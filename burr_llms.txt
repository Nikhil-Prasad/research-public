Directory Structure:

â””â”€â”€ ./
    â””â”€â”€ burr
        â”œâ”€â”€ burr
        â”‚   â”œâ”€â”€ cli
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ __main__.py
        â”‚   â”‚   â””â”€â”€ demo_data.py
        â”‚   â”œâ”€â”€ common
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ async_utils.py
        â”‚   â”‚   â””â”€â”€ types.py
        â”‚   â”œâ”€â”€ core
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ action.py
        â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â”œâ”€â”€ graph.py
        â”‚   â”‚   â”œâ”€â”€ implementations.py
        â”‚   â”‚   â”œâ”€â”€ parallelism.py
        â”‚   â”‚   â”œâ”€â”€ persistence.py
        â”‚   â”‚   â”œâ”€â”€ serde.py
        â”‚   â”‚   â”œâ”€â”€ state.py
        â”‚   â”‚   â”œâ”€â”€ typing.py
        â”‚   â”‚   â””â”€â”€ validation.py
        â”‚   â”œâ”€â”€ integrations
        â”‚   â”‚   â”œâ”€â”€ persisters
        â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ b_aiosqlite.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ b_asyncpg.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ b_mongodb.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ b_psycopg2.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ b_pymongo.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ b_redis.py
        â”‚   â”‚   â”‚   â””â”€â”€ postgresql.py
        â”‚   â”‚   â”œâ”€â”€ serde
        â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ langchain.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ pandas.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ pickle.py
        â”‚   â”‚   â”‚   â””â”€â”€ pydantic.py
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ base.py
        â”‚   â”‚   â”œâ”€â”€ hamilton.py
        â”‚   â”‚   â”œâ”€â”€ haystack.py
        â”‚   â”‚   â”œâ”€â”€ notebook.py
        â”‚   â”‚   â”œâ”€â”€ opentelemetry.py
        â”‚   â”‚   â”œâ”€â”€ pydantic.py
        â”‚   â”‚   â”œâ”€â”€ ray.py
        â”‚   â”‚   â””â”€â”€ streamlit.py
        â”‚   â”œâ”€â”€ lifecycle
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ base.py
        â”‚   â”‚   â”œâ”€â”€ default.py
        â”‚   â”‚   â””â”€â”€ internal.py
        â”‚   â”œâ”€â”€ testing
        â”‚   â”‚   â””â”€â”€ __init__.py
        â”‚   â”œâ”€â”€ tracking
        â”‚   â”‚   â”œâ”€â”€ common
        â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”‚   â””â”€â”€ models.py
        â”‚   â”‚   â”œâ”€â”€ server
        â”‚   â”‚   â”‚   â”œâ”€â”€ s3
        â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ backend.py
        â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ initialize_db.py
        â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ models.py
        â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ settings.py
        â”‚   â”‚   â”‚   â”‚   â””â”€â”€ utils.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ backend.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ run.py
        â”‚   â”‚   â”‚   â””â”€â”€ schema.py
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ base.py
        â”‚   â”‚   â”œâ”€â”€ client.py
        â”‚   â”‚   â”œâ”€â”€ s3client.py
        â”‚   â”‚   â””â”€â”€ utils.py
        â”‚   â”œâ”€â”€ visibility
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ tracing.py
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ log_setup.py
        â”‚   â”œâ”€â”€ system.py
        â”‚   â”œâ”€â”€ telemetry.py
        â”‚   â””â”€â”€ version.py
        â”œâ”€â”€ docs
        â”‚   â”œâ”€â”€ _static
        â”‚   â”‚   â”œâ”€â”€ custom.css
        â”‚   â”‚   â””â”€â”€ testimonials.css
        â”‚   â”œâ”€â”€ conf.py
        â”‚   â””â”€â”€ make_testimonials.py
        â”œâ”€â”€ examples
        â”‚   â”œâ”€â”€ adaptive-crag
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ conversational-rag
        â”‚   â”‚   â”œâ”€â”€ graph_db_example
        â”‚   â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ graph_schema.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ hamilton_ingest.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ ingest_fighters.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ ingest_fights.py
        â”‚   â”‚   â”‚   â””â”€â”€ utils.py
        â”‚   â”‚   â”œâ”€â”€ simple_example
        â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”‚   â””â”€â”€ __init__.py
        â”‚   â”œâ”€â”€ custom-serde
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â””â”€â”€ run.py
        â”‚   â”œâ”€â”€ deep-researcher
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â”œâ”€â”€ prompts.py
        â”‚   â”‚   â”œâ”€â”€ server.py
        â”‚   â”‚   â””â”€â”€ utils.py
        â”‚   â”œâ”€â”€ deployment
        â”‚   â”‚   â””â”€â”€ aws
        â”‚   â”‚       â””â”€â”€ lambda
        â”‚   â”‚           â””â”€â”€ app
        â”‚   â”‚               â”œâ”€â”€ __init__.py
        â”‚   â”‚               â”œâ”€â”€ counter_app.py
        â”‚   â”‚               â””â”€â”€ lambda_handler.py
        â”‚   â”œâ”€â”€ email-assistant
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â””â”€â”€ server.py
        â”‚   â”œâ”€â”€ hamilton-integration
        â”‚   â”‚   â”œâ”€â”€ actions
        â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ ask_question.py
        â”‚   â”‚   â”‚   â””â”€â”€ ingest_blog.py
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ haystack-integration
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ hello-world-counter
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ application_classbased.py
        â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â””â”€â”€ streamlit_app.py
        â”‚   â”œâ”€â”€ image-telephone
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ instructor-gemini-flash
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ integrations
        â”‚   â”‚   â””â”€â”€ hamilton
        â”‚   â”‚       â””â”€â”€ image-telephone
        â”‚   â”‚           â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ llm-adventure-game
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ ml-training
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ multi-agent-collaboration
        â”‚   â”‚   â”œâ”€â”€ hamilton
        â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ alternative_implementation.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â”‚   â””â”€â”€ func_agent.py
        â”‚   â”‚   â”œâ”€â”€ lcel
        â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”‚   â””â”€â”€ __init__.py
        â”‚   â”œâ”€â”€ multi-modal-chatbot
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â”œâ”€â”€ server.py
        â”‚   â”‚   â”œâ”€â”€ simple_streamlit_app.py
        â”‚   â”‚   â””â”€â”€ streamlit_app.py
        â”‚   â”œâ”€â”€ openai-compatible-agent
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â””â”€â”€ server.py
        â”‚   â”œâ”€â”€ opentelemetry
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ other-examples
        â”‚   â”‚   â”œâ”€â”€ cowsay
        â”‚   â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â”‚   â””â”€â”€ streamlit_app.py
        â”‚   â”‚   â””â”€â”€ hamilton-multi-modal
        â”‚   â”‚       â”œâ”€â”€ __init__.py
        â”‚   â”‚       â”œâ”€â”€ application.py
        â”‚   â”‚       â””â”€â”€ dag.py
        â”‚   â”œâ”€â”€ pytest
        â”‚   â”‚   â”œâ”€â”€ conftest.py
        â”‚   â”‚   â”œâ”€â”€ some_actions.py
        â”‚   â”‚   â””â”€â”€ test_some_actions.py
        â”‚   â”œâ”€â”€ rag-lancedb-ingestion
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â”œâ”€â”€ ingestion.py
        â”‚   â”‚   â””â”€â”€ utils.py
        â”‚   â”œâ”€â”€ ray
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ recursive
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ simple-chatbot-intro
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ simulation
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ streaming-fastapi
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â”œâ”€â”€ server.py
        â”‚   â”‚   â””â”€â”€ streamlit_app.py
        â”‚   â”œâ”€â”€ streaming-overview
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â”œâ”€â”€ async_application.py
        â”‚   â”‚   â””â”€â”€ streamlit_app.py
        â”‚   â”œâ”€â”€ templates
        â”‚   â”‚   â”œâ”€â”€ agent_supervisor.py
        â”‚   â”‚   â”œâ”€â”€ hierarchical_agent_teams.py
        â”‚   â”‚   â”œâ”€â”€ multi_agent_collaboration.py
        â”‚   â”‚   â””â”€â”€ multi_modal_agent.py
        â”‚   â”œâ”€â”€ test-case-creation
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â””â”€â”€ test_application.py
        â”‚   â”œâ”€â”€ tool-calling
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ tracing-and-spans
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â””â”€â”€ application.py
        â”‚   â”œâ”€â”€ typed-state
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â””â”€â”€ server.py
        â”‚   â”œâ”€â”€ youtube-to-social-media-post
        â”‚   â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”‚   â”œâ”€â”€ application.py
        â”‚   â”‚   â””â”€â”€ server.py
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â””â”€â”€ validate_examples.py
        â”œâ”€â”€ telemetry
        â”‚   â””â”€â”€ ui
        â”‚       â”œâ”€â”€ public
        â”‚       â”‚   â””â”€â”€ index.html
        â”‚       â”œâ”€â”€ scripts
        â”‚       â”‚   â””â”€â”€ token_costs.py
        â”‚       â”œâ”€â”€ src
        â”‚       â”‚   â”œâ”€â”€ api
        â”‚       â”‚   â”‚   â”œâ”€â”€ core
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ApiError.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ApiRequestOptions.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ApiResult.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ CancelablePromise.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ OpenAPI.ts
        â”‚       â”‚   â”‚   â”‚   â””â”€â”€ request.ts
        â”‚       â”‚   â”‚   â”œâ”€â”€ models
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ActionModel.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ AnnotationCreate.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ AnnotationDataPointer.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ AnnotationObservation.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ AnnotationOut.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ AnnotationUpdate.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ApplicationLogs.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ApplicationModel.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ApplicationPage.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ApplicationSummary.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ AttributeModel.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ BackendSpec.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ BeginEntryModel.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ BeginSpanModel.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ChatItem.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ChildApplicationModel.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ DraftInit.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ EmailAssistantState.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ EndEntryModel.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ EndSpanModel.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ EndStreamModel.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ Feedback.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ FirstItemStreamModel.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ HTTPValidationError.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ IndexingJob.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ InitializeStreamModel.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ PointerModel.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ Project.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ PromptInput.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ QuestionAnswers.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ ResearchSummary.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ Span.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ Step.ts
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ TransitionModel.ts
        â”‚       â”‚   â”‚   â”‚   â””â”€â”€ ValidationError.ts
        â”‚       â”‚   â”‚   â”œâ”€â”€ services
        â”‚       â”‚   â”‚   â”‚   â””â”€â”€ DefaultService.ts
        â”‚       â”‚   â”‚   â””â”€â”€ index.ts
        â”‚       â”‚   â”œâ”€â”€ components
        â”‚       â”‚   â”‚   â”œâ”€â”€ common
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ button.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ chip.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ dates.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ drawer.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ fieldset.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ href.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ input.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ link.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ loading.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ modelCost.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ pagination.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ switch.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ table.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ tabs.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ text.tsx
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ textarea.tsx
        â”‚       â”‚   â”‚   â”‚   â””â”€â”€ tooltip.tsx
        â”‚       â”‚   â”‚   â”œâ”€â”€ nav
        â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ appcontainer.tsx
        â”‚       â”‚   â”‚   â”‚   â””â”€â”€ breadcrumb.tsx
        â”‚       â”‚   â”‚   â””â”€â”€ routes
        â”‚       â”‚   â”‚       â”œâ”€â”€ app
        â”‚       â”‚   â”‚       â”‚   â”œâ”€â”€ ActionView.tsx
        â”‚       â”‚   â”‚       â”‚   â”œâ”€â”€ AnnotationsView.tsx
        â”‚       â”‚   â”‚       â”‚   â”œâ”€â”€ AppView.tsx
        â”‚       â”‚   â”‚       â”‚   â”œâ”€â”€ DataView.tsx
        â”‚       â”‚   â”‚       â”‚   â”œâ”€â”€ GraphView.tsx
        â”‚       â”‚   â”‚       â”‚   â”œâ”€â”€ InsightsView.tsx
        â”‚       â”‚   â”‚       â”‚   â”œâ”€â”€ ReproduceView.tsx
        â”‚       â”‚   â”‚       â”‚   â”œâ”€â”€ StateMachine.tsx
        â”‚       â”‚   â”‚       â”‚   â””â”€â”€ StepList.tsx
        â”‚       â”‚   â”‚       â”œâ”€â”€ AdminView.tsx
        â”‚       â”‚   â”‚       â”œâ”€â”€ AppList.tsx
        â”‚       â”‚   â”‚       â””â”€â”€ ProjectList.tsx
        â”‚       â”‚   â”œâ”€â”€ examples
        â”‚       â”‚   â”‚   â”œâ”€â”€ Chatbot.tsx
        â”‚       â”‚   â”‚   â”œâ”€â”€ Common.tsx
        â”‚       â”‚   â”‚   â”œâ”€â”€ Counter.tsx
        â”‚       â”‚   â”‚   â”œâ”€â”€ DeepResearcher.tsx
        â”‚       â”‚   â”‚   â”œâ”€â”€ EmailAssistant.tsx
        â”‚       â”‚   â”‚   â”œâ”€â”€ MiniTelemetry.tsx
        â”‚       â”‚   â”‚   â””â”€â”€ StreamingChatbot.tsx
        â”‚       â”‚   â”œâ”€â”€ utils
        â”‚       â”‚   â”‚   â””â”€â”€ tailwind.ts
        â”‚       â”‚   â”œâ”€â”€ App.css
        â”‚       â”‚   â”œâ”€â”€ App.test.tsx
        â”‚       â”‚   â”œâ”€â”€ App.tsx
        â”‚       â”‚   â”œâ”€â”€ index.css
        â”‚       â”‚   â”œâ”€â”€ index.tsx
        â”‚       â”‚   â”œâ”€â”€ react-app-env.d.ts
        â”‚       â”‚   â”œâ”€â”€ reportWebVitals.ts
        â”‚       â”‚   â”œâ”€â”€ setupTests.ts
        â”‚       â”‚   â””â”€â”€ utils.tsx
        â”‚       â”œâ”€â”€ .eslintrc.js
        â”‚       â””â”€â”€ tailwind.config.js
        â”œâ”€â”€ tests
        â”‚   â”œâ”€â”€ common
        â”‚   â”‚   â””â”€â”€ test_async_utils.py
        â”‚   â”œâ”€â”€ core
        â”‚   â”‚   â”œâ”€â”€ test_action.py
        â”‚   â”‚   â”œâ”€â”€ test_application.py
        â”‚   â”‚   â”œâ”€â”€ test_graph.py
        â”‚   â”‚   â”œâ”€â”€ test_graphviz_display.py
        â”‚   â”‚   â”œâ”€â”€ test_implementations.py
        â”‚   â”‚   â”œâ”€â”€ test_parallelism.py
        â”‚   â”‚   â”œâ”€â”€ test_persistence.py
        â”‚   â”‚   â”œâ”€â”€ test_serde.py
        â”‚   â”‚   â”œâ”€â”€ test_state.py
        â”‚   â”‚   â””â”€â”€ test_validation.py
        â”‚   â”œâ”€â”€ integration_tests
        â”‚   â”‚   â””â”€â”€ test_app.py
        â”‚   â”œâ”€â”€ integrations
        â”‚   â”‚   â”œâ”€â”€ persisters
        â”‚   â”‚   â”‚   â”œâ”€â”€ test_b_aiosqlite.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ test_b_mongodb.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ test_b_redis.py
        â”‚   â”‚   â”‚   â””â”€â”€ test_postgresql.py
        â”‚   â”‚   â”œâ”€â”€ serde
        â”‚   â”‚   â”‚   â”œâ”€â”€ test_langchain.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ test_pandas.py
        â”‚   â”‚   â”‚   â”œâ”€â”€ test_pickle.py
        â”‚   â”‚   â”‚   â””â”€â”€ test_pydantic.py
        â”‚   â”‚   â”œâ”€â”€ test_burr_hamilton.py
        â”‚   â”‚   â”œâ”€â”€ test_burr_haystack.py
        â”‚   â”‚   â”œâ”€â”€ test_burr_opentelemetry.py
        â”‚   â”‚   â”œâ”€â”€ test_burr_pydantic_future_annotations.py
        â”‚   â”‚   â”œâ”€â”€ test_burr_pydantic.py
        â”‚   â”‚   â”œâ”€â”€ test_burr_ray.py
        â”‚   â”‚   â””â”€â”€ test_opentelemetry.py
        â”‚   â”œâ”€â”€ tracking
        â”‚   â”‚   â”œâ”€â”€ test_common_models.py
        â”‚   â”‚   â””â”€â”€ test_local_tracking_client.py
        â”‚   â”œâ”€â”€ visibility
        â”‚   â”‚   â””â”€â”€ test_tracing.py
        â”‚   â”œâ”€â”€ conftest.py
        â”‚   â””â”€â”€ test_end_to_end.py
        â””â”€â”€ __init__.py



---
File: /burr/burr/cli/__init__.py
---




---
File: /burr/burr/cli/__main__.py
---

import importlib.util
import json
import logging
import os
import shutil
import subprocess
import sys
import threading
import time
import webbrowser
from contextlib import contextmanager
from importlib.resources import files
from types import ModuleType

from burr import system, telemetry
from burr.core.persistence import PersistedStateData
from burr.integrations.base import require_plugin
from burr.log_setup import setup_logging

try:
    import click
    import requests
    from loguru import logger
except ImportError as e:
    require_plugin(
        e,
        "start",
    )

# Clear default handlers
setup_logging(logging.INFO)


# TODO -- add this as a general callback to the CLI
def _telemetry_if_enabled(event: str):
    if telemetry.is_telemetry_enabled():
        telemetry.create_and_send_cli_event(event)


def _command(command: str, capture_output: bool, addl_env: dict = None) -> str:
    """Runs a simple command"""
    if addl_env is None:
        addl_env = {}
    env = os.environ.copy()
    env.update(addl_env)
    logger.info(f"Running command: {command}")
    if isinstance(command, str):
        command = command.split(" ")
        if capture_output:
            try:
                return (
                    subprocess.check_output(command, stderr=subprocess.PIPE, shell=False, env=env)
                    .decode()
                    .strip()
                )
            except subprocess.CalledProcessError as e:
                print(e.stdout.decode())
                print(e.stderr.decode())
                raise e
        subprocess.run(command, shell=False, check=True, env=env)


def _get_git_root() -> str:
    return _command("git rev-parse --show-toplevel", capture_output=True)


def open_when_ready(check_url: str, open_url: str):
    while True:
        try:
            response = requests.get(check_url)
            if response.status_code == 200:
                webbrowser.open(open_url)
                return
            else:
                pass
        except requests.exceptions.RequestException:
            pass
        time.sleep(1)


@contextmanager
def cd(path):
    old_dir = os.getcwd()
    os.chdir(path)
    try:
        yield
    finally:
        os.chdir(old_dir)


@click.group()
def cli():
    pass


def _build_ui():
    cmd = "npm install --prefix telemetry/ui"
    _command(cmd, capture_output=False)
    cmd = "npm run build --prefix telemetry/ui"
    _command(cmd, capture_output=False)
    # create a symlink so we can get packages inside it...
    cmd = "rm -rf burr/tracking/server/build"
    _command(cmd, capture_output=False)
    cmd = "cp -R telemetry/ui/build burr/tracking/server/build"
    _command(cmd, capture_output=False)


@cli.command()
def build_ui():
    git_root = _get_git_root()
    with cd(git_root):
        _build_ui()


BACKEND_MODULES = {
    "local": "burr.tracking.server.backend.LocalBackend",
    "s3": "burr.tracking.server.s3.backend.SQLiteS3Backend",
}


def _run_server(
    port: int,
    dev_mode: bool,
    no_open: bool,
    no_copy_demo_data: bool,
    initial_page="",
    host: str = "127.0.0.1",
    backend: str = "local",
):
    _telemetry_if_enabled("run_server")
    # TODO: Implement server running logic here
    # Example: Start a web server, configure ports, etc.
    logger.info(f"Starting server on port {port}")
    cmd = f"uvicorn burr.tracking.server.run:app --port {port} --host {host}"
    if dev_mode:
        cmd += " --reload"

    if backend == "local" and not no_copy_demo_data:
        # TODO: fix this so we don't leak burr_path here since that's the value the local backend users
        base_dir = os.environ.get("burr_path", os.path.expanduser("~/.burr"))
        logger.info(f"Copying demo data over to {base_dir}...")
        demo_data_path = files("burr").joinpath("tracking/server/demo_data")
        for top_level in os.listdir(demo_data_path):
            if not os.path.exists(f"{base_dir}/{top_level}"):
                # this is purely for legacy -- we used to name with `demo_`
                if not system.IS_WINDOWS and os.path.exists(
                    f"{base_dir}/{top_level.replace('demo_', 'demo:')}"
                ):
                    # in this case we don't need to copy it over, it already exists in the right place...
                    continue
                logger.info(f"Copying {top_level} over...")
                shutil.copytree(f"{demo_data_path}/{top_level}", f"{base_dir}/{top_level}")

    if not no_open:
        thread = threading.Thread(
            target=open_when_ready,
            kwargs={
                "open_url": (open_url := f"http://localhost:{port}/{initial_page}"),
                "check_url": f"{open_url}/ready",
            },
            daemon=True,
        )
        thread.start()
    env = {
        "BURR_BACKEND_IMPL": BACKEND_MODULES[backend],
    }
    _command(cmd, capture_output=False, addl_env=env)


@cli.command()
@click.option("--port", default=7241, help="Port to run the server on")
@click.option("--dev-mode", is_flag=True, help="Run the server in development mode")
@click.option("--no-open", is_flag=True, help="Run the server without opening it")
@click.option("--no-copy-demo_data", is_flag=True, help="Don't copy demo data over.")
@click.option(
    "--host",
    default="127.0.0.1",
    help="Host to run the server on -- use 0.0.0.0 if you want "
    "to expose it to the network (E.G. in a docker image)",
)
@click.option(
    "--backend",
    default="local",
    help="Backend to use for the server.",
    type=click.Choice(["local", "s3"]),
)
def run_server(
    port: int, dev_mode: bool, no_open: bool, no_copy_demo_data: bool, host: str, backend: str
):
    _run_server(port, dev_mode, no_open, no_copy_demo_data, host=host, backend=backend)


@cli.command()
@click.option("--port", default=7241, help="Port to run the server on")
def demo_server(port: int):
    _run_server(port, True, False, False, "demos/chatbot")


@cli.command(help="Publishes the package to a repository")
@click.option("--prod", is_flag=True, help="Publish to pypi (rather than test pypi)")
@click.option("--no-wipe-dist", is_flag=True, help="Wipe the dist/ directory before building")
def build_and_publish(prod: bool, no_wipe_dist: bool):
    _telemetry_if_enabled("build_and_publish")
    git_root = _get_git_root()
    with cd(git_root):
        logger.info("Building UI -- this may take a bit...")
        _build_ui()
        logger.info("Built UI!")
        if not no_wipe_dist:
            logger.info("Wiping dist/ directory for a clean publish.")
            shutil.rmtree("dist", ignore_errors=True)
        _command("python3 -m build", capture_output=False)
        repository = "pypi" if prod else "testpypi"
        _command(f"python3 -m twine upload --repository {repository} dist/*", capture_output=False)
        logger.info(f"Published to {repository}! ðŸŽ‰")


@cli.command(help="generate demo data for the UI")
@click.option(
    "--s3-bucket", help="S3 URI to save to, will use the s3 tracker, not local mode", required=False
)
@click.option(
    "--data-dir",
    help="Local directory to save to",
    required=False,
    default="burr/tracking/server/demo_data",
)
@click.option("--unique-app-names", help="Use unique app names", is_flag=True)
@click.option("--no-clear-current-data", help="Don't clear current data", is_flag=True)
def generate_demo_data(s3_bucket, data_dir, unique_app_names: bool, no_clear_current_data: bool):
    _telemetry_if_enabled("generate_demo_data")
    git_root = _get_git_root()
    # We need to add the examples directory to the path so we have all the imports
    # The GPT-one relies on a local import
    sys.path.extend([git_root, f"{git_root}/examples/multi-modal-chatbot"])
    from burr.cli.demo_data import generate_all

    # local mode
    if s3_bucket is None:
        with cd(git_root):
            logger.info("Removing old demo data")
            if not no_clear_current_data:
                shutil.rmtree(data_dir, ignore_errors=True)
            generate_all(data_dir=data_dir, unique_app_names=unique_app_names)
    else:
        generate_all(s3_bucket=s3_bucket, unique_app_names=unique_app_names)


def _remove_private_keys(state: dict):
    return {key: value for key, value in state.items() if not key.startswith("__")}


def _transform_state_to_test_case(
    before_action_state: dict, after_action_state: dict, action_name: str, test_name: str
) -> dict:
    """Helper function to transform a state into a test case.

    :param state:
    :param action_name:
    :param test_name:
    :return:
    """
    # Remove private Burr keys -- don't want to expose these in the test case since we could change them
    before_action_state = _remove_private_keys(before_action_state)
    after_action_state = _remove_private_keys(after_action_state)
    return {
        "action": action_name,
        "name": test_name,
        "input_state": before_action_state,
        "expected_state": after_action_state,
    }


@click.group()
def test_case():
    """Test case related commands."""
    pass


PYTEST_TEMPLATE = """import pytest
from burr.core import state
from burr.testing import pytest_generate_tests  # noqa: F401
# TODO: import action you're testing, i.e. import {ACTION_NAME}.

@pytest.mark.file_name("{FILE_NAME}")
def test_{ACTION_NAME}(input_state, expected_state):
    \"\"\"Function for testing the action\"\"\"
    input_state = state.State.deserialize(input_state)
    expected_state = state.State.deserialize(expected_state)
    _, output_state = {ACTION_NAME}(input_state)  # exercise the action
    # TODO: choose appropriate way to evaluate the output
    # e.g. exact match, fuzzy match, LLM grade, etc.
    # this is exact match here on all values in state
    assert output_state == expected_state
    # e.g.
    # assert 'some value' in output_state["response"]["content"]
    # assert llm_evaluator(..., ...) == "Y"
"""


def import_from_file(file_path: str) -> ModuleType:
    """Import a module from a file path.

    :param file_path: file path. Assume it exists.
    :return:
    """
    # Extract the module name and directory from the file path
    module_name = os.path.basename(file_path).replace(".py", "")
    module_dir = os.path.dirname(file_path)

    # Add the module directory to sys.path if not already included
    if module_dir not in sys.path:
        sys.path.append(module_dir)

    # Import the module
    module = importlib.import_module(module_name)

    return module


@click.command()
@click.option("--project-name", required=True, help="Name of the project.")
@click.option("--partition-key", required=False, help="Partition key to look at.")
@click.option("--app-id", required=True, help="App ID to pull from.")
@click.option("--sequence-id", required=True, help="Sequence ID to pull.")
@click.option(
    "--target-file-name",
    required=False,
    help="What file to write the data to. Else print to console.",
)
@click.option(
    "--serde-module",
    required=False,
    help="Python file or fully qualified python module to import for custom serialization/deserialization.",
)
@click.option(
    "--action-name",
    required=False,
    help="Provide name of the action for the test case. Defaults to the action name at the sequence ID.",
)
def create_test_case(
    project_name: str,
    partition_key: str,  # will be None if not passed in.
    app_id: str,
    sequence_id: str,
    target_file_name: str = None,
    serde_module: str = None,
    action_name: str = None,
):
    """Create a test case from a persisted state.

    Does two things:

    1. Pulls data specified and saves it/prints to console.
    2. Prints a pytest test case to the console for you to cut and paste.

    See examples/test-case-creation/notebook.ipynb for example usage.
    See examples/test-case-creation/test_application.py for details.

    If you have custom serialization/deserialization then pass in the name of a
    python module, or a path to a python file, to import with your custom serialization/deserialization
    registration functions. This will be imported so that they can be registered.

    Note:
        - when burr fails, then the state retrieved for a sequence ID is the state at the start of the sequence ID.
          The fixture created will only have input state.
        - when burr completes successfully, then the state retrieved for a sequence ID is the final modified state.
          The fixture created will have input and output state, as we will grab the prior sequence ID's state
          as the input state.
        - we strip any keys prefixed with __ from the state to avoid exposing private keys in the test case.
    """
    if serde_module:
        if os.path.exists(serde_module):
            logger.info(f"Importing from file {serde_module}")
            import_from_file(serde_module)
        else:
            logger.info(f"Importing module {serde_module}")
            import importlib

            importlib.import_module(serde_module)

    # TODO: make this handle instantiating/using a persister other than local tracker
    from burr.tracking.client import LocalTrackingClient

    local_tracker = LocalTrackingClient(project=project_name)
    sequence_id = int(sequence_id)

    after_action: PersistedStateData = local_tracker.load(
        partition_key=partition_key, app_id=app_id, sequence_id=sequence_id
    )
    if not after_action:
        print(f"No state found for {project_name}, {partition_key}, {app_id}, {sequence_id}.")
        return

    action_name = action_name if action_name else after_action["position"]
    """
    Explanation of the logic here.
    If there's an error:
     - status would be failed vs completed
     - state loaded would be the starting state
    If there's no error:
     - status would be completed
     - state loaded would be the ending state, so to get the starting state, we need to look at the prior sequence ID
    """
    if after_action["status"] == "completed":
        print("Action was successful so loading initial and expected state into test fixture.")
        after_action_state = after_action["state"].serialize()
        # if it's completed, then we need to look at the prior sequence ID to get the input state
        prior_sequence_id = sequence_id - 1
        try:
            # TODO: handle forked case. i.e. prior doesn't exist because it was a forked sequence.
            before_action: PersistedStateData = local_tracker.load(
                partition_key=partition_key, app_id=app_id, sequence_id=prior_sequence_id
            )
        except ValueError:
            before_action = None

        if not before_action:
            # there was no initial state saved for this sequence id
            print(
                f"Warning: there was no initial state found sequence ID {sequence_id}. That is, we looked for the "
                f"prior state corresponding to the prior sequence ID {prior_sequence_id}, but did not find a state "
                f"value. Was this application ID forked? Defaulting to empty state - please fill this in."
            )
            before_action_state = {"TODO:": "fill this in"}
        else:
            before_action_state = before_action["state"].serialize()
    else:
        print("Action was not successful so loading initial state into test fixture.")
        # there was an error so state is the starting state
        before_action_state = after_action["state"].serialize()
        after_action_state = {"TODO:": "fill this in"}

    # test case json
    tc_json = _transform_state_to_test_case(
        before_action_state,
        after_action_state,
        action_name,
        f"{action_name}_{app_id[:8] + '_' + str(sequence_id)}",
    )

    if target_file_name:
        # if it already exists, load it up and append to it
        if os.path.exists(target_file_name):
            with open(target_file_name, "r") as f:
                # assumes it's a list of test cases
                current_testcases = json.load(f)
            current_testcases.append(tc_json)
        else:
            current_testcases = [tc_json]
        print(f"\nWriting data to file {target_file_name}")
        with open(target_file_name, "w") as f:
            json.dump(current_testcases, f, indent=2)
    else:
        logger.info(json.dumps(tc_json, indent=2))
    # print out python test to add
    print("\nAdd the following to your test file:\n")
    print(PYTEST_TEMPLATE.format(FILE_NAME=target_file_name, ACTION_NAME=action_name))


test_case.add_command(create_test_case, name="create")
cli.add_command(test_case)

# quick trick to expose every subcommand as a variable
# will create a command called `cli_{command}` for every command we have
for key, command in cli.commands.items():
    globals()[f'cli_{key.replace("-", "_")}'] = command

if __name__ == "__main__":
    cli()



---
File: /burr/burr/cli/demo_data.py
---

import importlib
import logging
import os
import uuid
from typing import Optional

from burr.core import ApplicationBuilder, Result, default, expr
from burr.core.graph import GraphBuilder
from burr.tracking import LocalTrackingClient
from burr.tracking.s3client import S3TrackingClient

logger = logging.getLogger(__name__)

conversational_rag_application = importlib.import_module(
    "examples.conversational-rag.simple_example.application"
)
counter_application = importlib.import_module("examples.hello-world-counter.application")
chatbot_application = importlib.import_module("examples.multi-modal-chatbot.application")
chatbot_application_with_traces = importlib.import_module("examples.tracing-and-spans.application")

from opentelemetry.instrumentation.openai import OpenAIInstrumentor

OpenAIInstrumentor().instrument()


def generate_chatbot_data(
    data_dir: Optional[str] = None,
    s3_bucket: Optional[str] = None,
    use_traces: bool = False,
    unique_app_names: bool = False,
):
    project_id = "demo_chatbot" if not use_traces else "demo_chatbot_with_traces"
    run_prefix = str(uuid.uuid4())[0:8] + "-" if unique_app_names else ""
    working_conversations = {
        "chat-1-giraffe": [
            "Please draw a giraffe.",  # Answered by the image mode
            "Please write a function that queries the internet for the height of a giraffe",  # answered by the code mode
            "OK, just tell me, how tall is a giraffe?",  # answered by the question mode
            "Please build me a giraffe",  # Answered by nothing
            "If Aaron burr were an animal, would he be a giraffe?",  # answered by the question mode
        ],
        "chat-2-geography": [
            "What is the capital of France?",  # answered by the question mode
            "Please draw a map of the world",  # answered by the image mode
            "Please write code to compute the circumpherence of the earth",  # answered by the code mode
            "Geography! Geography! Geography!",  # answered by nothing
        ],
        "chat-3-physics": [
            "Please draw a free-body diagram of a block on a ramp",  # answered by the image mode
            "Please write code to compute the force of gravity on the moon",  # answered by the code mode
            "What is the force of gravity on the moon?",  # answered by the question mode
            "Please build me a block on a ramp",  # answered by nothing
        ],
        "chat-4-philosophy": [
            "Please draw a picture of a philosopher",  # answered by the image mode
            "Please write code to compute the meaning of life (hint, its 42)",  # answered by the code mode
            "What is the meaning of life?",  # answered by the question mode (ish)
        ],
        "chat-5-jokes": [
            "Please draw a picture of a good joke",  # answered by the image mode
            "Please write code for an interactive knock-knock joke",  # answered by the code mode
            "What is a good joke?",  # answered by the question mode
            "The chicken crossed the road because it was a free-range chicken",  # answered by nothing
        ],
    }
    broken_conversations = {"chat-6-demonstrate-errors": working_conversations["chat-1-giraffe"]}

    def _modify(app_id: str) -> str:
        return run_prefix + app_id

    def _run_conversation(app_id, prompts):
        tracker = (
            LocalTrackingClient(project=project_id + "_otel", storage_dir=data_dir)
            if not s3_bucket
            else S3TrackingClient(project=project_id, bucket=s3_bucket)
        )

        graph = (chatbot_application_with_traces if use_traces else chatbot_application).graph
        app = (
            ApplicationBuilder()
            .with_graph(graph)
            .with_identifiers(app_id=app_id)
            .with_tracker(tracker, use_otel_tracing=True)
            .with_entrypoint("prompt")
            .build()
        )
        for prompt in prompts:
            app.run(halt_after=["response"], inputs={"prompt": prompt})

    for app_id, prompts in sorted(working_conversations.items()):
        _run_conversation(_modify(app_id), prompts)
    old_api_key = os.environ.get("OPENAI_API_KEY")
    os.environ["OPENAI_API_KEY"] = "fake"
    for app_id, prompts in sorted(broken_conversations.items()):
        try:
            _run_conversation(_modify(app_id), prompts)
        except Exception as e:
            print(f"Got an exception: {e}")
    os.environ["OPENAI_API_KEY"] = old_api_key


def generate_counter_data(
    data_dir: str = "~/.burr", s3_bucket: Optional[str] = None, unique_app_names: bool = False
):
    counter = counter_application.counter
    tracker = (
        LocalTrackingClient(project="demo_counter", storage_dir=data_dir)
        if not s3_bucket
        else S3TrackingClient(project="demo_counter", bucket=s3_bucket)
    )

    counts = [1, 10, 100, 50, 42]
    # This is just cause we don't want to change the code
    # TODO -- add ability to grab graph from application or something like that
    graph = (
        GraphBuilder()
        .with_actions(counter=counter, result=Result("counter"))
        .with_transitions(
            ("counter", "counter", expr("counter < count_to")),
            ("counter", "result", default),
        )
        .build()
    )
    for i, count in enumerate(counts):
        app_id = f"count-to-{count}"
        if unique_app_names:
            suffix = str(uuid.uuid4())[0:8]
            app_id = f"{app_id}-{suffix}"
        app = (
            ApplicationBuilder()
            .with_graph(graph)
            .with_identifiers(app_id=app_id, partition_key=f"user_{i}")
            .with_state(count_to=count, counter=0)
            .with_tracker(tracker)
            .with_entrypoint("counter")
            .build()
        )
        app.run(halt_after=["result"])


def generate_rag_data(
    data_dir: Optional[str] = None, s3_bucket: Optional[str] = None, unique_app_names: bool = False
):
    conversations = {
        "rag-1-food": [
            "What is Elijah's favorite food?",
            "What is Stefan's favorite food?",
            "What is Aaron's favorite food?",  # unknown
            "exit",
        ],
        "rag-2-work-history": [
            "Where did Elijah work?",
            "Where did Stefan work?",
            "Where did Harrison work?",
            "Where did Jonathan work?",
            "Did Stefan and Harrison work together?",
            "exit",
        ],
        "rag-3-activities": [
            "What does Elijah like to do?",
            "What does Stefan like to do?",
            "exit",
        ],
        "rag-4-everything": [
            "What is Elijah's favorite food?",
            "Where did Elijah work?",
            "Where did Stefan work" "What does Elijah like to do?",
            "What is Stefan's favorite food?",
            "Whose favorite food is better, Elijah's or Stefan's?" "exit",
        ],
    }
    prefix = str(uuid.uuid4())[0:8] + "-" if unique_app_names else ""
    for app_id, prompts in sorted(conversations.items()):
        graph = conversational_rag_application.graph()
        tracker = (
            LocalTrackingClient(project="demo_conversational-rag", storage_dir=data_dir)
            if not s3_bucket
            else S3TrackingClient(project="demo_conversational-rag", bucket=s3_bucket)
        )
        app_id = f"{prefix}{app_id}" if unique_app_names else app_id
        app = (
            ApplicationBuilder()
            .with_graph(graph)
            .with_identifiers(app_id=app_id)
            .with_tracker(tracker)
            .with_entrypoint("human_converse")
            .build()
        )
        logger.warning(f"Running {app_id}...")
        for prompt in prompts:
            app.run(halt_after=["ai_converse", "terminal"], inputs={"user_question": prompt})


def generate_all(
    data_dir: Optional[str] = None, s3_bucket: Optional[str] = None, unique_app_names: bool = False
):
    logger.info("Generating chatbot data")
    generate_chatbot_data(
        data_dir=data_dir, s3_bucket=s3_bucket, use_traces=False, unique_app_names=unique_app_names
    )
    logger.info("Generating chatbot data with traces")
    generate_chatbot_data(
        data_dir=data_dir, s3_bucket=s3_bucket, use_traces=True, unique_app_names=unique_app_names
    )
    logger.info("Generating counter data")
    generate_counter_data(data_dir=data_dir, s3_bucket=s3_bucket, unique_app_names=unique_app_names)
    logger.info("Generating RAG data")
    generate_rag_data(data_dir=data_dir, s3_bucket=s3_bucket, unique_app_names=unique_app_names)



---
File: /burr/burr/common/__init__.py
---




---
File: /burr/burr/common/async_utils.py
---

import inspect
from typing import AsyncGenerator, AsyncIterable, Generator, List, TypeVar, Union

T = TypeVar("T")

GenType = TypeVar("GenType")

SyncOrAsyncIterable = Union[AsyncIterable[T], List[T]]
SyncOrAsyncGenerator = Union[Generator[GenType, None, None], AsyncGenerator[GenType, None]]
SyncOrAsyncGeneratorOrItemOrList = Union[SyncOrAsyncGenerator[GenType], List[GenType], GenType]


async def asyncify_generator(
    generator: SyncOrAsyncGenerator[GenType],
) -> AsyncGenerator[GenType, None]:
    """Convert a sync generator to an async generator.

    :param generator: sync generator
    :return: async generator
    """
    if inspect.isasyncgen(generator):
        async for item in generator:
            yield item
    else:
        for item in generator:
            yield item


async def arealize(maybe_async_generator: SyncOrAsyncGenerator[GenType]) -> List[GenType]:
    """Realize an async generator or async iterable to a list.

    :param maybe_async_generator: async generator or async iterable
    :return: list of items -- fully realized
    """
    if inspect.isasyncgen(maybe_async_generator):
        out = [item async for item in maybe_async_generator]
    else:
        out = [item for item in maybe_async_generator]
    return out



---
File: /burr/burr/common/types.py
---

import abc
import dataclasses
from typing import Any, Optional

try:
    from typing import Self
except ImportError:
    Self = Any


# This contains common types
# Currently the types are a little closer to the logic than we'd like
# We'll want to break them out into interfaces and put more here eventually
# This will help avoid the ugly if TYPE_CHECKING imports;
@dataclasses.dataclass
class ParentPointer:
    app_id: str
    partition_key: Optional[str]
    sequence_id: Optional[int]


class BaseCopyable(abc.ABC):
    """Interface for copying objects. This is used internally."""

    @abc.abstractmethod
    def copy(self) -> "Self":
        pass



---
File: /burr/burr/core/__init__.py
---

from burr.core.action import Action, Condition, Result, action, default, expr, when
from burr.core.application import (
    Application,
    ApplicationBuilder,
    ApplicationContext,
    ApplicationGraph,
)
from burr.core.graph import Graph, GraphBuilder
from burr.core.state import State

__all__ = [
    "action",
    "Action",
    "Application",
    "ApplicationBuilder",
    "ApplicationGraph",
    "ApplicationContext",
    "Condition",
    "default",
    "expr",
    "Result",
    "State",
    "when",
    "Graph",
    "GraphBuilder",
]



---
File: /burr/burr/core/action.py
---

import abc
import ast
import builtins
import copy
import inspect
import sys
import types
import typing
from collections.abc import AsyncIterator
from typing import (
    TYPE_CHECKING,
    Any,
    AsyncGenerator,
    Callable,
    Coroutine,
    Dict,
    Generator,
    Generic,
    Iterator,
    List,
    Optional,
    Protocol,
    Tuple,
    Type,
    TypeVar,
    Union,
)

if sys.version_info <= (3, 11):
    Self = Any
else:
    from typing import Self

from burr.core.state import State
from burr.core.typing import ActionSchema

# This is here to make accessing the pydantic actions easier
# we just attach them to action so you can call `@action.pyddantic...`
# The IDE will like it better and thus be able to auto-complete/type-check
# TODO - come up with a better way to attach integrations to core objects
imported_pydantic = False
if TYPE_CHECKING:
    try:
        from pydantic import BaseModel
    except ImportError:
        pass


class Function(abc.ABC):
    """Interface to represent the 'computing' part of an action"""

    @property
    @abc.abstractmethod
    def reads(self) -> list[str]:
        """Returns the keys from the state that this function reads

        :return: A list of keys
        """
        pass

    @abc.abstractmethod
    def run(self, state: State, **run_kwargs) -> dict:
        """Runs the function on the given state and returns the result.
        The result is just a key/value dictionary.

        :param state: State to run the function on
        :param run_kwargs: Additional arguments to the function passed at runtime.
        :return: Result of the function
        """
        pass

    @property
    def inputs(self) -> Union[list[str], tuple[list[str], list[str]]]:
        """Represents inputs that are used for this to run.
        These correspond to the ``**run_kwargs`` in `run` above.

        Note that this has two possible return values:
        1. A list of strings -- these are the keys that are required to run the function
        2. A tuple of two lists of strings -- the first list is the required keys, the second is the optional keys

        :return: Either a list of strings (required inputs) or a tuple of two lists of strings (required and optional inputs)
        """
        return []

    @property
    def optional_and_required_inputs(self) -> tuple[set[str], set[str]]:
        """Returns a tuple of two sets of strings -- the first set is the required keys, the second is the optional keys.
        This is internal and not meant to override.

        :return: Tuple of required keys and optional keys
        """
        inputs = self.inputs
        if isinstance(inputs, tuple):
            return set(inputs[0]), set(inputs[1])
        return set(inputs), set()

    def validate_inputs(self, inputs: Optional[Dict[str, Any]]) -> None:
        """Validates the inputs to the function. This is a convenience method
        to allow for validation of inputs before running the function.

        :param inputs: Inputs to validate
        :raises ValueError: If the inputs are invalid
        """
        if inputs is None:
            inputs = {}
        required_inputs, optional_inputs = self.optional_and_required_inputs
        given_inputs = set(inputs.keys())
        missing_inputs = required_inputs - given_inputs
        additional_inputs = given_inputs - required_inputs - optional_inputs
        if missing_inputs or additional_inputs:
            raise ValueError(
                f"Inputs to function {self} are invalid. "
                + f"Missing the following inputs: {', '.join(missing_inputs)}."
                if missing_inputs
                else (
                    "" f"Additional inputs: {','.join(additional_inputs)}."
                    if additional_inputs
                    else ""
                )
            )

    def is_async(self) -> bool:
        """Convenience method to check if the function is async or not.
        This can be used by the application to run it.

        :return: True if the function is async, False otherwise
        """
        return inspect.iscoroutinefunction(self.run)


class Reducer(abc.ABC):
    """Interface to represent the 'updating' part of an action"""

    @property
    @abc.abstractmethod
    def writes(self) -> list[str]:
        """Returns the keys from the state that this reducer writes.

        :return: A list of keys
        """
        pass

    @abc.abstractmethod
    def update(self, result: dict, state: State) -> State:
        """Performs a state update given a result and the current state.
        Returns a new, modified :py:class:`State <burr.core.state.State>`
        (recall state is immutable -- simply changing the state in place will not work).

        In the context of Burr, this is only applied in the two-step actions, where
        the :py:meth:`run <burr.core.action.Function.run>` and update() functions are separate. The
        function-based APIs for Burr use the SingleStepAction class, which performs them both at once.
        This is not (yet) exposed as an interface for users to extend.

        :param result: Result of a function executing on the state
        :param state: State to update
        :return: A new, modified state.
        """
        pass


class DefaultSchema(ActionSchema):
    def state_input_type(self) -> type[State]:
        raise NotImplementedError

    def state_output_type(self) -> type[State]:
        raise NotImplementedError

    def intermediate_result_type(self) -> type[dict]:
        return dict


DEFAULT_SCHEMA = DefaultSchema()


class Action(Function, Reducer, abc.ABC):
    def __init__(self):
        """Represents an action in a state machine. This is the base class from which
        actions extend. Note that this class needs to have a name set after the fact.
        """
        self._name = None

    def with_name(self, name: str) -> Self:
        """Returns a copy of the given action with the given name. Why do we need this?
        We instantiate actions without names, and then set them later. This is a way to
        make the API cleaner/consolidate it, and the ApplicationBuilder will end up handling it
        for you, in the with_actions(...) method, which is the only way to use actions.

        Note they can also take in names in the constructor for testing, but otherwise this is
        not something users will ever have to think about.

        :param name: Name to set
        :return: A new action with the given name
        """
        if self._name is not None:
            raise ValueError(
                f"Name of {self} already set to {self._name} -- cannot set name to {name}"
            )
        # TODO -- ensure that we're not mutating anything later on
        # If we are, we may want to copy more intelligently
        new_action = copy.copy(self)
        new_action._name = name
        return new_action

    @property
    def name(self) -> str:
        """Gives the name of this action. This should be unique
        across your application."""
        return self._name

    @property
    def single_step(self) -> bool:
        return False

    @property
    def streaming(self) -> bool:
        return False

    @property
    def schema(self) -> ActionSchema:
        return DEFAULT_SCHEMA

    def get_source(self) -> str:
        """Returns the source code of the action. This will default to
        the source code of the class in which the action is implemented,
        but can be overwritten." Override if you want debugging/tracking
        to display a different source"""
        try:
            return inspect.getsource(self.__class__)
        except Exception:
            return "No source available"

    def input_schema(self) -> Any:
        """Returns the input schema for the action.
        The input schema is a type that can be used to validate the input to the action"""
        return None

    def __repr__(self):
        read_repr = ", ".join(self.reads) if self.reads else "{}"
        write_repr = ", ".join(self.writes) if self.writes else "{}"
        return f"{self.name}: {read_repr} -> {write_repr}"

    @property
    def tags(self) -> list[str]:
        """Returns the tags associated with this action.
        Tags are effectively action aliases -- names that apply towards multiple actions.

        :return: List of string tags
        """
        return []


class Condition(Function):
    KEY = "PROCEED"

    def __init__(
        self,
        keys: List[str],
        resolver: Callable[[State], bool],
        name: str = None,
        optional_keys: List[str] = None,
    ):
        """Base condition class. Chooses keys to read from the state and a resolver function.
        If you want a condition that defaults to true, use Condition.default or just default.

        Note that you can use a few fundamental operators to build more complex conditions:

         - ``~`` operator allows you to automatically invert the condition.
         - ``|`` operator allows you to OR two conditions together.
         - ``&`` operator allows you to AND two conditions together.

        :param keys: Keys to read from the state
        :param resolver:  Function to resolve the condition to True or False
        :param name: Name of the condition
        """
        self._resolver = resolver
        self._keys = keys
        self._optional_keys = optional_keys if optional_keys is not None else []
        self._name = name

    @staticmethod
    def expr(expr: str) -> "Condition":
        """Returns a condition that evaluates the given expression. Expression must use
        only state variables and Python operators. Do not trust that anything else will work.

        Do not accept expressions generated from user-inputted text, this has the potential to be unsafe.

        You can also refer to this as ``from burr.core import expr`` in the API.

        :param expr: Expression to evaluate
        :return: A condition that evaluates the given expression
        """
        tree = ast.parse(expr, mode="eval")
        all_builtins = builtins.__dict__

        # Visitor class to collect variable names
        class NameVisitor(ast.NodeVisitor):
            def __init__(self):
                self.names = set()

            def visit_Name(self, node):
                if node.id not in all_builtins:
                    self.names.add(node.id)

        # Visit the nodes and collect variable names
        visitor = NameVisitor()
        visitor.visit(tree)
        keys = list(visitor.names)

        # Compile the expression into a callable function
        def condition_func(state: State) -> bool:
            __globals = state.get_all()  # we can get all because externally we will subset
            return eval(compile(tree, "<string>", "eval"), {}, __globals)

        return Condition(keys, condition_func, name=expr)

    @staticmethod
    def lmda(resolver: Callable[[State], bool], state_keys: List[str]) -> "Condition":
        """Returns a condition that evaluates the given function of State.
        Note that this is just a simple wrapper over the Condition object.

        This does not (yet) support optional (default) arguments.

        :param fn:
        :param state_keys:
        :return:
        """
        return Condition(state_keys, resolver, name=f"lmda_{resolver.__name__}_")

    # TODO -- decide what to do with this when we have optional keys
    # @staticmethod
    # def exists(*keys: str) -> "Condition":
    #     """Returns a condition that checks if the given key exists in the state.
    #
    #     :param key: Key to check for existence
    #     :return: A condition that checks if the given key exists in the state
    #     """
    #     return Condition(
    #         list(keys),
    #         lambda state: all(item in state for item in keys),
    #         name=f"exists_{'_and_'.join(sorted(keys))}"
    #     )

    def _validate(self, state: State):
        missing_keys = set(self._keys) - set(state.keys())
        if missing_keys:
            raise ValueError(
                f"Missing keys in state required by condition: {self} {', '.join(missing_keys)}"
            )

    def run(self, state: State, **run_kwargs) -> dict:
        self._validate(state)
        return {Condition.KEY: self._resolver(state)}

    @property
    def reads(self) -> list[str]:
        return self._keys

    @classmethod
    def when(cls, **kwargs):
        """Returns a condition that checks if the given keys are in the
        state and equal to the given values.

        You can also refer to this as ``from burr.core import when`` in the API.

        :param kwargs: Keyword arguments of keys and values to check -- will be an AND condition
        :return: A condition that checks if the given keys are in the state and equal to the given values
        """
        keys = list(kwargs.keys())

        def condition_func(state: State) -> bool:
            for key, value in kwargs.items():
                if state.get(key) != value:
                    return False
            return True

        name = f"{', '.join(f'{key}={value}' for key, value in sorted(kwargs.items()))}"
        return Condition(keys, condition_func, name=name)

    def __repr__(self):
        return f"condition: {self._name}"

    @property
    def name(self) -> str:
        return self._name

    def __or__(self, other: "Condition") -> "Condition":
        """Combines two conditions with an OR operator. This will return a new condition
        that is the OR of the two conditions.

        To check if either foo is bar or baz is qux:

        .. code-block:: python

            condition = Condition.when(foo="bar") | Condition.when(baz="qux")

        :param other: Other condition to OR with
        :return: A new condition that is the OR of the two conditions
        """
        if not isinstance(other, Condition):
            raise ValueError(f"Cannot OR a Condition with {other}")
        return Condition(
            self._keys + other._keys,
            lambda state: self._resolver(state) or other.resolver(state),
            name=f"{self._name} | {other._name}",
        )

    def __and__(self, other: "Condition") -> "Condition":
        """Combines two conditions with an AND operator. This will return a new condition
        that is the AND of the two conditions.

        To check if both foo is bar and baz is qux:

        .. code-block:: python

            condition = Condition.when(foo="bar") & Condition.when(baz="qux")
            # equivalent to
            condition = Condition.when(foo="bar", baz="qux")

        :param other: Other condition to AND with
        :return:  A new condition that is the AND of the two conditions
        """
        if not isinstance(other, Condition):
            raise ValueError(f"Cannot AND a Condition with {other}")
        return Condition(
            self._keys + other._keys,
            lambda state: self._resolver(state) and other.resolver(state),
            name=f"{self._name} & {other._name}",
        )

    @property
    def resolver(self) -> Callable[[State], bool]:
        return self._resolver

    def __invert__(self):
        return Condition(self._keys, lambda state: not self._resolver(state), name=f"~{self._name}")


Condition.default = Condition([], lambda _: True, name="default")

default = Condition.default
when = Condition.when
expr = Condition.expr
lmda = Condition.lmda
# exists = Condition.exists


class Result(Action):
    def __init__(self, *fields: str):
        """Represents a result action. This is purely a convenience class to
        pull data from state and give it out to the result. It does nothing to
        the state itself.

        :param fields: Fields to pull from the state and put into results
        """
        super(Result, self).__init__()
        self._fields = fields

    def run(self, state: State) -> dict:
        return {key: value for key, value in state.get_all().items() if key in self._fields}

    def update(self, result: dict, state: State) -> State:
        return state  # does not modify state in any way

    @property
    def reads(self) -> list[str]:
        return list(self._fields)

    @property
    def writes(self) -> list[str]:
        return []


class Input(Action):
    def __init__(self, *fields: str):
        """Represents an input action -- this reads something from an input
        then writes that directly to state. This is a convenience class for when you don't
        need to process the input and just want to put it in state for later use.

        :param fields: Fields to pull from the inputs and put into state
        """
        super(Input, self).__init__()
        self._fields = fields

    @property
    def reads(self) -> list[str]:
        return []  # nothing from state

    def run(self, state: State, **run_kwargs) -> dict:
        return {key: run_kwargs[key] for key in self._fields}

    @property
    def writes(self) -> list[str]:
        return list(self._fields)

    @property
    def inputs(self) -> list[str]:
        return list(self._fields)

    def update(self, result: dict, state: State) -> State:
        return state.update(**result)


class SingleStepAction(Action, abc.ABC):
    """Internal representation of a "single-step" action. While most actions will have
    a run and an update, this is a convenience class for actions that return them both at the same time.
    Note this is not user-facing, as the internal API is meant to change. This is largely special-cased
    for the function-based action, which users will not be extending.

    Currently this keeps a cache of the state created, which is not ideal. This is a temporary
    measure to make the API work, and will be removed in the future.
    """

    def __init__(self):
        super(SingleStepAction, self).__init__()
        self._state_created = None

    @property
    def single_step(self) -> bool:
        return True

    @abc.abstractmethod
    def run_and_update(self, state: State, **run_kwargs) -> tuple[dict, State]:
        """Performs a run/update at the same time.

        :param state: State to run the action on
        :param run_kwargs: Additional arguments to the function passed at runtime.
        :return: Result of the action and the new state
        """
        pass

    def run(self, state: State) -> dict:
        """This should never really get called.
        That said, this is an action so we have this in for now.
        TODO -- rethink the hierarchy. This is not user-facing, so its OK to change,
        and there's a bug we want to fix that requires this.

        :param state:
        :return:
        """
        raise ValueError(
            "SingleStepAction.run should never be called independently -- use run_and_update instead."
        )

    def update(self, result: dict, state: State) -> State:
        """Same with the above"""
        raise ValueError(
            "SingleStepAction.update should never be called independently -- use run_and_update instead."
        )

    def is_async(self) -> bool:
        """Convenience method to check if the function is async or not.
        We'll want to clean up the class hierarchy, but this is all internal.
        See note on ``run`` and ``update`` above

        :return: True if the function is async, False otherwise
        """
        return inspect.iscoroutinefunction(self.run_and_update)


# the following exist to share implementation between FunctionBasedStreamingAction and FunctionBasedAction
# TODO -- think through the class hierarchy to simplify, for now this is OK
def derive_inputs_from_fn(bound_params: dict, fn: Callable) -> tuple[list[str], list[str]]:
    """Derives inputs from the function, given the bound parameters. This assumes that the function
    has inputs named `state`, as well as any number of other kwarg-boundable parameters.

    :param bound_params: Parameters that are already bound to the function
    :param fn: Function to derive inputs from
    :return: Required and optional inputs
    """
    sig = inspect.signature(fn)
    required_inputs, optional_inputs = [], []
    for param_name, param in sig.parameters.items():
        if param_name != "state" and param_name not in bound_params:
            if param.default is inspect.Parameter.empty:
                # has no default means its required
                required_inputs.append(param_name)
            else:
                # has a default means its optional
                optional_inputs.append(param_name)
    return required_inputs, optional_inputs


FunctionBasedActionType = Union["FunctionBasedAction", "FunctionBasedStreamingAction"]


class FunctionBasedAction(SingleStepAction):
    ACTION_FUNCTION = "action_function"

    def __init__(
        self,
        fn: Callable,
        reads: List[str],
        writes: List[str],
        bound_params: Optional[dict] = None,
        input_spec: Optional[tuple[list[str], list[str]]] = None,
        originating_fn: Optional[Callable] = None,
        schema: ActionSchema = DEFAULT_SCHEMA,
        tags: Optional[List[str]] = None,
    ):
        """Instantiates a function-based action with the given function, reads, and writes.
        The function must take in a state and return a tuple of (result, new_state).

        :param fn: Function to run
        :param reads: Keys that the function reads from the state
        :param writes: Keys that the function writes to the state
        :param bound_params: Prior bound parameters
        :param input_spec: Specification for inputs. Will derive from function if not provided.
        """
        super(FunctionBasedAction, self).__init__()
        self._originating_fn = originating_fn if originating_fn is not None else fn
        self._fn = fn
        self._reads = reads
        self._writes = writes
        self._bound_params = bound_params if bound_params is not None else {}
        self._inputs = (
            derive_inputs_from_fn(self._bound_params, self._fn)
            if input_spec is None
            else (
                [item for item in input_spec[0] if item not in self._bound_params],
                [item for item in input_spec[1] if item not in self._bound_params],
            )
        )
        self._schema = schema
        self._tags = tags if tags is not None else []

    @property
    def fn(self) -> Callable:
        return self._fn

    @property
    def reads(self) -> list[str]:
        return self._reads

    @property
    def writes(self) -> list[str]:
        return self._writes

    @property
    def inputs(self) -> tuple[list[str], list[str]]:
        return self._inputs

    @property
    def schema(self) -> ActionSchema:
        return self._schema

    @property
    def tags(self) -> list[str]:
        return self._tags

    def with_params(self, **kwargs: Any) -> "FunctionBasedAction":
        """Binds parameters to the function.
        Note that there is no reason to call this by the user. This *could*
        be done at the class level, but given that API allows for constructor parameters
        (which do the same thing in a cleaner way), it is best to keep it here for now.

        :param kwargs:
        :return:
        """
        return FunctionBasedAction(
            self._fn,
            self._reads,
            self._writes,
            {**self._bound_params, **kwargs},
            input_spec=self._inputs,
            originating_fn=self._originating_fn,
            schema=self._schema,
            tags=self._tags,
        )

    def run_and_update(self, state: State, **run_kwargs) -> tuple[dict, State]:
        return self._fn(state, **self._bound_params, **run_kwargs)

    def is_async(self) -> bool:
        return inspect.iscoroutinefunction(self._fn)

    def get_source(self) -> str:
        """Return the source of the code for this action."""
        return inspect.getsource(self._originating_fn)


StateType = TypeVar("StateType")

StreamType = Tuple[dict, Optional[State[StateType]]]

GeneratorReturnType = Generator[StreamType, None, None]
AsyncGeneratorReturnType = AsyncGenerator[StreamType, None]

StreamingFn = Callable[..., GeneratorReturnType]
StreamingFnAsync = Callable[..., AsyncGeneratorReturnType]


class StreamingAction(Action, abc.ABC):
    """Base class for Streaming action. These are "multi-step", meaning that
    they run in multiple passes (run -> update)"""

    @abc.abstractmethod
    def stream_run(self, state: State[StateType], **run_kwargs) -> Generator[dict, None, None]:
        """Streaming action ``stream_run`` is different than standard action run. It:
        1. streams in an intermediate result (the dict output)
        2. yields the final result at the end

        Note that the user, in this case, is responsible for joining the result.

        For instance, you could have:

        .. code-block:: python

            def stream_run(state: State) -> Generator[dict, None, dict]:
                buffer = [] # you might want to be more efficient than simple strcat
                for token in query(state['prompt']):
                    yield {'response' : token}
                    buffer.append(token)
                yield {'response' : "".join(buffer)}

        This would utilize a simple string buffer (implemented by a list) to store the results
        and then join them at the end. We return the final result.

        :param state: State to run the action on
        :param run_kwargs: parameters passed to the run function -- these are specified by `inputs`
        :return: A generator that streams in a result and returns the final result
        """
        pass

    def run(self, state: State[StateType], **run_kwargs) -> dict:
        """Runs the streaming action through to completion."""
        gen = self.stream_run(state, **run_kwargs)
        last_result = None
        for item in gen:
            last_result = item
        return last_result

    @property
    def streaming(self) -> bool:
        return True


class AsyncStreamingAction(Action, abc.ABC):
    """Asynchronous version of the streaming action. This is a base class for streaming actions.
    Currently this is separate from the synchronous version, but we may want to merge them in the future.
    Note this is the "multi-step" variant, in which run/update are separate."""

    @abc.abstractmethod
    async def stream_run(self, state, **run_kwargs) -> AsyncGenerator[dict, None]:
        """Asynchronous streaming action ``stream_run`` is different than the standard action run. It:
        1. streams in an intermediate result (the dict output)
        2. yields the final result at the end

        Note that the user, in this case, is responsible for joining the result.

        For instance, you could have:

        .. code-block:: python

            async def stream_run(state: State) -> Generator[dict, None, dict]:
                buffer = [] # you might want to be more efficient than simple strcat
                async for token in query(state['prompt']): # asynchronous generator
                    yield {'response' : token}
                    buffer.append(token)
                yield {'response' : "".join(buffer)}

        This would utilize a simple string buffer (implemented by a list) to store the results
        and then join them at the end. We return the final result.

        :param state: State to run the action on
        :param run_kwargs: parameters passed to the run function -- these are specified by `inputs`
        :return: A generator that streams in a result and returns the final result
        """
        pass

    async def run(self, state: State[StateType], **run_kwargs) -> dict:
        """Runs the streaming action through to completion.
        Returns the final result. This is used if we want a streaming action
        as an intermediate.

        :param state: State to run the action on
        :param run_kwargs: Additional arguments to the function passed at runtime.
        :return: Final result
        """
        gen = self.stream_run(state, **run_kwargs)
        result = None
        async for item in gen:
            result = item
        return result

    @property
    def streaming(self) -> bool:
        return True

    def is_async(self) -> bool:
        return True


StreamResultType = TypeVar("StreamResultType")


class StreamingResultContainer(Generic[StateType, StreamResultType], Iterator[StreamResultType]):
    """Container for a streaming result. This allows you to:

    1. Iterate over the result as it comes in
    2. Get the final result/state at the end

    If you're familiar with generators/iterators in python, this is effectively an
    iterator that caches the final result after calling it. This is meant to be used
    exclusively with the streaming action calls in `Application`. Note that you will
    never instantiate this class directly, but you will use it in the API when it is returned
    by :py:meth:`stream_result <burr.core.application.Application.stream_result>`.
    For reference, here's how you would use it:

    .. code-block:: python

        action_we_just_ran, streaming_result_container = application.stream_result(...)
        print(f"getting streaming results for action={action_we_just_ran.name}")

        for result_component in streaming_result_container:
            print(result_component['response']) # this assumes you have a response key in your result

        final_result, final_state = streaming_result_container.get()
    """

    @staticmethod
    def pass_through(
        results: StreamResultType, final_state: State[StateType]
    ) -> "StreamingResultContainer[StreamResultType, StateType]":
        """Instantiates a streaming result container that just passes through the given results
        This is to be used internally -- it allows us to wrap non-streaming action results in a streaming
        result container."""

        def empty_generator() -> (
            Generator[Tuple[StreamResultType, Optional[State[StateType]]], None, None]
        ):
            yield results, final_state

        return StreamingResultContainer(
            empty_generator(),
            final_state,
            lambda result, state: (result, state),
            lambda result, state, exc: None,
        )

    def __init__(
        self,
        streaming_result_generator: GeneratorReturnType,
        initial_state: State[StateType],
        process_result: Callable[[dict, State], tuple[dict, State]],
        callback: Callable[[Optional[dict], State, Optional[Exception]], None],
    ):
        """Initializes a streaming result container. User will never call directly.

        :param streaming_result_generator: Generator of streaming results. Note that this
            will always yield result, Optional[State] -- regardless of the API used to create it.
        :param initial_state:  The initial state
        :param process_result:  Function to process the result -- this gets called after the generator is exhausted,
            prior to returning the final result
        :param callback: Callback to call at the very end. This will only get called *once*, and will be called during the finally block of the generator
        """
        self.streaming_result_generator = streaming_result_generator
        self._action = action
        self._callback = callback
        self._process_result = process_result
        self._initial_state = initial_state
        self._result = None
        self._callback_realized = False

    def __next__(self) -> StreamResultType:
        if self._result is not None:
            # we're done, and we've run through it
            raise StopIteration
        result, state = self.streaming_result_generator.__next__()
        if state is not None:  # we're done -- we've hit the last one
            self._result = self._process_result(result, state)
            raise StopIteration
        return result

    def __iter__(self) -> Iterator[StreamResultType]:
        def gen_fn():
            try:
                while True:
                    out = self.__next__()
                    yield out
            except StopIteration:
                return
            finally:
                if self._result is None:
                    self._result = None, self._initial_state
                if not self._callback_realized:
                    exc = sys.exc_info()[1]
                    self._callback_realized = True
                    self._callback(*self._result, exc)

        # We really don't need an internal generator function but this was done to keep it the same
        # as the async version
        return gen_fn()

    def get(self) -> Tuple[StreamResultType, State[StateType]]:
        # exhaust the generator
        for _ in self:
            pass

        return self._result


class AsyncStreamingResultContainer(
    Generic[StateType, StreamResultType],
    AsyncIterator[StreamResultType],
):
    """Container for an async streaming result. This allows you to:
    1. Iterate over the result as it comes in
    2. Await the final result/state at the end

    If you're familiar with generators/iterators in python, this is effectively an
    iterator that caches the final result after calling it. This is meant to be used
    exclusively with the streaming action calls in `Application`. Note that you will
    never instantiate this class directly, but you will use it in the API when it is returned
    by :py:meth:`astream_result <burr.core.application.Application.stream_result>`.
    For reference, here's how you would use it:

    .. code-block:: python

        action_we_just_ran, streaming_result_container = await application.stream_result(...)
        print(f"getting streaming results for action={action_we_just_ran.name}")

        async for result_component in streaming_result_container:
            print(result_component['response']) # this assumes you have a response key in your result

        final_result, final_state = await streaming_result_container.get()
    """

    def __init__(
        self,
        streaming_result_generator: AsyncGeneratorReturnType,
        initial_state: State[StateType],
        process_result: Callable[
            [StreamResultType, State[StateType]], tuple[StreamResultType, State[StateType]]
        ],
        callback: Callable[
            [Optional[StreamResultType], State[StateType], Optional[Exception]],
            typing.Coroutine[None, None, None],
        ],
    ):
        """Initializes an async streaming result container. User will never call directly.

        :param streaming_result_generator: Generator of streaming results. Note that this
            will always yield result, Optional[State] -- regardless of the API used to create it.
        :param initial_state:  The initial state
        :param process_result:  Function to process the result -- this gets called after the generator is exhausted,
            prior to returning the final result
        :param callback: Callback to call at the very end. This will only get called *once*, and will be called during the finally block of the generator
        """
        self.streaming_result_generator = streaming_result_generator
        self._initial_state = initial_state
        self._process_result = process_result
        self._callback = callback
        self._result = None
        self._callback_realized = False

    async def __anext__(self) -> StreamResultType:
        """Moves to the next state in the streaming result"""
        if self._result is not None:
            # we're done, and we've run through it
            raise StopAsyncIteration
        result, state = await self.streaming_result_generator.__anext__()
        if state is not None:  # we're done -- we've hit the last one
            self._result = self._process_result(result, state)
            raise StopAsyncIteration
        return result

    def __aiter__(self) -> AsyncIterator[StreamResultType]:
        """Gives the iterator. Just calls anext, assigning the result in the finally block.
        Note this may not be perfect due to the complexity of callbacks for async generators,
        but it works in most cases."""

        async def gen_fn():
            try:
                while True:
                    yield await self.__anext__()
            except StopAsyncIteration:
                return
            finally:
                if self._result is None:
                    self._result = None, self._initial_state
                if not self._callback_realized:
                    exc = sys.exc_info()[1]
                    self._callback_realized = True
                    await self._callback(*self._result, exc)

        # return it as `__aiter__` cannot be async/have awaits :/
        return gen_fn()

    async def get(self) -> tuple[Optional[StreamResultType], State[StateType]]:
        # exhaust the generator
        async for _ in self:
            pass

        return self._result

    @staticmethod
    def pass_through(
        results: StreamResultType, final_state: State[StateType]
    ) -> "AsyncStreamingResultContainer[StateType]":
        """Creates a streaming result container that just passes through the given results.
        This is not a public facing API."""

        async def just_results() -> AsyncGeneratorReturnType:
            yield results, final_state

        async def empty_callback(
            result: Optional[StreamResultType], state: State, exc: Optional[Exception]
        ):
            pass

        return AsyncStreamingResultContainer[StateType, StreamResultType](
            just_results(), final_state, lambda result, state: (result, state), empty_callback
        )


class SingleStepStreamingAction(SingleStepAction, abc.ABC):
    """Class to represent a "single-step" streaming action. This is meant to
    work with the functional API. Note this is not user-facing -- the user will
    only interact with this by using the ``@streaming_action`` decorator.
    """

    @abc.abstractmethod
    def stream_run_and_update(
        self, state: State, **run_kwargs
    ) -> Union[GeneratorReturnType, AsyncGeneratorReturnType]:
        """Streaming version of the run and update function. This
        return type is a generator that streams in a result, has no "send"
        value, and returns the final result (new result + state).
        """
        pass

    def _run_and_update(self, state: State, **run_kwargs) -> tuple[dict, State]:
        gen = self.stream_run_and_update(state, **run_kwargs)
        result = None
        new_state = state
        for result, new_state in gen:
            pass
        # TODO -- validate that it has a single length output
        return result, new_state

    async def _arun_and_update(self, state: State, **run_kwargs) -> tuple[dict, State]:
        gen = self.stream_run_and_update(state, **run_kwargs)
        last_result = None
        new_state = state
        async for last_result, new_state in gen:
            pass
        return last_result, new_state

    def run_and_update(
        self, state: State, **run_kwargs
    ) -> Union[tuple[dict, State], Coroutine[Any, Any, tuple[dict, State]]]:
        """Runs the action and returns the final result. This allows us to run this as a
        single step action. This is helpful for when the streaming result needs to be
        run as an intermediate."""
        if self.is_async():
            return self._arun_and_update(state, **run_kwargs)
        return self._run_and_update(state, **run_kwargs)

    @property
    def streaming(self) -> bool:
        return True

    def is_async(self) -> bool:
        return inspect.isasyncgenfunction(self.stream_run_and_update)


class FunctionBasedStreamingAction(SingleStepStreamingAction):
    _fn: Union[StreamingFn, StreamingFnAsync]

    def __init__(
        self,
        fn: Union[
            StreamingFn,
            StreamingFnAsync,
        ],
        reads: List[str],
        writes: List[str],
        bound_params: Optional[dict] = None,
        input_spec: Optional[tuple[list[str], list[str]]] = None,
        originating_fn: Optional[Callable] = None,
        schema: ActionSchema = DEFAULT_SCHEMA,
        tags: Optional[List[str]] = None,
    ):
        """Instantiates a function-based streaming action with the given function, reads, and writes.
        The function must take in a state (and inputs) and return a generator of (result, new_state).

        :param fn: Function to use
        :param reads:
        :param writes:
        """
        super(FunctionBasedStreamingAction, self).__init__()
        self._fn = fn
        self._reads = reads
        self._writes = writes
        self._bound_params = bound_params if bound_params is not None else {}
        self._inputs = (
            derive_inputs_from_fn(self._bound_params, self._fn)
            if input_spec is None
            else (
                [item for item in input_spec[0] if item not in self._bound_params],
                [item for item in input_spec[1] if item not in self._bound_params],
            )
        )
        self._originating_fn = originating_fn if originating_fn is not None else fn
        self._schema = schema
        self._tags = tags if tags is not None else []

    async def _a_stream_run_and_update(
        self, state: State, **run_kwargs
    ) -> AsyncGeneratorReturnType:
        async for result in self._fn(state, **self._bound_params, **run_kwargs):
            yield result

    def _stream_run_and_update(self, state: State, **run_kwargs) -> GeneratorReturnType:
        yield from self._fn(state, **self._bound_params, **run_kwargs)

    def stream_run_and_update(
        self, state: State, **run_kwargs
    ) -> Union[AsyncGeneratorReturnType, GeneratorReturnType]:
        if self.is_async():
            return self._a_stream_run_and_update(state, **run_kwargs)
        return self._stream_run_and_update(state, **run_kwargs)

    @property
    def reads(self) -> list[str]:
        return self._reads

    @property
    def writes(self) -> list[str]:
        return self._writes

    @property
    def streaming(self) -> bool:
        return True

    @property
    def tags(self) -> list[str]:
        return self._tags

    def with_params(self, **kwargs: Any) -> "FunctionBasedStreamingAction":
        """Binds parameters to the function. This is not user-facing -- this is
        meant to be used internally by the API.

        :param kwargs:
        :return:
        """
        return FunctionBasedStreamingAction(
            self._fn,
            self._reads,
            self._writes,
            {**self._bound_params, **kwargs},
            input_spec=self._inputs,
            originating_fn=self._originating_fn,
            schema=self._schema,
            tags=self._tags,
        )

    @property
    def inputs(self) -> tuple[list[str], list[str]]:
        return self._inputs

    @property
    def fn(self) -> Union[StreamingFn, StreamingFnAsync]:
        return self._fn

    @property
    def schema(self) -> ActionSchema:
        return self._schema

    def is_async(self) -> bool:
        return inspect.isasyncgenfunction(self._fn)

    def get_source(self) -> str:
        """Return the source of the code for this action"""
        return inspect.getsource(self._originating_fn)


C = TypeVar("C", bound=Callable)  # placeholder for any Callable


class FunctionRepresentingAction(Protocol[C]):
    action_function: FunctionBasedActionType
    __call__: C

    def bind(self, **kwargs: Any) -> Self:
        ...


def copy_func(f: types.FunctionType) -> types.FunctionType:
    """Copies a function. This is used internally to bind parameters to a function
    so we don't accidentally overwrite them.

    :param f: Function to copy
    :return: The copied function
    """
    fn = types.FunctionType(f.__code__, f.__globals__, f.__name__, f.__defaults__, f.__closure__)
    fn.__dict__.update(f.__dict__)
    return fn


def bind(self: FunctionRepresentingAction, **kwargs: Any) -> FunctionRepresentingAction:
    """Binds an action to the given parameters. This is functionally equivalent to
    functools.partial, but is more explicit and is meant to be used in the API. This only works with
    the :py:meth:`@action <burr.core.action.action>`  functional API and not with the class-based API.

    .. code-block:: python

        @action(["x"], ["y"])
        def my_action(state: State, z: int) -> tuple[dict, State]:
            return {"y": state.get("x") + z}, state

        my_action.bind(z=2)

    :param self: The decorated function
    :param kwargs: The keyword arguments to bind
    :return: The decorated function with the given parameters bound
    """
    self = copy_func(self)  # we have to bind to a copy of the function, otherwise it will override
    self.action_function = self.action_function.with_params(**kwargs)
    return self


class action:
    @staticmethod
    def pydantic(
        reads: List[str],
        writes: List[str],
        state_input_type: Optional[Type["BaseModel"]] = None,
        state_output_type: Optional[Type["BaseModel"]] = None,
        tags: Optional[List[str]] = None,
    ) -> Callable:
        """Action that specifies inputs/outputs using pydantic models.
        This should make it easier to develop with guardrails.

        :param reads: keys that this model reads. Note that this will be a subset of the pydantic model with which this is decorated.
            We will be validating that the keys are present in the model.
        :param writes: keys that this model writes. Note that this will be a subset of the pydantic model with which this is decorated.
            We will be validating that the keys are present in the model.
        :param state_input_type: The pydantic model type that is used to represent the input state.
            If this is None it will attempt to derive from the signature.
        :param state_output_type: The pydantic model type that is used to represent the output state.
            If this is None it will attempt to derive from the signature.
        :param tags: Optional list of tags to associate with this action
        :return:
        """
        try:
            from burr.integrations.pydantic import pydantic_action
        except ImportError:
            raise ImportError(
                "Please install pydantic to use the pydantic decorator. pip install burr[pydantic]"
            )

        return pydantic_action(
            reads=reads,
            writes=writes,
            state_input_type=state_input_type,
            state_output_type=state_output_type,
            tags=tags,
        )

    def __init__(self, reads: List[str], writes: List[str], tags: Optional[List[str]] = None):
        """Decorator to create a function-based action. This is user-facing.
        Note that, in the future, with typed state, we may not need this for
        all cases.

        If parameters are not bound, they will be interpreted as inputs and must
        be passed in at runtime. If they have default values, they will be recorded
        as optional inputs. These can (optionally) be provided at runtime.

        :param reads: Items to read from the state
        :param writes: Items to write to the state
        :return: The decorator to assign the function as an action
        """
        self.reads = reads
        self.writes = writes
        self.tags = tags

    def __call__(self, fn) -> FunctionRepresentingAction:
        setattr(
            fn,
            FunctionBasedAction.ACTION_FUNCTION,
            FunctionBasedAction(fn, self.reads, self.writes, tags=self.tags),
        )
        setattr(fn, "bind", types.MethodType(bind, fn))
        return fn


class streaming_action:
    @staticmethod
    def pydantic(
        reads: List[str],
        writes: List[str],
        state_input_type: Type["BaseModel"],
        state_output_type: Type["BaseModel"],
        stream_type: Union[Type["BaseModel"], Type[dict]],
        tags: Optional[List[str]] = None,
    ) -> Callable:
        """Creates a streaming action that uses pydantic models.

        :param reads: The fields this consumes from the state.
        :param writes: The fields this writes to the state.
        :param stream_type: The pydantic model or dictionary type that is used to represent the partial results.
            Use a dict if you want this untyped.
        :param state_input_type: The pydantic model type that is used to represent the input state.
        :param state_output_type: The pydantic model type that is used to represent the output state.
        :param tags: Optional list of tags to associate with this action
        :return: The same function, decorated function.
        """
        try:
            from burr.integrations.pydantic import pydantic_streaming_action
        except ImportError:
            raise ImportError(
                "Please install pydantic to use the pydantic decorator. pip install 'burr[pydantic]'"
            )

        return pydantic_streaming_action(
            reads=reads,
            writes=writes,
            state_input_type=state_input_type,
            state_output_type=state_output_type,
            stream_type=stream_type,
            tags=tags,
        )

    def __init__(self, reads: List[str], writes: List[str], tags: Optional[List[str]] = None):
        """Decorator to create a streaming function-based action. This is user-facing.

        If parameters are not bound, they will be interpreted as inputs and must be passed in at runtime.

        See the following example for how to use this decorator -- this reads ``prompt`` from the state and writes
        ``response`` back out, yielding all intermediate chunks.

        Note that this *must* return a value. If it does not, we will not know how to update the state, and
        we will error out.

        .. code-block:: python

            @streaming_action(reads=["prompt"], writes=['response'])
            def streaming_response(state: State) -> Generator[dict, None, tuple[dict, State]]:
                response = client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[{
                        'role': 'user',
                        'content': state["prompt"]
                        }],
                    temperature=0,
                )
                buffer = []
                for chunk in response:
                    delta = chunk.choices[0].delta.content
                    buffer.append(delta)
                    # yield partial results
                    yield {'response': delta}, None
                full_response = ''.join(buffer)
                # return the final result
                return {'response': full_response}, state.update(response=full_response)

        :param reads: The fields this consumes from the state.
        :param writes: The fields this writes to the state.
        :param tags: Optional list of tags to associate with this action
        """
        self.reads = reads
        self.writes = writes
        self.tags = tags

    def __call__(self, fn: Callable) -> FunctionRepresentingAction:
        fn = copy_func(fn)
        setattr(
            fn,
            FunctionBasedAction.ACTION_FUNCTION,
            FunctionBasedStreamingAction(fn, self.reads, self.writes, tags=self.tags),
        )
        setattr(fn, "bind", types.MethodType(bind, fn))
        return fn


ActionT = TypeVar("ActionT", bound=Action)


def create_action(action_: Union[Callable, ActionT], name: str) -> ActionT:
    """Factory function to create an action. This is meant to be called by
    the ApplicationBuilder, and not by the user. The internal API may change.

    :param action_: Object to create an action from
    :param name: The name to assign the action
    :return: An action with the given name
    """
    if hasattr(action_, FunctionBasedAction.ACTION_FUNCTION):
        action_ = getattr(action_, FunctionBasedAction.ACTION_FUNCTION)
    elif not isinstance(action_, Action):
        raise ValueError(
            f"Object {action_} is not a valid action. Have you decorated it with @action or @streaming_action?"
        )
    return action_.with_name(name)



---
File: /burr/burr/core/application.py
---

from __future__ import annotations

import contextvars
import dataclasses
import functools
import inspect
import logging
import pprint
import uuid
from concurrent.futures import Executor, ThreadPoolExecutor
from contextlib import AbstractContextManager
from typing import (
    TYPE_CHECKING,
    Any,
    AsyncGenerator,
    Callable,
    Dict,
    Generator,
    Generic,
    List,
    Literal,
    Optional,
    Set,
    Tuple,
    TypeVar,
    Union,
    cast,
)

from burr import system, telemetry, visibility
from burr.common import types as burr_types
from burr.core import persistence, validation
from burr.core.action import (
    DEFAULT_SCHEMA,
    Action,
    AsyncStreamingAction,
    AsyncStreamingResultContainer,
    Condition,
    Function,
    Reducer,
    SingleStepAction,
    SingleStepStreamingAction,
    StreamingAction,
    StreamingResultContainer,
)
from burr.core.graph import Graph, GraphBuilder
from burr.core.persistence import (
    AsyncBaseStateLoader,
    AsyncBaseStateSaver,
    BaseStateLoader,
    BaseStateSaver,
)
from burr.core.state import State
from burr.core.typing import ActionSchema, DictBasedTypingSystem, TypingSystem
from burr.core.validation import BASE_ERROR_MESSAGE
from burr.lifecycle.base import ExecuteMethod, LifecycleAdapter, PostRunStepHook, PreRunStepHook
from burr.lifecycle.internal import LifecycleAdapterSet
from burr.visibility import tracing
from burr.visibility.tracing import tracer_factory_context_var

if TYPE_CHECKING:
    # TODO --  figure out whether we want to just do if TYPE_CHECKING
    # for all first-class imports as Ruff suggests...
    from burr.tracking.base import TrackingClient

logger = logging.getLogger(__name__)

PRIOR_STEP = "__PRIOR_STEP"
SEQUENCE_ID = "__SEQUENCE_ID"

StateType = TypeVar("StateType")
StateTypeToSet = TypeVar("StateTypeToSet")


def _validate_result(result: Any, name: str, schema: ActionSchema = DEFAULT_SCHEMA) -> None:
    # TODO -- split out the action schema into input/output schema types
    # Currently they're tied together, but this doesn't make as much sense for single-step actions
    result_type = schema.intermediate_result_type()
    if not isinstance(result, result_type):
        raise ValueError(
            f"Action {name} returned a non-{result_type.__name__} result: {result}. "
            f"All results must be dictionaries."
        )


def _raise_fn_return_validation_error(output: Any, action_name: str):
    raise ValueError(
        BASE_ERROR_MESSAGE
        + f"Single step action: {action_name} must return either *just* the newly updated State or a tuple of (result: dict, state: State). "
        f"Got: {output} of type {type(output)} instead, which is not valid."
    )


def _adjust_single_step_output(
    output: Union[State, Tuple[dict, State]],
    action_name: str,
    action_schema: ActionSchema,
):
    """Adjusts the output of a single step action to be a tuple of (result, state) or just state"""

    if isinstance(output, tuple):
        if not len(output) == 2:
            _raise_fn_return_validation_error(output, action_name)
        _validate_result(output[0], action_name, action_schema)
        if not isinstance(output[1], State):
            _raise_fn_return_validation_error(output, action_name)
        return output
    if isinstance(output, State):
        return {}, output
    _raise_fn_return_validation_error(output, action_name)


def _remap_dunder_parameters(
    run_method: Callable, inputs: Dict[str, Any], vars_to_remap: List[str]
) -> dict:
    """This is a utility function to remap the __dunder parameters to the mangled version in the function signature.

    Python mangles __parameter names in the function signature, so we need to remap it to the correct parameter name.

    :param run_method: the run method to inspect.
    :param inputs: the inputs to inspect
    :param vars_to_remap: the variables to remap
    :return: potentially new dict with the remapped variable, else the original dict.
    """
    # Get the signature of the method being run. This should be Function.run() or similar.
    signature = inspect.signature(run_method)
    mangled_params: Dict[str, Optional[str]] = {v: None for v in vars_to_remap}
    # Find the name-mangled __context variable
    for dunder_param in mangled_params.keys():
        for param in signature.parameters.values():
            if param.name.endswith(dunder_param):
                mangled_params[dunder_param] = param.name
                break

    # If any mangled __parameter is found, remap the value in inputs
    if any(mangled_params.values()):
        inputs = inputs.copy()
        for dunder_param, mangled_name in mangled_params.items():
            if mangled_name and dunder_param in inputs:
                inputs[mangled_name] = inputs.pop(dunder_param)
    return inputs


def _run_function(function: Function, state: State, inputs: Dict[str, Any], name: str) -> dict:
    """Runs a function, returning the result of running the function.
    Note this restricts the keys in the state to only those that the
    function reads.

    :param function: Function to run
    :param state: State at time of execution
    :param inputs: Inputs to the function
    :return:
    """
    if function.is_async():
        raise ValueError(
            f"Cannot run async: {name} "
            "in non-async context. Use astep()/aiterate()/arun() "
            "instead...)"
        )
    state_to_use = state.subset(*function.reads)
    function.validate_inputs(inputs)
    if "__context" in inputs or "__tracer" in inputs:
        # potentially need to remap the __context & __tracer variables
        inputs = _remap_dunder_parameters(function.run, inputs, ["__context", "__tracer"])
    result = function.run(state_to_use, **inputs)
    _validate_result(result, name)
    return result


async def _arun_function(
    function: Function, state: State, inputs: Dict[str, Any], name: str
) -> dict:
    """Runs a function, returning the result of running the function.
    Async version of the above."""
    state_to_use = state.subset(*function.reads)
    function.validate_inputs(inputs)
    result = await function.run(state_to_use, **inputs)
    _validate_result(result, name)
    return result


def _state_update(state_to_modify: State, modified_state: State) -> State:
    """Performs a state update for an action -- allowing for deletes.

    This is suboptimal -- we should not be observing the state, we should be using the state commands and layering in deltas.
    That said, we currently eagerly evaluate the state at all operations, which means we have to do it this way. See
    https://github.com/DAGWorks-Inc/burr/issues/33 for a more detailed plan.

    This function was written to solve this issue: https://github.com/DAGWorks-Inc/burr/issues/28.


    :param state_subset_pre_update: The subset of state passed to the update() function
    :param modified_state: The subset of state realized after the update() function
    :param state_to_modify: The state to modify-- this is the original
    :return:
    """
    # We want to wipe these, as these should never be changed by the action
    # an action that modifies these is going to incur "undefined behavior" -- we can effectively
    # do anything we want -- in this case we're dropping it, but we may error out in the future
    # This is really an issue in the case of parallelism -- E.G. when an action reduces multiple states
    # a common failure mode is to modify.
    # TODO -- unify the logic of choosing whether a key is internal or not
    private_fields = [item for item in modified_state.keys() if item.startswith("__")]
    modified_state_without_private_fields = modified_state.wipe(delete=private_fields)
    # we filter here as we don't want to
    deleted_keys = [
        item
        for item in (
            set(state_to_modify.keys()) - set(modified_state_without_private_fields.keys())
        )
        if not item.startswith("__")
    ]
    return state_to_modify.merge(modified_state_without_private_fields).wipe(delete=deleted_keys)


def _validate_reducer_writes(reducer: Reducer, state: State, name: str) -> None:
    required_writes = reducer.writes
    missing_writes = set(reducer.writes) - state.keys()
    if len(missing_writes) > 0:
        raise ValueError(
            f"State is missing write keys after running: {name}. Missing keys are: {missing_writes}. "
            f"Has writes: {required_writes}"
        )


def _run_reducer(reducer: Reducer, state: State, result: dict, name: str) -> State:
    """Runs the reducer, returning the new state. Note this restricts the
    keys in the state to only those that the function writes.

    :param reducer:
    :param state:
    :param result:
    :return:
    """
    # TODO -- better guarding on state reads/writes
    new_state = reducer.update(result, state)
    keys_in_new_state = set(new_state.keys())
    new_keys = keys_in_new_state - set(state.keys())
    extra_keys = new_keys - set(reducer.writes)
    if len(extra_keys) > 0:
        raise ValueError(
            f"Action {name} attempted to write to keys {extra_keys} "
            f"that it did not declare. It declared: ({reducer.writes})!"
        )
    _validate_reducer_writes(reducer, new_state, name)
    return _state_update(state, new_state)


def _create_dict_string(kwargs: dict) -> str:
    """This is a utility function to create a string representation of a dict.
    This is the state that was passed into the function usually. This is useful for debugging,
    as it can be printed out to see what the state was.

    :param kwargs: The inputs to the function that errored.
    :return: The string representation of the inputs, truncated appropriately.
    """
    pp = pprint.PrettyPrinter(width=80)
    inputs = {}
    for k, v in kwargs.items():
        item_repr = repr(v)
        if len(item_repr) > 50:
            item_repr = item_repr[:50] + "..."
        else:
            item_repr = v
        inputs[k] = item_repr
    input_string = pp.pformat(inputs)
    if len(input_string) > 1000:
        input_string = input_string[:1000] + "..."
    return input_string


def _format_BASE_ERROR_MESSAGE(action: Action, input_state: State, inputs: dict) -> str:
    """Formats the error string, given that we're inside an action"""
    message = BASE_ERROR_MESSAGE
    message += f"> Action: `{action.name}` encountered an error!"
    padding = " " * (80 - len(message) - 1)
    message += padding + "<"
    message += "\n> State (at time of action):\n" + _create_dict_string(input_state.get_all())
    message += "\n> Inputs (at time of action):\n" + _create_dict_string(inputs)
    border = "*" * 80
    return "\n" + border + "\n" + message + "\n" + border


def _run_single_step_action(
    action: SingleStepAction, state: State, inputs: Optional[Dict[str, Any]]
) -> Tuple[Dict[str, Any], State]:
    """Runs a single step action. This API is internal-facing and a bit in flux, but
    it corresponds to the SingleStepAction class.

    :param action: Action to run
    :param state: State to run with
    :param inputs: Inputs to pass directly to the action
    :return: The result of running the action, and the new state
    """
    # TODO -- guard all reads/writes with a subset of the state
    action.validate_inputs(inputs)
    result, new_state = _adjust_single_step_output(
        action.run_and_update(state, **inputs), action.name, action.schema
    )
    _validate_result(result, action.name, action.schema)
    out = result, _state_update(state, new_state)
    _validate_reducer_writes(action, new_state, action.name)
    return out


def _run_single_step_streaming_action(
    action: SingleStepStreamingAction,
    state: State,
    inputs: Optional[Dict[str, Any]],
    sequence_id: int,
    app_id: str,
    partition_key: Optional[str],
    lifecycle_adapters: LifecycleAdapterSet = LifecycleAdapterSet(),
) -> Generator[Tuple[dict, Optional[State]], None, None]:
    """Runs a single step streaming action. This API is internal-facing.
    This normalizes + validates the output."""
    action.validate_inputs(inputs)
    stream_initialize_time = system.now()
    first_stream_start_time = None
    generator = action.stream_run_and_update(state, **inputs)
    result = None
    state_update = None
    count = 0
    for item in generator:
        if not isinstance(item, tuple):
            # TODO -- consider adding support for just returning a result.
            raise ValueError(
                f"Action {action.name} must yield a tuple of (result, state_update). "
                f"For all non-final results (intermediate),"
                f"the state update must be None"
            )
        result, state_update = item
        count += 1
        if state_update is None:
            if first_stream_start_time is None:
                first_stream_start_time = system.now()
            lifecycle_adapters.call_all_lifecycle_hooks_sync(
                "post_stream_item",
                item=result,
                item_index=count,
                stream_initialize_time=stream_initialize_time,
                first_stream_item_start_time=first_stream_start_time,
                action=action.name,
                app_id=app_id,
                partition_key=partition_key,
                sequence_id=sequence_id,
            )
            yield result, None

    if state_update is None:
        raise ValueError(
            f"Action {action.name} did not return a state update. For streaming actions, the last yield "
            f"statement must be a tuple of (result, state_update). For example, yield dict(foo='bar'), state.update(foo='bar')"
        )
    _validate_result(result, action.name, action.schema)
    _validate_reducer_writes(action, state_update, action.name)
    yield result, state_update


async def _arun_single_step_streaming_action(
    action: SingleStepStreamingAction,
    state: State,
    inputs: Optional[Dict[str, Any]],
    sequence_id: int,
    app_id: str,
    partition_key: Optional[str],
    lifecycle_adapters: LifecycleAdapterSet = LifecycleAdapterSet(),
) -> AsyncGenerator[Tuple[dict, Optional[State]], None]:
    """Runs a single step streaming action in async. See the synchronous version for more details."""
    action.validate_inputs(inputs)
    stream_initialize_time = system.now()
    first_stream_start_time = None
    generator = action.stream_run_and_update(state, **inputs)
    result = None
    state_update = None
    count = 0
    async for item in generator:
        if not isinstance(item, tuple):
            # TODO -- consider adding support for just returning a result.
            raise ValueError(
                f"Action {action.name} must yield a tuple of (result, state_update). "
                f"For all non-final results (intermediate),"
                f"the state update must be None"
            )
        result, state_update = item
        if state_update is None:
            if first_stream_start_time is None:
                first_stream_start_time = system.now()
            await lifecycle_adapters.call_all_lifecycle_hooks_sync_and_async(
                "post_stream_item",
                item=result,
                item_index=count,
                stream_initialize_time=stream_initialize_time,
                first_stream_item_start_time=first_stream_start_time,
                action=action.name,
                app_id=app_id,
                partition_key=partition_key,
                sequence_id=sequence_id,
            )
            count += 1
            yield result, None
    if state_update is None:
        raise ValueError(
            f"Action {action.name} did not return a state update. For async actions, the last yield "
            f"statement must be a tuple of (result, state_update). For example, yield dict(foo='bar'), state.update(foo='bar')"
        )
    # TODO -- add back in validation when we have a schema
    _validate_result(result, action.name, action.schema)
    _validate_reducer_writes(action, state_update, action.name)
    # TODO -- add guard against zero-length stream
    yield result, state_update


def _run_multi_step_streaming_action(
    action: StreamingAction,
    state: State,
    inputs: Optional[Dict[str, Any]],
    sequence_id: int,
    app_id: str,
    partition_key: Optional[str],
    lifecycle_adapters: LifecycleAdapterSet = LifecycleAdapterSet(),
) -> Generator[Tuple[dict, Optional[State]], None, None]:
    """Runs a multi-step streaming action. E.G. one with a run/reduce step.
    This API is internal-facing. Note that this converts the shape of a
    multi-step streaming action to yielding the results of the run step
    as well as the state update, which is None for all the finaly ones.

    This peeks ahead by one so we know when this is done (and when to validate).
    """
    action.validate_inputs(inputs)
    stream_initialize_time = system.now()
    generator = action.stream_run(state, **inputs)
    result = None
    first_stream_start_time = None
    count = 0
    for item in generator:
        # We want to peek ahead so we can return the last one
        # This is slightly eager, but only in the case in which we
        # are using a multi-step streaming action
        next_result = result
        result = item
        if next_result is not None:
            if first_stream_start_time is None:
                first_stream_start_time = system.now()
            lifecycle_adapters.call_all_lifecycle_hooks_sync(
                "post_stream_item",
                item=next_result,
                item_index=count,
                stream_initialize_time=stream_initialize_time,
                first_stream_item_start_time=first_stream_start_time,
                action=action.name,
                app_id=app_id,
                partition_key=partition_key,
                sequence_id=sequence_id,
            )
            count += 1
            yield next_result, None
    state_update = _run_reducer(action, state, result, action.name)
    _validate_result(result, action.name, action.schema)
    _validate_reducer_writes(action, state_update, action.name)
    yield result, state_update


async def _arun_multi_step_streaming_action(
    action: AsyncStreamingAction,
    state: State,
    inputs: Optional[Dict[str, Any]],
    sequence_id: int,
    app_id: str,
    partition_key: Optional[str],
    lifecycle_adapters: LifecycleAdapterSet = LifecycleAdapterSet(),
) -> AsyncGenerator[Tuple[dict, Optional[State]], None]:
    """Runs a multi-step streaming action in async. See the synchronous version for more details."""
    action.validate_inputs(inputs)
    stream_initialize_time = system.now()
    generator = action.stream_run(state, **inputs)
    result = None
    first_stream_start_time = None
    count = 0
    async for item in generator:
        # We want to peek ahead so we can return the last one
        # This is slightly eager, but only in the case in which we
        # are using a multi-step streaming action
        next_result = result
        result = item
        if next_result is not None:
            if first_stream_start_time is None:
                first_stream_start_time = system.now()
            await lifecycle_adapters.call_all_lifecycle_hooks_sync_and_async(
                "post_stream_item",
                item=next_result,
                stream_initialize_time=stream_initialize_time,
                item_index=count,
                first_stream_item_start_time=first_stream_start_time,
                action=action.name,
                app_id=app_id,
                partition_key=partition_key,
                sequence_id=sequence_id,
            )
            count += 1
            yield next_result, None
    state_update = _run_reducer(action, state, result, action.name)
    _validate_result(result, action.name, action.schema)
    _validate_reducer_writes(action, state_update, action.name)
    yield result, state_update


async def _arun_single_step_action(
    action: SingleStepAction, state: State, inputs: Optional[Dict[str, Any]]
) -> Tuple[dict, State]:
    """Runs a single step action in async. See the synchronous version for more details."""
    state_to_use = state
    action.validate_inputs(inputs)
    result, new_state = _adjust_single_step_output(
        await action.run_and_update(state_to_use, **inputs), action.name, action.schema
    )
    _validate_result(result, action.name, action.schema)
    _validate_reducer_writes(action, new_state, action.name)
    return result, _state_update(state, new_state)


@dataclasses.dataclass
class ApplicationGraph(Graph):
    """User-facing representation of the state machine. This has

    #. All the action objects
    #. All the transition objects
    #. The entrypoint action
    """

    entrypoint: Action


@dataclasses.dataclass
class ApplicationIdentifiers:
    app_id: str
    partition_key: Optional[str]
    sequence_id: Optional[int]


@dataclasses.dataclass
class ApplicationContext(AbstractContextManager, ApplicationIdentifiers):
    """Application context. This is anything your node might need to know about the application.
    Often used for recursive tracking.

    Note this is also a context manager (allowing you to pass context to sub-applications).

    To access this object in a running application, you can use the `__context` variable in the
    action signature:

    .. code-block:: python

        from burr.core import action, State, ApplicationContext

        @action(reads=[...], writes=[...])
        def my_action(state: State, __context: ApplicationContext) -> State:
            app_id = __context.app_id
            partition_key = __context.partition_key
            current_action_name = __context.action_name  # Access the current action name
            ...

    """

    tracker: Optional["TrackingClient"]
    parallel_executor_factory: Callable[[], Executor]
    state_initializer: Optional[BaseStateLoader]
    state_persister: Optional[BaseStateSaver]
    action_name: Optional[str]  # Store just the action name

    @staticmethod
    def get() -> Optional["ApplicationContext"]:
        """Provides the context-local application context.
        You can use this instead of declaring `__context` in an application.
        You really should only be using this if you're wiring through multiple layers of abstraction
        and want to connect two applications.

        :return: The ApplicationContext you'll want to use
        """
        return _application_context.get()

    def __enter__(self) -> "ApplicationContext":
        _application_context.set(self)
        return self

    def __exit__(self, __exc_type, __exc_value, __traceback):
        _application_context.set(None)


_application_context = contextvars.ContextVar[Optional[ApplicationContext]](
    "application_context", default=None
)
# The purpose of this is to ensure that we don't run the run call hooks multiple times
# We do this as some functions call out to others (but they do not call out to themselves, recursively)
# Thus we keep track of a sentinel, unique per execute method, which tells us if we have already logged
# If we want to call out recursively inside applications, we'll need to think this through
# That said, this is all context/thread safe -- multiple threads can call at the same time
# Furthermore, if one app delegates to another, their app ID will be different, so it will trace as well.
_run_call_var = contextvars.ContextVar[Optional[Dict[str, ExecuteMethod]]](
    "run_call_var", default=None
)

CallableT = TypeVar("CallableT")


class _call_execute_method_pre_post:
    """Wrapper class to ensure that the lifecycle hooks are called before/after application calls.
    This is not called for streaming as there is special logic involved "post-return" that we need to handle.
    We will likely add additional hooks for streaming as well, but for now these just decorate the function.
    This is not a publicly exposed API -- it is used internally to ensure that the lifecycle hooks are called.
    """

    def __init__(self, method: ExecuteMethod):
        self.method = method

    def call_pre(self, app) -> bool:
        if should_run_hooks := (app.uid not in _run_call_var.get({})):
            _run_call_var.set({**_run_call_var.get({}), **{app.uid: self.method}})
            app._adapter_set.call_all_lifecycle_hooks_sync(
                "pre_run_execute_call",
                app_id=app._uid,
                partition_key=app._partition_key,
                state=app.state,
                method=self.method,
            )
        return should_run_hooks

    def call_post(self, app, exc) -> bool:
        if should_run_hooks := (
            app.uid in _run_call_var.get(dict) and _run_call_var.get()[app.uid] == self.method
        ):
            _run_call_var.set({k: v for k, v in _run_call_var.get().items() if k != app.uid})
            app._adapter_set.call_all_lifecycle_hooks_sync(
                "post_run_execute_call",
                app_id=app.uid,
                partition_key=app._partition_key,
                state=app.state,
                method=self.method,
                exception=exc,
            )
        return should_run_hooks

    async def acall_pre(self, app) -> bool:
        if should_run_hooks := (app.uid not in _run_call_var.get({})):
            _run_call_var.set({**_run_call_var.get({}), **{app.uid: self.method}})
            await app._adapter_set.call_all_lifecycle_hooks_sync_and_async(
                "pre_run_execute_call",
                app_id=app._uid,
                partition_key=app._partition_key,
                state=app.state,
                method=self.method,
            )
        return should_run_hooks

    async def acall_post(self, app, exc) -> bool:
        if should_run_hooks := (
            app.uid in _run_call_var.get(dict) and _run_call_var.get()[app.uid] == self.method
        ):
            _run_call_var.set({k: v for k, v in _run_call_var.get().items() if k != app.uid})
            await app._adapter_set.call_all_lifecycle_hooks_sync_and_async(
                "post_run_execute_call",
                app_id=app._uid,
                partition_key=app._partition_key,
                state=app.state,
                method=self.method,
                exception=exc,
            )
        return should_run_hooks

    def __call__(self, fn: CallableT) -> CallableT:
        @functools.wraps(fn)
        async def wrapper_async(app_self, *args, **kwargs):
            # We only run at the top level, so we decorate it if we're there
            await self.acall_pre(app_self)
            exc = None
            try:
                return await fn(app_self, *args, **kwargs)
            finally:
                await self.acall_post(app_self, exc)

        @functools.wraps(fn)
        def wrapper_sync(app_self, *args, **kwargs):
            self.call_pre(app_self)
            exc = None
            try:
                return fn(app_self, *args, **kwargs)
            finally:
                self.call_post(app_self, exc)

        return wrapper_async if inspect.iscoroutinefunction(fn) else wrapper_sync


class TracerFactoryContextHook(PreRunStepHook, PostRunStepHook):
    """Passes the tracer factory into context. This allows us to do out-of-band visibility processing"""

    def __init__(self, adapter_set: LifecycleAdapterSet):
        self._adapter_set = adapter_set
        self.token_pointer_map = {}

    def pre_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        inputs: Dict[str, Any],
        **future_kwargs: Any,
    ):
        if "__tracer" in inputs:
            tracer_factory = inputs["__tracer"]
        else:
            tracer_factory = tracing.TracerFactory(
                action=action.name,
                app_id=app_id,
                lifecycle_adapters=self._adapter_set,
                partition_key=partition_key,
                sequence_id=sequence_id,
            )
        assert tracer_factory is not None
        # TODO -- store token/context manager by app_id, sequence_id
        self.token_pointer_map[(app_id, sequence_id)] = tracer_factory_context_var.set(
            tracer_factory
        )

    def post_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        result: Optional[Dict[str, Any]],
        exception: Exception,
        **future_kwargs: Any,
    ):
        try:
            tracer_factory_context_var.reset(self.token_pointer_map[(app_id, sequence_id)])
        except ValueError:  # Token var = ContextVar created in a different context
            # This occurs when we're at the finally block of an async streaming action
            logger.debug(
                "Token var = ContextVar created in a different context -- this happens with streaming APIs. "
                "See: https://github.com/open-telemetry/opentelemetry-python/issues/2606 for a similar issue"
            )
            tracer_factory_context_var.set(None)

        del self.token_pointer_map[(app_id, sequence_id)]


ApplicationStateType = TypeVar("ApplicationStateType")
StreamResultType = TypeVar("StreamResultType", bound=Union[dict, Any])


def _create_default_executor() -> Executor:
    return ThreadPoolExecutor()


class Application(Generic[ApplicationStateType]):
    def __init__(
        self,
        graph: Graph,
        state: State[ApplicationStateType],
        partition_key: Optional[str],
        uid: str,
        entrypoint: str,
        sequence_id: Optional[int] = None,
        adapter_set: Optional[LifecycleAdapterSet] = None,
        builder: Optional["ApplicationBuilder"] = None,
        fork_parent_pointer: Optional[burr_types.ParentPointer] = None,
        spawning_parent_pointer: Optional[burr_types.ParentPointer] = None,
        tracker: Optional["TrackingClient"] = None,
        parallel_executor_factory: Optional[Executor] = None,
        state_persister: Union[BaseStateSaver, LifecycleAdapter, None] = None,
        state_initializer: Union[BaseStateLoader, LifecycleAdapter, None] = None,
    ):
        """Instantiates an Application. This is an internal API -- use the builder!

        :param actions: Actions to run
        :param transitions: Transitions between actions
        :param state: State to run with
        :param initial_step: Step name to start at
        :param partition_key: Partition key for the application (optional)
        :param uid: Unique identifier for the application
        :param sequence_id: Sequence ID for the application. Note this will be incremented every run.
            So if this starts at 0, the first one you will see will be 1.
        :param adapter_set: Set of lifecycle adapters
        :param builder: Builder that created this application
        """
        self._partition_key = partition_key
        self._uid = uid
        self.entrypoint = entrypoint

        self._graph = graph
        self._public_facing_graph = ApplicationGraph(
            actions=graph.actions,
            transitions=graph.transitions,
            entrypoint=graph.get_action(entrypoint),
        )
        self._state = state
        adapter_set = adapter_set if adapter_set is not None else LifecycleAdapterSet()
        self._adapter_set = adapter_set.with_new_adapters(TracerFactoryContextHook(adapter_set))
        # TODO -- consider adding global inputs + global input factories to the builder
        self._tracker = tracker

        if sequence_id is not None:
            self._set_sequence_id(sequence_id)
        self._builder = builder
        self._parent_pointer = fork_parent_pointer
        self._parallel_executor_factory = (
            parallel_executor_factory
            if parallel_executor_factory is not None
            else _create_default_executor
        )
        self._dependency_factory = {
            "__tracer": functools.partial(
                visibility.tracing.TracerFactory,
                lifecycle_adapters=self._adapter_set,
                app_id=self._uid,
                partition_key=self._partition_key,
            ),
            "__context": self._context_factory,
        }
        self._spawning_parent_pointer = spawning_parent_pointer
        self._state_initializer = state_initializer
        self._state_persister = state_persister
        self._adapter_set.call_all_lifecycle_hooks_sync(
            "post_application_create",
            state=self._state,
            application_graph=self._public_facing_graph,
            app_id=self._uid,
            partition_key=self._partition_key,
            parent_pointer=fork_parent_pointer,
            spawning_parent_pointer=spawning_parent_pointer,
        )

    # @telemetry.capture_function_usage # todo -- capture usage when we break this up into one that isn't called internally
    # This will be doable when we move sequence ID to the beginning of the function https://github.com/DAGWorks-Inc/burr/pull/73
    @_call_execute_method_pre_post(ExecuteMethod.step)
    def step(self, inputs: Optional[Dict[str, Any]] = None) -> Optional[Tuple[Action, dict, State]]:
        """Performs a single step, advancing the state machine along.
        This returns a tuple of the action that was run, the result of running
        the action, and the new state.

        Use this if you just want to do something with the state and not rely on generators.
        E.G. press forward/backwards, human in the loop, etc... Odds are this is not
        the method you want -- you'll want iterate() (if you want to see the state/
        results along the way), or run() (if you just want the final state/results).

        :param inputs: Inputs to the action -- this is if this action requires an input that is passed in from the outside world
        :return: Tuple[Function, dict, State] -- the function that was just ran, the result of running it, and the new state
        """
        # we need to increment the sequence before we start computing
        # that way if we're replaying from state, we don't get stuck
        self.validate_correct_async_use()
        self._increment_sequence_id()
        out = self._step(inputs=inputs, _run_hooks=True)
        return out

    def _context_factory(self, action: Action, sequence_id: int) -> ApplicationContext:
        """Helper function to create an application context, in the form of the dependency factories we inject to nodes."""
        return ApplicationContext(
            app_id=self._uid,
            tracker=self._tracker,
            partition_key=self._partition_key,
            sequence_id=sequence_id,
            parallel_executor_factory=self._parallel_executor_factory,
            state_initializer=self._state_initializer,
            state_persister=self._state_persister,
            action_name=action.name if action else None,  # Pass just the action name
        )

    def _step(
        self, inputs: Optional[Dict[str, Any]], _run_hooks: bool = True
    ) -> Optional[Tuple[Action, dict, State]]:
        """Internal-facing version of step. This is the same as step, but with an additional
        parameter to hide hook execution so async can leverage it."""
        with self.context:
            next_action = self.get_next_action()
            if next_action is None:
                return None
            if inputs is None:
                inputs = {}
            action_inputs = self._process_inputs(inputs, next_action)
            if _run_hooks:
                self._adapter_set.call_all_lifecycle_hooks_sync(
                    "pre_run_step",
                    action=next_action,
                    state=self._state,
                    inputs=action_inputs,
                    sequence_id=self.sequence_id,
                    app_id=self._uid,
                    partition_key=self._partition_key,
                )
            exc = None
            result = None
            new_state = self._state
            try:
                if next_action.single_step:
                    result, new_state = _run_single_step_action(
                        next_action, self._state, action_inputs
                    )
                else:
                    result = _run_function(
                        next_action, self._state, action_inputs, name=next_action.name
                    )
                    new_state = _run_reducer(next_action, self._state, result, next_action.name)

                new_state = self._update_internal_state_value(new_state, next_action)
                self._set_state(new_state)
            except Exception as e:
                exc = e
                logger.exception(_format_BASE_ERROR_MESSAGE(next_action, self._state, inputs))
                raise e
            finally:
                if _run_hooks:
                    self._adapter_set.call_all_lifecycle_hooks_sync(
                        "post_run_step",
                        app_id=self._uid,
                        partition_key=self._partition_key,
                        action=next_action,
                        state=new_state,
                        result=result,
                        sequence_id=self.sequence_id,
                        exception=exc,
                    )
            return next_action, result, new_state

    def reset_to_entrypoint(self) -> None:
        """Resets the state machine to the entrypoint action -- you probably want to consider having a loop
        in your graph, but this will do the trick if you need it!"""
        self._set_state(self._state.wipe(delete=[PRIOR_STEP]))

    def _update_internal_state_value(
        self, new_state: State[ApplicationStateType], next_action: Action
    ) -> State[ApplicationStateType]:
        """Updates the internal state values of the new state."""
        new_state = new_state.update(
            **{
                PRIOR_STEP: next_action.name,
            }
        )
        return new_state

    def _process_inputs(self, inputs: Dict[str, Any], action: Action) -> Dict[str, Any]:
        """Processes inputs, injecting the common inputs and ensuring that all required inputs are present."""
        starting_with_double_underscore = {key for key in inputs.keys() if key.startswith("__")}
        if len(starting_with_double_underscore) > 0:
            raise ValueError(
                BASE_ERROR_MESSAGE
                + f"Inputs starting with a double underscore ({starting_with_double_underscore}) "
                f"are reserved for internal use/injected inputs. "
                "Please do not directly pass keys starting with a double underscore."
            )
        inputs = inputs.copy()
        processed_inputs = {}
        required_inputs, optional_inputs = action.optional_and_required_inputs
        for key in list(inputs.keys()):
            if key in required_inputs or key in optional_inputs:
                processed_inputs[key] = inputs.pop(key)
        if len(inputs) > 0 and logger.isEnabledFor(logging.DEBUG):
            logger.debug(
                f"Keys {inputs.keys()} were passed in as inputs to action "
                f"{action.name}, but not declared by the action as an input. "
                f"Action only needs: {required_inputs} (optionally: {optional_inputs}) "
                f"so we're just letting you know some inputs are being skipped."
            )
        missing_inputs = required_inputs - set(processed_inputs.keys())
        additional_inputs = optional_inputs - set(processed_inputs.keys())
        for input_ in missing_inputs | additional_inputs:
            # if we can find it in the dependency factory, we'll use that
            # TODO -- figure out what happens if people attempt to override default factory
            # inputs
            if input_ in self._dependency_factory:
                processed_inputs[input_] = self._dependency_factory[input_](
                    action, self.sequence_id
                )
                if input_ in missing_inputs:
                    missing_inputs.remove(input_)
        if len(missing_inputs) > 0:
            missing_inputs_dict = {key: "FILL ME IN" for key in missing_inputs}
            missing_inputs_dict.update({key: "..." for key in inputs.keys()})
            addendum = (
                "\nPlease double check the values passed to the keyword argument `inputs` to however you're running "
                "the burr application.\n"
                "e.g.\n"
                f"   app.run( # or app.step, app.iterate, app.astep, etc.\n"
                f"       halt_..., # your halt logic\n"
                f"       inputs={missing_inputs_dict}  # <-- this is what you need to adjust\n"
                f"   )"
            )
            raise ValueError(
                f"Action {action.name} is missing required inputs: {missing_inputs}. "
                f"Has inputs: {processed_inputs}. " + addendum
            )
        return processed_inputs

    # @telemetry.capture_function_usage
    # ditto with step()
    @_call_execute_method_pre_post(ExecuteMethod.astep)
    async def astep(
        self, inputs: Optional[Dict[str, Any]] = None
    ) -> Optional[Tuple[Action, dict, State[ApplicationStateType]]]:
        """Asynchronous version of step.

        :param inputs: Inputs to the action -- this is if this action
            requires an input that is passed in from the outside world

        :return: Tuple[Function, dict, State] -- the action that was just ran, the result of running it, and the new state
        """
        self._increment_sequence_id()
        out = await self._astep(inputs=inputs, _run_hooks=True)
        return out

    async def _astep(self, inputs: Optional[Dict[str, Any]], _run_hooks: bool = True):
        # we want to increment regardless of failure
        with self.context:
            next_action = self.get_next_action()
            if next_action is None:
                return None
            if inputs is None:
                inputs = {}
            if _run_hooks:
                await self._adapter_set.call_all_lifecycle_hooks_sync_and_async(
                    "pre_run_step",
                    action=next_action,
                    state=self._state,
                    inputs=inputs,
                    sequence_id=self.sequence_id,
                    app_id=self._uid,
                    partition_key=self._partition_key,
                )
            exc = None
            result = None
            new_state = self._state
            try:
                if not next_action.is_async():
                    # we can just delegate to the synchronous version, it will block the event loop,
                    # but that's safer than assuming its OK to launch a thread
                    # TODO -- add an option/configuration to launch a thread (yikes, not super safe, but for a pure function
                    # which this is supposed to be its OK).
                    # this delegates hooks to the synchronous version, so we'll call all of them as well
                    # In this case we allow the self._step to do input processing
                    return self._step(
                        inputs=inputs, _run_hooks=False
                    )  # Skip hooks as we already ran all of them/will run all of them in this function's finally
                # In this case we want to process inputs because we run the function directly
                action_inputs = self._process_inputs(inputs, next_action)
                if next_action.single_step:
                    result, new_state = await _arun_single_step_action(
                        next_action, self._state, inputs=action_inputs
                    )
                else:
                    result = await _arun_function(
                        next_action,
                        self._state,
                        inputs=action_inputs,
                        name=next_action.name,
                    )
                    new_state = _run_reducer(next_action, self._state, result, next_action.name)
                new_state = self._update_internal_state_value(new_state, next_action)
                self._set_state(new_state)
            except Exception as e:
                exc = e
                logger.exception(_format_BASE_ERROR_MESSAGE(next_action, self._state, inputs))
                raise e
            finally:
                if _run_hooks:
                    await self._adapter_set.call_all_lifecycle_hooks_sync_and_async(
                        "post_run_step",
                        action=next_action,
                        state=new_state,
                        result=result,
                        sequence_id=self.sequence_id,
                        exception=exc,
                        app_id=self._uid,
                        partition_key=self._partition_key,
                    )

            return next_action, result, new_state

    def _parse_action_list(self, action_list: list[str]) -> Tuple[List[str], List[str]]:
        """Utility function to parse a list of actions/tags into a list of actions and a list of tags."""
        actions = []
        tags = []
        for action in action_list:
            if action.startswith("@tag"):
                tags.append(action[len("@tag:") :])
            else:
                actions.append(action)
        return actions, tags

    def _process_control_flow_params(
        self,
        halt_before: list[str] = None,
        halt_after: list[str] = None,
        inputs: Optional[Dict[str, Any]] = None,
    ) -> Tuple[list[str], list[str], Dict[str, Any]]:
        """Utility function to clean out iterate params so we have less duplication between iterate/aiterate
        and the logic is cleaner later.
        """
        if halt_before is None and halt_after is None:
            logger.warning(
                "No halt termination specified -- this has the possibility of running forever!"
            )
        halt_before = halt_before or []
        halt_after = halt_after or []
        inputs = inputs or {}
        halt_before_actions, halt_before_tags = self._parse_action_list(halt_before)
        halt_after_actions, halt_after_tags = self._parse_action_list(halt_after)
        halt_before_expanded = set(halt_before_actions)
        for tag in halt_before_tags:
            halt_before_expanded.update([item.name for item in self.graph.get_actions_by_tag(tag)])
        halt_after_expanded = set(halt_after_actions)
        for tag in halt_after_tags:
            halt_after_expanded.update([item.name for item in self.graph.get_actions_by_tag(tag)])
        return list(halt_before_expanded), list(halt_after_expanded), inputs

    def _validate_halt_conditions(self, halt_before: list[str], halt_after: list[str]) -> None:
        """Utility function to validate halt conditions"""
        missing_actions = set(halt_before + halt_after) - set(
            action.name for action in self.graph.actions
        )
        if len(missing_actions) > 0:
            raise ValueError(
                BASE_ERROR_MESSAGE
                + f"Halt conditions {missing_actions} are not registered actions. Please ensure that they have been "
                f"registered as actions in the application and that you've spelled them correctly!"
                f"Valid actions are: {[action.name for action in self.graph.actions]}"
            )

    def has_next_action(self) -> bool:
        """Returns whether or not there is a next action to run.

        :return: True if there is a next action, False otherwise
        """
        return self.get_next_action() is not None

    def _should_halt_iterate(
        self, halt_before: list[str], halt_after: list[str], prior_action: Action
    ) -> bool:
        """Internal utility function to determine whether or not to halt during iteration"""
        if self.has_next_action() and self.get_next_action().name in halt_before:
            logger.debug(f"Halting before executing {self.get_next_action().name}")
            return True
        elif prior_action.name in halt_after:
            logger.debug(f"Halting after executing {prior_action.name}")
            return True
        return False

    def _return_value_iterate(
        self,
        halt_before: list[str],
        halt_after: list[str],
        prior_action: Optional[Action],
        result: Optional[dict],
    ) -> Tuple[Optional[Action], Optional[dict], State[ApplicationStateType]]:
        """Utility function to decide what to return for iterate/arun. Note that run() will delegate to the return value of
        iterate, whereas arun cannot delegate to the return value of aiterate (as async generators cannot return a value).
        We put the code centrally to clean up the logic.
        """
        if self.has_next_action() and self.get_next_action().name in halt_before:
            logger.debug(
                f"We have hit halt_before condition with next action: {self.get_next_action().name}. "
                f"Returning: next_action={self.get_next_action()}, None, and state"
            )
            return self.get_next_action(), None, self._state
        if prior_action is not None and prior_action.name in halt_after:
            prior_action_name = prior_action.name if prior_action is not None else None
            logger.debug(
                f"We have hit halt_after condition with prior action: {prior_action_name}. "
                f"Returning: prior_action={prior_action}, result, and state"
            )
            return prior_action, result, self._state
        logger.warning(
            "This is trying to return without having computed a single action -- "
            "we'll end up just returning some Nones. This means that nothing was executed "
            "(E.G. that the state machine had nowhere to go). Either fix the state machine or"
            f"the halt conditions, or both... Halt conditions are: halt_before={halt_before}, halt_after={halt_after}."
            f"Note that this is considered undefined behavior -- if you get here, you should fix!"
        )
        return prior_action, result, self._state

    @telemetry.capture_function_usage
    @_call_execute_method_pre_post(ExecuteMethod.iterate)
    def iterate(
        self,
        *,
        halt_before: list[str] = None,
        halt_after: list[str] = None,
        inputs: Optional[Dict[str, Any]] = None,
    ) -> Generator[
        Tuple[Action, dict, State[ApplicationStateType]],
        None,
        Tuple[Action, Optional[dict], State[ApplicationStateType]],
    ]:
        """Returns a generator that calls step() in a row, enabling you to see the state
        of the system as it updates. Note this returns a generator, and also the final result
        (for convenience).

        Note that tags must be specified by the form "@tag:<tag_name>" to differentiate them from actions.

        Note the nuance with halt_before and halt_after. halt_before conditions will take precedence to halt_after. Furthermore,
        a single iteration will always be executed prior to testing for any halting conditions.

        :param halt_before: The list of actions/tags to halt before execution of. It will halt prior to the execution of the first one it sees.
        :param halt_after: The list of actions/tags to halt after execution of. It will halt after the execution of the first one it sees.
        :param inputs: Inputs to the action -- this is if this action requires an input that is passed in from the outside world.
            Note that this is only used for the first iteration -- subsequent iterations will not use this.
        :return: Each iteration returns the result of running `step`. This generator also returns a tuple of
            [action, result, current state]
        """
        self.validate_correct_async_use()
        halt_before, halt_after, inputs = self._process_control_flow_params(
            halt_before,
            halt_after,
            inputs,
        )
        self._validate_halt_conditions(halt_before, halt_after)

        result = None
        prior_action: Optional[Action] = None
        while self.has_next_action():
            # self.step will only return None if there is no next action, so we can rely on tuple unpacking
            prior_action, result, state = self.step(inputs=inputs)
            yield prior_action, result, state
            if self._should_halt_iterate(halt_before, halt_after, prior_action):
                break
        return self._return_value_iterate(halt_before, halt_after, prior_action, result)

    @telemetry.capture_function_usage
    @_call_execute_method_pre_post(ExecuteMethod.aiterate)
    async def aiterate(
        self,
        *,
        halt_before: list[str] = None,
        halt_after: list[str] = None,
        inputs: Optional[Dict[str, Any]] = None,
    ) -> AsyncGenerator[Tuple[Action, dict, State[ApplicationStateType]], None]:
        """Returns a generator that calls step() in a row, enabling you to see the state
        of the system as it updates. This is the asynchronous version so it has no capability of t

        Note that tags must be specified by the form "@tag:<tag_name>" to differentiate them from actions.

        :param halt_before: The list of actions/tags to halt before execution of. It will halt on the first one.
        :param halt_after: The list of actions/tags to halt after execution of. It will halt on the first one.
        :param inputs: Inputs to the action -- this is if this action requires an input that is passed in from the outside world.
            Note that this is only used for the first iteration -- subsequent iterations will not use this.
        :return: Each iteration returns the result of running `step`. This returns nothing -- it's an async generator which is not
            allowed to have a return value.
        """
        halt_before, halt_after, inputs = self._process_control_flow_params(
            halt_before, halt_after, inputs
        )
        self._validate_halt_conditions(halt_before, halt_after)
        while self.has_next_action():
            # self.step will only return None if there is no next action, so we can rely on tuple unpacking
            prior_action, result, state = await self.astep(inputs=inputs)
            yield prior_action, result, state
            if self._should_halt_iterate(halt_before, halt_after, prior_action):
                break

    @telemetry.capture_function_usage
    @_call_execute_method_pre_post(ExecuteMethod.run)
    def run(
        self,
        *,
        halt_before: list[str] = None,
        halt_after: list[str] = None,
        inputs: Optional[Dict[str, Any]] = None,
    ) -> Tuple[Action, Optional[dict], State[ApplicationStateType]]:
        """Runs your application through until completion. Does
        not give access to the state along the way -- if you want that, use iterate().

        Note that tags must be specified by the form "@tag:<tag_name>" to differentiate them from actions.

        :param halt_before: The list of actions/tags to halt before execution of. It will halt on the first one.
        :param halt_after: The list of actions/tags to halt after execution of. It will halt on the first one.
        :param inputs: Inputs to the action -- this is if this action requires an input that is passed in from the outside world.
            Note that this is only used for the first iteration -- subsequent iterations will not use this.
        :return: The final state, and the results of running the actions in the order that they were specified.
        """
        self.validate_correct_async_use()
        gen = self.iterate(halt_before=halt_before, halt_after=halt_after, inputs=inputs)
        while True:
            try:
                next(gen)
            except StopIteration as e:
                result = e.value
                return result

    @telemetry.capture_function_usage
    @_call_execute_method_pre_post(ExecuteMethod.arun)
    async def arun(
        self,
        *,
        halt_before: list[str] = None,
        halt_after: list[str] = None,
        inputs: Optional[Dict[str, Any]] = None,
    ) -> Tuple[Action, Optional[dict], State]:
        """Runs your application through until completion, using async. Does
        not give access to the state along the way -- if you want that, use iterate().

        Note that tags must be specified by the form "@tag:<tag_name>" to differentiate them from actions.

        :param halt_before: The list of actions/tags to halt before execution of. It will halt on the first one.
        :param halt_after: The list of actions/tags to halt after execution of. It will halt on the first one.
        :param inputs: Inputs to the action -- this is if this action requires an input that is passed in from the outside world
        :return: The final state, and the results of running the actions in the order that they were specified.
        """

        prior_action = None
        result = None
        halt_before, halt_after, inputs = self._process_control_flow_params(
            halt_before, halt_after, inputs
        )
        self._validate_halt_conditions(halt_before, halt_after)
        async for prior_action, result, state in self.aiterate(
            halt_before=halt_before, halt_after=halt_after, inputs=inputs
        ):
            pass
        return self._return_value_iterate(halt_before, halt_after, prior_action, result)

    @telemetry.capture_function_usage
    def stream_result(
        self,
        halt_after: list[str],
        halt_before: Optional[list[str]] = None,
        inputs: Optional[Dict[str, Any]] = None,
    ) -> Tuple[Action, StreamingResultContainer[ApplicationStateType, Union[dict, Any]]]:
        """Streams a result out.

        :param halt_after: The list of actions/tags to halt after execution of. It will halt on the first one.
        :param halt_before: The list of actions/tags to halt before execution of. It will halt on the first one. Note that
            if this is met, the streaming result container will be empty (and return None) for the result, having an empty generator.
        :param inputs: Inputs to the action -- this is if this action requires an input that is passed in from the outside world
        :return: A streaming result container, which is a generator that will yield results as they come in, as well as cache/give you the final result,
            and update state accordingly.

        This is meant to be used with streaming actions -- :py:meth:`streaming_action <burr.core.action.streaming_action>`
        or :py:class:`StreamingAction <burr.core.action.StreamingAction>` It returns a
        :py:class:`StreamingResultContainer <burr.core.action.StreamingResultContainer>`, which has two capabilities:

        1. It is a generator that streams out the intermediate results of the action
        2. It has a ``.get()`` method that returns the final result of the action, and the final state.

        If ``.get()`` is called before the generator is exhausted, it will block until the generator is exhausted.

        While this container is meant to work with streaming actions, it can also be used with non-streaming actions. In this case,
        the generator will be empty, and the ``.get()`` method will return the final result and state.

        The rules for halt_before and halt_after are the same as for :py:meth:`iterate <burr.core.application.Application.iterate>`,
        and :py:meth:`run <burr.core.application.Application.run>`. In this case, `halt_before` will indicate a *non* streaming action,
        which will be empty. Thus ``halt_after`` takes precedence -- if it is met, the streaming result container will contain the result of the
        halt_after condition.

        The :py:class:`StreamingResultContainer <burr.core.action.StreamingResultContainer>` is meant as a convenience -- specifically this allows for
        hooks, callbacks, etc... so you can take the control flow and still have state updated afterwards. Hooks/state update will be called after an exception
        is thrown during streaming, or the stream is completed. Note that it is undefined behavior to attempt to execute another action while a stream is in progress.


        To see how this works, let's take the following action (simplified as a single-node workflow) as an example:

        .. code-block:: python

            @streaming_action(reads=[], writes=['response'])
            def streaming_response(state: State, prompt: str) -> Generator[dict, None, Tuple[dict, State]]:
                response = client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[{
                        'role': 'user',
                        'content': prompt
                        }],
                    temperature=0,
                )
                buffer = []
                for chunk in response:
                    delta = chunk.choices[0].delta.content
                    buffer.append(delta)
                    # yield partial results
                    yield {'response': delta}, None # indicate that we are not done by returning a `None` state!
                full_response = ''.join(buffer)
                # return the final result
                yield {'response': full_response}, state.update(response=full_response)

        To use streaming_result, you pass in names of streaming actions (such as the one above) to the halt_after
        parameter:

        .. code-block:: python

            application = ApplicationBuilder().with_actions(streaming_response=streaming_response)...build()
            prompt = "Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ..."
            action, streaming_result = application.stream_result(halt_after='streaming_response', inputs={"prompt": prompt})
            for result in streaming_result:
                print(result['response']) # one by one

            result, state = streaming_result.get()
            print(result) #  all at once

        Note that if you have multiple halt_after conditions, you can use the ``.action`` attribute to get the action that
        was run.

        .. code-block:: python

            application = ApplicationBuilder().with_actions(
                streaming_response=streaming_response,
                error=error # another function that outputs an error, streaming
            )...build()
            prompt = "Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ..."
            action, streaming_result = application.stream_result(halt_after='streaming_response', inputs={"prompt": prompt})
            color = "red" if action.name == "error" else "green"
            for result in streaming_result:
                print(format(result['response'], color)) # assumes that error and streaming_response both have the same output shape

        .. code-block:: python

            application = ApplicationBuilder().with_actions(
                streaming_response=streaming_response,
                error=non_streaming_error # a non-streaming function that outputs an error
            )...build()
            prompt = "Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ..."
            action, streaming_result = application.stream_result(halt_after='streaming_response', inputs={"prompt": prompt})
            color = "red" if action.name == "error" else "green"
            if action.name == "streaming_response": # can also use the ``.streaming`` attribute of action
                for result in output:
                     print(format(result['response'], color)) # assumes that error and streaming_response both have the same output shape
            else:
                result, state = output.get()
                print(format(result['response'], color))
        """
        self.validate_correct_async_use()
        call_execute_method_wrapper = _call_execute_method_pre_post(ExecuteMethod.stream_result)
        call_execute_method_wrapper.call_pre(self)
        halt_before, halt_after, inputs = self._process_control_flow_params(
            halt_before, halt_after, inputs
        )
        self._validate_halt_conditions(halt_before, halt_after)
        next_action = self.get_next_action()
        if next_action is None:
            raise ValueError(
                f"Cannot stream result -- no next action found! Prior action was: {self._state[PRIOR_STEP]}"
            )
        if next_action.name not in halt_after:
            # fast forward until we get to the action
            # run already handles incrementing sequence IDs, nothing to worry about here
            next_action, results, state = self.run(
                halt_before=halt_after + halt_before, inputs=inputs
            )
            # In this case, we are ready to halt and return an empty generator
            # The results will be None, and the state will be the final state
            # For context, this is specifically for the case in which you want to have
            # multiple terminal points with a unified API, where some are streaming, and some are not.
            if next_action.name in halt_before and next_action.name not in halt_after:
                call_execute_method_wrapper.call_post(self, None)
                return next_action, StreamingResultContainer.pass_through(
                    results=results, final_state=state
                )
        self._increment_sequence_id()
        self._adapter_set.call_all_lifecycle_hooks_sync(
            "pre_run_step",
            action=next_action,
            state=self._state,
            inputs=inputs,
            sequence_id=self.sequence_id,
            app_id=self._uid,
            partition_key=self._partition_key,
        )

        # we need to track if there's any exceptions that occur during this
        try:

            def process_result(result: dict, state: State) -> Tuple[Dict[str, Any], State]:
                new_state = self._update_internal_state_value(state, next_action)
                self._set_state(new_state)
                return result, new_state

            def callback(
                result: Optional[dict],
                state: State,
                exc: Optional[Exception] = None,
            ):
                self._adapter_set.call_all_lifecycle_hooks_sync(
                    "post_run_step",
                    app_id=self._uid,
                    partition_key=self._partition_key,
                    action=next_action,
                    state=state,
                    result=result,
                    sequence_id=self.sequence_id,
                    exception=exc,
                )
                call_execute_method_wrapper.call_post(self, exc)
                self._adapter_set.call_all_lifecycle_hooks_sync(
                    "post_end_stream",
                    action=next_action.name,
                    sequence_id=self.sequence_id,
                    app_id=self._uid,
                    partition_key=self._partition_key,
                )

            action_inputs = self._process_inputs(inputs, next_action)
            if not next_action.streaming:
                # In this case we are halting at a non-streaming condition
                # This is allowed as we want to maintain a more consistent API
                action, result, state = self._step(inputs=inputs, _run_hooks=False)
                self._adapter_set.call_all_lifecycle_hooks_sync(
                    "post_run_step",
                    app_id=self._uid,
                    partition_key=self._partition_key,
                    action=next_action,
                    state=self._state,
                    result=result,
                    sequence_id=self.sequence_id,
                    exception=None,
                )
                out = action, StreamingResultContainer.pass_through(
                    results=result, final_state=state
                )
                call_execute_method_wrapper.call_post(self, None)
                return out
            # Both the next cases start with this
            self._adapter_set.call_all_lifecycle_hooks_sync(
                "pre_start_stream",
                action=next_action.name,
                sequence_id=self.sequence_id,
                app_id=self._uid,
                partition_key=self._partition_key,
            )
            if next_action.single_step:
                next_action = cast(SingleStepStreamingAction, next_action)
                generator = _run_single_step_streaming_action(
                    action=next_action,
                    state=self._state,
                    inputs=action_inputs,
                    sequence_id=self.sequence_id,
                    app_id=self._uid,
                    partition_key=self._partition_key,
                    lifecycle_adapters=self._adapter_set,
                )
                return next_action, StreamingResultContainer(
                    generator, self._state, process_result, callback
                )
            else:
                next_action = cast(StreamingAction, next_action)
                generator = _run_multi_step_streaming_action(
                    action=next_action,
                    state=self._state,
                    inputs=action_inputs,
                    sequence_id=self.sequence_id,
                    app_id=self._uid,
                    partition_key=self._partition_key,
                    lifecycle_adapters=self._adapter_set,
                )
        except Exception as e:
            # We only want to raise this in the case of an exception
            # otherwise, this will get delegated to the finally
            # block of the streaming result container
            self._adapter_set.call_all_lifecycle_hooks_sync(
                "post_run_step",
                app_id=self._uid,
                partition_key=self._partition_key,
                action=next_action,
                state=self._state,
                result=None,
                sequence_id=self.sequence_id,
                exception=e,
            )
            call_execute_method_wrapper.call_post(self, e)
            raise
        return next_action, StreamingResultContainer(
            generator, self._state, process_result, callback
        )

    @telemetry.capture_function_usage
    async def astream_result(
        self,
        halt_after: list[str],
        halt_before: Optional[list[str]] = None,
        inputs: Optional[Dict[str, Any]] = None,
    ) -> Tuple[Action, AsyncStreamingResultContainer[ApplicationStateType, Union[dict, Any]]]:
        """Streams a result out in an asynchronous manner.

        :param halt_after: The list of actions/tags to halt after execution of. It will halt on the first one.
        :param halt_before: The list of actions/tags to halt before execution of. It will halt on the first one. Note that
            if this is met, the streaming result container will be empty (and return None) for the result, having an empty generator.
        :param inputs: Inputs to the action -- this is if this action requires an input that is passed in from the outside world
        :return: An asynchronous :py:class:`AsyncStreamingResultContainer <burr.core.action.AsyncStreamingResultContainer>`, which is a generator that will yield results as they come in, as well as cache/give you the final result,
            and update state accordingly.

        This is meant to be used with streaming actions -- :py:meth:`streaming_action <burr.core.action.streaming_action>`
        or :py:class:`StreamingAction <burr.core.action.StreamingAction>` It returns a
        :py:class:`StreamingResultContainer <burr.core.action.StreamingResultContainer>`, which has two capabilities:

        1. It is a generator that streams out the intermediate results of the action
        2. It has an async ``.get()`` method that returns the final result of the action, and the final state.

        If ``.get()`` is called before the generator is exhausted, it will block until the generator is exhausted.

        While this container is meant to work with streaming actions, it can also be used with non-streaming actions. In this case,
        the generator will be empty, and the ``.get()`` method will return the final result and state.

        The rules for halt_before and halt_after are the same as for :py:meth:`iterate <burr.core.application.Application.iterate>`,
        and :py:meth:`run <burr.core.application.Application.run>`. In this case, `halt_before` will indicate a *non* streaming action,
        which will be empty. Thus ``halt_after`` takes precedence -- if it is met, the streaming result container will contain the result of the
        halt_after condition.

        The :py:class:`AsyncStreamingResultContainer <burr.core.action.AsyncStreamingResultContainer>` is meant as a convenience -- specifically this allows for
        hooks, callbacks, etc... so you can take the control flow and still have state updated afterwards. Hooks/state update will be called after an exception
        is thrown during streaming, or the stream is completed. Note that it is undefined behavior to attempt to execute another action while a stream is in progress.


        To see how this works, let's take the following action (simplified as a single-node workflow) as an example:

        .. code-block:: python

            client = openai.AsyncClient()

            @streaming_action(reads=[], writes=['response'])
            async def streaming_response(state: State, prompt: str) -> Generator[dict, None, Tuple[dict, State]]:
                response = client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[{
                        'role': 'user',
                        'content': prompt
                        }],
                    temperature=0,
                )
                buffer = []
                async for chunk in response: # use an async for loop
                    delta = chunk.choices[0].delta.content
                    buffer.append(delta)
                    # yield partial results
                    yield {'response': delta}, None # indicate that we are not done by returning a `None` state!
                # make sure to join with the buffer!
                full_response = ''.join(buffer)
                # yield the final result at the end + the state update
                yield {'response': full_response}, state.update(response=full_response)

        To use streaming_result, you pass in names of streaming actions (such as the one above) to the halt_after
        parameter:

        .. code-block:: python

            application = ApplicationBuilder().with_actions(streaming_response=streaming_response)...build()
            prompt = "Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ..."
            action, streaming_result = application.astream_result(halt_after='streaming_response', inputs={"prompt": prompt})
            async for result in streaming_result:
                print(result['response']) # one by one

            result, state = await streaming_result.get()
            print(result['response']) #  all at once

        Note that if you have multiple halt_after conditions, you can use the ``.action`` attribute to get the action that
        was run.

        .. code-block:: python

            application = ApplicationBuilder().with_actions(
                streaming_response=streaming_response,
                error=error # another function that outputs an error, streaming
            )...build()
            prompt = "Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ..."
            action, streaming_result = await application.astream_result(halt_after='streaming_response', inputs={"prompt": prompt})
            color = "red" if action.name == "error" else "green"
            for result in streaming_result:
                print(format(result['response'], color)) # assumes that error and streaming_response both have the same output shape

        .. code-block:: python

            application = ApplicationBuilder().with_actions(
                streaming_response=streaming_response,
                error=non_streaming_error # a non-streaming function that outputs an error
            )...build()
            prompt = "Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ..."
            action, streaming_result = await application.astream_result(halt_after='streaming_response', inputs={"prompt": prompt})
            color = "red" if action.name == "error" else "green"
            if action.name == "streaming_response": # can also use the ``.streaming`` attribute of action
                async for result in streaming_result:
                     print(format(result['response'], color)) # assumes that error and streaming_response both have the same output shape
            else:
                result, state = await output.get()
                print(format(result['response'], color))
        """
        call_execute_method_wrapper = _call_execute_method_pre_post(ExecuteMethod.stream_result)
        await call_execute_method_wrapper.acall_pre(self)
        halt_before, halt_after, inputs = self._process_control_flow_params(
            halt_before, halt_after, inputs
        )
        self._validate_halt_conditions(halt_before, halt_after)
        next_action = self.get_next_action()
        if next_action is None:
            raise ValueError(
                f"Cannot stream result -- no next action found! Prior action was: {self._state[PRIOR_STEP]}"
            )
        if next_action.name not in halt_after:
            # fast forward until we get to the action
            # run already handles incrementing sequence IDs, nothing to worry about here
            next_action, results, state = await self.arun(
                halt_before=halt_after + halt_before, inputs=inputs
            )
            # In this case, we are ready to halt and return an empty generator
            # The results will be None, and the state will be the final state
            # For context, this is specifically for the case in which you want to have
            # multiple terminal points with a unified API, where some are streaming, and some are not.
            if next_action.name in halt_before and next_action.name not in halt_after:
                await call_execute_method_wrapper.acall_post(self, None)
                return next_action, AsyncStreamingResultContainer.pass_through(
                    results=results, final_state=state
                )
        self._increment_sequence_id()
        await self._adapter_set.call_all_lifecycle_hooks_sync_and_async(
            "pre_run_step",
            action=next_action,
            state=self._state,
            inputs=inputs,
            sequence_id=self.sequence_id,
            app_id=self._uid,
            partition_key=self._partition_key,
        )
        try:

            def process_result(result: dict, state: State) -> Tuple[Dict[str, Any], State]:
                new_state = self._update_internal_state_value(state, next_action)
                self._set_state(new_state)
                return result, new_state

            async def callback(
                result: Optional[dict],
                state: State,
                exc: Optional[Exception] = None,
            ):
                await self._adapter_set.call_all_lifecycle_hooks_sync_and_async(
                    "post_run_step",
                    app_id=self._uid,
                    partition_key=self._partition_key,
                    action=next_action,
                    state=state,
                    result=result,
                    sequence_id=self.sequence_id,
                    exception=exc,
                )
                await self._adapter_set.call_all_lifecycle_hooks_sync_and_async(
                    "post_end_stream",
                    action=next_action.name,
                    sequence_id=self.sequence_id,
                    app_id=self._uid,
                    partition_key=self._partition_key,
                )
                await call_execute_method_wrapper.acall_post(self, exc)

            action_inputs = self._process_inputs(inputs, next_action)
            if not next_action.streaming:
                # In this case we are halting at a non-streaming condition
                # This is allowed as we want to maintain a more consistent API
                action, result, state = await self._astep(inputs=inputs, _run_hooks=False)
                await self._adapter_set.call_all_lifecycle_hooks_sync_and_async(
                    "post_run_step",
                    app_id=self._uid,
                    partition_key=self._partition_key,
                    action=next_action,
                    state=self._state,
                    result=result,
                    sequence_id=self.sequence_id,
                    exception=None,
                )
                await call_execute_method_wrapper.acall_post(self, None)
                return action, AsyncStreamingResultContainer.pass_through(
                    results=result, final_state=state
                )
            await self._adapter_set.call_all_lifecycle_hooks_sync_and_async(
                "pre_start_stream",
                action=next_action.name,
                sequence_id=self.sequence_id,
                app_id=self._uid,
                partition_key=self._partition_key,
            )
            if next_action.single_step:
                next_action = cast(SingleStepStreamingAction, next_action)
                if not next_action.is_async():
                    raise ValueError(
                        f"Action: {next_action.name} is not"
                        "an async action, but is marked as streaming. "
                        "Currently we do not support running synchronous "
                        "streaming actions with astream_result, although we plan to in the future. "
                        "For now, convert the action to async. Please open up an issue if you hit this. "
                    )
                generator = _arun_single_step_streaming_action(
                    action=next_action,
                    state=self._state,
                    inputs=action_inputs,
                    sequence_id=self.sequence_id,
                    app_id=self._uid,
                    partition_key=self._partition_key,
                    lifecycle_adapters=self._adapter_set,
                )
                return next_action, AsyncStreamingResultContainer(
                    generator, self._state, process_result, callback
                )
            else:
                if not next_action.is_async():
                    raise ValueError(
                        f"Action: {next_action.name} is not"
                        "an async action, but is marked as streaming. "
                        "Currently we do not support running synchronous "
                        "streaming actions with astream_result, although we plan to in the future. "
                        "For now, convert the action to async. Please open up an issue if you hit this. "
                    )
                next_action = cast(AsyncStreamingAction, next_action)
                generator = _arun_multi_step_streaming_action(
                    action=next_action,
                    state=self._state,
                    inputs=action_inputs,
                    sequence_id=self.sequence_id,
                    app_id=self._uid,
                    partition_key=self._partition_key,
                    lifecycle_adapters=self._adapter_set,
                )
        except Exception as e:
            # We only want to raise this in the case of an exception
            # otherwise, this will get delegated to the finally
            # block of the streaming result container
            await self._adapter_set.call_all_lifecycle_hooks_sync_and_async(
                "post_run_step",
                app_id=self._uid,
                partition_key=self._partition_key,
                action=next_action,
                state=self._state,
                result=None,
                sequence_id=self.sequence_id,
                exception=e,
            )
            await call_execute_method_wrapper.acall_post(self, e)
            await self._adapter_set.call_all_lifecycle_hooks_sync_and_async(
                "post_end_stream",
                action=next_action.name,
                sequence_id=self.sequence_id,
                app_id=self._uid,
                partition_key=self._partition_key,
            )
            raise
        return next_action, AsyncStreamingResultContainer(
            generator, self._state, process_result, callback
        )

    @telemetry.capture_function_usage
    @_call_execute_method_pre_post(ExecuteMethod.stream_iterate)
    def stream_iterate(
        self,
        halt_after: Optional[Union[str, List[str]]] = None,
        halt_before: Optional[Union[str, List[str]]] = None,
        inputs: Optional[Dict[str, Any]] = None,
    ) -> Generator[
        Tuple[Action, StreamingResultContainer[ApplicationStateType, Union[dict, Any]]], None, None
    ]:
        """Produces an iterator that iterates through intermediate streams. You may want
        to use this in something like deep research mode in which:

        1. The user queries for a result
        2. The application runs through a workflow
        3. For each step of the workflow, the application
            a. Streams out an intermediate result
            b. Selects the next action/transition when the intermediate results is complete

        Note that there are control-flow complexities involved here -- we need to ensure that the iterator
        is properly pushed along and the prior streaming results containers are all finished before going to the next one.

        :param halt_after: Action names/tags to halt before
        :param halt_before: _description_, defaults to None
        :param inputs: _description_, defaults to None
        :return: _description_
        :yield: _description_
        """
        self.validate_correct_async_use()
        halt_before, halt_after, inputs = self._process_control_flow_params(
            halt_before, halt_after, inputs
        )
        self._validate_halt_conditions(halt_before, halt_after)
        while self.has_next_action():
            next_action = self.get_next_action()
            _, streaming_result = self.stream_result(
                halt_after=[next_action.name], halt_before=None, inputs=inputs
            )
            yield next_action, streaming_result
            # We need to ensure it's fully exhausted before going to the next action
            streaming_result.get()
            if self._should_halt_iterate(halt_before, halt_after, next_action):
                break

    @telemetry.capture_function_usage
    @_call_execute_method_pre_post(ExecuteMethod.astream_iterate)
    async def astream_iterate(
        self,
        halt_after: Optional[Union[str, List[str]]] = None,
        halt_before: Optional[Union[str, List[str]]] = None,
        inputs: Optional[Dict[str, Any]] = None,
    ) -> AsyncGenerator[
        Tuple[Action, AsyncStreamingResultContainer[ApplicationStateType, Union[dict, Any]]], None
    ]:
        """Async version of stream_iterate. Produces an async generator that iterates
        through intermediate streams. See stream_iterate for more details.

        :param halt_after: Action names/tags to halt after the action completes
        :param halt_before: Action names/tags to halt after
        :param inputs: Inputs to the first action run
        :return: Async generator yielding tuples of (action, streaming_result_container)
        :yield: Tuples of (action, streaming_result_container)
        """
        halt_before, halt_after, inputs = self._process_control_flow_params(
            halt_before, halt_after, inputs
        )
        self._validate_halt_conditions(halt_before, halt_after)
        while self.has_next_action():
            next_action = self.get_next_action()
            _, streaming_result = await self.astream_result(  # Use astream_result
                halt_after=[next_action.name], halt_before=None, inputs=inputs
            )
            yield next_action, streaming_result
            # We need to ensure it's fully exhausted before going to the next action
            await streaming_result.get()  # await the get call
            if self._should_halt_iterate(halt_before, halt_after, next_action):
                break

    @telemetry.capture_function_usage
    def visualize(
        self,
        output_file_path: Optional[str] = None,
        include_conditions: bool = False,
        include_state: bool = False,
        view: bool = False,
        engine: Literal["graphviz"] = "graphviz",
        write_dot: bool = False,
        **engine_kwargs: Any,
    ) -> Optional["graphviz.Digraph"]:  # noqa: F821
        """Visualizes the application graph using graphviz. This will render the graph.

        :param output_file_path: The path to save this to, None if you don't want to save. Do not pass an extension
            for graphviz, instead pass `format` in `engine_kwargs` (e.g. `format="png"`)
        :param include_conditions: Whether to include condition strings on the edges (this can get noisy)
        :param include_state: Whether to indicate the action "signature" (reads/writes) on the nodes
        :param view: Whether to bring up a view
        :param engine: The engine to use -- only graphviz is supported for now
        :param write_dot: If True, produce a graphviz dot file
        :param engine_kwargs: Additional kwargs to pass to the engine
        :return: The graphviz object
        """
        return self.graph.visualize(
            output_file_path=output_file_path,
            include_conditions=include_conditions,
            include_state=include_state,
            view=view,
            engine=engine,
            write_dot=write_dot,
            **engine_kwargs,
        )

    def _set_state(self, new_state: State[ApplicationStateType]):
        self._state = new_state

    def get_next_action(self) -> Optional[Action]:
        return self._graph.get_next_node(self._state.get(PRIOR_STEP), self._state, self.entrypoint)

    def update_state(self, new_state: State[ApplicationStateType]):
        """Updates state -- this is meant to be called if you need to do
        anything with the state. For example:
        1. Reset it (after going through a loop)
        2. Store to some external source/log out

        :param new_state:
        :return:
        """
        self._state = new_state

    @property
    def state(self) -> State[ApplicationStateType]:
        """Gives the state. Recall that state is purely immutable
        -- anything you do with this state will not be persisted unless you
        subsequently call update_state.

        :return: The current state object.
        """
        return self._state

    @property
    def parent_pointer(self) -> Optional[burr_types.ParentPointer]:
        """Gives the parent pointer of an application (from where it was forked).
        This is None if it was not forked.

        Forking is the process of starting an application off of another.

        :return: The parent pointer object.
        """
        return self._parent_pointer

    @property
    def spawning_parent_pointer(self) -> Optional[burr_types.ParentPointer]:
        """Gives the parent pointer of an application (from where it was spawned).
        This is None if it was not spawned.

        Spawning is the process of launching an application from within
        a step of another. This is used for recursive tracking.

        :return: The parent pointer object.
        """
        return self._spawning_parent_pointer

    @property
    def graph(self) -> ApplicationGraph:
        """Application graph object -- if you want to inspect, visualize, etc..
        this is what you want.

        :return: The application graph object
        """
        return self._public_facing_graph

    @property
    def sequence_id(self) -> Optional[int]:
        """gives the sequence ID of the current (next) action.
        This is incremented prior to every step. Any logging, etc... will use the current
        step's sequence ID

        :return: The sequence ID of the current (next) action
        """
        return self._state.get(SEQUENCE_ID)

    @property
    def context(self) -> ApplicationContext:
        """Gives the application context.
        This has information you need for the tracker, sequence ID, application, etc...

        :return: Application context
        """
        return self._context_factory(self.get_next_action(), self.sequence_id)

    def _increment_sequence_id(self):
        if SEQUENCE_ID not in self._state:
            self._state = self._state.update(**{SEQUENCE_ID: 0})
        else:
            self._state = self._state.update(**{SEQUENCE_ID: self.sequence_id + 1})

    def _set_sequence_id(self, sequence_id: int):
        self._state = self._state.update(**{SEQUENCE_ID: sequence_id})

    @property
    def uid(self) -> str:
        """Unique ID for the application. This must be unique across *all* applications in a search space.
        This is used by persistence/tracking to ensure that applications have meanings.

        Every application has this -- if not assigned, it will be randomly generated.

        :return: The unique ID for the application
        """
        return self._uid

    @property
    def partition_key(self) -> Optional[str]:
        """Partition key for the application. This is designed to add semantic meaning to
        the application, and be leveraged by persistence systems to select/find applications.

        Note this is optional -- if it is not included, you will need to use a persister that
        supports a null partition key.

        :return: The partition key, None if not set
        """
        return self._partition_key

    @property
    def builder(self) -> Optional["ApplicationBuilder[ApplicationStateType]"]:
        """Returns the application builder that was used to build this application.
        Note that this asusmes the application was built using the builder. Otherwise,

        :return: The application builder
        """
        return self._builder

    def _repr_mimebundle_(self, include=None, exclude=None, **kwargs):
        """Attribute read by notebook renderers
        This returns the attribute of the `graphviz.Digraph` returned by `self.display_all_functions()`

        The parameters `include`, `exclude`, and `**kwargs` are required, but not explicitly used
        ref: https://ipython.readthedocs.io/en/stable/config/integrating.html
        """
        dot = self.visualize(include_conditions=True, include_state=False)
        return dot._repr_mimebundle_(include=include, exclude=exclude, **kwargs)

    def validate_correct_async_use(self):
        """Validates that the application is meant to run async.
        This validation is performed in synchronous application methods."""

        # This is a gentle warning for existing users to use the async application
        if self._adapter_set.async_hooks:
            logger.warning(
                "There are asynchronous hooks present in the application that will be ignored. "
                "Please use async methods to run the application and have them executed. "
                f"The application has following asynchronous hooks: {self._adapter_set.async_hooks} "
            )

        # We check that if:
        # - we have build the application using .abuild()
        # - we have async hooks present
        # this application is meant to be run in async mode.
        if self._builder and self._builder.is_async:
            raise ValueError(
                "The application was build with .abuild() "
                "which needs to be executed in an asynchronous run. "
                "Please use the async run methods to run the application."
            )


def _validate_app_id(app_id: Optional[str]):
    if app_id is None:
        raise ValueError(
            "App ID was None. Please ensure that you set an app ID using with_identifiers(app_id=...), or default"
            "not setting it and letting the system generate one for you."
        )


def _validate_start(start: Optional[str], actions: Set[str]):
    validation.assert_set(start, "_start", "with_entrypoint")
    if start not in actions:
        raise ValueError(
            f"Entrypoint: {start} not found in actions. Please add "
            f"using with_actions({start}=...)"
        )


class ApplicationBuilder(Generic[StateType]):
    def __init__(self):
        self.start = None
        self.state: Optional[State[StateType]] = None
        self.lifecycle_adapters: List[LifecycleAdapter] = list()
        self.app_id: str = str(uuid.uuid4())
        self.partition_key: Optional[str] = None
        self.sequence_id: Optional[int] = None
        self.state_initializer = None
        self.use_entrypoint_from_save_state: Optional[bool] = None
        self.default_state: Optional[dict] = None
        self.fork_from_app_id: Optional[str] = None
        self.fork_from_partition_key: Optional[str] = None
        self.fork_from_sequence_id: Optional[int] = None
        self.spawn_from_app_id: Optional[str] = None
        self.spawn_from_partition_key: Optional[str] = None
        self.spawn_from_sequence_id: Optional[int] = None
        self.loaded_from_fork: bool = False
        self.tracker = None
        self.graph_builder = None
        self.typing_system = None
        self.parallel_executor_factory = None
        self.state_persister = None
        self._is_async: bool = False

    def with_identifiers(
        self, app_id: str = None, partition_key: str = None, sequence_id: int = None
    ) -> "ApplicationBuilder[StateType]":
        """Assigns various identifiers to the application. This is used for tracking, persistence, etc...

        :param app_id: Application ID -- this will be assigned to a uuid if not set.
        :param partition_key: Partition key -- this is used for disambiguating groups of applications. For instance, a unique user ID, etc...
            This is coupled to persistence, and is used to query for/select application runs.
        :param sequence_id: Sequence ID that we want this to start at. If you're using ``.initialize``, this will be set. Otherwise this is
            solely for resetting/starting at a specified position.
        :return: The application builder for future chaining.
        """
        if app_id is not None:
            self.app_id = app_id
        if partition_key is not None:
            self.partition_key = partition_key
        if sequence_id is not None:
            self.sequence_id = sequence_id
        return self

    def with_typing(
        self, typing_system: TypingSystem[StateTypeToSet]
    ) -> "ApplicationBuilder[StateTypeToSet]":
        """Sets the typing system for the application. This is used to enforce typing on the state.

        :param typing_system: Typing system to use
        :return: The application builder for future chaining.
        """
        if typing_system is not None:
            self.typing_system = typing_system
        return self  # type: ignore

    def with_state(
        self, state: Optional[Union[State, StateTypeToSet]] = None, **kwargs
    ) -> "ApplicationBuilder[StateType]":
        """Sets initial values in the state. If you want to load from a prior state,
        you can do so here and pass the values in.

        TODO -- enable passing in a `state` object instead of `**kwargs`

        :param kwargs: Key-value pairs to set in the state
        :return: The application builder for future chaining.
        """
        if self.state_initializer is not None:
            raise ValueError(
                BASE_ERROR_MESSAGE + "You cannot set state if you are loading state"
                "the .initialize_from() API. Either allow the persister to set the "
                "state, or set the state manually."
            )
        if state is not None:
            if self.state is not None:
                raise ValueError(
                    BASE_ERROR_MESSAGE
                    + "State items have already been set -- you cannot use the type-based API as well."
                    " Either set state with with_state(**kwargs) or pass in a state/typed object."
                )
            if isinstance(state, State):
                self.state = state
            elif self.typing_system is not None:
                self.state = self.typing_system.construct_state(state)
            else:
                raise ValueError(
                    BASE_ERROR_MESSAGE
                    + "You have not set a typing system, and you are passing in a typed state object."
                    " Please set a typing system using with_typing before doing so."
                )
        elif self.state is not None:
            self.state = self.state.update(**kwargs)
        else:
            self.state = State(kwargs)
        return self

    def with_graphs(self, *graphs) -> "ApplicationBuilder[StateType]":
        """Adds multiple prebuilt graphs -- this just calls :py:meth:`with_graph <burr.core.application.ApplicationBuilder.with_graph>`
        in a loop! See caveats in :py:meth:`with_graph <burr.core.application.ApplicationBuilder.with_graph>`.

        :param graphs: Graphs to add to the application
        :return: The application builder for future chaining.
        """
        for graph in graphs:
            self.with_graph(graph)
        return self

    def with_graph(self, graph: Graph) -> "ApplicationBuilder[StateType]":
        """Adds a prebuilt graph -- this can work in addition to using with_actions and with_transitions methods.
        This will add all nodes + edges from a prebuilt graph to the current graph. Note that if you add two
        graphs (or a combination of graphs/nodes/edges), you will need to ensure that there are no node name conflicts.

        1. You want to reuse the same graph object for different applications
        2. You want the logic that constructs the graph to be separate from that which constructs the application
        3. You want to serialize/deserialize a graph object and run it in an application

        :param graph: Graph object built with the :py:class:`GraphBuilder <burr.core.graph.GraphBuilder>`
        :return: The application builder for future chaining.
        """
        self._initialize_graph_builder()
        self.graph_builder = self.graph_builder.with_graph(graph)
        return self

    def with_parallel_executor(self, executor_factory: lambda: Executor):
        """Assigns a default executor to be used for recursive/parallel sub-actions. This effectively allows
        for executing multiple Burr apps in parallel. See https://burr.dagworks.io/concepts/parallelism/
        for more details.

        This will default to a simple threadpool executor, meaning that you will be bound by the number of threads
        your computer can handle. If you want to use a more advanced executor, you can pass it in here -- any subclass
        of concurrent.futures.Executor will work.

        If you specify executors for specific tasks, this will default to that.

        Note that, if you are using asyncio, you cannot specify an executor. It will default to using
        asyncio.gather with asyncio's event loop.

        :param executor:
        :return:
        """
        if self.parallel_executor_factory is not None:
            raise ValueError(
                BASE_ERROR_MESSAGE
                + "You have already set an executor. You cannot set multiple executors. Current executor is:"
                f"{self.parallel_executor_factory}"
            )

        self.parallel_executor_factory = executor_factory
        return self

    def _initialize_graph_builder(self):
        if self.graph_builder is None:
            self.graph_builder = GraphBuilder()

    def with_entrypoint(self, action: str) -> "ApplicationBuilder[StateType]":
        """Adds an entrypoint to the application. This is the action that will be run first.
        This can only be called once.

        :param action: The name of the action to set as the entrypoint
        :return: The application builder for future chaining.
        """
        if self.start is not None:
            raise ValueError(
                BASE_ERROR_MESSAGE
                + "You cannot set the entrypoint if you are loading a persister using "
                "the .initialize_from() API. Either allow the persister to set the "
                "entrypoint/provide a default, or set the entrypoint + state manually."
            )
        self.start = action
        return self

    def with_actions(
        self,
        *action_list: Union[Action, Callable],
        **action_dict: Union[Action, Callable],
    ) -> "ApplicationBuilder[StateType]":
        """Adds an action to the application. The actions are granted names (using the with_name)
        method post-adding, using the kw argument. If it already has a name (or you wish to use the function name, raw, and
        it is a function-based-action), then you can use the *args* parameter. This is the only supported way to add actions.

        :param action_list: Actions to add -- these must have a name or be function-based (in which case we will use the function-name)
        :param action_dict: Actions to add, keyed by name
        :return: The application builder for future chaining.
        """
        self._initialize_graph_builder()
        self.graph_builder = self.graph_builder.with_actions(*action_list, **action_dict)
        return self

    def with_transitions(
        self,
        *transitions: Union[
            Tuple[Union[str, list[str]], str],
            Tuple[Union[str, list[str]], str, Condition],
        ],
    ) -> "ApplicationBuilder[StateType]":
        """Adds transitions to the application. Transitions are specified as tuples of either:
            1. (from, to, condition)
            2. (from, to)  -- condition is set to DEFAULT (which is a fallback)

        Transitions will be evaluated in order of specification -- if one is met, the others will not be evaluated.
        Note that one transition can be terminal -- the system doesn't have


        :param transitions: Transitions to add
        :return: The application builder for future chaining.
        """
        self._initialize_graph_builder()
        self.graph_builder = self.graph_builder.with_transitions(*transitions)
        return self

    def with_hooks(self, *adapters: LifecycleAdapter) -> "ApplicationBuilder[StateType]":
        """Adds a lifecycle adapter to the application. This is a way to add hooks to the application so that
        they are run at the appropriate times. You can use this to synchronize state out, log results, etc...

        :param adapters: Adapter to add
        :return: The application builder for future chaining.
        """
        self.lifecycle_adapters.extend(adapters)
        return self

    def with_tracker(
        self,
        tracker: Union[Literal["local"], "TrackingClient"] = "local",
        project: str = "default",
        params: Dict[str, Any] = None,
        use_otel_tracing: bool = False,
    ):
        """Adds a "tracker" to the application. The tracker specifies
        a project name (used for disambiguating groups of tracers), and plugs into the
        Burr UI. This can either be:

        1. A string (the only supported one right now is "local"), and a set of parameters for a set of supported trackers.
        2. A lifecycle adapter object that does tracking (up to you how to implement it).

        (1) internally creates a :py:class:`LocalTrackingClient <burr.tracking.client.LocalTrackingClient>` object, and adds it to the lifecycle adapters.
        (2) adds the lifecycle adapter to the lifecycle adapters.

        :param tracker: Tracker to use. ``local`` creates one, else pass one in.
        :param project: Project name -- used if the tracker is string-specified (local).
        :param params: Parameters to pass to the tracker if it's string-specified (local).
        :param use_otel_tracing: Whether to log opentelemetry traces to the Burr UI. This is experimental but we will be adding
            full support shortly. This requires burr[opentelemetry] installed. Note you can also log burr to OpenTelemetry using the OpenTelemetry adapter
        :return: The application builder for future chaining.
        """
        # if it's a lifecycle adapter, just add it
        instantiated_tracker = tracker
        if isinstance(tracker, str):
            if params is None:
                params = {}
            if tracker == "local":
                from burr.tracking.client import LocalTrackingClient

                kwargs = {"project": project}
                kwargs.update(params)
                instantiated_tracker = LocalTrackingClient(**kwargs)
            else:
                raise ValueError(f"Tracker {tracker}:{project} not supported")
        else:
            if params is not None:
                raise ValueError(
                    "Params are not supported for object-specified trackers, these are already initialized!"
                )
        if use_otel_tracing:
            from burr.integrations.opentelemetry import OpenTelemetryTracker

            instantiated_tracker = OpenTelemetryTracker(burr_tracker=instantiated_tracker)
        self.lifecycle_adapters.append(instantiated_tracker)
        self.tracker = instantiated_tracker
        return self

    def initialize_from(
        self,
        initializer: Union[BaseStateLoader, AsyncBaseStateLoader],
        resume_at_next_action: bool,
        default_state: dict,
        default_entrypoint: str,
        fork_from_app_id: str = None,
        fork_from_partition_key: str = None,
        fork_from_sequence_id: int = None,
    ) -> "ApplicationBuilder[StateType]":
        """Initializes the application we will build from some prior state object.

        Note (1) that you can *either* call this or use `with_state` and `with_entrypoint`.

        Note (2) if you want to continue a prior application and don't want to fork it into a new application ID,
        the values in `.with_identifiers()` will be used to query for prior state.

        :param initializer: The persister object to use for initialization. Likely the same one called with ``with_state_persister``.
        :param resume_at_next_action: Whether to resume at the next action, or default to the ``default_entrypoint``
        :param default_state: The default state to use if it does not exist. This is a dictionary.
        :param default_entrypoint: The default entry point to use if it does not exist or you elect not to resume_at_next_action.
        :param fork_from_app_id: The app ID to fork from, not to be confused with the current app_id that is set with `.with_identifiers()`. This is used to fork from a prior application run.
        :param fork_from_partition_key: The partition key to fork from a prior application. Optional. `fork_from_app_id` required.
        :param fork_from_sequence_id: The sequence ID to fork from a prior application run. Optional, defaults to latest. `fork_from_app_id` required.
        :return: The application builder for future chaining.
        """
        if self.start is not None or self.state is not None:
            raise ValueError(
                BASE_ERROR_MESSAGE
                + "Cannot call initialize_from if you have already set state or an entrypoint! "
                "You can either use the initializer *or* set the state and entrypoint manually."
            )
        if not fork_from_app_id and (fork_from_partition_key or fork_from_sequence_id):
            raise ValueError(
                BASE_ERROR_MESSAGE
                + "If you set fork_from_partition_key or fork_from_sequence_id, you must also set fork_from_app_id. "
                "See .initialize_from() documentation."
            )
        self.state_initializer = initializer
        self.resume_at_next_action = resume_at_next_action
        self.default_state = default_state
        self.start = default_entrypoint
        self.fork_from_app_id = fork_from_app_id
        self.fork_from_partition_key = fork_from_partition_key
        self.fork_from_sequence_id = fork_from_sequence_id
        return self

    def with_state_persister(
        self,
        persister: Union[BaseStateSaver, AsyncBaseStateSaver, LifecycleAdapter],
        on_every: str = "step",
    ) -> "ApplicationBuilder[StateType]":
        """Adds a state persister to the application. This is a way to persist state out to a database, file, etc...
        at the specified interval. This is one of two options:

        1. [normal mode] A BaseStateSaver object -- this is a utility class that makes it easy to save/load
        2. [power-user-mode] A lifecycle adapter -- this is a custom class that you use to save state.

        The framework will wrap the BaseStateSaver object in a PersisterHook, which is a post-run.

        :param persister: The persister to add
        :param on_every: The interval to persist state. Currently only "step" is supported.
        :return: The application builder for future chaining.
        """
        if on_every != "step":
            raise ValueError(f"on_every {on_every} not supported")

        self.state_persister = persister  # tracks for later; validates in build / abuild
        return self

    def with_spawning_parent(
        self, app_id: str, sequence_id: int, partition_key: Optional[str] = None
    ) -> "ApplicationBuilder[StateType]":
        """Sets the 'spawning' parent application that created this app.
        This is used for tracking purposes. Doing this creates a parent/child relationship.
        There can be many spawned children from a single sequence ID (just as there can be many forks of an app).

        Note the difference between this and forking. Forking allows you to create a new app
        where the old one left off. This suggests that this application is wholly contained
        within the parent application.

        :param app_id: ID of application that spawned this app
        :param sequence_id: Sequence ID of the parent app that spawned this app
        :param partition_key: Partition key of the parent app that spawned this app
        :return: The application builder for future chaining.
        """
        self.spawn_from_app_id = app_id
        self.spawn_from_sequence_id = sequence_id
        self.spawn_from_partition_key = partition_key
        return self

    def _set_sync_state_persister(self):
        """Inits the synchronous with_state_persister to save the state (local/DB/custom implementations).
        Moved here to mimic the async case.
        """
        if self.state_persister.is_async():
            raise ValueError(
                "You are building the sync application, but have used an "
                "async persister. Please use a sync persister or use the "
                ".abuild() method to build an async application."
            )

        if not isinstance(self.state_persister, persistence.BaseStateSaver):
            self.lifecycle_adapters.append(self.state_persister)
        else:
            # Check if 'is_initialized' exists and is False; raise RuntimeError, else continue if not implemented
            try:
                if not self.state_persister.is_initialized():
                    raise RuntimeError(
                        "RuntimeError: Uninitialized persister. Make sure to call .initialize() before passing it to "
                        "the ApplicationBuilder."
                    )
            except NotImplementedError:
                pass
            self.lifecycle_adapters.append(persistence.PersisterHook(self.state_persister))

    async def _set_async_state_persister(self):
        """Inits the asynchronous with_state_persister to save the state (local/DB/custom implementations).
        Moved here to be able to chain methods and delay the execution until we can chain coroutines in abuild().
        """
        if not self.state_persister.is_async():
            raise ValueError(
                "You are building the async application, but have used an "
                "sync persister. Please use an async persister or use the "
                ".build() method to build an sync application."
            )

        if not isinstance(self.state_persister, persistence.AsyncBaseStateSaver):
            self.lifecycle_adapters.append(self.state_persister)
        else:
            # Check if 'is_initialized' exists and is False; raise RuntimeError, else continue if not implemented
            try:
                if not await self.state_persister.is_initialized():
                    raise RuntimeError(
                        "RuntimeError: Uninitialized persister. Make sure to call .initialize() before passing it to "
                        "the ApplicationBuilder."
                    )
            except NotImplementedError:
                pass
            self.lifecycle_adapters.append(persistence.PersisterHookAsync(self.state_persister))

    def _identify_state_to_load(self):
        """Helper to determine which state to load."""
        if self.fork_from_app_id is not None:
            if self.app_id == self.fork_from_app_id:
                raise ValueError(
                    BASE_ERROR_MESSAGE + "Cannot fork and save to the same app_id. "
                    "Please update the app_id passed in via with_identifiers(), "
                    "or don't pass in a fork_from_app_id value to `initialize_from()`."
                )
            _partition_key = self.fork_from_partition_key
            _app_id = self.fork_from_app_id
            _sequence_id = self.fork_from_sequence_id
        else:
            # only use the with_identifier values if we're not forking from a previous app
            _partition_key = self.partition_key
            _app_id = self.app_id
            _sequence_id = self.sequence_id

        return _partition_key, _app_id, _sequence_id

    def _init_state_from_persister(
        self,
        load_result: Optional[persistence.PersistedStateData],
        partition_key: str,
        app_id: Optional[str],
        sequence_id: Optional[int],
    ):
        """Initializes the state of the application.

        Either there is a loaded configuration provided or we use the default state to initialize.
        """
        if load_result is None:
            if self.fork_from_app_id is not None:
                logger.warning(
                    f"{self.state_initializer.__class__.__name__} returned None while trying to fork from: "
                    f"partition_key:{partition_key}, app_id:{app_id}, "
                    f"sequence_id:{sequence_id}. "
                    "You explicitly requested to fork from a prior application run, but it does not exist. "
                    "Defaulting to state defaults instead."
                )
            # there was nothing to load -- use default state
            self.state = self.state.update(**self.default_state)
            self.sequence_id = None  # has to start at None
        else:
            self.loaded_from_fork = True
            if load_result["state"] is None:
                raise ValueError(
                    BASE_ERROR_MESSAGE
                    + f"Error: {self.state_initializer.__class__.__name__} returned {load_result} for "
                    f"partition_key:{partition_key}, app_id:{app_id}, "
                    f"sequence_id:{sequence_id}, "
                    "but value for state was None! This is not allowed. Please return just None in this case, "
                    "or double check that persisted state can never be a None value."
                )
            # TODO: capture parent app ID relationship & wire it through
            # there was something
            last_position = load_result["position"]
            self.state = load_result["state"]
            self.sequence_id = load_result["sequence_id"]
            status = load_result["status"]
            if self.resume_at_next_action:
                # if we're supposed to resume where we saved from
                if status == "completed":
                    # completed means we set prior step to current to go to next action
                    self.state = self.state.update(**{PRIOR_STEP: last_position})
                else:
                    # else we failed we just start at that node
                    self.start = last_position
                    self.reset_to_entrypoint()
            else:
                # self.start is already set to the default. We don't need to do anything.
                pass

    def _load_from_sync_persister(self):
        """Loads from the set sync persister and into this current object.

        Mutates:
         - self.state
         - self.sequence_id
         - maybe self.start

        """
        if self.state_initializer.is_async():
            raise ValueError(
                "You are building the sync application, but have used an "
                "async initializer. Please use a sync initializer or use the "
                ".abuild() method to build an async application."
            )

        _partition_key, _app_id, _sequence_id = self._identify_state_to_load()

        # load state from persister
        load_result = self.state_initializer.load(_partition_key, _app_id, _sequence_id)
        self._init_state_from_persister(load_result, _partition_key, _app_id, _sequence_id)

    async def _load_from_async_persister(self):
        """Loads from the set async persister and into this current object.

        Mutates:
         - self.state
         - self.sequence_id
         - maybe self.start

        """
        if not self.state_initializer.is_async():
            raise ValueError(
                "You are building the async application, but have used an "
                "sync initializer. Please use an async initializer or use the "
                ".build() method to build an sync application."
            )

        _partition_key, _app_id, _sequence_id = self._identify_state_to_load()

        # load state from persister
        load_result = await self.state_initializer.load(_partition_key, _app_id, _sequence_id)
        self._init_state_from_persister(load_result, _partition_key, _app_id, _sequence_id)

    def reset_to_entrypoint(self):
        self.state = self.state.wipe(delete=[PRIOR_STEP])

    def _get_built_graph(self) -> Graph:
        if self.graph_builder is None:
            raise ValueError(
                BASE_ERROR_MESSAGE
                + "No graph constructs exist. You must call some combination of with_graph, with_entrypoint, "
                "with_actions, and with_transitions"
            )
        return self.graph_builder.build()

    def _build_common(self) -> Application:
        graph = self._get_built_graph()
        _validate_start(self.start, {action.name for action in graph.actions})
        typing_system: TypingSystem[StateType] = (
            self.typing_system if self.typing_system is not None else DictBasedTypingSystem()
        )  # type: ignore
        self.state = self.state.with_typing_system(typing_system=typing_system)
        return Application(
            graph=graph,
            state=self.state,
            uid=self.app_id,
            partition_key=self.partition_key,
            sequence_id=self.sequence_id,
            entrypoint=self.start,
            adapter_set=LifecycleAdapterSet(*self.lifecycle_adapters),
            builder=self,
            fork_parent_pointer=(
                burr_types.ParentPointer(
                    app_id=self.fork_from_app_id,
                    partition_key=self.fork_from_partition_key,
                    sequence_id=self.fork_from_sequence_id,
                )
                if self.loaded_from_fork
                else None
            ),
            tracker=self.tracker,
            spawning_parent_pointer=(
                burr_types.ParentPointer(
                    app_id=self.spawn_from_app_id,
                    partition_key=self.spawn_from_partition_key,
                    sequence_id=self.spawn_from_sequence_id,
                )
                if self.spawn_from_app_id is not None
                else None
            ),
            parallel_executor_factory=self.parallel_executor_factory,
            state_persister=self.state_persister,
            state_initializer=self.state_initializer,
        )

    @telemetry.capture_function_usage
    def build(self) -> Application[StateType]:
        """Builds the application for synchronous runs.

        We support both synchronous and asynchronous applications. In case you are using state initializers
        and persisters, the synchronous application should be used in the following cases:

        .. table:: When to use .build()
            :widths: auto

            +-----------------------------------------+----------+------------------------+
            | Cases (persister and app methods)       | Status   | Remarks                |
            +=========================================+==========+========================+
            | Sync and Sync                           |  âœ…      |                        |
            +-----------------------------------------+----------+------------------------+
            | Sync and Async                          |  âœ… âš ï¸   | Will be deprecated     |
            +-----------------------------------------+----------+------------------------+
            | Async and Sync                          |  âŒ      |                        |
            +-----------------------------------------+----------+------------------------+
            | Async and Async                         |  âŒ      |                        |
            +-----------------------------------------+----------+------------------------+

        We originally only had sync persistence and as such this still can be used when running the
        app async. However, we strongly encourage to switch to async persisters if you are running
        an async application.

        :return: The application object.
        """
        _validate_app_id(self.app_id)
        if self.state is None:
            self.state = State()

        if self.state_persister:
            self._set_sync_state_persister()  # this is used to save the state during application run
        if self.state_initializer:
            # sets state, sequence_id, and maybe start
            self._load_from_sync_persister()  # this is used to load application from a previously saved state

        return self._build_common()

    @telemetry.capture_function_usage
    async def abuild(self) -> Application[StateType]:
        """Builds the application for asynchronous runs.

        We support both synchronous and asynchronous applications. To save/load in an asynchronous
        manner add the async persister versions and use this method to initialize them. This will
        also enforce the application to be run with async methods as an additional safety check that
        you are not introducing unnecessary bottlenecks.

        Note: When you run an async application you can still use the normal sync functionalities, i.e.
        sync hooks of other adapters, but they will block the async event loop until finished.

        In case you are using state initializers and persisters, the asynchronous application should
        be used in the following cases:

        .. table:: When to use .abuild()
            :widths: auto

            +-----------------------------------------+----------+
            | Cases (persister and app methods)       | Status   |
            +=========================================+==========+
            | Sync and Sync                           |  âŒ      |
            +-----------------------------------------+----------+
            | Sync and Async                          |  âŒ      |
            +-----------------------------------------+----------+
            | Async and Sync                          |  âŒ      |
            +-----------------------------------------+----------+
            | Async and Async                         |  âœ…      |
            +-----------------------------------------+----------+

        :return: The application object.
        """
        self._is_async = True

        _validate_app_id(self.app_id)
        if self.state is None:
            self.state = State()

        if self.state_persister:
            await self._set_async_state_persister()  # this is used to save the state during application run

        if self.state_initializer:
            # sets state, sequence_id, and maybe start
            await self._load_from_async_persister()  # this is used to load application from a previously saved state

        return self._build_common()

    @property
    def is_async(self) -> bool:
        return self._is_async



---
File: /burr/burr/core/graph.py
---

import collections
import dataclasses
import inspect
import logging
import pathlib
from typing import Any, Callable, List, Literal, Optional, Set, Tuple, Union

from burr import telemetry
from burr.core.action import Action, Condition, create_action, default
from burr.core.state import State
from burr.core.validation import BASE_ERROR_MESSAGE, assert_set

logger = logging.getLogger(__name__)


@dataclasses.dataclass
class Transition:
    """Internal, utility class"""

    from_: Action
    to: Action
    condition: Condition


def _validate_actions(actions: Optional[List[Action]]):
    assert_set(actions, "_actions", "with_actions")
    if len(actions) == 0:
        raise ValueError("Must have at least one action in the application!")
    seen_action_names = set()
    for action in actions:
        if action.name in seen_action_names:
            raise ValueError(
                f"Action: {action.name} is duplicated in the actions list. "
                "actions have unique names. This could happen "
                "if you add two actions with the same name or add a graph that "
                "has actions with the same name as any that already exist."
            )
        seen_action_names.add(action.name)


def _validate_transitions(
    transitions: Optional[List[Tuple[str, str, Condition]]], actions: Set[str]
):
    exhausted = {}  # items for which we have seen a default transition
    for from_, to, condition in transitions:
        if from_ not in actions:
            raise ValueError(
                f"Transition source: `{from_}` not found in actions! "
                f"Please add to actions using with_actions({from_}=...)"
            )
        if to not in actions:
            raise ValueError(
                f"Transition target: `{to}` not found in actions! "
                f"Please add to actions using with_actions({to}=...)"
            )
        if condition.name == "default":  # we have seen a default transition
            if from_ in exhausted:
                raise ValueError(
                    f"Transition `{from_}` -> `{to}` is redundant -- "
                    f"a default transition has already been set for `{from_}`"
                )
            exhausted[from_] = True
    return True


def _render_graphviz(
    graphviz_obj,
    output_file_path: Union[str, pathlib.Path],
    view: bool = False,
    write_dot: bool = False,
) -> None:
    """Generate an image file for the graphviz object.

    Use graphviz's `.render()` to produce a dot file or open the image
    on the OS (i.e., `view` arg)
    Use .pipe()` to not produce a not file; can't open the image
    """
    output_file_path = pathlib.Path(output_file_path)

    if output_file_path.suffix != "":
        # infer format from path; i.e., extract `svg` from the `.svg` suffix
        format = output_file_path.suffix.partition(".")[-1]
    else:
        format = "png"

    path_without_suffix = pathlib.Path(output_file_path.parent, output_file_path.stem)
    if write_dot or view:
        # `.render()` appends the `format` kwarg to the filename
        # i.e., we need to pass `/my/filepath` to generate `/my/filepath.png`
        # otherwise, passing `/my/filepath.png` will generate `/my/filepath.png.png`
        graphviz_obj.render(path_without_suffix, format=format, view=view)
    else:
        # `.pipe()` doesn't append the format to the filename, so we do it explicitly
        pathlib.Path(f"{path_without_suffix}.{format}").write_bytes(
            graphviz_obj.pipe(format=format)
        )


@dataclasses.dataclass
class Graph:
    """Graph class allows you to specify actions and transitions between them.
    You will never instantiate this directly, just through the GraphBuilder,
    or indirectly through the ApplicationBuilder."""

    actions: List[Action]
    transitions: List[Transition]

    def __post_init__(self):
        """Sets up initial state for easy graph operations later"""
        self._action_map = {action.name: action for action in self.actions}
        self._adjacency_map = self._create_adjacency_map(self.transitions)
        self._action_tag_map = self._create_action_tag_map(self.actions)

    @staticmethod
    def _create_adjacency_map(transitions: List[Transition]) -> dict:
        adjacency_map = collections.defaultdict(list)
        for transition in transitions:
            from_ = transition.from_
            to = transition.to
            adjacency_map[from_.name].append((to.name, transition.condition))
        return adjacency_map

    @staticmethod
    def _create_action_tag_map(actions: List[Action]) -> dict[str, List[Action]]:
        """Creates a mapping of action tags to lists of corresponding actions.

        :param actions: List of Action objects
        :return: Dictionary mapping action tags to lists of matching Action objects
        """
        tag_map = collections.defaultdict(list)
        for action in actions:
            for tag in action.tags:
                tag_map[tag].append(action)
        return dict(tag_map)

    def get_next_node(
        self, prior_step: Optional[str], state: State, entrypoint: str
    ) -> Optional[Action]:
        """Gives the next node to execute given state + prior step."""
        if prior_step is None:
            return self._action_map[entrypoint]
        possibilities = self._adjacency_map[prior_step]
        for next_action, condition in possibilities:
            if condition.run(state)[Condition.KEY]:
                return self._action_map[next_action]
        return None

    def get_action(self, action_name: str) -> Optional[Action]:
        """Gets an action object given the action name"""
        if action_name not in self._action_map:
            raise ValueError(
                BASE_ERROR_MESSAGE + f"Action: {action_name} not found in graph. "
                f"Actions are: {self._action_map.keys()}"
            )
        return self._action_map.get(action_name)

    def get_actions_by_tag(self, tag: str) -> List[Action]:
        """Gets a list of actions given a tag. Note that, if no tags match the action a ValueError will be raised."""
        if tag not in self._action_tag_map:
            raise ValueError(
                BASE_ERROR_MESSAGE + f"Tag: {tag} not found in graph. "
                f"Tags are: {self._action_tag_map.keys()}."
            )
        return self._action_tag_map.get(tag)

    @telemetry.capture_function_usage
    def visualize(
        self,
        output_file_path: Optional[Union[str, pathlib.Path]] = None,
        include_conditions: bool = False,
        include_state: bool = False,
        view: bool = False,
        engine: Literal["graphviz"] = "graphviz",
        write_dot: bool = False,
        **engine_kwargs: Any,
    ) -> Optional["graphviz.Digraph"]:  # noqa: F821
        """Visualizes the graph using graphviz. This will render the graph.

        :param output_file_path: The path to save this to, None if you don't want to save. Do not pass an extension
            for graphviz, instead pass `format` in `engine_kwargs` (e.g. `format="png"`)
        :param include_conditions: Whether to include condition strings on the edges (this can get noisy)
        :param include_state: Whether to indicate the action "signature" (reads/writes) on the nodes
        :param view: Whether to bring up a view
        :param engine: The engine to use -- only graphviz is supported for now
        :param write_dot: If True, produce a graphviz dot file
        :param engine_kwargs: Additional kwargs to pass to the engine
        :return: The graphviz object
        """
        if engine != "graphviz":
            raise ValueError(f"Only graphviz is supported for now, not {engine}")
        try:
            import graphviz  # noqa: F401
        except ModuleNotFoundError:
            logger.exception(
                " graphviz is required for visualizing the application graph. Install it with:"
                '\n\n  pip install "burr[graphviz]" or pip install graphviz \n\n'
            )
            return
        digraph_attr = dict(
            graph_attr=dict(
                rankdir="TB",
                ranksep="0.4",
                compound="false",
                concentrate="false",
            ),
            node_attr=dict(
                fontname="Helvetica",
                margin="0.15",
                fillcolor="#b4d8e4",
            ),
        )
        for g_key, g_value in engine_kwargs.items():
            if isinstance(g_value, dict):
                digraph_attr[g_key].update(**g_value)
            else:
                digraph_attr[g_key] = g_value
        digraph = graphviz.Digraph(**digraph_attr)
        for action in self.actions:
            label = (
                action.name
                if not include_state
                else f"{action.name}({', '.join(action.reads)}): {', '.join(action.writes)}"
            )
            digraph.node(action.name, label=label, shape="box", style="rounded,filled")
            required_inputs, optional_inputs = action.optional_and_required_inputs
            for input_ in required_inputs | optional_inputs:
                if input_.startswith("__"):
                    # These are internally injected by the framework
                    continue
                input_name = f"input__{input_}"
                digraph.node(input_name, shape="rect", style="dashed", label=f"input: {input_}")
                digraph.edge(input_name, action.name)
        for transition in self.transitions:
            condition = transition.condition
            digraph.edge(
                transition.from_.name,
                transition.to.name,
                label=condition.name if include_conditions and condition is not default else None,
                style="dashed" if transition.condition is not default else "solid",
            )

        if output_file_path:
            _render_graphviz(
                graphviz_obj=digraph,
                output_file_path=output_file_path,
                view=view,
                write_dot=write_dot,
            )

        return digraph

    def _repr_mimebundle_(self, include=None, exclude=None, **kwargs):
        """Attribute read by notebook renderers
        This returns the attribute of the `graphviz.Digraph` returned by `self.display_all_functions()`

        The parameters `include`, `exclude`, and `**kwargs` are required, but not explicitly used
        ref: https://ipython.readthedocs.io/en/stable/config/integrating.html
        """
        dot = self.visualize(include_conditions=True, include_state=False)
        return dot._repr_mimebundle_(include=include, exclude=exclude, **kwargs)


class GraphBuilder:
    """GraphBuilder class. This allows you to construct a graph without considering application concerns.
    While you can (and at first, should) use the ApplicationBuidler (which has the same API), this is lower level
    and allows you to decouple concerns, reuse the same graph object, etc..."""

    def __init__(self):
        """Initializes the graph builder."""
        self.transitions: Optional[List[Tuple[str, str, Condition]]] = []
        self.actions: Optional[List[Action]] = None

    def with_actions(
        self, *action_list: Union[Action, Callable], **action_dict: Union[Action, Callable]
    ) -> "GraphBuilder":
        """Adds an action to the application. The actions are granted names (using the with_name)
        method post-adding, using the kw argument. If it already has a name (or you wish to use the function name, raw, and
        it is a function-based-action), then you can use the *args* parameter. This is the only supported way to add actions.

        :param action_list: Actions to add -- these must have a name or be function-based (in which case we will use the function-name)
        :param action_dict: Actions to add, keyed by name
        :return: The application builder for future chaining.
        """
        if self.actions is None:
            self.actions = []
        for action_value in action_list:
            if inspect.isfunction(action_value):
                self.actions.append(create_action(action_value, action_value.__name__))
            elif isinstance(action_value, Action):
                if not action_value.name:
                    raise ValueError(
                        BASE_ERROR_MESSAGE + f"Action: {action_value} must have a name set. "
                        "If you hit this, you should probably be using the "
                        "**kwargs parameter (or call with_name(your_name) on the action)."
                    )
                self.actions.append(action_value)
        for key, value in action_dict.items():
            self.actions.append(create_action(value, key))
        return self

    def with_transitions(
        self,
        *transitions: Union[
            Tuple[Union[str, list[str]], str], Tuple[Union[str, list[str]], str, Condition]
        ],
    ) -> "GraphBuilder":
        """Adds transitions to the graph. Transitions are specified as tuples of either:
            1. (from, to, condition)
            2. (from, to)  -- condition is set to DEFAULT (which is a fallback)

        Transitions will be evaluated in order of specification -- if one is met, the others will not be evaluated.
        Note that one transition can be terminal -- the system doesn't have


        :param transitions: Transitions to add
        :return: The application builder for future chaining.
        """
        for transition in transitions:
            from_, to_, *conditions = transition
            if len(conditions) > 0:
                condition = conditions[0]
            else:
                condition = default
            if not isinstance(from_, list):
                from_ = [from_]
            for action in from_:
                if not isinstance(action, str):
                    raise ValueError(f"Transition source must be a string, not {action}")
                if not isinstance(to_, str):
                    raise ValueError(f"Transition target must be a string, not {to_}")
                self.transitions.append((action, to_, condition))
        return self

    def with_graph(self, graph: Graph) -> "GraphBuilder":
        """Adds an existing graph to the builder. Note that if you have any name clashes
        this will error out. This would happen if you add actions with the same name as actions
        that already exist.

        :param graph: The graph to add
        :return: The application builder for future chaining.
        """
        if self.actions is None:
            self.actions = []
        if self.transitions is None:
            self.transitions = []
        self.actions.extend(graph.actions)
        self.transitions.extend(
            (transition.from_.name, transition.to.name, transition.condition)
            for transition in graph.transitions
        )
        return self

    def build(self) -> Graph:
        """Builds/finalizes the graph.

        :return: The graph object
        """
        _validate_actions(self.actions)
        actions_by_name = {action.name: action for action in self.actions}
        all_actions = set(actions_by_name.keys())
        _validate_transitions(self.transitions, all_actions)
        return Graph(
            actions=self.actions,
            transitions=[
                Transition(
                    from_=actions_by_name[from_],
                    to=actions_by_name[to],
                    condition=condition,
                )
                for from_, to, condition in self.transitions
            ],
        )



---
File: /burr/burr/core/implementations.py
---

from burr.core import State
from burr.core.action import Action


class Placeholder(Action):
    """This is a placeholder action -- you would expect it to break if you tried to run it. It is specifically
    for the following workflow:
    1. Create your state machine out of placeholders to model it
    2. Visualize the state machine
    2. Replace the placeholders with real actions as you see fit
    """

    def __init__(self, reads: list[str], writes: list[str]):
        super().__init__()
        self._reads = reads
        self._writes = writes

    def run(self, state: State) -> dict:
        raise NotImplementedError(
            f"This is a placeholder action and thus you are unable to run. Please implement: {self}!"
        )

    def update(self, result: dict, state: State) -> State:
        raise NotImplementedError(
            f"This is a placeholder action and thus cannot update state. Please implement: {self}!"
        )

    @property
    def reads(self) -> list[str]:
        return self._reads

    @property
    def writes(self) -> list[str]:
        return self._writes



---
File: /burr/burr/core/parallelism.py
---

import abc
import asyncio
import dataclasses
import hashlib
import inspect
import logging
from typing import (
    Any,
    AsyncGenerator,
    Callable,
    Dict,
    Generator,
    List,
    Literal,
    Optional,
    Tuple,
    TypeVar,
    Union,
)

from burr.common import async_utils
from burr.common.async_utils import SyncOrAsyncGenerator, SyncOrAsyncGeneratorOrItemOrList
from burr.core import Action, ApplicationBuilder, ApplicationContext, Graph, State
from burr.core.action import SingleStepAction
from burr.core.application import ApplicationIdentifiers
from burr.core.graph import GraphBuilder
from burr.core.persistence import BaseStateLoader, BaseStateSaver
from burr.lifecycle import LifecycleAdapter
from burr.tracking.base import TrackingClient

SubgraphType = Union[Action, Callable, "RunnableGraph"]
logger = logging.getLogger(__name__)


@dataclasses.dataclass
class RunnableGraph:
    """Contains a graph with information it needs to run.
    This is a bit more than a graph -- we have entrypoints + halt_after points.
    This is the core element of a recursive action -- your recursive generators can yield these
    (as well as actions/functions, which both get turned into single-node graphs...)
    """

    graph: Graph
    entrypoint: str
    halt_after: List[str]

    @staticmethod
    def create(from_: SubgraphType) -> "RunnableGraph":
        """Creates a RunnableGraph from a callable/action. This will create a single-node runnable graph,
        so we can wrap it up in a task.

        :param from_: Callable or Action to wrap
        :return: RunnableGraph
        """
        if isinstance(from_, RunnableGraph):
            return from_
        if isinstance(from_, Action):
            assert (
                from_.name is not None
            ), "Action must have a name to be run, internal error, reach out to devs"
        graph = GraphBuilder().with_actions(from_).build()
        (action,) = graph.actions
        return RunnableGraph(graph=graph, entrypoint=action.name, halt_after=[action.name])


TrackerBehavior = Union[Literal["cascade"], None, TrackingClient]
StatePersisterBehavior = Union[Literal["cascade"], BaseStateSaver, LifecycleAdapter, None]
StateInitializerBehavior = Union[Literal["cascade"], BaseStateLoader, LifecycleAdapter, None]

AdapterType = TypeVar("AdapterType", bound=Union[BaseStateSaver, BaseStateLoader, LifecycleAdapter])


@dataclasses.dataclass
class SubGraphTask:
    """Task to run a subgraph. Has runtime-specific information, like inputs, state, and
    the application ID. This is the lower-level component -- the user will only directly interact
    with this if they use the TaskBasedParallelAction interface, which produces a generator of these.
    """

    graph: RunnableGraph
    inputs: Dict[str, Any]
    state: State
    application_id: str
    # The following require you to specify a tracking client, base state saver, etc...
    tracker: Optional[TrackingClient] = None
    state_persister: Optional[BaseStateSaver] = None
    state_initializer: Optional[BaseStateLoader] = None

    def _create_app_builder(self, parent_context: ApplicationIdentifiers) -> ApplicationBuilder:
        builder = (
            ApplicationBuilder()
            .with_graph(self.graph.graph)
            .with_spawning_parent(
                app_id=parent_context.app_id,
                sequence_id=parent_context.sequence_id,
                partition_key=parent_context.partition_key,
            )
            # TODO -- handle persistence...
            .with_identifiers(
                app_id=self.application_id,
                partition_key=parent_context.partition_key,  # cascade the partition key
            )
        )
        if self.tracker is not None:
            builder = builder.with_tracker(self.tracker)  # TODO -- move this into the adapter

        # In this case we want to persist the state for the app
        if self.state_persister is not None:
            builder = builder.with_state_persister(self.state_persister)
        # In this case we want to initialize from it
        # We're going to use default settings (initialize from the latest
        # TODO -- consider if there's a case in which we want to initialize
        # in a custom manner
        # if state_initializer is not None and self.cascade_state_initializer:
        if self.state_initializer is not None:
            builder = builder.initialize_from(
                self.state_initializer,
                default_state=self.state.get_all(),  # TODO _- ensure that any hidden variables aren't used...
                default_entrypoint=self.graph.entrypoint,
                resume_at_next_action=True,
            )
        else:
            builder = builder.with_entrypoint(self.graph.entrypoint).with_state(self.state)

        return builder

    def run(
        self,
        parent_context: ApplicationContext,
    ) -> State:
        """Runs the task -- this simply executes it by instantiating a sub-application"""
        app = self._create_app_builder(parent_context).build()
        action, result, state = app.run(
            halt_after=self.graph.halt_after,
            inputs={key: value for key, value in self.inputs.items() if not key.startswith("__")},
        )
        return state

    async def arun(self, parent_context: ApplicationContext):
        # Here for backwards compatibility, not ideal
        if (self.state_initializer is not None and not self.state_initializer.is_async()) or (
            self.state_persister is not None and not self.state_persister.is_async()
        ):
            logger.warning(
                "You are using sync persisters for an async application which is not optimal. "
                "Consider switching to an async persister implementation. We will make this an error soon."
            )
            app = self._create_app_builder(parent_context).build()
        else:
            app = await self._create_app_builder(parent_context).abuild()
        action, result, state = await app.arun(
            halt_after=self.graph.halt_after,
            inputs={key: value for key, value in self.inputs.items() if not key.startswith("__")},
        )
        return state


def _stable_app_id_hash(app_id: str, child_key: str) -> str:
    """Gives a stable hash for an application. Given the parent app_id and a child key,
    this will give a hash that will be stable across runs.

    :param app_id: Parent application ID
    :param child_key: Child key to hash
    :return: Stable hash of the parent app_id and child key
    """
    return hashlib.sha256(f"{app_id}:{child_key}".encode()).hexdigest()


class TaskBasedParallelAction(SingleStepAction):
    """The base class for actions that run a set of tasks in parallel and reduce the results.
    This is more power-user mode -- if you need fine-grained control over the set of tasks
    your parallel action utilizes, then this is for you. If not, you'll want to see:

    - :py:class:`MapActionsAndStates` -- a cartesian product of actions/states
    - :py:class:`MapActions` -- a map of actions over a single state
    - :py:class:`MapStates` -- a map of a single action over multiple states

    If you're unfamiliar about where to start, you'll want to see the docs on :ref:`parallelism <parallelism>`.

    This is responsible for two things:

    1. Creating a set of tasks to run in parallel
    2. Reducing the results of those tasks into a single state for the action to return.

    The following example shows how to call a set of prompts over a set of different models in parallel and return the result.

    .. code-block:: python

        from burr.core import action, state, ApplicationContext
        from burr.core.parallelism import MapStates, RunnableGraph
        from typing import Callable, Generator, List

        @action(reads=["prompt", "model"], writes=["llm_output"])
        def query_llm(state: State, model: str) -> State:
            # TODO -- implement _query_my_llm to call litellm or something
            return state.update(llm_output=_query_my_llm(prompt=state["prompt"], model=model))

        class MultipleTaskExample(TaskBasedParallelAction):
            def tasks(state: State, context: ApplicationContext) -> Generator[SubGraphTask, None, None]:
                for prompt in state["prompts"]:
                    for action in [
                        query_llm.bind(model="gpt-4").with_name("gpt_4_answer"),
                        query_llm.bind(model="o1").with_name("o1_answer"),
                        query_llm.bind(model="claude").with_name("claude_answer"),
                    ]
                        yield SubGraphTask(
                            action=action, # can be a RunnableGraph as well
                            state=state.update(prompt=prompt),
                            inputs={},
                            # stable hash -- up to you to ensure uniqueness
                            application_id=hashlib.sha256(context.application_id + action.name + prompt).hexdigest(),
                            # a few other parameters we might add -- see advanced usage -- failure conditions, etc...
                        )

            def reduce(self, state: State, states: Generator[State, None, None]) -> State:
                all_llm_outputs = []
                for sub_state in states:
                    all_llm_outputs.append(
                        {
                            "output" : sub_state["llm_output"],
                            "model" : sub_state["model"],
                            "prompt" : sub_state["prompt"],
                        }
                    )
                return state.update(all_llm_outputs=all_llm_outputs)

    Note that it can be synchronous *or* asynchronous. Synchronous implementations will use the standard/
    supplied executor. Asynchronous implementations will use asyncio.gather. Note that, while asynchronous
    implementations may implement the tasks as either synchronous or asynchronous generators, synchronous implementations
    can only use synchronous generators. Furthermore, with asynchronous implementations, the generator for reduce
    will be asynchronous as well (regardless of whether your task functions are asynchronous).
    """

    def __init__(self):
        super().__init__()

    def run_and_update(self, state: State, **run_kwargs) -> Tuple[dict, State]:
        """Runs and updates. This is not user-facing, so do not override it.
        This runs all actions in parallel (using the supplied executor, from the context),
        and then reduces the results.

        :param state: Input state
        :param run_kwargs: Additional inputs (runtime inputs)
        :return: The results, updated state tuple. The results are empty, but we may add more in the future.
        """

        def _run_and_update():
            context: ApplicationContext = run_kwargs.get("__context")
            if context is None:
                raise ValueError("This action requires a context to run")
            state_without_internals = state.wipe(
                delete=[item for item in state.keys() if item.startswith("__")]
            )
            task_generator = self.tasks(state_without_internals, context, run_kwargs)

            def execute_task(task):
                return task.run(run_kwargs["__context"])

            with context.parallel_executor_factory() as executor:
                # Directly map the generator to the executor
                results = list(executor.map(execute_task, task_generator))

            def state_generator() -> Generator[Any, None, None]:
                yield from results

            return {}, self.reduce(state_without_internals, state_generator())

        async def _arun_and_update():
            context: ApplicationContext = run_kwargs.get("__context")
            if context is None:
                raise ValueError("This action requires a context to run")
            state_without_internals = state.wipe(
                delete=[item for item in state.keys() if item.startswith("__")]
            )
            task_generator = self.tasks(state_without_internals, context, run_kwargs)

            async def state_generator():
                """This makes it easier on the user -- if they don't have an async generator we can still exhause it
                This way we run through all of the task generators. These correspond to the task generation capabilities above (the map*/task generation stuff)
                """
                all_tasks = await async_utils.arealize(task_generator)
                coroutines = [item.arun(context) for item in all_tasks]
                results = await asyncio.gather(*coroutines)
                # TODO -- yield in order...
                for result in results:
                    yield result

            return {}, await self.reduce(state_without_internals, state_generator())

        if self.is_async():
            return _arun_and_update()  # type: ignore
        return _run_and_update()

    def is_async(self) -> bool:
        """This says whether or not the action is async. Note you have to override this if you have async tasks
        and want to use asyncio.gather on them. Otherwise leave this blank.

        :return: Whether or not the action is async
        """
        return False

    @property
    def inputs(self) -> Union[list[str], tuple[list[str], list[str]]]:
        """Inputs from this -- if you want to override you'll want to call super()
        first so you get these inputs.

        :return: the list of inputs that will populate kwargs.
        """
        return ["__context"]  # TODO -- add any additional input

    @abc.abstractmethod
    def tasks(
        self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
    ) -> SyncOrAsyncGenerator[SubGraphTask]:
        """Creates all tasks that this action will run, given the state/inputs.
        This produces a generator of SubGraphTasks that will be run in parallel.

        :param state: State prior to action's execution
        :param context: Context for the action
        :yield: SubGraphTasks to run
        """
        pass

    @abc.abstractmethod
    def reduce(self, state: State, states: SyncOrAsyncGenerator[State]) -> State:
        """Reduces the states from the tasks into a single state.

        :param states: State outputs from the subtasks
        :return: Reduced state
        """
        pass

    @property
    @abc.abstractmethod
    def writes(self) -> list[str]:
        pass

    @property
    @abc.abstractmethod
    def reads(self) -> list[str]:
        pass


def _cascade_adapter(
    behavior: Union[Literal["cascade"], AdapterType, None],
    adapter: Union[AdapterType, None],
) -> Union[AdapterType, None]:
    """Cascades the desired adapter given the specified behavior (cascade, None, or an adapter).
    If shared_instance is specified (non-null) as well as `cascade`, then the shared instance will be used,
    *if* it is the same instance as the specified adapter.

    This allows us to ensure that sharing, say, persisters will result in the same

    :param behavior:
    :param adapter:
    :param copy_from:
    :return:
    """
    if behavior is None:
        return None
    # TODO -- consider checking this in a cleaner way
    elif behavior == "cascade":
        if hasattr(adapter, "copy"):
            adapter = adapter.copy()
        return adapter
    return behavior


class MapActionsAndStates(TaskBasedParallelAction):
    """Base class to run a cartesian product of actions x states.

    For example, if you want to run the following:

    - n prompts
    - m models

    This will make it easy to do. If you need fine-grained control, you can use the :py:class:`TaskBasedParallelAction`,
    which allows you to specify the tasks individually. If you just want to vary actions/states  (and not both), use
    :py:class:`MapActions` or :py:class:`MapStates` implementations.

    The following shows how to run a set of prompts over a set of models in parallel and return the results.

    .. code-block:: python

        from burr.core import action, state
        from burr.core.parallelism import MapActionsAndStates, RunnableGraph
        from typing import Callable, Generator, List

        @action(reads=["prompt", "model"], writes=["llm_output"])
        def query_llm(state: State, model: str) -> State:
            # TODO -- implement _query_my_llm to call litellm or something
            return state.update(llm_output=_query_my_llm(prompt=state["prompt"], model=model))

        class TestModelsOverPrompts(MapActionsAndStates):

            def actions(self, state: State, context: ApplicationContext, inputs: Dict[str, Any]) -> Generator[Action | Callable | RunnableGraph, None, None]:
                # make sure to add a name to the action
                # This is not necessary for subgraphs, as actions will already have names
                for action in [
                    query_llm.bind(model="gpt-4").with_name("gpt_4_answer"),
                    query_llm.bind(model="o1").with_name("o1_answer"),
                    query_llm.bind(model="claude").with_name("claude_answer"),
                ]:
                    yield action

            def states(self, state: State, context: ApplicationContext, inputs: Dict[str, Any]) -> Generator[State, None, None]:
                for prompt in [
                    "What is the meaning of life?",
                    "What is the airspeed velocity of an unladen swallow?",
                    "What is the best way to cook a steak?",
                ]:
                    yield state.update(prompt=prompt)

            def reduce(self, state: State, states: Generator[State, None, None]) -> State:
                all_llm_outputs = []
                for sub_state in states:
                    all_llm_outputs.append(
                        {
                            "output" : sub_state["llm_output"],
                            "model" : sub_state["model"],
                            "prompt" : sub_state["prompt"],
                        }
                    )
                return state.update(all_llm_outputs=all_llm_outputs)

            @property
            def reads(self) -> list[str]:
                return ["prompts"]

            @property
            def writes(self) -> list[str]:
                return ["all_llm_outputs"]

    """

    @abc.abstractmethod
    def actions(
        self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
    ) -> SyncOrAsyncGenerator[SubgraphType]:
        """Yields actions to run in parallel. These will be merged with the states as a cartesian product.

        :param state: Input state at the time of running the "parent" action.
        :param inputs: Runtime Inputs to the action
        :return: Generator of actions to run
        """
        pass

    @abc.abstractmethod
    def states(
        self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
    ) -> SyncOrAsyncGenerator[State]:
        """Yields states to run in parallel. These will be merged with the actions as a cartesian product.

        :param state: Input state at the time of running the "parent" action.
        :param context: Context for the action
        :param inputs: Runtime Inputs to the action
        :return: Generator of states to run
        """
        pass

    def tasks(
        self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
    ) -> SyncOrAsyncGenerator[SubGraphTask]:
        """Takes the cartesian product of actions and states, creating tasks for each.

        :param state: Input state at the time of running the "parent" action.
        :param context: Context for the action
        :param inputs: Runtime Inputs to the action
        :return: Generator of tasks to run
        """

        def _create_task(key: str, action: Action, substate: State) -> SubGraphTask:
            tracker = _cascade_adapter(self.tracker(), context.tracker)
            state_initializer_behavior = self.state_initializer()
            state_initializer = _cascade_adapter(
                self.state_initializer(), context.state_initializer
            )
            state_persister_behavior = self.state_persister()
            # In this case we want to ensure they share the same instance as they do in the parent
            # This enables us to mirror expected behavior, and is the standard case
            # Specifically, that the persister is the same as the initializer
            if (
                state_initializer_behavior == "cascade"
                and state_persister_behavior == "cascade"
                and context.state_persister is context.state_initializer
            ):
                state_persister = state_initializer
            # In the case that they are not the same, we want to cascade the persister separately
            else:
                state_persister = _cascade_adapter(self.state_persister(), context.state_persister)
            return SubGraphTask(
                graph=RunnableGraph.create(action),
                inputs=inputs,
                state=substate,
                application_id=_stable_app_id_hash(context.app_id, key),
                tracker=tracker,
                state_persister=state_persister,
                state_initializer=state_initializer,
            )

        def _tasks() -> Generator[SubGraphTask, None, None]:
            for i, action in enumerate(self.actions(state, context, inputs)):
                for j, substate in enumerate(self.states(state, context, inputs)):
                    key = f"{i}-{j}"  # this is a stable hash for now but will not handle caching
                    yield _create_task(key, action, substate)

        async def _atasks() -> AsyncGenerator[SubGraphTask, None]:
            action_generator = async_utils.asyncify_generator(self.actions(state, context, inputs))
            state_generator = async_utils.asyncify_generator(self.states(state, context, inputs))
            actions = await async_utils.arealize(action_generator)
            states = await async_utils.arealize(state_generator)
            for i, action in enumerate(actions):
                for j, substate in enumerate(states):
                    key = f"{i}-{j}"
                    yield _create_task(key, action, substate)

        return _atasks() if self.is_async() else _tasks()

    @abc.abstractmethod
    def reduce(self, state: State, states: Generator[State, None, None]) -> State:
        """Reduces the states from the tasks into a single state.

        :param states: State outputs from the subtasks
        :return: Reduced state
        """
        pass

    def state_persister(self, **kwargs) -> StatePersisterBehavior:
        """Persister for the action -- what persister does the sub-application use?

        This can either be:
        - "cascade": inherit from parent
        - None: don't use a persister
        - Object: use the specified persister

        Note this is global -- if you want to override it on a per-subgraph basis, you'll need to use the task-level API.

        :param kwargs: Additional arguments, reserverd for later
        :return: The specified behavior.
        """
        return "cascade"

    def state_initializer(self, **kwargs) -> StateInitializerBehavior:
        """State initializer for the action -- what initializer does the sub-application use?

        This can either be:
        - "cascade": inherit from parent
        - None: don't use an initializer
        - Object: use the specified initializer

        Note this is global -- if you want to override it on a per-subgraph basis, you'll need to use the task-level API.

        :param kwargs: Additional arguments, reserverd for later

        :return: the specified behavior
        """
        return "cascade"

    def tracker(self, **kwargs) -> TrackerBehavior:
        """Tracker for the action -- what tracker does the sub-application use?

        This can either be:
        - "cascade": inherit from parent
        - None: don't use a tracker
        - Object: use the specified tracker

        Note this is global -- if you want to override it on a per-subgraph basis, you'll need to use the task-level API.


        :param kwargs: Additional arguments, reserverd for later
        :return: the specified behavior
        """
        return "cascade"


class MapActions(MapActionsAndStates, abc.ABC):
    """Base class to run a set of actions over the same state. Actions can be functions (decorated with @action),
    action objects, or subdags implemented as :py:class:`RunnableGraph` objects. With this, you can do the following:

    1. Specify the actions to run
    2. Specify the state to run the actions over
    3. Reduce the results into a single state

    This is useful, for example, to run different LLMs over the same set of prompts,

    Here is an example (with some pseudocode) of doing just that:

    .. code-block:: python

        from burr.core import action, state
        from burr.core.parallelism import MapActions, RunnableGraph
        from typing import Callable, Generator, List

        @action(reads=["prompt", "model"], writes=["llm_output"])
        def query_llm(state: State, model: str) -> State:
            # TODO -- implement _query_my_llm to call litellm or something
            return state.update(llm_output=_query_my_llm(prompt=state["prompt"], model=model))

        class TestMultipleModels(MapActions):

            def actions(self, state: State, context: ApplicationContext, inputs: Dict[str, Any]) -> Generator[Action | Callable | RunnableGraph, None, None]:
                # Make sure to add a name to the action if you use bind() with a function,
                # note that these can be different actions, functions, etc...
                # in this case we're using `.bind()` to create multiple actions, but we can use some mix of
                # subgraphs, functions, action objects, etc...
                for action in [
                    query_llm.bind(model="gpt-4").with_name("gpt_4_answer"),
                    query_llm.bind(model="o1").with_name("o1_answer"),
                    query_llm.bind(model="claude").with_name("claude_answer"),
                ]:
                    yield action

            def state(self, state: State, inputs: Dict[str, Any]) -> State:
                return state.update(prompt="What is the meaning of life?")

            def reduce(self, states: Generator[State, None, None]) -> State:
                all_llm_outputs = []
                for state in states:
                    all_llm_outputs.append(state["llm_output"])
                return state.update(all_llm_outputs=all_llm_outputs)

            @property
            def reads(self) -> List[str]:
                return ["prompt"] # we're just running this on a single prompt, for multiple actions

            @property
            def writes(self) -> List[str]:
                return ["all_llm_outputs"]

    """

    @abc.abstractmethod
    def actions(
        self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
    ) -> SyncOrAsyncGenerator[SubgraphType]:
        """Gives all actions to map over, given the state/inputs.

        :param state: State at the time of running the action
        :param inputs: Runtime Inputs to the action
        :param context: Context for the action
        :return: Generator of actions to run
        """

    def state(self, state: State, inputs: Dict[str, Any]):
        """Gives the state for each of the actions.
        By default, this will give out the current state. That said,
        you may want to adjust this -- E.G. to translate state into
        a format the sub-actions would expect.

        :param state: State at the time of running the action
        :param inputs: Runtime inputs to the action
        :return: State for the action
        """
        return state

    def states(
        self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
    ) -> Generator[State, None, None]:
        """Just converts the state into a generator of 1, so we can use the superclass. This is internal."""
        yield self.state(state, inputs)

    @abc.abstractmethod
    def reduce(self, state: State, states: SyncOrAsyncGenerator[State]) -> State:
        """Reduces the task's results into a single state. Runs through all outputs
        and combines them together, to form the final state for the action.

        :param states: State outputs from the subtasks
        :return: Reduced state
        """
        pass


class MapStates(MapActionsAndStates, abc.ABC):
    """Base class to run a single action over a set of states. States are given as
    updates (manipulations) of the action's input state, specified by the `states`
    generator.

    With this, you can do the following:

    1. Specify the states to run
    2. Specify the action to run over all the states
    3. Reduce the results into a single state

    This is useful, for example, to run different prompts over the same LLM,

    Here is an example (with some pseudocode) of doing just that:

    .. code-block:: python

        from burr.core import action, state
        from burr.core.parallelism import MapStates, RunnableGraph
        from typing import Callable, Generator, List

        @action(reads=["prompt"], writes=["llm_output"])
        def query_llm(state: State) -> State:
            return state.update(llm_output=_query_my_llm(prompt=state["prompt"]))

        class TestMultiplePrompts(MapStates):

            def action(self, state: State, inputs: Dict[str, Any]) -> Action | Callable | RunnableGraph:
                # make sure to add a name to the action
                # This is not necessary for subgraphs, as actions will already have names
                return query_llm.with_name("query_llm")

            def states(self, state: State, inputs: Dict[str, Any], context: ApplicationContext) -> Generator[State, None, None]:
                # You could easily have a list_prompts upstream action that writes to "prompts" in state
                # And loop through those
                # This hardcodes for simplicity
                for prompt in [
                    "What is the meaning of life?",
                    "What is the airspeed velocity of an unladen swallow?",
                    "What is the best way to cook a steak?",
                ]:
                    yield state.update(prompt=prompt)

            def reduce(self, state: State, states: Generator[State, None, None]) -> State:
                all_llm_outputs = []
                for sub_state in states:
                    all_llm_outputs.append(sub_state["llm_output"])
                return state.update(all_llm_outputs=all_llm_outputs)

            @property
            def reads(self) -> List[str]:
                return ["prompts"]

            @property
            def writes(self) -> List[str]:
                return ["all_llm_outputs"]
    """

    @abc.abstractmethod
    def states(
        self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
    ) -> SyncOrAsyncGenerator[State]:
        """Generates all states to map over, given the state and inputs.
        Each state will be an update to the input state.

        For instance, you may want to take an input state that has a list field, and expand it
        into a set of states, each with a different value from the list.

        For example:

        .. code-block:: python

            def states(self, state: State, context: ApplicationContext, inputs: Dict[str, Any]) -> Generator[State, None, None]:
                for item in state["multiple_fields"]:
                    yield state.update(individual_field=item)

        :param state: Initial state
        :param context: Context for the action
        :param inputs: Runtime inputs to the action
        :return: Generator of states to run
        """
        pass

    @abc.abstractmethod
    def action(self, state: State, inputs: Dict[str, Any]) -> SubgraphType:
        """The single action to apply to each state.
        This can be a function (decorated with `@action`, action object, or subdag).

        :param state: State to run the action over
        :param inputs: Runtime inputs to the action
        :return: Action to run
        """
        pass

    def actions(
        self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
    ) -> SyncOrAsyncGenerator[SubgraphType]:
        """Maps the action over each state generated by the `states` method.
        Internally used, do not implement."""
        yield self.action(state, inputs)

    @abc.abstractmethod
    def reduce(self, state: State, results: SyncOrAsyncGenerator[State]) -> State:
        """Reduces the task's results

        :param results:
        :return:
        """
        pass


class PassThroughMapActionsAndStates(MapActionsAndStates):
    def __init__(
        self,
        action: Union[
            SubgraphType,
            List[SubgraphType],
            Callable[
                [State, ApplicationContext, Dict[str, Any]], SyncOrAsyncGenerator[SubgraphType]
            ],
        ],
        state: Callable[[State, ApplicationContext, Dict[str, Any]], SyncOrAsyncGenerator[State]],
        reducer: Callable[[State, SyncOrAsyncGenerator[State]], State],
        reads: List[str],
        writes: List[str],
        inputs: List[str],
    ):
        super().__init__()
        self._action_or_generator = action
        self._state_or_generator = state
        self._reducer = reducer
        self._reads = reads
        self._writes = writes
        self._inputs = inputs

    def actions(
        self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
    ) -> SyncOrAsyncGenerator[SubgraphType]:
        if isinstance(self._action_or_generator, list):
            for action in self._action_or_generator:
                yield action
            return
        if isinstance(self._action_or_generator, SubgraphType):
            yield self._action_or_generator
        else:
            gen = self._action_or_generator(state, context, inputs)
            if inspect.isasyncgen(gen):

                async def gen():
                    async for item in self._action_or_generator(state, context, inputs):
                        yield item

                return gen()
            else:
                yield from self._action_or_generator(state, context, inputs)

    def states(
        self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
    ) -> Generator[State, None, None]:
        gen = self._state_or_generator(state, context, inputs)
        if isinstance(gen, State):
            yield gen
        if inspect.isasyncgen(gen):

            async def gen():
                async for item in self._state_or_generator(state, context, inputs):
                    yield item

            return gen()
        else:
            yield from gen

    def reduce(self, state: State, states: SyncOrAsyncGenerator[State]) -> State:
        return self._reducer(state, states)

    @property
    def writes(self) -> list[str]:
        return self._writes

    @property
    def reads(self) -> list[str]:
        return self._reads


def map_reduce_action(
    # action: Optional[SubgraphType]=None,
    action: Union[
        SubgraphType,
        List[SubgraphType],
        Callable[
            [State, ApplicationContext, Dict[str, Any]],
            SyncOrAsyncGeneratorOrItemOrList[SubgraphType],
        ],
    ],
    state: Callable[
        [State, ApplicationContext, Dict[str, Any]], SyncOrAsyncGeneratorOrItemOrList[State]
    ],
    reducer: Callable[[State, SyncOrAsyncGenerator[State]], State],
    reads: List[str],
    writes: List[str],
    inputs: List[str],
):
    """Experimental API for creating a map-reduce action easily. We'll be improving this."""
    return PassThroughMapActionsAndStates(
        action=action, state=state, reducer=reducer, reads=reads, writes=writes, inputs=inputs
    )


---
File: /burr/burr/core/persistence.py
---

import abc
import datetime
import json
import sqlite3
from abc import ABCMeta
from collections import defaultdict
from typing import Any, Dict, Literal, Optional, TypedDict

from burr.common.types import BaseCopyable
from burr.core import Action
from burr.core.state import State, logger
from burr.lifecycle import PostRunStepHook, PostRunStepHookAsync

try:
    from typing import Self
except ImportError:
    Self = None


class PersistedStateData(TypedDict):
    partition_key: str
    app_id: str
    sequence_id: int
    position: str
    state: State
    created_at: str
    status: str


class BaseStateLoader(abc.ABC):
    """Base class for state initialization. This goes together with a BaseStateSaver to form the
    database for your application."""

    @abc.abstractmethod
    def load(
        self, partition_key: str, app_id: Optional[str], sequence_id: Optional[int] = None, **kwargs
    ) -> Optional[PersistedStateData]:
        """Loads the state for a given app_id

        :param partition_key: the partition key. Note this could be None, but it's up to the persistor to whether
            that is a valid value it can handle.
        :param app_id: the identifier for the app instance being recorded.
        :param sequence_id: optional, the state corresponding to a specific point in time. Specifically state at the
            end of the action with this sequence_id. If sequence_id is not provided, persistor should return the state
            from the latest fully completed action.
        :return: PersistedStateData or None
        """
        pass

    @abc.abstractmethod
    def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        """Returns list of app IDs for a given primary key"""
        pass

    def is_async(self) -> bool:
        return False


class AsyncBaseStateLoader(abc.ABC):
    """Asynchronous base class for state initialization. This goes together with a AsyncBaseStateSaver
    to form the database for your application."""

    @abc.abstractmethod
    async def load(
        self, partition_key: str, app_id: Optional[str], sequence_id: Optional[int] = None, **kwargs
    ) -> Optional[PersistedStateData]:
        """Loads the state for a given app_id

        :param partition_key: the partition key. Note this could be None, but it's up to the persistor to whether
            that is a valid value it can handle.
        :param app_id: the identifier for the app instance being recorded.
        :param sequence_id: optional, the state corresponding to a specific point in time. Specifically state at the
            end of the action with this sequence_id. If sequence_id is not provided, persistor should return the state
            from the latest fully completed action.
        :return: PersistedStateData or None
        """
        pass

    @abc.abstractmethod
    async def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        """Returns list of app IDs for a given primary key"""
        pass

    def is_async(self) -> bool:
        return True


class BaseStateSaver(abc.ABC):
    """Base class for state writing. This goes together with a BaseStateLoader to form the
    database for your application.
    """

    def initialize(self):
        """Initializes the app for saving, set up any databases, etc.. you want to here."""
        pass

    def is_initialized(self) -> bool:
        """Check if the persister has been initialized appropriately."""
        raise NotImplementedError("Implement this method in your subclass if you need to.")

    @abc.abstractmethod
    def save(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int,
        position: str,
        state: State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        """Saves the state for a given app_id, sequence_id, position

        (PK, App_id, sequence_id, position) are a unique identifier for the state. Why not just
        (PK, App_id, sequence_id)? Because we're over-engineering this here. We're always going to have
        a position so might as well make it a quadruple.

        :param partition_key: the partition key. Note this could be None, but it's up to the persistor to whether
            that is a valid value it can handle.
        :param app_id: Appliaction UID to write with
        :param sequence_id: Sequence ID of the last executed step
        :param position: The action name that was implemented
        :param state: The current state of the application
        :param status: The status of this state, either "completed" or "failed". If "failed" the state is what it was
            before the action was applied.
        """
        pass

    def is_async(self) -> bool:
        return False


class AsyncBaseStateSaver(abc.ABC):
    """Asynchronous base class for state writing. This goes together with a AsyncBaseStateLoader
    to form the database for your application.
    """

    async def initialize(self):
        """Initializes the app for saving, set up any databases, etc.. you want to here."""
        pass

    async def is_initialized(self) -> bool:
        """Check if the persister has been initialized appropriately."""
        raise NotImplementedError("Implement this method in your subclass if you need to.")

    @abc.abstractmethod
    async def save(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int,
        position: str,
        state: State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        """Saves the state for a given app_id, sequence_id, position

        (PK, App_id, sequence_id, position) are a unique identifier for the state. Why not just
        (PK, App_id, sequence_id)? Because we're over-engineering this here. We're always going to have
        a position so might as well make it a quadruple.

        :param partition_key: the partition key. Note this could be None, but it's up to the persistor to whether
            that is a valid value it can handle.
        :param app_id: Appliaction UID to write with
        :param sequence_id: Sequence ID of the last executed step
        :param position: The action name that was implemented
        :param state: The current state of the application
        :param status: The status of this state, either "completed" or "failed". If "failed" the state is what it was
            before the action was applied.
        """
        pass

    def is_async(self) -> bool:
        return True


class BaseStatePersister(BaseStateLoader, BaseStateSaver, metaclass=ABCMeta):
    """Utility interface for a state reader/writer. This both persists and initializes state.
    Extend this class if you want an easy way to implement custom state storage.
    """


class AsyncBaseStatePersister(AsyncBaseStateLoader, AsyncBaseStateSaver, metaclass=ABCMeta):
    """Utility interface for an asynchronous state reader/writer. This both persists and initializes state.
    Extend this class if you want an easy way to implement custom state storage.
    """


class PersisterHook(PostRunStepHook):
    """Wrapper class for bridging the persistence interface with lifecycle hooks. This is used internally."""

    def __init__(self, persister: BaseStateSaver):
        self.persister = persister

    def post_run_step(
        self,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        result: Optional[Dict[str, Any]],
        exception: Exception,
        **future_kwargs: Any,
    ):
        if exception is None:
            self.persister.save(partition_key, app_id, sequence_id, action.name, state, "completed")
        else:
            self.persister.save(partition_key, app_id, sequence_id, action.name, state, "failed")


class PersisterHookAsync(PostRunStepHookAsync):
    """Wrapper class for bridging the persistence interface with asynchronous lifecycle hooks. This is used internally."""

    def __init__(self, persister: AsyncBaseStateSaver):
        self.persister = persister

    async def post_run_step(
        self,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        result: Optional[Dict[str, Any]],
        exception: Exception,
        **future_kwargs: Any,
    ):
        if exception is None:
            await self.persister.save(
                partition_key, app_id, sequence_id, action.name, state, "completed"
            )
        else:
            await self.persister.save(
                partition_key, app_id, sequence_id, action.name, state, "failed"
            )


class DevNullPersister(BaseStatePersister):
    """Does nothing, do not use this. This is for testing only."""

    def load(
        self, partition_key: str, app_id: Optional[str], sequence_id: Optional[int] = None, **kwargs
    ) -> Optional[PersistedStateData]:
        return None

    def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        return []

    def save(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int,
        position: str,
        state: State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        return


class AsyncDevNullPersister(AsyncBaseStatePersister):
    """Does nothing asynchronously, do not use this. This is for testing only."""

    async def load(
        self, partition_key: str, app_id: Optional[str], sequence_id: Optional[int] = None, **kwargs
    ) -> Optional[PersistedStateData]:
        return None

    async def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        return []

    async def save(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int,
        position: str,
        state: State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        return


class SQLitePersister(BaseStatePersister, BaseCopyable):
    """Class for SQLite persistence of state. This is a simple implementation."""

    @classmethod
    def from_config(cls, config: dict) -> "SQLitePersister":
        """Creates a new instance of the SQLitePersister from a configuration dictionary.

        The config key:value pair needed are:
        db_path: str,
        table_name: str,
        serde_kwargs: dict,
        connect_kwargs: dict,
        """
        return cls.from_values(**config)

    @classmethod
    def from_values(
        cls,
        db_path: str,
        table_name: str = "burr_state",
        serde_kwargs: dict = None,
        connect_kwargs: dict = None,
    ) -> "SQLitePersister":
        """Creates a new instance of the SQLitePersister from passed in values.

        :param db_path: the path the DB will be stored.
        :param table_name: the table name to store things under.
        :param serde_kwargs: kwargs for state serialization/deserialization.
        :param connect_kwargs: kwargs to pass to the aiosqlite.connect method.
        :return: async sqlite persister instance with an open connection. You are responsible
            for closing the connection yourself.
        """
        connection = sqlite3.connect(
            db_path, **connect_kwargs if connect_kwargs is not None else {}
        )
        return cls(db_path, table_name, serde_kwargs, connection=connection)

    def copy(self) -> "Self":
        return SQLitePersister(
            db_path=self.db_path,
            table_name=self.table_name,
            serde_kwargs=self.serde_kwargs,
            connect_kwargs=self._connect_kwargs,
        )

    PARTITION_KEY_DEFAULT = ""

    def __init__(
        self,
        db_path: str,
        table_name: str = "burr_state",
        serde_kwargs: dict = None,
        connect_kwargs: dict = None,
        connection: sqlite3.Connection = None,
    ):
        """Constructor

        :param db_path: the path the DB will be stored.
        :param table_name: the table name to store things under.
        :param serde_kwargs: kwargs for state serialization/deserialization.
        :param connect_kwargs: kwargs to pass to the sqlite3.connect method.
            Use check_same_thread=False to enable use ina  multithreaded context
        :param connection: ability to instantiate a db outside of the persister and pass
            in the connection to be used.
        """
        self.db_path = db_path
        self.table_name = table_name

        # Here for backwards compatibility, the original idea was to create the connection
        # but later we realized it also makes sense to pass the connection to the class and
        # handle cleanup manually.
        if connection is None:
            self.connection = sqlite3.connect(
                db_path, **connect_kwargs if connect_kwargs is not None else {}
            )
        else:
            self.connection = connection

        self.serde_kwargs = serde_kwargs or {}
        self._initialized = False
        self._connect_kwargs = connect_kwargs

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.connection.close()
        return False

    def set_serde_kwargs(self, serde_kwargs: dict):
        """Sets the serde_kwargs for the persister."""
        self.serde_kwargs = serde_kwargs

    def create_table_if_not_exists(self, table_name: str):
        """Helper function to create the table where things are stored if it doesn't exist."""
        cursor = self.connection.cursor()
        cursor.execute(
            f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                partition_key TEXT DEFAULT '{SQLitePersister.PARTITION_KEY_DEFAULT}',
                app_id TEXT NOT NULL,
                sequence_id INTEGER NOT NULL,
                position TEXT NOT NULL,
                status TEXT NOT NULL,
                state TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                PRIMARY KEY (partition_key, app_id, sequence_id, position)
            )"""
        )
        cursor.execute(
            f"""
            CREATE INDEX IF NOT EXISTS {table_name}_created_at_index ON {table_name} (created_at);
        """
        )
        self.connection.commit()

    def initialize(self):
        """Creates the table if it doesn't exist"""
        # Usage
        self.create_table_if_not_exists(self.table_name)
        self._initialized = True

    def is_initialized(self) -> bool:
        """This checks to see if the table has been created in the database or not.
        It defaults to using the initialized field, else queries the database to see if the table exists.
        It then sets the initialized field to True if the table exists.
        """
        if self._initialized:
            return True
        cursor = self.connection.cursor()
        cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name=?", (self.table_name,)
        )
        self._initialized = cursor.fetchone() is not None
        return self._initialized

    def list_app_ids(self, partition_key: Optional[str], **kwargs) -> list[str]:
        partition_key = (
            partition_key if partition_key is not None else SQLitePersister.PARTITION_KEY_DEFAULT
        )

        cursor = self.connection.cursor()
        cursor.execute(
            f"SELECT DISTINCT app_id FROM {self.table_name} "
            f"WHERE partition_key = ? "
            f"ORDER BY created_at DESC",
            (partition_key,),
        )
        app_ids = [row[0] for row in cursor.fetchall()]
        return app_ids

    def load(
        self,
        partition_key: Optional[str],
        app_id: Optional[str],
        sequence_id: Optional[int] = None,
        **kwargs,
    ) -> Optional[PersistedStateData]:
        """Loads state for a given partition id.

        Depending on the parameters, this will return the last thing written, the last thing written for a given app_id,
        or a specific sequence_id for a given app_id.

        :param partition_key:
        :param app_id:
        :param sequence_id:
        :return:
        """
        partition_key = (
            partition_key if partition_key is not None else SQLitePersister.PARTITION_KEY_DEFAULT
        )
        logger.debug("Loading %s, %s, %s", partition_key, app_id, sequence_id)
        cursor = self.connection.cursor()
        if app_id is None:
            # get latest for all app_ids
            cursor.execute(
                f"SELECT position, state, sequence_id, app_id, created_at, status FROM {self.table_name} "
                f"WHERE partition_key = ? "
                f"ORDER BY CREATED_AT DESC LIMIT 1",
                (partition_key,),
            )
        elif sequence_id is None:
            cursor.execute(
                f"SELECT position, state, sequence_id, app_id, created_at, status FROM {self.table_name} "
                f"WHERE partition_key = ? AND app_id = ? "
                f"ORDER BY sequence_id DESC LIMIT 1",
                (partition_key, app_id),
            )
        else:
            cursor.execute(
                f"SELECT position, state, sequence_id, app_id, created_at, status FROM {self.table_name} "
                f"WHERE partition_key = ? AND app_id = ? AND sequence_id = ?",
                (partition_key, app_id, sequence_id),
            )
        row = cursor.fetchone()
        if row is None:
            return None
        _state = State.deserialize(json.loads(row[1]), **self.serde_kwargs)
        return {
            "partition_key": partition_key,
            "app_id": row[3],
            "sequence_id": row[2],
            "position": row[0],
            "state": _state,
            "created_at": row[4],
            "status": row[5],
        }

    def save(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int,
        position: str,
        state: State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        """
        Saves the state for a given app_id, sequence_id, and position.

        This method connects to the SQLite database, converts the state to a JSON string, and inserts a new record
        into the table with the provided partition_key, app_id, sequence_id, position, and state. After the operation,
        it commits the changes and closes the connection to the database.

        :param partition_key: The partition key. This could be None, but it's up to the persistor to whether
            that is a valid value it can handle.
        :param app_id: The identifier for the app instance being recorded.
        :param sequence_id: The state corresponding to a specific point in time.
        :param position: The position in the sequence of states.
        :param state: The state to be saved, an instance of the State class.
        :param status: The status of this state, either "completed" or "failed". If "failed" the state is what it was
            before the action was applied.
        :return: None
        """
        logger.debug(
            "saving %s, %s, %s, %s, %s, %s",
            partition_key,
            app_id,
            sequence_id,
            position,
            state,
            status,
        )
        partition_key = (
            partition_key if partition_key is not None else SQLitePersister.PARTITION_KEY_DEFAULT
        )
        cursor = self.connection.cursor()
        json_state = json.dumps(state.serialize(**self.serde_kwargs))
        cursor.execute(
            f"INSERT INTO {self.table_name} (partition_key, app_id, sequence_id, position, state, status) "
            f"VALUES (?, ?, ?, ?, ?, ?)",
            (partition_key, app_id, sequence_id, position, json_state, status),
        )
        self.connection.commit()

    def cleanup(self):
        """Closes the connection to the database."""
        self.connection.close()

    def __del__(self):
        # This should be deprecated -- using __del__ is unreliable for closing connections to db's;
        # the preferred way should be for the user to use a context manager or use the `.cleanup()`
        # method within a REST API framework.

        # closes connection at end when things are being shutdown.
        self.connection.close()

    def __getstate__(self):
        return {key: value for key, value in self.__dict__.items() if key != "connection"}

    def __setstate__(self, state):
        for key, value in state.items():
            setattr(self, key, value)
        self.connection = sqlite3.connect(
            self.db_path, **self._connect_kwargs if self._connect_kwargs is not None else {}
        )


class InMemoryPersister(BaseStatePersister):
    """In-memory persister for testing purposes. This is not recommended for production use."""

    def __init__(self):
        self._storage = defaultdict(lambda: defaultdict(list))

    def load(
        self, partition_key: str, app_id: Optional[str], sequence_id: Optional[int] = None, **kwargs
    ) -> Optional[PersistedStateData]:
        # If no app_id provided, return None
        if app_id is None:
            return None

        if not (states := self._storage[partition_key][app_id]):
            return None

        if sequence_id is None:
            return states[-1]

        # Find states matching the specific sequence_id
        matching_states = [state for state in states if state["sequence_id"] == sequence_id]

        # Return the latest state for this sequence_id, if exists
        return matching_states[-1] if matching_states else None

    def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        return list(self._storage[partition_key].keys())

    def save(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int,
        position: str,
        state: State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        # Create a PersistedStateData entry
        persisted_state: PersistedStateData = {
            "partition_key": partition_key or "",
            "app_id": app_id,
            "sequence_id": sequence_id,
            "position": position,
            "state": state,
            "created_at": datetime.datetime.now().isoformat(),
            "status": status,
        }

        # Store the state
        self._storage[partition_key][app_id].append(persisted_state)


class AsyncInMemoryPersister(AsyncBaseStatePersister):
    """Sync in-memory persister for testing purposes. This is not recommended for production use."""

    def __init__(self):
        self._storage = defaultdict(lambda: defaultdict(list))

    async def load(
        self, partition_key: str, app_id: Optional[str], sequence_id: Optional[int] = None, **kwargs
    ) -> Optional[PersistedStateData]:
        # If no app_id provided, return None
        if app_id is None:
            return None

        if not (states := self._storage[partition_key][app_id]):
            return None

        if sequence_id is None:
            return states[-1]

        # Find states matching the specific sequence_id
        matching_states = [state for state in states if state["sequence_id"] == sequence_id]

        # Return the latest state for this sequence_id, if exists
        return matching_states[-1] if matching_states else None

    async def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        return list(self._storage[partition_key].keys())

    async def save(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int,
        position: str,
        state: State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        # Create a PersistedStateData entry
        persisted_state: PersistedStateData = {
            "partition_key": partition_key or "",
            "app_id": app_id,
            "sequence_id": sequence_id,
            "position": position,
            "state": state,
            "created_at": datetime.datetime.now().isoformat(),
            "status": status,
        }

        # Store the state
        self._storage[partition_key][app_id].append(persisted_state)


SQLLitePersister = SQLitePersister

if __name__ == "__main__":
    s = SQLitePersister(db_path=".SQLite.db", table_name="test1")
    s.initialize()
    s.save("pk", "app_id", 1, "pos", State({"a": 1, "b": 2}), "completed")
    print(s.list_app_ids("pk"))
    print(s.load("pk", "app_id"))



---
File: /burr/burr/core/serde.py
---

from functools import singledispatch
from typing import Any, Union

KEY = "__burr_serde__"


class StringDispatch:
    """Class to capture how to deserialize something.

    We register a key with a deserializer function. It's like single dispatch
    but based on a string key value.

    Example usage:

    .. code-block:: python

        from burr.core import serde

        @serde.deserializer.register("pickle")
        def deserialize_pickle(value: dict, pickle_kwargs: dict = None, **kwargs) -> cls:
            if pickle_kwargs is None:
                pickle_kwargs = {}
            return pickle.loads(value["value"], **pickle_kwargs)

    What this does is register the function `deserialize_pickle` with the key "pickle".
    This should mirror the appropriate serialization function - which is what sets the key value
    to match the deserializer function against.

    Notice that this namespaces its kwargs. This is important because we don't want to have
    a collision with other kwargs that might be passed in.
    """

    def __init__(self):
        self.func_map = {}

    def register(self, key):
        def decorator(func):
            self.func_map[key] = func
            return func

        return decorator

    def call(self, key, *args, **kwargs):
        if key in self.func_map:
            return self.func_map[key](*args, **kwargs)
        else:
            raise ValueError(f"No function registered for key: {key}")


deserializer = StringDispatch()


def deserialize(value: Any, **kwargs) -> Any:
    """Main function to deserialize a value.

    Looks for a key in the value if it's a dictionary and calls the appropriate deserializer function.
    """
    if isinstance(value, dict):
        class_to_instantiate = value.get(KEY, None)
        if class_to_instantiate is not None:
            return deserializer.call(class_to_instantiate, value, **kwargs)
        else:
            return {k: deserialize(v, **kwargs) for k, v in value.items()}
    elif isinstance(value, list):
        return [deserialize(v, **kwargs) for v in value]
    else:
        return value


@singledispatch
def serialize(value, **kwargs) -> Any:
    """This is the default implementation for serializing a value.

    All other implementations should be registered with the `@serialize.register` decorator.

    Each function should output a dictionary, and include the `KEY` & value to use for deserialization.

    :param value: The value to serialize
    :param kwargs: Any additional keyword arguments. Each implementation should namespace their kwargs.
    :return: A dictionary representation of the value
    """
    if value is None:
        return None
    return str(value)


@serialize.register(str)
@serialize.register(int)
@serialize.register(float)
@serialize.register(bool)
def serialize_primitive(value, **kwargs) -> Union[str, int, float, bool]:
    return value


@serialize.register(dict)
def serialize_dict(value: dict, **kwargs) -> dict[str, Any]:
    return {k: serialize(v, **kwargs) for k, v in value.items()}


@serialize.register(list)
def serialize_list(value: list, **kwargs) -> list[Any]:
    return [serialize(v, **kwargs) for v in value]



---
File: /burr/burr/core/state.py
---

import abc
import copy
import dataclasses
import importlib
import inspect
import logging
from functools import cached_property
from typing import Any, Callable, Dict, Generic, Iterator, Mapping, Optional, TypeVar, Union

from burr.core import serde
from burr.core.typing import DictBasedTypingSystem, TypingSystem

logger = logging.getLogger(__name__)

FIELD_SERIALIZATION = {}


def register_field_serde(field_name: str, serializer: Callable, deserializer: Callable):
    """Registers a custom serializer + deserializer for a field globally.

    This is useful for really controlling how a field is serialized and deserialized for
    tracking / persistence.

    .. code-block:: python

        def my_field_serializer(value: MyType, **kwargs) -> dict:
            serde_value = _do_something_to_serialize(value)
            return {"value": serde_value}

        def my_field_deserializer(value: dict, **kwargs) -> MyType:
            serde_value = value["value"]
            return _do_something_to_deserialize(serde_value)

        register_field_serde("my_field", my_field_serializer, my_field_deserializer)

    :param field_name: The name of the field to register the serializer for.
    :param serializer: A function that takes the field value and returns a JSON serializable object.
    :param deserializer: A function that takes the JSON serializable object and returns the field value.
    """
    # assert that the serializer has **kwargs argument; it also needs to return a dict but we can't check that.
    # def name(value: Any, **kwargs) -> dict
    serializer_sig = inspect.signature(serializer)
    if not any(param.kind == param.VAR_KEYWORD for param in serializer_sig.parameters.values()):
        raise ValueError(f"Serializer for [{field_name}] must have **kwargs argument.")

    # assert that the deserializer has **kwargs argument; it also needs to return a dict but we can't check that.
    deserializer_sig = inspect.signature(deserializer)
    if not any(param.kind == param.VAR_KEYWORD for param in deserializer_sig.parameters.values()):
        raise ValueError(f"Deserializer for [{field_name}] must have **kwargs argument.")

    FIELD_SERIALIZATION[field_name] = (serializer, deserializer)


class StateDelta(abc.ABC):
    """Represents a delta operation for state. This represents a transaction.
    Note it has the ability to mutate state in-place, but will be layered behind an immutable
    state object."""

    @classmethod
    @abc.abstractmethod
    def name(cls) -> str:
        """Unique name of this operation for ser/deser"""
        pass

    def serialize(self) -> dict:
        """Converts the state delta to a JSON object"""
        if not dataclasses.is_dataclass(self):
            raise TypeError("serialize method is only supported for dataclass instances")
        return dataclasses.asdict(self)

    def base_serialize(self) -> dict:
        """Converts the state delta to a JSON object"""
        return {"name": self.name(), "operation": self.serialize()}

    def validate(self, input_state: Dict[str, Any]):
        """Validates the input state given the state delta. This is a no-op by default, as most operations are valid"""
        pass

    @classmethod
    def deserialize(cls, json_dict: dict) -> "StateDelta":
        """Converts a JSON object to a state delta"""
        if not dataclasses.is_dataclass(cls):
            raise TypeError("deserialize method is only supported for dataclass types")
        return cls(**json_dict)  # Assumes all fields in the dataclass match keys in json_dict

    def base_deserialize(self, json_dict: dict) -> "StateDelta":
        """Converts a JSON object to a state delta"""
        return self.deserialize(json_dict)

    @abc.abstractmethod
    def reads(self) -> list[str]:
        """Returns the keys that this state delta reads"""
        pass

    @abc.abstractmethod
    def writes(self) -> list[str]:
        """Returns the keys that this state delta writes"""
        pass

    @abc.abstractmethod
    def apply_mutate(self, inputs: dict):
        """Applies the state delta to the inputs"""
        pass


@dataclasses.dataclass
class SetFields(StateDelta):
    """State delta that sets fields in the state"""

    values: Mapping[str, Any]

    @classmethod
    def name(cls) -> str:
        return "set"

    def reads(self) -> list[str]:
        return list(self.values.keys())

    def writes(self) -> list[str]:
        return list(self.values.keys())

    def apply_mutate(self, inputs: dict):
        inputs.update(self.values)


@dataclasses.dataclass
class AppendFields(StateDelta):
    """State delta that appends to fields in the state"""

    values: Mapping[str, Any]

    @classmethod
    def name(cls) -> str:
        return "append"

    def reads(self) -> list[str]:
        return list(self.values.keys())

    def writes(self) -> list[str]:
        return list(self.values.keys())

    def apply_mutate(self, inputs: dict):
        for key, value in self.values.items():
            if key not in inputs:
                inputs[key] = []
            if not isinstance(inputs[key], list):
                raise ValueError(f"Cannot append to non-list value {key}={inputs[key]}")
            inputs[key].append(value)

    def validate(self, input_state: Dict[str, Any]):
        incorrect_types = {}
        for write_key in self.writes():
            if write_key in input_state and not hasattr(input_state[write_key], "append"):
                incorrect_types[write_key] = type(input_state[write_key])
        if incorrect_types:
            raise ValueError(
                f"Cannot append to non-appendable values: {incorrect_types}. "
                f"Please ensure that all fields are list-like."
            )


@dataclasses.dataclass
class ExtendFields(StateDelta):
    """State delta that extends fields in the state"""

    values: Mapping[str, list[Any]]

    @classmethod
    def name(cls) -> str:
        return "extend"

    def reads(self) -> list[str]:
        return list(self.values.keys())

    def writes(self) -> list[str]:
        return list(self.values.keys())

    def apply_mutate(self, inputs: dict):
        for key, value in self.values.items():
            if key not in inputs:
                inputs[key] = []
            if not isinstance(inputs[key], list):
                raise ValueError(f"Cannot extend non-list value {key}={inputs[key]}")
            inputs[key].extend(value)

    def validate(self, input_state: Dict[str, Any]):
        incorrect_types = {}
        for write_key in self.writes():
            if write_key in input_state and not hasattr(input_state[write_key], "extend"):
                incorrect_types[write_key] = type(input_state[write_key])
        if incorrect_types:
            raise ValueError(
                f"Cannot extend non-extendable values: {incorrect_types}. "
                f"Please ensure that all fields are list-like."
            )


@dataclasses.dataclass
class IncrementFields(StateDelta):
    values: Mapping[str, int]

    @classmethod
    def name(cls) -> str:
        return "increment"

    def reads(self) -> list[str]:
        return list(self.values.keys())

    def writes(self) -> list[str]:
        return list(self.values.keys())

    def validate(self, input_state: Dict[str, Any]):
        incorrect_types = {}
        for write_key in self.writes():
            if write_key in input_state and not isinstance(input_state[write_key], int):
                incorrect_types[write_key] = type(input_state[write_key])
        if incorrect_types:
            raise ValueError(
                f"Cannot increment non-integer values: {incorrect_types}. "
                f"Please ensure that all fields are integers."
            )

    def apply_mutate(self, inputs: dict):
        for key, value in self.values.items():
            if key not in inputs:
                inputs[key] = value
            else:
                inputs[key] += value


@dataclasses.dataclass
class DeleteField(StateDelta):
    """State delta that deletes fields from the state"""

    keys: list[str]

    @classmethod
    def name(cls) -> str:
        return "delete"

    def reads(self) -> list[str]:
        return list(self.keys)

    def writes(self) -> list[str]:
        return []

    def apply_mutate(self, inputs: dict):
        for key in self.keys:
            inputs.pop(key, None)


StateType = TypeVar("StateType", bound=Union[Dict[str, Any], Any])
AssignedStateType = TypeVar("AssignedStateType")


class State(Mapping, Generic[StateType]):
    """An immutable state object. This is the only way to interact with state in Burr."""

    def __init__(
        self,
        initial_values: Optional[Dict[str, Any]] = None,
        typing_system: Optional[TypingSystem[StateType]] = None,
    ):
        if initial_values is None:
            initial_values = dict()
        self._typing_system = (
            typing_system if typing_system is not None else DictBasedTypingSystem()  # type: ignore
        )
        self._state = initial_values

    @property
    def typing_system(self) -> TypingSystem[StateType]:
        return self._typing_system

    def with_typing_system(
        self, typing_system: TypingSystem[AssignedStateType]
    ) -> "State[AssignedStateType]":
        """Copies state with a specific typing system"""
        return State(self._state, typing_system=typing_system)

    @cached_property
    def data(self) -> StateType:
        return self._typing_system.construct_data(self)  # type: ignore

    def apply_operation(self, operation: StateDelta) -> "State[StateType]":
        """Applies a given operation to the state, returning a new state"""

        new_state = copy.copy(self._state)  # TODO -- restrict to just the read keys
        for field in operation.reads():
            # This ensures we only copy the fields that are read by value
            # and copy the others by value
            # TODO -- make this more efficient when we have immutable transactions
            # with event-based history: https://github.com/DAGWorks-Inc/burr/issues/33
            if field in new_state:
                # currently the reads() includes optional fields
                # We should clean that up, but this is an internal API so not worried now
                new_state[field] = copy.deepcopy(new_state[field])
        operation.validate(new_state)
        operation.apply_mutate(
            new_state
        )  # todo -- validate that the write keys are the only different ones
        return State(new_state, typing_system=self._typing_system)

    def get_all(self) -> Dict[str, Any]:
        """Returns the entire state, realize as a dictionary. This is a copy."""
        return dict(self)

    def serialize(self, **kwargs) -> dict:
        """Converts the state to a JSON serializable object"""
        _dict = self.get_all()

        def _serialize(k, v, **extrakwargs) -> Union[dict, str]:
            """chooses the correct serde function for the given key and calls it"""
            if k in FIELD_SERIALIZATION:
                result = FIELD_SERIALIZATION[k][0](v, **extrakwargs)
                if not isinstance(result, dict):
                    raise ValueError(
                        f"Field serde for {k} must return a dict,"
                        f" but {FIELD_SERIALIZATION[k][0].__name__} returned {type(result)} ({str(result)[0:10]})."
                    )
                return result
            return serde.serialize(v, **extrakwargs)

        return {k: _serialize(k, v, **kwargs) for k, v in _dict.items()}

    @classmethod
    def deserialize(cls, json_dict: dict, **kwargs) -> "State[StateType]":
        """Converts a dictionary representing a JSON object back into a state"""

        def _deserialize(k, v: Union[str, dict], **extrakwargs) -> Callable:
            """chooses the correct serde function for the given key and calls it"""
            if k in FIELD_SERIALIZATION:
                return FIELD_SERIALIZATION[k][1](v, **extrakwargs)
            return serde.deserialize(v, **extrakwargs)

        return State({k: _deserialize(k, v, **kwargs) for k, v in json_dict.items()})

    def update(self, **updates: Any) -> "State[StateType]":
        """Updates the state with a set of key-value pairs
        Does an upsert operation (if the keys exist their value will be overwritten,
        otherwise they will be created)

        .. code-block:: python

            state = State({"a": 1})
            state.update(a=2, b=3)  # State({"a": 2, "b": 3})

        :param updates: Updates to apply
        :return: A new state with the updates applied
        """
        return self.apply_operation(SetFields(updates))

    def append(self, **updates: Any) -> "State[StateType]":
        """Appends to the state with a set of key-value pairs. Each one
        must correspond to a list-like object, or an error will be raised.

        This is an upsert operation, meaning that if the key does not
        exist, a new list will be created with the value in it.

        .. code-block:: python

            state = State({"a": [1]})
            state.append(a=2)  # State({"a": [1, 2]})

        :param updates: updates to apply
        :return: new state object
        """

        return self.apply_operation(AppendFields(updates))

    def extend(self, **updates: list[Any]) -> "State[StateType]":
        """Extends the state with a set of key-value pairs. Each one
        must correspond to a list-like object, or an error will be raised.

        This is an upsert operation, meaning that if the key does not
        exist, a new list will be created and extended with the values.

        .. code-block:: python

            state = State({"a": [1]})
            state.extend(a=[2, 3])  # State({"a": [1, 2, 3]})

        :param updates: updates to apply
        :return: new state object
        """

        return self.apply_operation(ExtendFields(updates))

    def increment(self, **updates: int) -> "State[StateType]":
        """Increments the state with a set of key-value pairs. Each one
        must correspond to an integer, or an error will be raised.

        :param updates: updates to apply
        :return: new state object
        """ ""
        return self.apply_operation(IncrementFields(updates))

    def wipe(self, delete: Optional[list[str]] = None, keep: Optional[list[str]] = None):
        """Wipes the state, either by deleting the keys in delete and keeping everything else
         or keeping the keys in keep. and deleting everything else. If you pass nothing in
         it will delete the whole thing.

        :param delete: Keys to delete
        :param keep: Keys to keep
        :return: A new state object
        """
        if delete is not None and keep is not None:
            raise ValueError(
                f"You cannot specify both delete and keep -- not both! "
                f"You have specified: delete={delete}, keep={keep}"
            )
        if delete is not None:
            fields_to_delete = delete
        else:
            fields_to_delete = [key for key in self._state if key not in keep]
        return self.apply_operation(DeleteField(fields_to_delete))

    def merge(self, other: "State") -> "State[StateType]":
        """Merges two states together, overwriting the values in self
        with those in other."""
        return State({**self.get_all(), **other.get_all()}, self.typing_system)

    def subset(self, *keys: str, ignore_missing: bool = True) -> "State[StateType]":
        """Returns a subset of the state, with only the given keys"""
        return State(
            {key: self[key] for key in keys if key in self or not ignore_missing},
            self.typing_system,
        )

    def __getitem__(self, __k: str) -> Any:
        if __k not in self._state:
            raise KeyError(
                f"Key \"{__k}\" not found in state. Keys state knows about are: {[key for key in self._state.keys() if not key.startswith('__')]}. "
                "If you hit this within the context of an application, you want to "
                "(a) ensure that an upstream action has produced this state/it is set as an initial state value and "
                "(b) ensure that your action declares this as a read key."
            )
        return self._state[__k]

    def __len__(self) -> int:
        return len(self._state)

    def __iter__(self) -> Iterator[Any]:
        return iter(self._state)

    def __repr__(self):
        return self.get_all().__repr__()  # quick hack


# We register the serde plugins here that we'll automatically try to load.
# In the future if we need to reorder/replace, we'll just have some
# check here that can skip loading plugins/override which ones to load.
# Note for pickle, we require people to manually register the type for that.
for serde_plugin in ["langchain", "pydantic", "pandas"]:
    try:
        importlib.import_module(f"burr.integrations.serde.{serde_plugin}")
    except ImportError:
        logger.debug(f"Skipped registering {serde_plugin} serde plugin.")



---
File: /burr/burr/core/typing.py
---

from __future__ import annotations

import abc
from typing import TYPE_CHECKING, Generic, Type, TypeVar

BaseType = TypeVar("BaseType")
# SpecificType = TypeVar('SpecificType', bound=BaseType)

if TYPE_CHECKING:
    from burr.core import Action, Graph, State

try:
    from typing import Self
except ImportError:
    Self = "TypingSystem"


class TypingSystem(abc.ABC, Generic[BaseType]):
    @abc.abstractmethod
    def state_type(self) -> Type[BaseType]:
        """Gives the type that represents the state of the
        application at any given time. Note that this must have
        adequate support for Optionals (E.G. non-required values).

        :return:
        """

    @abc.abstractmethod
    def state_pre_action_run_type(self, action: Action, graph: Graph) -> Type[BaseType]:
        """Gives the type that represents the state after an action has completed.
        Note that this could be smart -- E.g. it should have all possible upstream
        types filled in.

        :param action:
        :return:
        """

    @abc.abstractmethod
    def state_post_action_run_type(self, action: Action, graph: Graph) -> Type[BaseType]:
        """Gives the type that represents the state after an action has completed.
        Note that this could be smart -- E.g. it should have all possible upstream
        types filled in.

        :param action:
        :return:
        """

    def validate_state(self, state: State) -> None:
        """Validates the state to ensure it is valid.

        :param state:
        :return:
        """

    @abc.abstractmethod
    def construct_data(self, state: State[BaseType]) -> BaseType:
        """Constructs a type based on the arguments passed in.

        :param kwargs:
        :return:
        """

    @abc.abstractmethod
    def construct_state(self, data: BaseType) -> State[BaseType]:
        """Constructs a state based on the arguments passed in.

        :param kwargs:
        :return:
        """


StateInputType = TypeVar("StateInputType")
StateOutputType = TypeVar("StateOutputType")
IntermediateResultType = TypeVar("IntermediateResultType")


class ActionSchema(
    abc.ABC,
    Generic[
        StateInputType,
        StateOutputType,
        IntermediateResultType,
    ],
):
    """Quick wrapper class to represent a schema. Note that this is currently used internally,
    just to store the appropriate information. This does not validate or do conversion, currently that
    is done within the pydantic model state typing system (which is also internal in its implementation).



    We will likely centralize that logic at some point when we get more -- it would look something like this:
    1. Action is passed an ActionSchema
    2. Action is parameterized on the ActionSchema types
    3. Action takes state, validates the type and converts to StateInputType
    4. Action runs, returns intermediate result + state
    5. Action validates intermediate result type (or converts to dict? Probably just keeps it
    6. Action converts StateOutputType to State
    """

    @abc.abstractmethod
    def state_input_type() -> Type[StateInputType]:
        pass

    @abc.abstractmethod
    def state_output_type() -> Type[StateOutputType]:
        pass

    @abc.abstractmethod
    def intermediate_result_type() -> Type[IntermediateResultType]:
        pass


class DictBasedTypingSystem(TypingSystem[dict]):
    """Effectively a no-op. State is backed by a dictionary, which allows every state item
    to... be a dictionary."""

    def state_type(self) -> Type[dict]:
        return dict

    def state_pre_action_run_type(self, action: Action, graph: Graph) -> Type[dict]:
        return dict

    def state_post_action_run_type(self, action: Action, graph: Graph) -> Type[dict]:
        return dict

    def construct_data(self, state: State[dict]) -> dict:
        return state.get_all()

    def construct_state(self, data: dict) -> State[dict]:
        return State(data, typing_system=self)



---
File: /burr/burr/core/validation.py
---

from typing import Any, Optional

BASE_ERROR_MESSAGE = (
    "-------------------------------------------------------------------\n"
    "Oh no an error! Need help with Burr?\n"
    "Join our discord and ask for help! https://discord.gg/4FxBMyzW5n\n"
    "-------------------------------------------------------------------\n"
)


def assert_set(value: Optional[Any], field: str, method: str):
    if value is None:
        raise ValueError(
            BASE_ERROR_MESSAGE
            + f"Must call `{method}` before building application! Do so with ApplicationBuilder."
        )



---
File: /burr/burr/integrations/persisters/__init__.py
---




---
File: /burr/burr/integrations/persisters/b_aiosqlite.py
---

import json
import logging
from typing import Literal, Optional

import aiosqlite

from burr.common.types import BaseCopyable
from burr.core import State
from burr.core.persistence import AsyncBaseStatePersister, PersistedStateData

logger = logging.getLogger()

try:
    from typing import Self
except ImportError:
    Self = None


class AsyncSQLitePersister(AsyncBaseStatePersister, BaseCopyable):
    """Class for asynchronous SQLite persistence of state. This is a simple implementation.

    .. warning::
        The synchronous persister closes the connection on deletion of the class using the ``__del__`` method.
        In an async context that is not reliable (the event loop may already be closed by the time ``__del__``
        gets invoked). Therefore, you are responsible for closing the connection yourself (i.e. manual cleanup).
        We suggest to use the persister either as a context manager through the ``async with`` clause or
        using the method ``.cleanup()``.

    .. note::
        SQLite is specifically single-threaded and `aiosqlite <https://aiosqlite.omnilib.dev/en/latest/index.html>`_
        creates async support through multi-threading. This persister is mainly here for quick prototyping and testing;
        we suggest to consider a different database with native async support for production.

        Note the third-party library `aiosqlite <https://aiosqlite.omnilib.dev/en/latest/index.html>`_,
        is maintained and considered stable considered stable: https://github.com/omnilib/aiosqlite/issues/309.
    """

    def copy(self) -> "Self":
        return AsyncSQLitePersister(
            connection=self.connection, table_name=self.table_name, serde_kwargs=self.serde_kwargs
        )

    PARTITION_KEY_DEFAULT = ""

    @classmethod
    async def from_config(cls, config: dict) -> "AsyncSQLitePersister":
        """Creates a new instance of the AsyncSQLitePersister from a configuration dictionary.

        The config key:value pair needed are:
        db_path: str,
        table_name: str,
        serde_kwargs: dict,
        connect_kwargs: dict,
        """
        return await cls.from_values(**config)

    @classmethod
    async def from_values(
        cls,
        db_path: str,
        table_name: str = "burr_state",
        serde_kwargs: dict = None,
        connect_kwargs: dict = None,
    ) -> "AsyncSQLitePersister":
        """Creates a new instance of the AsyncSQLitePersister from passed in values.

        :param db_path: the path the DB will be stored.
        :param table_name: the table name to store things under.
        :param serde_kwargs: kwargs for state serialization/deserialization.
        :param connect_kwargs: kwargs to pass to the aiosqlite.connect method.
        :return: async sqlite persister instance with an open connection. You are responsible
            for closing the connection yourself.
        """
        connection = await aiosqlite.connect(
            db_path, **connect_kwargs if connect_kwargs is not None else {}
        )
        return cls(connection, table_name, serde_kwargs)

    def __init__(
        self,
        connection,
        table_name: str = "burr_state",
        serde_kwargs: dict = None,
    ):
        """Constructor.

        NOTE: you are responsible to handle closing of the connection / teardown manually. To help,
        we provide a close() method.

        :param connection: the path the DB will be stored.
        :param table_name: the table name to store things under.
        :param serde_kwargs: kwargs for state serialization/deserialization.
        """
        self.connection = connection
        self.table_name = table_name
        self.serde_kwargs = serde_kwargs or {}
        self._initialized = False

    def set_serde_kwargs(self, serde_kwargs: dict):
        """Sets the serde_kwargs for the persister."""
        self.serde_kwargs = serde_kwargs

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_value, traceback):
        await self.connection.close()
        return False

    async def create_table_if_not_exists(self, table_name: str):
        """Helper function to create the table where things are stored if it doesn't exist."""
        cursor = await self.connection.cursor()
        await cursor.execute(
            f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                partition_key TEXT DEFAULT '{AsyncSQLitePersister.PARTITION_KEY_DEFAULT}',
                app_id TEXT NOT NULL,
                sequence_id INTEGER NOT NULL,
                position TEXT NOT NULL,
                status TEXT NOT NULL,
                state TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                PRIMARY KEY (partition_key, app_id, sequence_id, position)
            )"""
        )
        await cursor.execute(
            f"""
            CREATE INDEX IF NOT EXISTS {table_name}_created_at_index ON {table_name} (created_at);
        """
        )
        await self.connection.commit()

    async def initialize(self):
        """Asynchronously creates the table if it doesn't exist"""
        # Usage
        await self.create_table_if_not_exists(self.table_name)
        self._initialized = True

    async def is_initialized(self) -> bool:
        """This checks to see if the table has been created in the database or not.
        It defaults to using the initialized field, else queries the database to see if the table exists.
        It then sets the initialized field to True if the table exists.
        """
        if self._initialized:
            return True

        cursor = await self.connection.cursor()
        await cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name=?", (self.table_name,)
        )
        self._initialized = await cursor.fetchone() is not None
        return self._initialized

    async def list_app_ids(self, partition_key: Optional[str] = None, **kwargs) -> list[str]:
        partition_key = (
            partition_key
            if partition_key is not None
            else AsyncSQLitePersister.PARTITION_KEY_DEFAULT
        )

        cursor = await self.connection.cursor()
        await cursor.execute(
            f"SELECT DISTINCT app_id FROM {self.table_name} "
            f"WHERE partition_key = ? "
            f"ORDER BY created_at DESC",
            (partition_key,),
        )
        app_ids = [row[0] for row in await cursor.fetchall()]
        return app_ids

    async def load(
        self,
        partition_key: Optional[str],
        app_id: Optional[str],
        sequence_id: Optional[int] = None,
        **kwargs,
    ) -> Optional[PersistedStateData]:
        """Asynchronously loads state for a given partition id.

        Depending on the parameters, this will return the last thing written, the last thing written for a given app_id,
        or a specific sequence_id for a given app_id.

        :param partition_key:
        :param app_id:
        :param sequence_id:
        :return:
        """
        partition_key = (
            partition_key
            if partition_key is not None
            else AsyncSQLitePersister.PARTITION_KEY_DEFAULT
        )
        logger.debug("Loading %s, %s, %s", partition_key, app_id, sequence_id)
        cursor = await self.connection.cursor()
        if app_id is None:
            # get latest for all app_ids
            await cursor.execute(
                f"SELECT position, state, sequence_id, app_id, created_at, status FROM {self.table_name} "
                f"WHERE partition_key = ? "
                f"ORDER BY CREATED_AT DESC LIMIT 1",
                (partition_key,),
            )
        elif sequence_id is None:
            await cursor.execute(
                f"SELECT position, state, sequence_id, app_id, created_at, status FROM {self.table_name} "
                f"WHERE partition_key = ? AND app_id = ? "
                f"ORDER BY sequence_id DESC LIMIT 1",
                (partition_key, app_id),
            )
        else:
            await cursor.execute(
                f"SELECT position, state, sequence_id, app_id, created_at, status FROM {self.table_name} "
                f"WHERE partition_key = ? AND app_id = ? AND sequence_id = ?",
                (partition_key, app_id, sequence_id),
            )
        row = await cursor.fetchone()
        if row is None:
            return None
        _state = State.deserialize(json.loads(row[1]), **self.serde_kwargs)
        return {
            "partition_key": partition_key,
            "app_id": row[3],
            "sequence_id": row[2],
            "position": row[0],
            "state": _state,
            "created_at": row[4],
            "status": row[5],
        }

    async def save(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int,
        position: str,
        state: State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        """
        Asynchronously saves the state for a given app_id, sequence_id, and position.

        This method connects to the SQLite database, converts the state to a JSON string, and inserts a new record
        into the table with the provided partition_key, app_id, sequence_id, position, and state. After the operation,
        it commits the changes and closes the connection to the database.

        :param partition_key: The partition key. This could be None, but it's up to the persister to whether
            that is a valid value it can handle.
        :param app_id: The identifier for the app instance being recorded.
        :param sequence_id: The state corresponding to a specific point in time.
        :param position: The position in the sequence of states.
        :param state: The state to be saved, an instance of the State class.
        :param status: The status of this state, either "completed" or "failed". If "failed" the state is what it was
            before the action was applied.
        :return: None
        """
        logger.debug(
            "saving %s, %s, %s, %s, %s, %s",
            partition_key,
            app_id,
            sequence_id,
            position,
            state,
            status,
        )
        partition_key = (
            partition_key
            if partition_key is not None
            else AsyncSQLitePersister.PARTITION_KEY_DEFAULT
        )
        cursor = await self.connection.cursor()
        json_state = json.dumps(state.serialize(**self.serde_kwargs))
        await cursor.execute(
            f"INSERT INTO {self.table_name} (partition_key, app_id, sequence_id, position, state, status) "
            f"VALUES (?, ?, ?, ?, ?, ?)",
            (partition_key, app_id, sequence_id, position, json_state, status),
        )
        await self.connection.commit()

    async def cleanup(self):
        """Closes the connection to the database."""
        await self.connection.close()

    async def close(self):
        """This is deprecated, please use .cleanup()"""
        logger.warning("The .close() method will be deprecated, please use .cleanup() instead.")
        await self.connection.close()



---
File: /burr/burr/integrations/persisters/b_asyncpg.py
---

import json
import logging
from typing import Literal, Optional, ClassVar
from typing import Any
from burr.common.types import BaseCopyable
from burr.core import persistence, state
from burr.integrations import base


try:
    import asyncpg
except ImportError as e:
    base.require_plugin(e, "asyncpg")

try:
    from typing import Self
except ImportError:
    Self = Any


logger = logging.getLogger(__name__)


class AsyncPostgreSQLPersister(persistence.AsyncBaseStatePersister, BaseCopyable):
    """Class for async PostgreSQL persistence of state.

    .. warning::
        The synchronous persister closes the connection on deletion of the class using the ``__del__`` method.
        In an async context that is not reliable (the event loop may already be closed by the time ``__del__``
        gets invoked). Therefore, you are responsible for closing the connection yourself (i.e. manual cleanup).
        We suggest to use the persister either as a context manager through the ``async with`` clause or
        using the method ``.cleanup()``.

    .. warning::
        If you intend to use parallelism features or need to share this persister across multiple tasks,
        you should initialize it with a connection pool (set ``use_pool=True`` in ``from_values``).
        Direct connections cannot be shared across different tasks and may cause errors in concurrent scenarios.

    .. note::
        The implementation relies on the popular asyncpg library: https://github.com/MagicStack/asyncpg

        MagicStack wrote a blog post: http://magic.io/blog/asyncpg-1m-rows-from-postgres-to-python/
        explaining why a new implementation, performance review and comparison to aiopg (async interface of psycopg2).


    To try it out locally with docker -- here's a command -- change the values as appropriate.

    .. code:: bash

        docker run --name local-psql \\\\  # container name
                   -v local_psql_data:/SOME/FILE_PATH/ \\\\  # mounting a volume for data persistence
                   -p 54320:5432 \\\\  # port mapping
                   -e POSTGRES_PASSWORD=my_password \\\\  # superuser password
                   -d postgres  # database name

    Then you should be able to create the class like this:

    .. code:: python

        p = await AsyncPostgreSQLPersister.from_values("postgres", "postgres", "my_password",
                                           "localhost", 54320, table_name="burr_state")

    """

    PARTITION_KEY_DEFAULT = ""

    # Class variable to hold the connection pool
    _pool: ClassVar[Optional[asyncpg.Pool]] = None

    @classmethod
    async def create_pool(
        cls,
        user: str,
        password: str,
        database: str,
        host: str,
        port: int,
        **pool_kwargs,
    ) -> asyncpg.Pool:
        """Creates a connection pool that can be shared across persisters."""
        if cls._pool is None:
            cls._pool = await asyncpg.create_pool(
                user=user,
                password=password,
                database=database,
                host=host,
                port=port,
                **pool_kwargs,
            )
        return cls._pool

    @classmethod
    async def from_config(cls, config: dict) -> "AsyncPostgreSQLPersister":
        """Creates a new instance of the PostgreSQLPersister from a configuration dictionary."""
        return await cls.from_values(**config)

    @classmethod
    async def from_values(
        cls,
        db_name: str,
        user: str,
        password: str,
        host: str,
        port: int,
        table_name: str = "burr_state",
        use_pool: bool = False,
        **pool_kwargs,
    ) -> "AsyncPostgreSQLPersister":
        """Builds a new instance of the PostgreSQLPersister from the provided values.

        :param db_name: the name of the PostgreSQL database.
        :param user: the username to connect to the PostgreSQL database.
        :param password: the password to connect to the PostgreSQL database.
        :param host: the host of the PostgreSQL database.
        :param port: the port of the PostgreSQL database.
        :param table_name:  the table name to store things under.
        :param use_pool: whether to use a connection pool (True) or a direct connection (False)
        :param pool_kwargs: additional kwargs to pass to the pool creation
        """
        if use_pool:
            pool = await cls.create_pool(
                user=user,
                password=password,
                database=db_name,
                host=host,
                port=port,
                **pool_kwargs,
            )
            return cls(connection=None, pool=pool, table_name=table_name)
        else:
            # Original behavior - direct connection
            connection = await asyncpg.connect(
                user=user, password=password, database=db_name, host=host, port=port
            )
            return cls(connection=connection, table_name=table_name)

    def __init__(
        self,
        connection=None,
        pool=None,
        table_name: str = "burr_state",
        serde_kwargs: dict = None,
    ):
        """Constructor

        :param connection: the connection to the PostgreSQL database (optional if pool is provided)
        :param pool: a connection pool to use instead of a direct connection (optional if connection is provided)
        :param table_name:  the table name to store things under.
        :param serde_kwargs: kwargs for state serialization/deserialization
        """
        if connection is None and pool is None:
            raise ValueError("Either connection or pool must be provided")

        self.table_name = table_name
        self.connection = connection
        self.pool = pool
        self.serde_kwargs = serde_kwargs or {}
        self._initialized = False

    def copy(self) -> "Self":
        """Creates a copy of this persister.

        If using a pool, returns a new persister that will acquire its own connection from the pool.
        If using a direct connection, just returns a new persister with the same connection (won't work for async parallelism)
        """
        if self.pool is not None:
            return AsyncPostgreSQLPersister(
                connection=None,
                pool=self.pool,
                table_name=self.table_name,
                serde_kwargs=self.serde_kwargs,
            )
        else:
            return AsyncPostgreSQLPersister(
                connection=self.connection,
                table_name=self.table_name,
                serde_kwargs=self.serde_kwargs,
            )

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_value, traceback):
        await self.cleanup()
        return False

    async def _get_connection(self):
        """Gets a connection - either the dedicated one or one from the pool."""
        if self.pool is not None:
            return await self.pool.acquire(), True
        elif self.connection is not None:
            return self.connection, False
        else:
            raise ValueError("No connection or pool available")

    async def _release_connection(self, connection, acquired):
        """Releases a connection back to the pool if it was acquired."""
        if acquired and self.pool is not None:
            await self.pool.release(connection)

    def set_serde_kwargs(self, serde_kwargs: dict):
        """Sets the serde_kwargs for the persister."""
        self.serde_kwargs = serde_kwargs

    async def create_table(self, table_name: str):
        """Helper function to create the table where things are stored."""
        conn, acquired = await self._get_connection()
        try:
            async with conn.transaction():
                await conn.execute(
                    f"""
                    CREATE TABLE IF NOT EXISTS {table_name} (
                        partition_key TEXT DEFAULT '{self.PARTITION_KEY_DEFAULT}',
                        app_id TEXT NOT NULL,
                        sequence_id INTEGER NOT NULL,
                        position TEXT NOT NULL,
                        status TEXT NOT NULL,
                        state JSONB NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        PRIMARY KEY (partition_key, app_id, sequence_id, position)
                    )"""
                )
                await conn.execute(
                    f"""
                    CREATE INDEX IF NOT EXISTS {table_name}_created_at_index ON {table_name} (created_at);
                """
                )
        finally:
            await self._release_connection(conn, acquired)

    async def initialize(self):
        """Creates the table"""
        await self.create_table(self.table_name)
        self._initialized = True

    async def is_initialized(self) -> bool:
        """This checks to see if the table has been created in the database or not.
        It defaults to using the initialized field, else queries the database to see if the table exists.
        It then sets the initialized field to True if the table exists.
        """
        if self._initialized:
            return True

        conn, acquired = await self._get_connection()
        try:
            query = "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = $1)"
            self._initialized = await conn.fetchval(query, self.table_name, column=0)
            return self._initialized
        finally:
            await self._release_connection(conn, acquired)

    async def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        """Lists the app_ids for a given partition_key."""
        conn, acquired = await self._get_connection()
        try:
            query = (
                f"SELECT DISTINCT app_id, created_at FROM {self.table_name} "
                "WHERE partition_key = $1 "
                "ORDER BY created_at DESC"
            )
            fetched_data = await conn.fetch(query, partition_key)
            app_ids = [row[0] for row in fetched_data]
            return app_ids
        finally:
            await self._release_connection(conn, acquired)

    async def load(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int = None,
        **kwargs,
    ) -> Optional[persistence.PersistedStateData]:
        """Loads state for a given partition id.

        Depending on the parameters, this will return the last thing written, the last thing written for a given app_id,
        or a specific sequence_id for a given app_id.

        :param partition_key:
        :param app_id:
        :param sequence_id:
        :return:
        """
        if partition_key is None:
            partition_key = self.PARTITION_KEY_DEFAULT
        logger.debug("Loading %s, %s, %s", partition_key, app_id, sequence_id)

        conn, acquired = await self._get_connection()
        try:
            row = None
            if app_id is None:
                # get latest for all app_ids
                query = (
                    f"SELECT position, state, sequence_id, app_id, created_at, status FROM {self.table_name} "
                    "WHERE partition_key = $1 "
                    f"ORDER BY CREATED_AT DESC LIMIT 1"
                )
                row = await conn.fetchrow(query, partition_key)
            elif sequence_id is None:
                query = (
                    f"SELECT position, state, sequence_id, app_id, created_at, status FROM {self.table_name} "
                    "WHERE partition_key = $1 AND app_id = $2 "
                    f"ORDER BY sequence_id DESC LIMIT 1"
                )
                row = await conn.fetchrow(query, partition_key, app_id)
            else:
                query = (
                    f"SELECT position, state, sequence_id, app_id, created_at, status FROM {self.table_name} "
                    "WHERE partition_key = $1 AND app_id = $2 AND sequence_id = $3 "
                )
                row = await conn.fetchrow(
                    query,
                    partition_key,
                    app_id,
                    sequence_id,
                )

            if row is None:
                return None

            # converts from asyncpg str to dict
            json_row = json.loads(row[1])
            _state = state.State.deserialize(json_row, **self.serde_kwargs)
            return {
                "partition_key": partition_key,
                "app_id": row[3],
                "sequence_id": row[2],
                "position": row[0],
                "state": _state,
                "created_at": row[4],
                "status": row[5],
            }
        finally:
            await self._release_connection(conn, acquired)

    async def save(
        self,
        partition_key: str,
        app_id: str,
        sequence_id: int,
        position: str,
        state: state.State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        """
        Saves the state for a given app_id, sequence_id, and position.

        This method connects to the SQLite database, converts the state to a JSON string, and inserts a new record
        into the table with the provided partition_key, app_id, sequence_id, position, and state. After the operation,
        it commits the changes and closes the connection to the database.

        :param partition_key: The partition key. This could be None, but it's up to the persistor to whether
            that is a valid value it can handle.
        :param app_id: The identifier for the app instance being recorded.
        :param sequence_id: The state corresponding to a specific point in time.
        :param position: The position in the sequence of states.
        :param state: The state to be saved, an instance of the State class.
        :param status: The status of this state, either "completed" or "failed". If "failed" the state is what it was
            before the action was applied.
        :return: None
        """
        logger.debug(
            "saving %s, %s, %s, %s, %s, %s",
            partition_key,
            app_id,
            sequence_id,
            position,
            state,
            status,
        )

        conn, acquired = await self._get_connection()
        try:
            json_state = json.dumps(state.serialize(**self.serde_kwargs))
            query = (
                f"INSERT INTO {self.table_name} (partition_key, app_id, sequence_id, position, state, status) "
                "VALUES ($1, $2, $3, $4, $5, $6)"
            )
            await conn.execute(
                query, partition_key, app_id, sequence_id, position, json_state, status
            )
        finally:
            await self._release_connection(conn, acquired)

    async def cleanup(self):
        """Closes the connection to the database."""
        if self.connection is not None:
            await self.connection.close()
            self.connection = None



---
File: /burr/burr/integrations/persisters/b_mongodb.py
---

"""This module will be deprecated. Please use b_pymongo.py for imports."""

import logging

from pymongo import MongoClient

from burr.integrations.persisters.b_pymongo import MongoDBBasePersister as PymongoPersister

logger = logging.getLogger(__name__)

logger.warning(
    "This class is deprecated and has been moved. "
    "Please import MongoDBBasePersister from b_pymongo.py."
)


class MongoDBBasePersister(PymongoPersister):
    """A class used to represent the MongoDB Persister.

    This class is deprecated and has been moved to b_pymongo.py.
    """

    @classmethod
    def from_values(
        cls,
        uri="mongodb://localhost:27017",
        db_name="mydatabase",
        collection_name="mystates",
        serde_kwargs: dict = None,
        mongo_client_kwargs: dict = None,
    ) -> "MongoDBBasePersister":
        """Initializes the MongoDBBasePersister class."""

        if mongo_client_kwargs is None:
            mongo_client_kwargs = {}
        client = MongoClient(uri, **mongo_client_kwargs)
        return PymongoPersister(
            client=client,
            db_name=db_name,
            collection_name=collection_name,
            serde_kwargs=serde_kwargs,
        )


class MongoDBPersister(PymongoPersister):
    """A class used to represent a MongoDB Persister.

    This class is deprecated. Please use MongoDBBasePersister instead.
    """

    def __init__(
        self,
        uri="mongodb://localhost:27017",
        db_name="mydatabase",
        collection_name="mystates",
        serde_kwargs: dict = None,
        mongo_client_kwargs: dict = None,
    ):
        """Initializes the MongoDBPersister class."""
        if mongo_client_kwargs is None:
            mongo_client_kwargs = {}
        client = MongoClient(uri, **mongo_client_kwargs)
        super(MongoDBPersister, self).__init__(
            client=client,
            db_name=db_name,
            collection_name=collection_name,
            serde_kwargs=serde_kwargs,
        )



---
File: /burr/burr/integrations/persisters/b_psycopg2.py
---

from burr.integrations import base

try:
    import psycopg2
except ImportError as e:
    base.require_plugin(e, "postgresql")

import json
import logging
from typing import Literal, Optional

from burr.core import persistence, state

logger = logging.getLogger(__name__)


class PostgreSQLPersister(persistence.BaseStatePersister):
    """Class for PostgreSQL persistence of state. This is a simple implementation.

    To try it out locally with docker -- here's a command -- change the values as appropriate.

    .. code:: bash

        docker run --name local-psql \  # container name
                   -v local_psql_data:/SOME/FILE_PATH/ \  # mounting a volume for data persistence
                   -p 54320:5432 \  # port mapping
                   -e POSTGRES_PASSWORD=my_password \  # superuser password
                   -d postgres  # database name

    Then you should be able to create the class like this:

    .. code:: python

        p = PostgreSQLPersister.from_values("postgres", "postgres", "my_password",
                                           "localhost", 54320, table_name="burr_state")


    """

    PARTITION_KEY_DEFAULT = ""

    @classmethod
    def from_config(cls, config: dict) -> "PostgreSQLPersister":
        """Creates a new instance of the PostgreSQLPersister from a configuration dictionary."""
        return cls.from_values(**config)

    @classmethod
    def from_values(
        cls,
        db_name: str,
        user: str,
        password: str,
        host: str,
        port: int,
        table_name: str = "burr_state",
    ):
        """Builds a new instance of the PostgreSQLPersister from the provided values.

        :param db_name: the name of the PostgreSQL database.
        :param user: the username to connect to the PostgreSQL database.
        :param password: the password to connect to the PostgreSQL database.
        :param host: the host of the PostgreSQL database.
        :param port: the port of the PostgreSQL database.
        :param table_name:  the table name to store things under.
        """
        connection = psycopg2.connect(
            dbname=db_name, user=user, password=password, host=host, port=port
        )
        return cls(connection, table_name)

    def __init__(self, connection, table_name: str = "burr_state", serde_kwargs: dict = None):
        """Constructor

        :param connection: the connection to the PostgreSQL database.
        :param table_name:  the table name to store things under.
        """
        self.table_name = table_name
        self.connection = connection
        self.serde_kwargs = serde_kwargs or {}
        self._initialized = False

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.connection.close()
        return False

    def set_serde_kwargs(self, serde_kwargs: dict):
        """Sets the serde_kwargs for the persister."""
        self.serde_kwargs = serde_kwargs

    def create_table(self, table_name: str):
        """Helper function to create the table where things are stored."""
        cursor = self.connection.cursor()
        cursor.execute(
            f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                partition_key TEXT DEFAULT '{self.PARTITION_KEY_DEFAULT}',
                app_id TEXT NOT NULL,
                sequence_id INTEGER NOT NULL,
                position TEXT NOT NULL,
                status TEXT NOT NULL,
                state JSONB NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                PRIMARY KEY (partition_key, app_id, sequence_id, position)
            )"""
        )
        cursor.execute(
            f"""
            CREATE INDEX IF NOT EXISTS {table_name}_created_at_index ON {table_name} (created_at);
        """
        )
        self.connection.commit()

    def initialize(self):
        """Creates the table"""
        self.create_table(self.table_name)
        self._initialized = True

    def is_initialized(self) -> bool:
        """This checks to see if the table has been created in the database or not.
        It defaults to using the initialized field, else queries the database to see if the table exists.
        It then sets the initialized field to True if the table exists.
        """
        if self._initialized:
            return True
        cursor = self.connection.cursor()
        cursor.execute(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = %s)",
            (self.table_name,),
        )
        self._initialized = cursor.fetchone()[0]
        return self._initialized

    def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        """Lists the app_ids for a given partition_key."""
        cursor = self.connection.cursor()
        cursor.execute(
            f"SELECT DISTINCT app_id, created_at FROM {self.table_name} "
            "WHERE partition_key = %s "
            "ORDER BY created_at DESC",
            (partition_key,),
        )
        app_ids = [row[0] for row in cursor.fetchall()]
        return app_ids

    def load(
        self, partition_key: Optional[str], app_id: str, sequence_id: int = None, **kwargs
    ) -> Optional[persistence.PersistedStateData]:
        """Loads state for a given partition id.

        Depending on the parameters, this will return the last thing written, the last thing written for a given app_id,
        or a specific sequence_id for a given app_id.

        :param partition_key:
        :param app_id:
        :param sequence_id:
        :return:
        """
        if partition_key is None:
            partition_key = self.PARTITION_KEY_DEFAULT
        logger.debug("Loading %s, %s, %s", partition_key, app_id, sequence_id)
        cursor = self.connection.cursor()
        if app_id is None:
            # get latest for all app_ids
            cursor.execute(
                f"SELECT position, state, sequence_id, app_id, created_at, status FROM {self.table_name} "
                f"WHERE partition_key = %s "
                f"ORDER BY CREATED_AT DESC LIMIT 1",
                (partition_key,),
            )
        elif sequence_id is None:
            cursor.execute(
                f"SELECT position, state, sequence_id, app_id, created_at, status FROM {self.table_name} "
                f"WHERE partition_key = %s AND app_id = %s "
                f"ORDER BY sequence_id DESC LIMIT 1",
                (partition_key, app_id),
            )
        else:
            cursor.execute(
                f"SELECT position, state, sequence_id, app_id, created_at, status FROM {self.table_name} "
                f"WHERE partition_key = %s AND app_id = %s AND sequence_id = %s ",
                (partition_key, app_id, sequence_id),
            )
        row = cursor.fetchone()
        if row is None:
            return None
        _state = state.State.deserialize(row[1], **self.serde_kwargs)
        return {
            "partition_key": partition_key,
            "app_id": row[3],
            "sequence_id": row[2],
            "position": row[0],
            "state": _state,
            "created_at": row[4],
            "status": row[5],
        }

    def save(
        self,
        partition_key: str,
        app_id: str,
        sequence_id: int,
        position: str,
        state: state.State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        """
        Saves the state for a given app_id, sequence_id, and position.

        This method connects to the SQLite database, converts the state to a JSON string, and inserts a new record
        into the table with the provided partition_key, app_id, sequence_id, position, and state. After the operation,
        it commits the changes and closes the connection to the database.

        :param partition_key: The partition key. This could be None, but it's up to the persistor to whether
            that is a valid value it can handle.
        :param app_id: The identifier for the app instance being recorded.
        :param sequence_id: The state corresponding to a specific point in time.
        :param position: The position in the sequence of states.
        :param state: The state to be saved, an instance of the State class.
        :param status: The status of this state, either "completed" or "failed". If "failed" the state is what it was
            before the action was applied.
        :return: None
        """
        logger.debug(
            "saving %s, %s, %s, %s, %s, %s",
            partition_key,
            app_id,
            sequence_id,
            position,
            state,
            status,
        )
        cursor = self.connection.cursor()
        json_state = json.dumps(state.serialize(**self.serde_kwargs))
        cursor.execute(
            f"INSERT INTO {self.table_name} (partition_key, app_id, sequence_id, position, state, status) "
            "VALUES (%s, %s, %s, %s, %s, %s)",
            (partition_key, app_id, sequence_id, position, json_state, status),
        )
        self.connection.commit()

    def cleanup(self):
        """Closes the connection to the database."""
        self.connection.close()

    def __del__(self):
        # This should be deprecated -- using __del__ is unreliable for closing connections to db's;
        # the preferred way should be for the user to use a context manager or use the `.cleanup()`
        # method within a REST API framework.

        # closes connection at end when things are being shutdown.
        self.connection.close()

    def __getstate__(self) -> dict:
        state = self.__dict__.copy()
        if not hasattr(self.connection, "info"):
            logger.warning(
                "Postgresql information for connection object not available. Cannot serialize persister."
            )
            return state
        state["connection_params"] = {
            "dbname": self.connection.info.dbname,
            "user": self.connection.info.user,
            "password": self.connection.info.password,
            "host": self.connection.info.host,
            "port": self.connection.info.port,
        }
        del state["connection"]
        return state

    def __setstate__(self, state: dict):
        connection_params = state.pop("connection_params")
        # we assume normal psycopg2 client.
        self.connection = psycopg2.connect(**connection_params)
        self.__dict__.update(state)


if __name__ == "__main__":
    # test the PostgreSQLPersister class
    persister = PostgreSQLPersister.from_values(
        "postgres", "postgres", "my_password", "localhost", 54320, table_name="burr_state"
    )

    persister.initialize()
    persister.save("pk", "app_id", 1, "pos", state.State({"a": 1, "b": 2}), "completed")
    print(persister.list_app_ids("pk"))
    print(persister.load("pk", "app_id"))



---
File: /burr/burr/integrations/persisters/b_pymongo.py
---

import json
import logging
from datetime import datetime, timezone
from typing import Literal, Optional

from pymongo import MongoClient

from burr.core import persistence, state

logger = logging.getLogger(__name__)


class MongoDBBasePersister(persistence.BaseStatePersister):
    """A class used to represent a MongoDB Persister.

    Example usage:

    .. code-block:: python

       persister = MongoDBBasePersister.from_values(uri='mongodb://user:pass@localhost:27017',
                                                    db_name='mydatabase',
                                                    collection_name='mystates')
       persister.save(
           partition_key='example_partition',
           app_id='example_app',
           sequence_id=1,
           position='example_position',
           state=state.State({'key': 'value'}),
           status='completed'
       )
       loaded_state = persister.load(partition_key='example_partition', app_id='example_app', sequence_id=1)
       print(loaded_state)

    Note: this is called MongoDBBasePersister because we had to change the constructor and wanted to make
     this change backwards compatible.
    """

    @classmethod
    def from_config(cls, config: dict) -> "MongoDBBasePersister":
        """Creates a new instance of the MongoDBBasePersister from a configuration dictionary."""
        return cls.from_values(**config)

    @classmethod
    def from_values(
        cls,
        uri="mongodb://localhost:27017",
        db_name="mydatabase",
        collection_name="mystates",
        serde_kwargs: dict = None,
        mongo_client_kwargs: dict = None,
    ) -> "MongoDBBasePersister":
        """Initializes the MongoDBBasePersister class."""
        if mongo_client_kwargs is None:
            mongo_client_kwargs = {}
        client = MongoClient(uri, **mongo_client_kwargs)
        return cls(
            client=client,
            db_name=db_name,
            collection_name=collection_name,
            serde_kwargs=serde_kwargs,
        )

    def __init__(
        self,
        client,
        db_name="mydatabase",
        collection_name="mystates",
        serde_kwargs: dict = None,
    ):
        """Initializes the MongoDBBasePersister class.

        :param client: the mongodb client to use
        :param db_name: the name of the database to use
        :param collection_name: the name of the collection to use
        :param serde_kwargs: serializer/deserializer keyword arguments to pass to the state object
        """
        self.client = client
        self.db = self.client[db_name]
        self.collection = self.db[collection_name]
        self.serde_kwargs = serde_kwargs or {}

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.connection.close()
        return False

    def set_serde_kwargs(self, serde_kwargs: dict):
        """Sets the serde_kwargs for the persister."""
        self.serde_kwargs = serde_kwargs

    def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        """List the app ids for a given partition key."""
        app_ids = self.collection.distinct("app_id", {"partition_key": partition_key})
        return app_ids

    def load(
        self, partition_key: Optional[str], app_id: str, sequence_id: int = None, **kwargs
    ) -> Optional[persistence.PersistedStateData]:
        """Loads the state data for a given partition key, app_id, and sequence_id.

        This method retrieves the most recent state data for the specified (partition_key, app_id) combination.
        If a sequence ID is provided, it will attempt to fetch the specific state at that sequence.

        :param partition_key: The partition key. Defaults to `None`. **Note:** The partition key defaults to `None`. If a partition key was used during saving, it must be provided
        consistently during retrieval, or no results will be returned.
        :param app_id: Application UID to read from.
        :param sequence_id: (Optional) The sequence ID to retrieve a specific state. If not provided,
            the latest state is returned.

        :returns: The state data if found, otherwise None.
        """
        query = {"partition_key": partition_key, "app_id": app_id}
        if sequence_id is not None:
            query["sequence_id"] = sequence_id
        document = self.collection.find_one(query, sort=[("sequence_id", -1)])
        if not document:
            return None
        _state = state.State.deserialize(json.loads(document["state"]), **self.serde_kwargs)
        return {
            "partition_key": partition_key,
            "app_id": app_id,
            "sequence_id": document["sequence_id"],
            "position": document["position"],
            "state": _state,
            "created_at": document["created_at"],
            "status": document["status"],
        }

    def save(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int,
        position: str,
        state: state.State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        """Save the state data to the MongoDB database.

        :param partition_key: the partition key. Note this could be None, but it's up to the persistor to whether
        that is a valid value it can handle. If a partition key was used during saving, it must be provided
        consistently during retrieval, or no results will be returned.
        :param app_id: Application UID to write with.
        :param sequence_id: Sequence ID of the last executed step.
        :param position: The action name that was implemented.
        :param state: The current state of the application.
        :param status: The status of this state, either "completed" or "failed". If "failed", the state is what it was
            before the action was applied.
        :return:
        """
        key = {"partition_key": partition_key, "app_id": app_id, "sequence_id": sequence_id}
        if self.collection.find_one(key):
            raise ValueError(f"partition_key:app_id:sequence_id[{key}] already exists.")
        json_state = json.dumps(state.serialize(**self.serde_kwargs))
        self.collection.insert_one(
            {
                "partition_key": partition_key,
                "app_id": app_id,
                "sequence_id": sequence_id,
                "position": position,
                "state": json_state,
                "status": status,
                "created_at": datetime.now(timezone.utc).isoformat(),
            }
        )

    def cleanup(self):
        """Closes the connection to the database."""
        self.connection.close()

    def __del__(self):
        # This should be deprecated -- using __del__ is unreliable for closing connections to db's;
        # the preferred way should be for the user to use a context manager or use the `.cleanup()`
        # method within a REST API framework.

        self.client.close()

    def __getstate__(self) -> dict:
        state = self.__dict__.copy()
        state["connection_params"] = {
            "uri": self.client.address[0],
            "port": self.client.address[1],
            "db_name": self.db.name,
            "collection_name": self.collection.name,
        }
        del state["client"]
        del state["db"]
        del state["collection"]
        return state

    def __setstate__(self, state: dict):
        connection_params = state.pop("connection_params")
        # we assume MongoClient.
        self.client = MongoClient(connection_params["uri"], connection_params["port"])
        self.db = self.client[connection_params["db_name"]]
        self.collection = self.db[connection_params["collection_name"]]
        self.__dict__.update(state)



---
File: /burr/burr/integrations/persisters/b_redis.py
---

from burr.integrations import base

try:
    import redis  # can't name module redis because this import wouldn't work.
    import redis.asyncio as aredis

except ImportError as e:
    base.require_plugin(e, "redis")

import json
import logging
from datetime import datetime, timezone
from typing import Literal, Optional

from burr.core import persistence, state

logger = logging.getLogger(__name__)


def add_namespace_to_partition_key(partition_key: str, namespace: Optional[str] = None) -> str:
    """Helper function to add namespace to partition key."""

    if namespace:
        return f"{namespace}:{partition_key}"
    return partition_key


class RedisBasePersister(persistence.BaseStatePersister):
    """Main class for Redis persister.

    Use this class if you want to directly control injecting the Redis client.

    This class is responsible for persisting state data to a Redis database.
    It inherits from the BaseStatePersister class.

    Note: We didn't create the right constructor for the initial implementation of the RedisPersister class,
    so this is an attempt to fix that in a backwards compatible way.
    """

    @classmethod
    def from_config(cls, config: dict) -> "RedisBasePersister":
        """Creates a new instance of the RedisBasePersister from a configuration dictionary."""
        return cls.from_values(**config)

    @classmethod
    def from_values(
        cls,
        host: str,
        port: int,
        db: int,
        password: str = None,
        serde_kwargs: dict = None,
        redis_client_kwargs: dict = None,
        namespace: str = None,
    ) -> "RedisBasePersister":
        """Creates a new instance of the RedisBasePersister from passed in values."""
        if redis_client_kwargs is None:
            redis_client_kwargs = {}
        connection = redis.Redis(
            host=host, port=port, db=db, password=password, **redis_client_kwargs
        )
        return cls(connection, serde_kwargs, namespace)

    def __init__(
        self,
        connection,
        serde_kwargs: dict = None,
        namespace: str = None,
    ):
        """Initializes the RedisPersister class.

        :param connection: the redis connection object.
        :param serde_kwargs: serialization and deserialization keyword arguments to pass to state SERDE.
        :param namespace: The name of the project to optionally use in the key prefix.
        """
        self.connection = connection
        self.serde_kwargs = serde_kwargs or {}
        self.namespace = namespace if namespace else ""

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.connection.close()
        return False

    def set_serde_kwargs(self, serde_kwargs: dict):
        """Sets the serde_kwargs for the persister."""
        self.serde_kwargs = serde_kwargs

    def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        """List the app ids for a given partition key."""
        namespaced_partition_key = add_namespace_to_partition_key(partition_key, self.namespace)
        app_ids = self.connection.zrevrange(namespaced_partition_key, 0, -1)
        return [app_id.decode() for app_id in app_ids]

    def load(
        self, partition_key: str, app_id: str, sequence_id: int = None, **kwargs
    ) -> Optional[persistence.PersistedStateData]:
        """Load the state data for a given partition key, app id, and sequence id.

        If the sequence id is not given, it will be looked up in the Redis database. If it is not found, None will be returned.

        :param partition_key:
        :param app_id:
        :param sequence_id:
        :param kwargs:
        :return: Value or None.
        """
        namespaced_partition_key = add_namespace_to_partition_key(partition_key, self.namespace)
        if sequence_id is None:
            sequence_id = self.connection.zscore(namespaced_partition_key, app_id)
            if sequence_id is None:
                return None
            sequence_id = int(sequence_id)
        key = self.create_key(app_id, partition_key, sequence_id)
        data = self.connection.hgetall(key)
        if not data:
            return None
        _state = state.State.deserialize(json.loads(data[b"state"].decode()), **self.serde_kwargs)
        return {
            "partition_key": partition_key,
            "app_id": app_id,
            "sequence_id": sequence_id,
            "position": data[b"position"].decode(),
            "state": _state,
            "created_at": data[b"created_at"].decode(),
            "status": data[b"status"].decode(),
        }

    def create_key(self, app_id, partition_key, sequence_id):
        """Create a key for the Redis database."""
        return add_namespace_to_partition_key(
            f"{partition_key}:{app_id}:{sequence_id}", self.namespace
        )

    def save(
        self,
        partition_key: str,
        app_id: str,
        sequence_id: int,
        position: str,
        state: state.State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        """Save the state data to the Redis database.

        :param partition_key:
        :param app_id:
        :param sequence_id:
        :param position:
        :param state:
        :param status:
        :param kwargs:
        :return:
        """
        key = self.create_key(app_id, partition_key, sequence_id)
        if self.connection.exists(key):
            raise ValueError(f"partition_key:app_id:sequence_id[{key}] already exists.")
        json_state = json.dumps(state.serialize(**self.serde_kwargs))
        self.connection.hset(
            key,
            mapping={
                "partition_key": partition_key,
                "app_id": app_id,
                "sequence_id": sequence_id,
                "position": position,
                "state": json_state,
                "status": status,
                "created_at": datetime.now(timezone.utc).isoformat(),
            },
        )
        namespaced_partition_key = add_namespace_to_partition_key(partition_key, self.namespace)
        self.connection.zadd(namespaced_partition_key, {app_id: sequence_id})

    def cleanup(self):
        """Closes the connection to the database."""
        self.connection.close()

    def __del__(self):
        # This should be deprecated -- using __del__ is unreliable for closing connections to db's;
        # the preferred way should be for the user to use a context manager or use the `.cleanup()`
        # method within a REST API framework.

        self.connection.close()

    def __getstate__(self) -> dict:
        state = self.__dict__.copy()
        state["connection_params"] = {
            "host": self.connection.connection_pool.connection_kwargs["host"],
            "port": self.connection.connection_pool.connection_kwargs["port"],
            "db": self.connection.connection_pool.connection_kwargs["db"],
            "password": self.connection.connection_pool.connection_kwargs["password"],
        }
        del state["connection"]
        return state

    def __setstate__(self, state: dict):
        connection_params = state.pop("connection_params")
        # we assume normal redis client.
        self.connection = redis.Redis(**connection_params)
        self.__dict__.update(state)


class AsyncRedisBasePersister(persistence.AsyncBaseStatePersister):
    """Main class for async Redis persister.

    .. warning::
        The synchronous persister closes the connection on deletion of the class using the ``__del__`` method.
        In an async context that is not reliable (the event loop may already be closed by the time ``__del__``
        gets invoked). Therefore, you are responsible for closing the connection yourself (i.e. manual cleanup).
        We suggest to use the persister either as a context manager through the ``async with`` clause or
        using the method ``.cleanup()``.


    This class is responsible for async persisting state data to a Redis database.
    It inherits from the AsyncBaseStatePersister class.
    """

    @classmethod
    def from_config(cls, config: dict) -> "AsyncRedisBasePersister":
        """Creates a new instance of the RedisBasePersister from a configuration dictionary."""
        return cls.from_values(**config)

    @classmethod
    def from_values(
        cls,
        host: str,
        port: int,
        db: int,
        password: str = None,
        serde_kwargs: dict = None,
        redis_client_kwargs: dict = None,
        namespace: str = None,
    ) -> "AsyncRedisBasePersister":
        """Creates a new instance of the AsyncRedisBasePersister from passed in values."""
        if redis_client_kwargs is None:
            redis_client_kwargs = {}
        connection = aredis.Redis(
            host=host, port=port, db=db, password=password, **redis_client_kwargs
        )
        return cls(connection, serde_kwargs, namespace)

    def __init__(
        self,
        connection,
        serde_kwargs: dict = None,
        namespace: str = None,
    ):
        """Initializes the AsyncRedisPersister class.

        :param connection: the redis connection object.
        :param serde_kwargs: serialization and deserialization keyword arguments to pass to state SERDE.
        :param namespace: The name of the project to optionally use in the key prefix.
        """
        self.connection = connection
        self.serde_kwargs = serde_kwargs or {}
        self.namespace = namespace if namespace else ""

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_value, traceback):
        await self.connection.aclose()
        return False

    def set_serde_kwargs(self, serde_kwargs: dict):
        """Sets the serde_kwargs for the persister."""
        self.serde_kwargs = serde_kwargs

    async def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        """List the app ids for a given partition key."""
        namespaced_partition_key = add_namespace_to_partition_key(partition_key, self.namespace)
        app_ids = await self.connection.zrevrange(namespaced_partition_key, 0, -1)
        return [app_id.decode() for app_id in app_ids]

    async def load(
        self, partition_key: str, app_id: str, sequence_id: int = None, **kwargs
    ) -> Optional[persistence.PersistedStateData]:
        """Load the state data for a given partition key, app id, and sequence id.

        If the sequence id is not given, it will be looked up in the Redis database. If it is not found, None will be returned.

        :param partition_key:
        :param app_id:
        :param sequence_id:
        :param kwargs:
        :return: Value or None.
        """
        namespaced_partition_key = add_namespace_to_partition_key(partition_key, self.namespace)
        if sequence_id is None:
            sequence_id = await self.connection.zscore(namespaced_partition_key, app_id)
            if sequence_id is None:
                return None
            sequence_id = int(sequence_id)
        key = self.create_key(app_id, partition_key, sequence_id)
        data = await self.connection.hgetall(key)
        if not data:
            return None
        _state = state.State.deserialize(json.loads(data[b"state"].decode()), **self.serde_kwargs)
        return {
            "partition_key": partition_key,
            "app_id": app_id,
            "sequence_id": sequence_id,
            "position": data[b"position"].decode(),
            "state": _state,
            "created_at": data[b"created_at"].decode(),
            "status": data[b"status"].decode(),
        }

    def create_key(self, app_id, partition_key, sequence_id):
        """Create a key for the Redis database."""
        return add_namespace_to_partition_key(
            f"{partition_key}:{app_id}:{sequence_id}", self.namespace
        )

    async def save(
        self,
        partition_key: str,
        app_id: str,
        sequence_id: int,
        position: str,
        state: state.State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        """Save the state data to the Redis database.

        :param partition_key:
        :param app_id:
        :param sequence_id:
        :param position:
        :param state:
        :param status:
        :param kwargs:
        :return:
        """
        key = self.create_key(app_id, partition_key, sequence_id)
        if await self.connection.exists(key):
            raise ValueError(f"partition_key:app_id:sequence_id[{key}] already exists.")
        json_state = json.dumps(state.serialize(**self.serde_kwargs))
        await self.connection.hset(
            key,
            mapping={
                "partition_key": partition_key,
                "app_id": app_id,
                "sequence_id": sequence_id,
                "position": position,
                "state": json_state,
                "status": status,
                "created_at": datetime.now(timezone.utc).isoformat(),
            },
        )
        namespaced_partition_key = add_namespace_to_partition_key(partition_key, self.namespace)
        await self.connection.zadd(namespaced_partition_key, {app_id: sequence_id})

    async def cleanup(self):
        """Closes the connection to the database."""
        await self.connection.aclose()


class RedisPersister(RedisBasePersister):
    """A class used to represent a Redis Persister.

    This class is deprecated. Use RedisBasePersister.from_values() instead.
    """

    def __init__(
        self,
        host: str,
        port: int,
        db: int,
        password: str = None,
        serde_kwargs: dict = None,
        redis_client_kwargs: dict = None,
        namespace: str = None,
    ):
        """Initializes the RedisPersister class.

        This is deprecated. Use RedisBasePersister.from_values() instead.

        :param host:
        :param port:
        :param db:
        :param password:
        :param serde_kwargs:
        :param redis_client_kwargs: Additional keyword arguments to pass to the redis.Redis client.
        :param namespace: The name of the project to optionally use in the key prefix.
        """
        if redis_client_kwargs is None:
            redis_client_kwargs = {}
        connection = redis.Redis(
            host=host, port=port, db=db, password=password, **redis_client_kwargs
        )
        super(RedisPersister, self).__init__(connection, serde_kwargs, namespace)


if __name__ == "__main__":
    # test the RedisBasePersister class
    persister = RedisBasePersister.from_values("localhost", 6379, 0)

    persister.initialize()
    persister.save("pk", "app_id", 2, "pos", state.State({"a": 1, "b": 2}), "completed")
    print(persister.list_app_ids("pk"))
    print(persister.load("pk", "app_id"))



---
File: /burr/burr/integrations/persisters/postgresql.py
---

"""This module is depricated. Please import from b_psycopg2.py."""

from burr.integrations import base
from burr.integrations.persisters.b_psycopg2 import PostgreSQLPersister as Psycopg2Persister

try:
    import psycopg2
except ImportError as e:
    base.require_plugin(e, "postgresql")


import logging

from burr.core import state

logger = logging.getLogger(__name__)
logger.warning(
    "This class is deprecated and has been moved. "
    "Please import PostgreSQLPersister from b_psycopg2.py."
)


class PostgreSQLPersister(Psycopg2Persister):
    """A class used to represent the Postgresql Persister.

    This class is deprecated and has been moved to b_psycopg2.py.
    """

    @classmethod
    def from_values(
        cls,
        db_name: str,
        user: str,
        password: str,
        host: str,
        port: int,
        table_name: str = "burr_state",
    ):
        """Builds a new instance of the PostgreSQLPersister from the provided values.

        :param db_name: the name of the PostgreSQL database.
        :param user: the username to connect to the PostgreSQL database.
        :param password: the password to connect to the PostgreSQL database.
        :param host: the host of the PostgreSQL database.
        :param port: the port of the PostgreSQL database.
        :param table_name:  the table name to store things under.
        """
        connection = psycopg2.connect(
            dbname=db_name, user=user, password=password, host=host, port=port
        )
        return Psycopg2Persister(connection, table_name)


if __name__ == "__main__":
    # test the PostgreSQLPersister class
    persister = PostgreSQLPersister.from_values(
        "postgres", "postgres", "my_password", "localhost", 54320, table_name="burr_state"
    )

    persister.initialize()
    persister.save("pk", "app_id", 1, "pos", state.State({"a": 1, "b": 2}), "completed")
    print(persister.list_app_ids("pk"))
    print(persister.load("pk", "app_id"))



---
File: /burr/burr/integrations/serde/__init__.py
---




---
File: /burr/burr/integrations/serde/langchain.py
---

# try to import to serialize Langchain messages
from langchain_core import documents as lc_documents
from langchain_core import load as lc_serde
from langchain_core import messages as lc_messages

from burr.core import serde


@serde.serialize.register(lc_documents.Document)
def serialize_lc_docs(value: lc_documents.Document, **kwargs) -> dict:
    """Serializes langchain documents."""
    if value.is_lc_serializable():
        lc_doc = lc_serde.dumpd(value)
        lc_doc[serde.KEY] = "lc_document"
        return lc_doc
    elif hasattr(value, "to_document") and hasattr(value, "state"):
        # attempt to serialize the state as well
        return {
            "doc": serialize_lc_docs(value.to_document()),
            "state": serde.serialize(value.state, **kwargs),
            serde.KEY: "lc_document_with_state",
        }
    elif hasattr(value, "to_document"):
        # we lose some state here, but it's better than nothing
        return serialize_lc_docs(value.to_document())
    else:
        # d.page_content  # hack because not all documents are serializable
        return {"value": value.page_content, serde.KEY: "lc_document_hack"}


@serde.deserializer.register("lc_document")
def deserialize_lc_document(value: dict, **kwargs) -> lc_documents.Document:
    """Deserializes langchain documents."""
    value.pop(serde.KEY)
    return lc_serde.load(value)


@serde.deserializer.register("lc_document_with_state")
def deserialize_lc_document_with_state(value: dict, **kwargs) -> lc_documents.Document:
    """Deserializes langchain documents with state."""
    from langchain_community.document_transformers.embeddings_redundant_filter import (
        _DocumentWithState,
    )

    value.pop(serde.KEY)
    doc = lc_serde.load(value["doc"])
    state = serde.deserialize(value["state"], **kwargs)
    return _DocumentWithState(page_content=doc.page_content, metadata=doc.metadata, state=state)


@serde.deserializer.register("lc_document_hack")
def deserialize_lc_document_hack(value: dict, **kwargs) -> lc_documents.Document:
    """Deserializes langchain documents that we didn't know about into a document."""
    return lc_documents.Document(page_content=value["value"])


@serde.serialize.register(lc_messages.BaseMessage)
def serialize_lc_messages(value: lc_messages.BaseMessage, **kwargs) -> dict:
    """Serializes langchain messages."""
    if value.is_lc_serializable():
        lc_message = lc_messages.message_to_dict(value)
        lc_message[serde.KEY] = "lc_message"
        return lc_message
    else:
        return {"value": value.content, "type": value.type, serde.KEY: "lc_message_hack"}


@serde.deserializer.register("lc_message")
def deserialize_lc_message(value: dict, **kwargs) -> lc_messages.BaseMessage:
    """Deserializes langchain messages."""
    value.pop(serde.KEY)  # note this mutates the dict
    return lc_messages._message_from_dict(value)


@serde.deserializer.register("lc_message_hack")
def deserialize_lc_message_hack(value: dict, **kwargs) -> lc_messages.BaseMessage:
    """Deserializes langchain messages that we didn't know how to serialize."""
    return lc_messages.BaseMessage(content=value["value"], type=value["type"])



---
File: /burr/burr/integrations/serde/pandas.py
---

# try to import to serialize Pandas Objects
import hashlib
import os

import pandas as pd

from burr.core import serde


@serde.serialize.register(pd.DataFrame)
def serialize_pandas_df(value: pd.DataFrame, pandas_kwargs: dict, **kwargs) -> dict:
    """Custom serde for pandas dataframes.

    Saves the dataframe to a parquet file and returns the path to the file.
    Requires a `path` key in the `pandas_kwargs` dictionary.

    :param value: the pandas dataframe to serialize.
    :param pandas_kwargs: `path` key is required -- this is the base path to save the parquet file. As \
    well as any other kwargs to pass to the pandas to_parquet function.
    :param kwargs:
    :return:
    """
    hash_object = hashlib.sha256()
    hash_value = str(value.columns) + str(value.shape) + str(value.dtypes)
    hash_object.update(hash_value.encode())

    # Return the hexadecimal representation of the hash
    file_name = f"df_{hash_object.hexdigest()}.parquet"
    kwargs = pandas_kwargs.copy()
    base_path: str = kwargs.pop("path")
    if not os.path.exists(base_path):
        os.makedirs(base_path)
    saved_to = os.path.join(base_path, file_name)
    value.to_parquet(path=saved_to, **kwargs)
    return {serde.KEY: "pandas.DataFrame", "path": saved_to}


@serde.deserializer.register("pandas.DataFrame")
def deserialize_pandas_df(value: dict, pandas_kwargs: dict, **kwargs) -> pd.DataFrame:
    """Custom deserializer for pandas dataframes.

    :param value: the dictionary to pull the path from to load the parquet file.
    :param pandas_kwargs: other args to pass to the pandas read_parquet function.
    :param kwargs:
    :return: pandas dataframe
    """
    kwargs = pandas_kwargs.copy()
    if "path" in kwargs:
        # remove this to not clash; we already have the full path.
        kwargs.pop("path")
    return pd.read_parquet(value["path"], **kwargs)



---
File: /burr/burr/integrations/serde/pickle.py
---

# Pickle serde registration
# This is not automatically registered because we want to register
# it based on class type.
import pickle

from burr.core import serde


def register_type_to_pickle(cls):
    """Register a class to be serialized/deserialized using pickle.

    Note: `pickle_kwargs` are passed to the pickle.dumps and pickle.loads functions.

    This will register the passed in class to be serialized/deserialized using pickle.

    .. code-block:: python

        class User:
            def __init__(self, name, email):
                self.name = name
                self.email = email

        from burr.integrations.serde import pickle
        pickle.register_type_to_pickle(User) # this will register the User class to be serialized/deserialized using pickle.


    :param cls: The class to register
    """

    @serde.serialize.register(cls)
    def serialize_pickle(value: cls, pickle_kwargs: dict = None, **kwargs) -> dict:
        """Serializes the value using pickle.

        :param value: the value to serialize.
        :param pickle_kwargs: not required. Optional.
        :param kwargs:
        :return: dictionary of serde.KEY and value
        """
        if pickle_kwargs is None:
            pickle_kwargs = {}
        return {
            serde.KEY: "pickle",
            "value": pickle.dumps(value, **pickle_kwargs),
        }

    @serde.deserializer.register("pickle")
    def deserialize_pickle(value: dict, pickle_kwargs: dict = None, **kwargs) -> cls:
        """Deserializes the value using pickle.

        :param value: the value to deserialize from.
        :param pickle_kwargs: note required. Optional.
        :param kwargs:
        :return: object of type cls
        """
        if pickle_kwargs is None:
            pickle_kwargs = {}
        return pickle.loads(value["value"], **pickle_kwargs)



---
File: /burr/burr/integrations/serde/pydantic.py
---

# try to import to serialize Pydantic Objects
import importlib

import pydantic

from burr.core import serde


@serde.serialize.register(pydantic.BaseModel)
def serialize_pydantic(value: pydantic.BaseModel, **kwargs) -> dict:
    """Uses pydantic to dump the model to a dictionary and then adds the __pydantic_class to the dictionary."""
    _dict = value.model_dump()
    _dict[serde.KEY] = "pydantic"
    # get qualified name of pydantic class. The module name should be fully qualified.
    _dict["__pydantic_class"] = f"{value.__class__.__module__}.{value.__class__.__name__}"
    return _dict


@serde.deserializer.register("pydantic")
def deserialize_pydantic(value: dict, **kwargs) -> pydantic.BaseModel:
    """Deserializes a pydantic object from a dictionary.
    This will pop the __pydantic_class and then import the class.
    """
    value.pop(serde.KEY)
    pydantic_class_name = value.pop("__pydantic_class")
    module_name, class_name = pydantic_class_name.rsplit(".", 1)
    module = importlib.import_module(module_name)
    pydantic_class = getattr(module, class_name)
    return pydantic_class.model_validate(value)



---
File: /burr/burr/integrations/__init__.py
---




---
File: /burr/burr/integrations/base.py
---

def require_plugin(import_error: ImportError, plugin_name: str):
    raise ImportError(
        f"Missing plugin {plugin_name}! To use the {plugin_name} plugin, you must install the 'extras' target [{plugin_name}] with burr[{plugin_name}] "
        f"(replace with your package manager of choice). Note that, if you're using poetry, you cannot install burr with burr[start], so "
        f"you'll have to install the components individually. See https://burr.dagworks.io/getting_started/install/ "
        f"for more details."
    ) from import_error



---
File: /burr/burr/integrations/hamilton.py
---

import dataclasses
from typing import Any, Dict, Literal, Tuple, Union

from burr.integrations.base import require_plugin

try:
    from hamilton.driver import Driver
except ImportError as e:
    require_plugin(
        e,
        ["sf-hamilton"],
        "hamilton",
    )

from burr.core import Action, State


@dataclasses.dataclass
class StateSource:
    state_key: str
    missing: Literal["drop", "error"] = "error"
    # default: Any = None


MissingAction = Literal["drop", "error"]


@dataclasses.dataclass
class LiteralSource:
    value: Any


Input = Union[StateSource, LiteralSource]


def from_state(
    key: str,
    missing: MissingAction = "error",
) -> StateSource:
    """Indicates that an input should come from state.
    Specify "missing" to allow for missing keys to be dropped or raise an error.

    :param key: Key in state to use
    :param missing: What to do if the key is missing
    :return: A StateSource object -- use by Hamilton(inputs=...)
    """
    return StateSource(key, missing)


def from_value(value: Any) -> LiteralSource:
    """Indicates that an input should come from a literal (variable/constant) value.
    Use this if you just want to fix a parameter into the Hamilton DAG.

    :param value: Value to use
    :return: A LiteralSource object -- use by Hamilton(inputs=...)
    """
    return LiteralSource(value)


@dataclasses.dataclass
class Output:
    key: str
    mode: Literal["update", "append"] = "update"


def update_state(key: str) -> Output:
    """At the update step of a Hamilton Action, call state.update to the key field of state.
    Used with outputs= parameter of Hamilton(...)

    :param key: Field in state to udpate
    :return: An Output object
    """
    return Output(key, "update")


def append_state(key: str):
    """At the update state of a Hamilton Action, call state.append to the key field of state.
    Used with outputs= parameter of Hamilton(...)

    :param key: Field in state to append to
    :return: An Output object
    """
    return Output(key, "append")


DEFAULT_DRIVER = None


class Hamilton(Action):
    @staticmethod
    def set_driver(driver: Driver):
        """Default method if all the hamilton nodes are using the same driver.
        Will set globally, so be careful.

        Note that the driver must have the default adapter (so that it returns a dict).

        :param driver: Driver to use
        """
        global DEFAULT_DRIVER
        DEFAULT_DRIVER = driver

    def __init__(
        self,
        inputs: Dict[str, Input],
        outputs: Dict[str, Output],
        driver: Driver = None,
    ):
        """Creates a Hamilton action. Allows youy to specify:
        1. How to wire state fields into hamilton inputs
        2  How to wire hamilton outputs into state fields

        Note that we o not distinguish between overrides and inputs -- we intelligently decide
        which are which based on the driver's available variables.

        :param inputs:
        :param outputs:
        :param driver:
        :param name:
        """
        super(Hamilton, self).__init__()
        if driver is None and DEFAULT_DRIVER is None:
            raise ValueError(
                "Driver must be set before creating a Hamilton function. "
                "You can do so with Hamilton.set_driver(...) to set it globally, "
                "or pass in driver to the Hamilton(...) constructor."
            )
        self._driver = driver if driver is not None else DEFAULT_DRIVER
        self._inputs = inputs
        self._outputs = outputs

    @property
    def driver(self):
        return self._driver

    def _extract_inputs_overrides(self, state: State) -> Tuple[dict, dict]:
        """Extracts the inputs and overrides from the state."""

        def resolve_value(source: Input) -> Any:
            if isinstance(source, StateSource):
                if source.state_key in state:
                    return state[source.state_key]
                else:
                    if source.missing == "error":
                        raise ValueError(f"Missing state key {source.state_key}")
                    else:
                        return None
            else:
                return source.value

        inputs = {}
        overrides = {}
        dr_vars = {node.name: node for node in self._driver.list_available_variables()}
        for key, source in self._inputs.items():
            if key not in dr_vars:
                raise ValueError(
                    f"Input {key} not available in driver -- "
                    f"available variables are: {list(self._driver.list_available_variables())}"
                )
            node = dr_vars[key]
            if node.is_external_input:
                inputs[node.name] = resolve_value(source)
            else:
                overrides[node.name] = resolve_value(source)
        return inputs, overrides

    def run(self, state: State) -> dict:
        """Runs a hamilton action, using the driver to execute the hamilton DAG.

        :param state: The state to use
        :return: The results of the hamilton DAG
        """
        inputs, overrides = self._extract_inputs_overrides(state)
        result = self._driver.raw_execute(
            list(self._outputs.keys()),
            overrides=overrides,
            inputs=inputs,
        )
        return result

    def update(self, result: dict, state: State) -> State:
        """Updates the state with the results of the hamilton action, as specified in the outputs.

        :param result: The results of the hamilton DAG
        :param state: The state to update
        """
        update_values = {
            self._outputs[key].key: value
            for key, value in result.items()
            if self._outputs[key].mode == "update"
        }
        append_values = {
            self._outputs[key].key: value
            for key, value in result.items()
            if self._outputs[key].mode == "append"
        }
        return state.update(**update_values).append(**append_values)

    @property
    def reads(self) -> list[str]:
        """The set of state items this action reads.
        TODO:
        1. Parse the inputs and overrides to determine which state items are read
        2. Return them
        """
        return [
            source.state_key for source in self._inputs.values() if isinstance(source, StateSource)
        ]

    @property
    def writes(self) -> list[str]:
        """The set of state items this action writes.
        TODO:
        0. Determine how we want to represent writing final vars to the state
        1. Parse the final_vars to determine which state items are written to
        3. Return them
        :return: The set of state items this action writes.
        """
        return [source.key for source in self._outputs.values()]

    def visualize_step(self, **kwargs):
        """Visualizes execution for a Hamilton step"""
        dr = self._driver
        inputs = {key: ... for key in self._inputs}
        overrides = inputs
        final_vars = list(self._outputs.keys())
        return dr.visualize_execution(
            final_vars=final_vars,
            inputs=inputs,
            overrides=overrides,
            bypass_validation=True,
            **kwargs,
        )



---
File: /burr/burr/integrations/haystack.py
---

import inspect
from collections.abc import Mapping
from typing import Any, Optional, Sequence, Union

from haystack import Pipeline
from haystack.core.component import Component
from haystack.core.component.types import _empty as haystack_empty

from burr.core.action import Action
from burr.core.graph import Graph, GraphBuilder
from burr.core.state import State


# TODO show OpenTelemetry integration
class HaystackAction(Action):
    """Burr ``Action`` wrapping a Haystack ``Component``.

    Haystack ``Component`` is the basic block of a Haystack ``Pipeline``.
    A ``Component`` is instantiated, then it receives inputs for its ``.run()`` method
    and returns output values.

    Learn more about components here: https://docs.haystack.deepset.ai/docs/custom-components
    """

    def __init__(
        self,
        component: Component,
        reads: Union[list[str], dict[str, str]],
        writes: Union[list[str], dict[str, str]],
        name: Optional[str] = None,
        bound_params: Optional[dict] = None,
        do_warm_up: bool = True,
    ):
        """Create a Burr ``Action`` from a Haystack ``Component``.

        :param component: Haystack ``Component`` to wrap
        :param reads: State fields read and passed to ``Component.run()``.
        Use a mapping {socket: state_field} to rename Haystack input sockets (see example).
        :param writes: State fields where results of ``Component.run()`` are written.
        Use a mapping {state_field: socket} to rename Haystack output sockets (see example).
        :param name: Name of the action. Can be set later via ``.with_name()``
        or in ``ApplicationBuilder.with_actions()``.
        :param bound_params: Parameters to bind to the ``Component.run()`` method.
        :param do_warm_up: If True, try to call ``Component.warm_up()`` if it exists.
        If False, we assume ``.warm_up()`` was called before creating the ``HaystackAction``.
        Read more about ``.warm_up()`` in the Haystack documentation: https://docs.haystack.deepset.ai/reference/pipeline-api#pipelinewarm_up

        Pass the mapping ``{"foo": "state_field"}`` to read the value of ``state_field`` on the Burr state
        and pass it to ``Component.run()`` as ``foo``.

            .. code-block:: python

                @component
                class HaystackComponent:
                    @component.output_types()
                    def run(self, foo: int) -> dict:
                        return {}

                HaystackAction(
                    component=HaystackComponent(),
                    reads={"foo": "state_field"},
                    writes=[]
                )

        Pass the mapping ``{"state_field": "bar"}`` to get the ``bar`` value from the results
        of ``.run()`` and set the field ``state_field`` on the Burr state

            .. code-block:: python

                @component
                class HaystackComponent:
                    @component.output_types(bar=int)
                    def run(self) -> dict:
                        return {"bar": 1}

                HaystackAction(
                    component=HaystackComponent(),
                    reads=[],
                    writes={"state_field": "bar"}
                )

        Basic usage:

            .. code-block:: python

                from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
                from haystack.document_stores.in_memory import InMemoryDocumentStore
                from burr.core import ApplicationBuilder
                from burr.integrations.haystack import HaystackAction

                retrieve_documents = HaystackAction(
                    component=InMemoryEmbeddingRetriever(InMemoryDocumentStore()),
                    name="retrieve_documents",
                    reads=["query_embedding"],
                    writes=["documents"],
                )

                app = (
                ApplicationBuilder()
                .with_actions(retrieve_documents)
                .with_transitions("retrieve_documents", "retrieve_documents")
                .with_entrypoint("retrieve_documents")
                .build()
                )
        """
        self._component = component
        self._name = name
        self._bound_params = bound_params if bound_params is not None else {}
        self._do_warm_up = do_warm_up

        if self._do_warm_up is True:
            self._try_warm_up()

        # NOTE input and output socket mappings are kept separately to avoid naming conflicts.
        if isinstance(reads, Mapping):
            self._input_socket_mapping = reads
            self._reads = list(set(reads.values()))
        elif isinstance(reads, Sequence):
            self._input_socket_mapping = {socket_name: socket_name for socket_name in reads}
            self._reads = reads
        else:
            raise TypeError(f"`reads` must be a sequence or mapping. Received: {type(reads)}")

        self._validate_input_sockets()

        if isinstance(writes, Mapping):
            self._output_socket_mapping = writes
            self._writes = list(writes.keys())
        elif isinstance(writes, Sequence):
            self._output_socket_mapping = {socket_name: socket_name for socket_name in writes}
            self._writes = writes
        else:
            raise TypeError(f"`writes` must be a sequence or mapping. Received: {type(writes)}")

        self._validate_output_sockets()

        self._required_inputs, self._optional_inputs = self._get_required_and_optional_inputs()

    def _try_warm_up(self) -> None:
        if hasattr(self._component, "warm_up") is True:
            self._component.warm_up()

    def _validate_input_sockets(self) -> None:
        """Check that input socket names passed by the user match the Component's input sockets"""
        # NOTE those are internal attributes, but we expect them be stable.
        # reference: https://github.com/deepset-ai/haystack/blob/906177329bcc54f6946af361fcd3d0e334e6ce5f/haystack/core/component/component.py#L371
        component_inputs = self._component.__haystack_input__._sockets_dict.keys()
        for socket_name in self._input_socket_mapping.keys():
            if socket_name not in component_inputs:
                raise ValueError(
                    f"Socket `{socket_name}` not found in `Component` inputs: {component_inputs}"
                )

    def _validate_output_sockets(self) -> None:
        """Check that output socket names passed by the user match the Component's output sockets"""
        # NOTE those are internal attributes, but we expect them be stable.
        # reference: https://github.com/deepset-ai/haystack/blob/906177329bcc54f6946af361fcd3d0e334e6ce5f/haystack/core/component/component.py#L449
        component_outputs = self._component.__haystack_output__._sockets_dict.keys()
        for socket_name in self._output_socket_mapping.values():
            if socket_name not in component_outputs:
                raise ValueError(
                    f"Socket `{socket_name}` not found in `Component` outputs: {component_outputs}"
                )

    @property
    def component(self) -> Component:
        """Haystack `Component` used by this action."""
        return self._component

    @property
    def reads(self) -> list[str]:
        """State fields read and passed to `Component.run()`"""
        return self._reads

    @property
    def writes(self) -> list[str]:
        """State fields where results of `Component.run()` are written."""
        return self._writes

    def _get_required_and_optional_inputs(self) -> tuple[set[str], set[str]]:
        """Iterate over Haystack Component input sockets and inspect default values.
        If we expect the value to come from state or it's a bound parameter, skip this socket.
        Otherwise, if it has a default value, it's optional.
        """
        required_inputs, optional_inputs = set(), set()
        # NOTE those are internal attributes, but we expect them be stable.
        # reference: https://github.com/deepset-ai/haystack/blob/906177329bcc54f6946af361fcd3d0e334e6ce5f/haystack/core/component/component.py#L371
        for socket_name, input_socket in self._component.__haystack_input__._sockets_dict.items():
            state_field_name = self._input_socket_mapping.get(socket_name, socket_name)
            if state_field_name in self.reads or state_field_name in self._bound_params:
                continue

            if input_socket.default_value == haystack_empty:
                required_inputs.add(state_field_name)
            else:
                optional_inputs.add(state_field_name)

        return required_inputs, optional_inputs

    @property
    def inputs(self) -> list[str]:
        """Return a list of required inputs for ``Component.run()``
        This corresponds to the Component's required input socket names.
        """
        return list(self._required_inputs)

    @property
    def optional_and_required_inputs(self) -> tuple[set[str], set[str]]:
        """Return a tuple of required and optional inputs for ``Component.run()``
        This corresponds to the Component's required and optional input socket names.
        """
        return self._required_inputs, self._optional_inputs

    def run(self, state: State, **run_kwargs) -> dict[str, Any]:
        """Call the Haystack `Component.run()` method.

        :param state: State object of the application. It contains some input values
           for ``Component.run()``.
        :param run_kwargs: User-provided inputs for ``Component.run()``.
        :return: Dictionary of results with mapping ``{socket_name: value}``.

        Note, values come from 3 sources:
        - state (from previous actions)
        - run_kwargs (inputs from ``Application.run()``)
        - bound parameters (from ``HaystackAction`` instantiation)
        """
        values = {}

        # here, precedence matters. Alternatively, we could unpack all dictionaries at once
        # which would throw an error for key collisions
        for input_socket_name, value in self._bound_params.items():
            values[input_socket_name] = value

        for input_socket_name, state_field_name in self._input_socket_mapping.items():
            try:
                values[input_socket_name] = state[state_field_name]
            except KeyError as e:
                raise ValueError(f"No value found in state for field: {state_field_name}") from e

        for input_socket_name, value in run_kwargs.items():
            values[input_socket_name] = value

        return self._component.run(**values)

    def update(self, result: dict, state: State) -> State:
        """Update the state using the results of ``Component.run()``.
        The output socket name is mapped to the Burr state field name.

        Values returned by ``Component.run()`` that aren't in ``writes`` are ignored.
        """
        # TODO we could want to handle ``.update()`` and ``.append()`` differently
        state_update = {}

        for state_field_name, output_socket_name in self._output_socket_mapping.items():
            if state_field_name in self.writes:
                try:
                    state_update[state_field_name] = result[output_socket_name]
                except KeyError as e:
                    raise ValueError(
                        f"Socket `{output_socket_name}` missing from output of `Component.run()`"
                    ) from e
        return state.update(**state_update)

    def get_source(self) -> str:
        """Return the source code of the Haystack ``Component``.

        NOTE. This doesn't include the initialization parameters of the ``Component``.
        This can be obtained using``HaystackAction().component.to_dict()``, but this
        method might is not implemented for all components.
        """
        return inspect.getsource(self._component.__class__)


def _socket_name_mapping(sockets_connections: list[tuple[str, str]]) -> dict[str, str]:
    """Map socket names to a single socket name.

    In Haystack, components communicate via sockets. A socket called
    "embedding" in one component can be renamed to "query_embedding" when
    passed to another component.

    In Burr, there is a single state object so we need a mapping to resolve
    that `embedding` and `query_embedding` point to the same value. This function
    creates a mapping {socket_name: state_field} to rename sockets when creating
    the Burr `Graph`.
    """
    all_connections: dict[str, set[str]] = {}
    for from_, to in sockets_connections:
        if from_ not in all_connections:
            all_connections[from_] = {from_}
        all_connections[from_].add(to)

        if to not in all_connections:
            all_connections[to] = {to}
        all_connections[to].add(from_)

    reduced_mapping: dict[str, str] = {}
    for key, values in all_connections.items():
        unique_name = min(values)
        reduced_mapping[key] = unique_name

    return reduced_mapping


def _connected_inputs(pipeline) -> dict[str, list[str]]:
    """Get all input sockets that are connected to other components."""
    return {
        name: [
            socket.name
            for socket in data.get("input_sockets", {}).values()
            if socket.is_variadic or socket.senders
        ]
        for name, data in pipeline.graph.nodes(data=True)
    }


def _connected_outputs(pipeline) -> dict[str, list[str]]:
    """Get all output sockets that are connected to other components."""
    return {
        name: [
            socket.name for socket in data.get("output_sockets", {}).values() if socket.receivers
        ]
        for name, data in pipeline.graph.nodes(data=True)
    }


def haystack_pipeline_to_burr_graph(pipeline: Pipeline) -> Graph:
    """Convert a Haystack `Pipeline` to a Burr `Graph`.

    NOTE. This currently doesn't support Haystack pipelines with
    parallel branches. Learn more https://docs.haystack.deepset.ai/docs/pipelines#branching

    From the Haystack `Pipeline`, we can easily retrieve transitions.
    For actions, we need to create `HaystackAction` from components
    and map their sockets to Burr state fields

    EXPERIMENTAL: This feature is experimental and may change in the future.
    Changes to Haystack or Burr could impact this function. Please let us know if
    you encounter any issues.
    """

    # get all socket connections in the pipeline
    sockets_connections = [
        (edge_data["from_socket"].name, edge_data["to_socket"].name)
        for _, _, edge_data in pipeline.graph.edges.data()
    ]
    socket_mapping = _socket_name_mapping(sockets_connections)

    transitions = [(from_, to) for from_, to, _ in pipeline.graph.edges]

    # get all input and output sockets that are connected to other components
    connected_inputs = _connected_inputs(pipeline)
    connected_outputs = _connected_outputs(pipeline)

    actions = []
    for component_name, component in pipeline.walk():
        inputs_mapping = {
            socket_name: socket_mapping[socket_name]
            for socket_name in connected_inputs[component_name]
        }
        outputs_mapping = {
            socket_mapping[socket_name]: socket_name
            for socket_name in connected_outputs[component_name]
        }

        haystack_action = HaystackAction(
            name=component_name,
            component=component,
            reads=inputs_mapping,
            writes=outputs_mapping,
        )
        actions.append(haystack_action)

    return GraphBuilder().with_actions(*actions).with_transitions(*transitions).build()



---
File: /burr/burr/integrations/notebook.py
---

import enum
import os
import subprocess

from IPython.core.magic import Magics, line_magic, magics_class
from IPython.core.magic_arguments import argument, magic_arguments, parse_argstring
from IPython.core.shellapp import InteractiveShellApp
from IPython.display import IFrame, display


class NotebookEnvironment(enum.Enum):
    JUPYTER = enum.auto()
    COLAB = enum.auto()
    VSCODE = enum.auto()
    DATABRICKS = enum.auto()
    KAGGLE = enum.auto()


def identify_notebook_environment(ipython: InteractiveShellApp) -> NotebookEnvironment:
    if os.environ.get("VSCODE_PID"):
        return NotebookEnvironment.VSCODE

    try:
        import google.colab  # noqa: F401

        return NotebookEnvironment.COLAB
    except ModuleNotFoundError:
        pass

    # TODO add Databricks implementation
    try:
        import dbruntime  # noqa: F401

        return NotebookEnvironment.DATABRICKS
    except ModuleNotFoundError:
        pass

    # TODO add Kaggle implementation
    try:
        import kaggle  # noqa: F401

        return NotebookEnvironment.KAGGLE
    except ModuleNotFoundError:
        pass

    # this is the base case. IPKernelApp should always be available
    if "IPKernelApp" in ipython.config:
        return NotebookEnvironment.JUPYTER

    raise RuntimeError(
        f"Unknown notebook environment. Known environments: {list(NotebookEnvironment)}"
    )


def launch_ui_colab():
    """Opens a Google Colab port and launches the Burr UI in a subprocess.

    Using a subprocess ensures that the Burr server logs aren't displayed in
    Colab cell outputs.

    NOTE. This will not work in a Jupyter notebook
    """
    from google.colab.output import eval_js

    PORT = 7241
    burr_ui_url = eval_js(f"google.colab.kernel.proxyPort({PORT})")
    process = subprocess.Popen(
        [
            "python",
            "-c",
            f"import uvicorn; from burr.tracking.server.run import app; uvicorn.run(app, host='127.0.0.1', port={PORT})",
        ],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    return process, burr_ui_url


def launch_ui_jupyter():
    """Launch the Burr UI in a subprocess.

    Using a subprocess ensures that the Burr server logs aren't displayed in Colab cell outputs
    """
    HOST = "127.0.0.1"
    PORT = 7241
    process = subprocess.Popen(
        [
            "python",
            "-c",
            f"import uvicorn; from burr.tracking.server.run import app; uvicorn.run(app, host='{HOST}', port={PORT})",
        ],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    return process, f"http://{HOST}:{PORT}"


def launch_ui(notebook_env: NotebookEnvironment) -> tuple:
    process, url = None, None
    if notebook_env == NotebookEnvironment.COLAB:
        process, url = launch_ui_colab()
    else:
        try:
            process, url = launch_ui_jupyter()
        except ModuleNotFoundError as e:
            raise RuntimeError(
                f"Failed to launch Burr UI for environment {notebook_env}. Please report this issue."
            ) from e

    return process, url


@magics_class
class NotebookMagics(Magics):
    def __init__(self, notebook_env: NotebookEnvironment, **kwargs):
        super().__init__(**kwargs)
        self.notebook_env = notebook_env
        self.process = None
        self.url = None

    @magic_arguments()
    @argument(
        "--height",
        "-h",
        default=400,
        type=int,
        help="Height of the Burr UI iframe specified as a number of pixels",
    )
    @argument(
        "--no-iframe",
        action="store_true",
        help="Passing this flag prints the URL of the launched Burr UI instead of displaying an iframe.",
    )
    @line_magic
    def burr_ui(self, line):
        """Launch the Burr UI from a notebook cell"""
        args = parse_argstring(self.burr_ui, line)

        if self.process is None:
            self.process, self.url = launch_ui(self.notebook_env)
        else:
            # if .poll() is not None, then subprocess exited. Try launching the server again
            # TODO investigate `.returncode` for better failure/retry handling
            if self.process.poll() is not None:
                self.process, self.url = launch_ui(self.notebook_env)

        if args.no_iframe is True:
            print(f"Burr UI: {self.url}")
        else:
            display(IFrame(self.url, width="100%", height=args.height))


def load_ipython_extension(ipython: InteractiveShellApp):
    """
    Any module file that define a function named `load_ipython_extension`
    can be loaded via `%load_ext module.path` or be configured to be
    autoloaded by IPython at startup time.
    """
    ipython.register_magics(NotebookMagics(notebook_env=identify_notebook_environment(ipython)))



---
File: /burr/burr/integrations/opentelemetry.py
---

import dataclasses
import datetime
import importlib
import importlib.metadata
import json
import logging
import random
import sys
from contextvars import ContextVar
from typing import Any, Dict, List, Literal, Optional, Sequence, Tuple

from burr.integrations.base import require_plugin

logger = logging.getLogger(__name__)

try:
    from opentelemetry import context
    from opentelemetry import context as context_api
    from opentelemetry import trace
    from opentelemetry.sdk.trace import Span, SpanProcessor, TracerProvider
    from opentelemetry.trace import get_current_span, use_span
except ImportError as e:
    require_plugin(
        e,
        "opentelemetry",
    )

from burr.common import types as burr_types
from burr.core import Action, ApplicationGraph, State, serde
from burr.lifecycle import (
    PostApplicationExecuteCallHook,
    PostRunStepHook,
    PreApplicationExecuteCallHook,
    PreRunStepHook,
)
from burr.lifecycle.base import DoLogAttributeHook, ExecuteMethod, PostEndSpanHook, PreStartSpanHook
from burr.tracking import LocalTrackingClient
from burr.tracking.base import SyncTrackingClient
from burr.visibility import ActionSpan

# We have to keep track of tokens for the span
# As OpenTel has some weird behavior around context managers, we have to account for the latest ones we started
# This way we can pop one off and know where to set the current one (as the parent, when the next one ends)
token_stack = ContextVar[Optional[List[Tuple[object, Span]]]]("token_stack", default=None)


@dataclasses.dataclass
class FullSpanContext:
    action_span: ActionSpan
    partition_key: str
    app_id: str


span_map = {}


def cache_span(span: Span, context: FullSpanContext) -> Span:
    span_map[span.get_span_context().span_id] = context
    return span


def uncache_span(span: Span) -> Span:
    del span_map[span.get_span_context().span_id]
    return span


def get_cached_span(span_id: int) -> Optional[FullSpanContext]:
    return span_map.get(span_id)


tracker_context = ContextVar[Optional[SyncTrackingClient]]("tracker_context", default=None)


def _is_homogeneous_sequence(value: Sequence):
    if len(value) == 0:
        return True
    first_type = type(value[0])
    return all([isinstance(val, first_type) for val in value])


def convert_to_otel_attribute(attr: Any):
    if isinstance(attr, (str, bool, float, int)):
        return attr
    elif isinstance(attr, Sequence):
        if _is_homogeneous_sequence(attr):
            return list(attr)
    try:
        return json.dumps(serde.serialize(attr))
    except Exception as e:
        logger.error(f"Failed to serialize attribute: {attr}, got error: {e}")
        return str(attr)


def _exit_span(exc: Optional[Exception] = None):
    """Ditto with _enter_span, but for exiting the span. Pops the token off the stack and detaches the context."""
    stack = token_stack.get()[:]
    token, span = stack.pop()
    token_stack.set(stack)
    context.detach(token)
    if exc:
        span.set_status(trace.Status(trace.StatusCode.ERROR, str(exc)))
    else:
        span.set_status(trace.Status(trace.StatusCode.OK))
    span.end()
    return span


def _enter_span(name: str, tracer: trace.Tracer):
    """Utility function to enter a span. Starts, sets the current context, and adds it to the token stack.

    See this for some background on why start_span doesn't really work. We could use start_as_current_span,
    but this is a bit more explicit.
    """
    span = tracer.start_span(
        name=name,
        record_exception=False,  # we'll handle this ourselves
        set_status_on_exception=False,
    )
    ctx = trace.set_span_in_context(span)
    token = context.attach(ctx)
    stack = (token_stack.get() or [])[:]
    stack.append((token, span))
    token_stack.set(stack)
    return span


class OpenTelemetryBridge(
    PreApplicationExecuteCallHook,
    PostApplicationExecuteCallHook,
    PreRunStepHook,
    PostRunStepHook,
    PreStartSpanHook,
    PostEndSpanHook,
    DoLogAttributeHook,
):
    """Adapter to log Burr events to OpenTelemetry. At a high level, this works as follows:

    1. On any of the start/pre hooks (pre_run_execute_call, pre_run_step, pre_start_span), we start a new span
    2. On any of the post ones we exit the span, accounting for the error (setting it if needed)
    3. On do_log_attributes, we log the attributes to the current span -- these are serialized using the serde module

    This works by logging to OpenTelemetry, and setting the span processor to be the right one (that knows about the tracker).

    You can use this as follows:

    .. code-block:: python

        # replace with instructions from your prefered vendor
        my_vendor_library_or_tracer_provider.init()

        app = (
            ApplicationBuilder()
            .with_entrypoint("prompt")
            .with_state(chat_history=[])
            .with_graph(graph)
            .with_hooks(OpenTelemetryBridge())
            .build()
        )

        app.run() # will log to OpenTelemetry
    """

    def __init__(self, tracer_name: str = None, tracer: trace.Tracer = None):
        """Initializes an OpenTel adapter. Passes in a tracer_name or a tracer object,
        should only pass one.

        :param tracer_name: Name of the tracer if you want it to initialize for you -- not including it will use a default
        :param tracer: Tracer object if you want to pass it in yourself
        """
        if tracer_name and tracer:
            raise ValueError(
                f"Only pass in one of tracer_name or tracer, not both, got: tracer_name={tracer_name} and tracer={tracer}"
            )
        if tracer:
            self.tracer = tracer
        else:
            self.tracer = trace.get_tracer(__name__ if tracer_name is None else tracer_name)

    def pre_run_execute_call(
        self,
        *,
        method: ExecuteMethod,
        **future_kwargs: Any,
    ):
        # TODO -- handle links -- we need to wire this through
        _enter_span(method.value, self.tracer)

    def do_log_attributes(
        self,
        *,
        attributes: Dict[str, Any],
        **future_kwargs: Any,
    ):
        otel_span = get_current_span()
        if otel_span is None:
            logger.warning(
                "Attempted to log attributes from the tracker outside of a span, ignoring"
            )
            return
        otel_span.set_attributes(
            {key: convert_to_otel_attribute(value) for key, value in attributes.items()}
        )

    def pre_run_step(
        self,
        *,
        action: "Action",
        **future_kwargs: Any,
    ):
        _enter_span(action.name, self.tracer)

    def pre_start_span(
        self,
        *,
        span: "ActionSpan",
        **future_kwargs: Any,
    ):
        _enter_span(span.name, self.tracer)

    def post_end_span(
        self,
        *,
        span: "ActionSpan",
        **future_kwargs: Any,
    ):
        # TODO -- wire through exceptions
        _exit_span()

    def post_run_step(
        self,
        *,
        exception: Exception,
        **future_kwargs: Any,
    ):
        _exit_span(exception)

    def post_run_execute_call(
        self,
        *,
        exception: Optional[Exception],
        **future_kwargs,
    ):
        _exit_span(exception)


class OpenTelemetryTracker(
    SyncTrackingClient,
):
    """Tracker that includes logging of OpenTelemetry events. Note you will be unlikely to instantiate this directly,
    rather, you will instantiate it through with_tracker(use_oteL_tracing=True) on the ApplicationBuilder.

    At a high level, this:
    1. Logs all events to OpenTelemetry
    2. Adds a span processor to opentelemetry

    Note that this globally sets a tracer provider -- it is possible that this will interfere with
    other tracers, and we are actively investigating it.
    TODO -- add stream start/end to opentel + TTFS, etc...
    """

    def pre_start_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        return self.burr_tracker.pre_start_stream(
            action=action,
            sequence_id=sequence_id,
            app_id=app_id,
            partition_key=partition_key,
            **future_kwargs,
        )

    def post_stream_item(
        self,
        *,
        item: Any,
        item_index: int,
        stream_initialize_time: datetime.datetime,
        first_stream_item_start_time: datetime.datetime,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        return self.burr_tracker.post_stream_item(
            item=item,
            item_index=item_index,
            stream_initialize_time=stream_initialize_time,
            first_stream_item_start_time=first_stream_item_start_time,
            action=action,
            sequence_id=sequence_id,
            app_id=app_id,
            partition_key=partition_key,
            **future_kwargs,
        )

    def post_end_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        return self.burr_tracker.post_end_stream(
            action=action,
            sequence_id=sequence_id,
            app_id=app_id,
            partition_key=partition_key,
            **future_kwargs,
        )

    def __init__(self, burr_tracker: SyncTrackingClient):
        initialize_tracer()
        self.tracer = trace.get_tracer("burr.integrations.opentelemetry")
        self.burr_tracker = burr_tracker

    def post_application_create(
        self,
        *,
        app_id: str,
        partition_key: Optional[str],
        state: "State",
        application_graph: "ApplicationGraph",
        parent_pointer: Optional[burr_types.ParentPointer],
        spawning_parent_pointer: Optional[burr_types.ParentPointer],
        **future_kwargs: Any,
    ):
        self.burr_tracker.post_application_create(
            app_id=app_id,
            partition_key=partition_key,
            state=state,
            application_graph=application_graph,
            parent_pointer=parent_pointer,
            spawning_parent_pointer=spawning_parent_pointer,
        )

    def do_log_attributes(
        self,
        *,
        attributes: Dict[str, Any],
        action: str,
        action_sequence_id: int,
        span: Optional["ActionSpan"],
        tags: dict,
        **future_kwargs: Any,
    ):
        # TODO -- get current span then call attributes
        # We need to serialize as well, attributes are not the right type to match 100%
        otel_span = get_current_span()
        if otel_span is None:
            # TODO -- see if this shows up then make it a les aggressive error
            raise ValueError("No current span")
        otel_span.set_attributes(
            {key: convert_to_otel_attribute(value) for key, value in attributes.items()}
        )

    def pre_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        inputs: Dict[str, Any],
        **future_kwargs: Any,
    ):
        self.burr_tracker.pre_run_step(
            app_id=app_id,
            partition_key=partition_key,
            sequence_id=sequence_id,
            state=state,
            action=action,
            inputs=inputs,
            **future_kwargs,
        )
        tracker_context.set(self.burr_tracker)
        span = _enter_span(action.name, self.tracer)
        cache_span(
            span,
            FullSpanContext(
                action_span=ActionSpan.create_initial(
                    action=action.name,
                    name=action.name,
                    sequence_id=0,
                    action_sequence_id=sequence_id,
                ),
                partition_key=partition_key,
                app_id=app_id,
            ),
        )

    def pre_start_span(
        self,
        *,
        action: str,
        action_sequence_id: int,
        span: "ActionSpan",
        span_dependencies: list[str],
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        otel_span = _enter_span(span.name, self.tracer)
        return otel_span

    def post_end_span(
        self,
        *,
        action: str,
        action_sequence_id: int,
        span: "ActionSpan",
        span_dependencies: list[str],
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        # TODO -- wire through exceptions
        _exit_span()

    def post_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        result: Optional[Dict[str, Any]],
        exception: Exception,
        **future_kwargs: Any,
    ):
        self.burr_tracker.post_run_step(
            app_id=app_id,
            partition_key=partition_key,
            sequence_id=sequence_id,
            state=state,
            action=action,
            result=result,
            exception=exception,
        )
        _exit_span(exception)
        tracker_context.set(None)

    def copy(self):
        return OpenTelemetryTracker(burr_tracker=self.burr_tracker.copy())


class BurrTrackingSpanProcessor(SpanProcessor):
    @property
    def tracker(self):
        """Quick trick to get closer to the right tracker. This is suboptimal as we don't really
        have guarentees that we'll be *in* the right context when it gets logged, but the way OpenTel
        is implemented we will (with the immediate span processor). TODO -- track a map of span ID -> tracker
        """
        return tracker_context.get()

    def on_start(
        self,
        span: "Span",
        parent_context: Optional[context_api.Context] = None,
    ) -> None:
        # First get the ID of the parent so we can retrieve from our cache
        parent_id = span.parent.span_id if span.parent is not None else None
        if parent_id is not None:
            parent_span = get_cached_span(span.parent.span_id)
            # If it exists, we can spawn a new span and cache that
            if parent_span is not None:
                cache_span(
                    span,
                    context := FullSpanContext(
                        action_span=parent_span.action_span.spawn(span.name),
                        partition_key=parent_span.partition_key,
                        app_id=parent_span.app_id,
                    ),
                )
                self.tracker.pre_start_span(
                    action=context.action_span.action,
                    action_sequence_id=context.action_span.action_sequence_id,
                    span=context.action_span,
                    span_dependencies=[],  # TODO -- log
                    app_id=context.app_id,
                    partition_key=context.partition_key,
                )

    def on_end(self, span: "Span") -> None:
        cached_span = get_cached_span(span.get_span_context().span_id)
        # If this is none it means we're outside of the burr context
        if cached_span is not None:
            # TODO -- get tracker context to work
            self.tracker.post_end_span(
                action=cached_span.action_span.action,
                action_sequence_id=cached_span.action_span.action_sequence_id,
                span=cached_span.action_span,
                span_dependencies=[],  # TODO -- log
                app_id=cached_span.app_id,
                partition_key=cached_span.partition_key,
            )
            uncache_span(span)
            if len(span.attributes) > 0:
                self.tracker.do_log_attributes(
                    attributes=dict(**span.attributes),
                    action=cached_span.action_span.action,
                    action_sequence_id=cached_span.action_span.action_sequence_id,
                    span=cached_span.action_span,
                    tags={},  # TODO -- log
                    app_id=cached_span.app_id,
                    partition_key=cached_span.partition_key,
                )


initialized = False


def initialize_tracer():
    """Initializes the tracer for OpenTel. Note this sets it globally.
    TODO -- ensure that it is initialized properly/do this in a cleaner manner.
    OpenTel does not make this easy as it's all global state.
    """
    global initialized
    if initialized:
        return
    initialized = True
    trace.set_tracer_provider(TracerProvider())
    trace.get_tracer_provider().add_span_processor(BurrTrackingSpanProcessor())


INSTRUMENTS_SPECS = {
    "openai": ("openai", "opentelemetry.instrumentation.openai", "OpenAIInstrumentor"),
    "anthropic": ("anthropic", "opentelemetry.instrumentation.anthropic", "AnthropicInstrumentor"),
    "cohere": ("cohere", "opentelemetry.instrumentation.cohere", "CohereInstrumentor"),
    "google_generativeai": (
        "google.generativeai",
        "opentelemetry.instrumentation.google_generativeai",
        "GoogleGenerativeAiInstrumentor",
    ),
    "mistral": ("mistralai", "opentelemetry.instrumentation.mistralai", "MistralAiInstrumentor"),
    "ollama": ("ollama", "opentelemetry.instrumentation.ollama", "OllamaInstrumentor"),
    "transformers": (
        "transformers",
        "opentelemetry.instrumentation.transformers",
        "TransformersInstrumentor",
    ),
    "together": ("together", "opentelemetry.instrumentation.together", "TogetherAiInstrumentor"),
    "bedrock": ("bedrock", "opentelemetry.instrumentation.bedrock", "BedrockInstrumentor"),
    "replicate": ("replicate", "opentelemetry.instrumentation.replicate", "ReplicateInstrumentor"),
    "vertexai": ("vertexai", "opentelemetry.instrumentation.vertexai", "VertexAIInstrumentor"),
    "groq": ("groq", "opentelemetry.instrumentation.groq", "GroqInstrumentor"),
    "watsonx": ("ibm-watsonx-ai", "opentelemetry.instrumentation.watsonx", "WatsonxInstrumentor"),
    "alephalpha": (
        "aleph_alpha_client",
        "opentelemetry.instrumentation.alephalpha",
        "AlephAlphaInstrumentor",
    ),
    "pinecone": ("pinecone", "opentelemetry.instrumentation.pinecone", "PineconeInstrumentor"),
    "qdrant": ("qdrant_client", "opentelemetry.instrumentation.qdrant", "QdrantInstrumentor"),
    "chroma": ("chromadb", "opentelemetry.instrumentation.chromadb", "ChromaInstrumentor"),
    "milvus": ("pymilvus", "opentelemetry.instrumentation.milvus", "MilvusInstrumentor"),
    "weaviate": ("weaviate", "opentelemetry.instrumentation.weaviate", "WeaviateInstrumentor"),
    "lancedb": ("lancedb", "opentelemetry.instrumentation.lancedb", "LanceInstrumentor"),
    "marqo": ("marqo", "opentelemetry.instrumentation.marqo", "MarqoInstrumentor"),
    "redis": ("redis", "opentelemetry.instrumentation.redis", "RedisInstrumentor"),
    "langchain": ("langchain", "opentelemetry.instrumentation.langchain", "LangchainInstrumentor"),
    "llama_index": (
        "llama_index",
        "opentelemetry.instrumentation.llamaindex",
        "LlamaIndexInstrumentor",
    ),
    "haystack": ("haystack", "opentelemetry.instrumentation.haystack", "HaystackInstrumentor"),
    "requests": ("requests", "opentelemetry.instrumentation.requests", "RequestsInstrumentor"),
    "httpx": ("httpx", "opentelemetry.instrumentation.httpx", "HTTPXClientInstrumentor"),
    "urllib": ("urllib", "opentelemetry.instrumentation.urllib", "URLLibInstrumentor"),
    "urllib3": ("urllib3", "opentelemetry.instrumentation.urllib3", "URLLib3Instrumentor"),
}

INSTRUMENTS = Literal[
    "openai",
    "anthropic",
    "cohere",
    "google_generativeai",
    "mistral",
    "ollama",
    "transformers",
    "together",
    "bedrock",
    "replicate",
    "vertexai",
    "groq",
    "watsonx",
    "alephalpha",
    "pinecone",
    "qdrant",
    "chroma",
    "milvus",
    "weaviate",
    "lancedb",
    "marqo",
    "redis",
    "langchain",
    "llama_index",
    "haystack",
    "requests",
    "httpx",
    "urllib",
    "urllib3",
]


def available_dists() -> set[str]:
    """Get the name of all available libraries in the current environment.

    ref for importlib.metadata: https://docs.python.org/3.11/library/importlib.metadata.html#metadata
    """
    return set((dist.name for dist in importlib.metadata.distributions()))


def _init_instrument(
    module_name: str, instrumentation_module_name: str, instrumentor_name: str
) -> None:
    """Instrument a Python library.
    Instrumentation will be skipped if module is not imported nor found in `sys.modules`
    Exit early if the instrumentation module isn't installed in the current environment.

    :param module_name: Name of the top-level module to instrument (e.g., `requests`, `openai`)
    :param instrumentation_module_name: Name of the module containing the instrumentor (e.g., opentelemetry.instrumentation.requests)
    :param instrumentor_name: Name of the object that has the `.instrument()` method. (e.g., OpenAIInstrumentor)
    :return:
    """
    if module_name not in sys.modules:
        logger.debug(f"`{module_name}` wasn't imported. Skipping instrumentation.")
        return

    instrumentation_package_name = instrumentation_module_name.replace(".", "-")
    if instrumentation_package_name not in available_dists():
        logger.info(
            f"Couldn't instrument `{module_name}`. Package `{instrumentation_package_name}` is missing."
        )
        return

    try:
        instrumentation_module = importlib.import_module(instrumentation_module_name)
        instrumentor = getattr(instrumentation_module, instrumentor_name)
        if instrumentor.is_instrumented_by_opentelemetry:
            logger.debug(f"`{module_name}` is already instrumented.")
        else:
            instrumentor.instrument()
            logger.info(f"`{module_name}` is now instrumented.")

    except BaseException:
        logger.error(f"Failed to instrument `{module_name}` with `{instrumentation_package_name}`.")


def init_instruments(*instruments: INSTRUMENTS, init_all: bool = False) -> None:
    """Instruments the specified libraries, or all that are installed if it is enabled.

    This will check if any libraries are available in the current environment and
    initialize if they are. See the ``INSTRUMENTS_SPECS`` field for the list of
    available libraries.

    :param instruments: Name of libraries to instrument (e.g., `requests`, `openai`)
    :param init_all: If True, initialize all available instruments for imported packages.
    :return:
    """
    # if no instrument explicitly passed, default to trying to instrument all available packages
    if init_all:
        logger.debug("Instrumenting all libraries.")
        instruments = INSTRUMENTS_SPECS.keys()

    for instrument in instruments:
        specs = INSTRUMENTS_SPECS[instrument]
        module_name, instrumentation_module_name, instrumentor_name = specs

        _init_instrument(module_name, instrumentation_module_name, instrumentor_name)


if __name__ == "__main__":
    initialize_tracer()
    tracer = trace.get_tracer(__name__)
    tracker = LocalTrackingClient("otel_test")
    opentel_adapter = OpenTelemetryTracker(burr_tracker=tracker)

    import time

    from burr.core import ApplicationBuilder, Result, action, default, expr
    from burr.visibility import TracerFactory

    def slp():
        time.sleep(random.random())

    @action(reads=["count"], writes=["count"])
    def counter(state: State, __tracer: TracerFactory) -> State:
        with __tracer("foo"):
            slp()
            with __tracer("bar"):
                slp()
                with tracer.start_span("baz") as span:
                    with use_span(span, end_on_exit=True):
                        slp()
                    with tracer.start_as_current_span("qux"):
                        slp()
                    with tracer.start_as_current_span("quux"):
                        slp()
                    slp()

                slp()
        return state.update(count=state["count"] + 1)

    result_action = Result("count").with_name("result")
    app = (
        ApplicationBuilder()
        .with_actions(result_action, counter=counter)
        .with_transitions(("counter", "counter", expr("count<10")))
        .with_transitions(("counter", "result", default))
        .with_hooks(opentel_adapter)
        .with_entrypoint("counter")
        # .with_tracker(tracker)
        .with_state(count=0)
        .build()
    )
    app.run(halt_after=["result"])



---
File: /burr/burr/integrations/pydantic.py
---

from __future__ import annotations

import copy
import inspect
import types
import typing
from typing import (
    AsyncGenerator,
    Awaitable,
    Callable,
    Generator,
    List,
    Optional,
    Tuple,
    Type,
    TypeVar,
    Union,
)

import pydantic
from pydantic_core import PydanticUndefined

from burr.core import Action, Graph, State
from burr.core.action import (
    FunctionBasedAction,
    FunctionBasedStreamingAction,
    bind,
    derive_inputs_from_fn,
)
from burr.core.typing import ActionSchema, TypingSystem

PydanticActionFunction = Callable[..., Union[pydantic.BaseModel, Awaitable[pydantic.BaseModel]]]


def model_to_dict(model: pydantic.BaseModel, include: Optional[List[str]] = None) -> dict:
    """Utility function to convert a pydantic model to a dictionary."""
    keys = model.model_fields.keys()
    keys = keys if include is None else [item for item in include if item in model.model_fields]
    return {key: getattr(model, key) for key in keys}


ModelType = TypeVar("ModelType", bound=pydantic.BaseModel)


def subset_model(
    model: Type[ModelType],
    fields: List[str],
    force_optional_fields: List[str],
    model_name_suffix: str,
) -> Type[ModelType]:
    """Creates a new pydantic model that is a subset of the original model.
    This is just to make it more efficient, as we can dynamically alter pydantic models

    :param fields: Fields that we want to include in the new model.
    :param force_optional_fields: Fields that we want to include in the new model, but that will always be optional.
    :param model: The model type to subset.
    :param model_name_suffix: The suffix to add to the model name.
    :return: The new model type.
    """
    new_fields = {}

    for name, field_info in model.model_fields.items():
        if name in fields:
            # copy directly
            # TODO -- handle cross-field validation
            new_fields[name] = (field_info.annotation, field_info)
        elif name in force_optional_fields:
            new_field_info = copy.deepcopy(field_info)
            if new_field_info.default_factory is None and (
                new_field_info.default is PydanticUndefined
            ):
                # in this case we can set to None
                new_field_info.default = None
                annotation = field_info.annotation
                if annotation is not None:
                    new_field_info.annotation = Optional[annotation]  # type: ignore
            new_fields[name] = (new_field_info.annotation, new_field_info)
    return pydantic.create_model(
        model.__name__ + model_name_suffix, __config__=model.model_config, **new_fields
    )  # type: ignore


def merge_to_state(model: pydantic.BaseModel, write_keys: List[str], state: State) -> State:
    """Merges a pydantic model that is a subset of the new state back into the state
    TODO -- implement
    TODO -- consider validating that the entire state is correct
    TODO -- consider validating just the deltas (if that's possible)
    """
    write_dict = model_to_dict(model=model, include=write_keys)
    return state.update(**write_dict)


def model_from_state(model: Type[ModelType], state: State) -> ModelType:
    """Creates a model from the state object -- capturing just the fields that are relevant to the model itself.

    :param model: model type to create
    :param state: state object to create from
    :return: model object
    """
    keys = [item for item in model.model_fields.keys() if item in state]
    return model(**{key: state[key] for key in keys})


def _validate_and_extract_signature_types(
    fn: PydanticActionFunction,
) -> Tuple[Type[pydantic.BaseModel], Type[pydantic.BaseModel]]:
    sig = inspect.signature(fn)
    if "state" not in sig.parameters:
        raise ValueError(
            f"Function fn: {fn.__qualname__} is not a valid pydantic action. "
            "The first argument of a pydantic "
            "action must be the state object. Got signature: {sig}."
        )
    type_hints = typing.get_type_hints(fn)

    if (state_model := type_hints["state"]) is inspect.Parameter.empty or not issubclass(
        state_model, pydantic.BaseModel
    ):
        raise ValueError(
            f"Function fn: {fn.__qualname__} is not a valid pydantic action. "
            "a type annotation of a type extending: pydantic.BaseModel. Got parameter "
            "state: {state_model.__qualname__}."
        )
    if (ret_hint := type_hints.get("return")) is None or not issubclass(
        ret_hint, pydantic.BaseModel
    ):
        raise ValueError(
            f"Function fn: {fn.__qualname__} is not a valid pydantic action. "
            "The return type must be a subclass of pydantic"
            ".BaseModel. Got return type: {sig.return_annotation}."
        )
    return state_model, ret_hint


def _validate_keys(model: Type[pydantic.BaseModel], keys: List[str], fn: Callable) -> None:
    missing_keys = [key for key in keys if key not in model.model_fields]
    if missing_keys:
        raise ValueError(
            f"Function fn: {fn.__qualname__} is not a valid pydantic action. "
            f"The keys: {missing_keys} are not present in the model: {model.__qualname__}."
        )


StateInputType = TypeVar("StateInputType", bound=pydantic.BaseModel)
StateOutputType = TypeVar("StateOutputType", bound=pydantic.BaseModel)
IntermediateResultType = TypeVar("IntermediateResultType", bound=Union[pydantic.BaseModel, dict])


class PydanticActionSchema(ActionSchema[StateInputType, StateOutputType, IntermediateResultType]):
    def __init__(
        self,
        input_type: Type[StateInputType],
        output_type: Type[StateOutputType],
        intermediate_result_type: Type[IntermediateResultType],
    ):
        self._input_type = input_type
        self._output_type = output_type
        self._intermediate_result_type = intermediate_result_type

    def state_input_type(self) -> Type[StateInputType]:
        return self._input_type

    def state_output_type(self) -> Type[StateOutputType]:
        return self._output_type

    def intermediate_result_type(self) -> type[IntermediateResultType]:
        return self._intermediate_result_type


def pydantic_action(
    reads: List[str],
    writes: List[str],
    state_input_type: Optional[Type[pydantic.BaseModel]] = None,
    state_output_type: Optional[Type[pydantic.BaseModel]] = None,
    tags: Optional[List[str]] = None,
) -> Callable[[PydanticActionFunction], PydanticActionFunction]:
    """See docstring for @action.pydantic"""

    def decorator(fn: PydanticActionFunction) -> PydanticActionFunction:
        if state_input_type is None and state_output_type is None:
            itype, otype = _validate_and_extract_signature_types(fn)

        elif state_input_type is not None and state_output_type is not None:
            itype, otype = state_input_type, state_output_type
        else:
            raise ValueError(
                "If you specify state_input_type or state_output_type, you must specify both."
            )
        _validate_keys(model=itype, keys=reads, fn=fn)
        _validate_keys(model=otype, keys=writes, fn=fn)
        SubsetInputType = subset_model(
            model=itype,
            fields=reads,
            force_optional_fields=[item for item in writes if item not in reads],
            model_name_suffix=f"{fn.__name__}_input",
        )
        SubsetOutputType = subset_model(
            model=otype,
            fields=writes,
            force_optional_fields=[],
            model_name_suffix=f"{fn.__name__}_input",
        )
        # TODO -- figure out

        def action_function(state: State, **kwargs) -> State:
            model_to_use = model_from_state(model=SubsetInputType, state=state)
            result = fn(state=model_to_use, **kwargs)
            # TODO -- validate that we can always construct this from the dict...
            # We really want a copy-type function
            output = SubsetOutputType(**model_to_dict(result, include=writes))
            return merge_to_state(model=output, write_keys=writes, state=state)

        async def async_action_function(state: State, **kwargs) -> State:
            model_to_use = model_from_state(model=SubsetInputType, state=state)
            result = await fn(state=model_to_use, **kwargs)
            output = SubsetOutputType(**model_to_dict(result, include=writes))
            return merge_to_state(model=output, write_keys=writes, state=state)

        is_async = inspect.iscoroutinefunction(fn)
        # This recreates the @action decorator
        # TODO -- use the @action decorator directly
        # TODO -- ensure that the function is the right one -- specifically it probably won't show code in the UI
        # now
        setattr(
            fn,
            FunctionBasedAction.ACTION_FUNCTION,
            FunctionBasedAction(
                async_action_function if is_async else action_function,
                reads,
                writes,
                input_spec=derive_inputs_from_fn({}, fn),
                originating_fn=fn,
                schema=PydanticActionSchema(
                    input_type=SubsetInputType,
                    output_type=SubsetOutputType,
                    intermediate_result_type=dict,
                ),
                tags=tags,
            ),
        )
        setattr(fn, "bind", types.MethodType(bind, fn))
        # TODO -- figure out typing
        # It's not smart enough to know that we have satisfied the type signature,
        # as we dynamically apply it using setattr
        return fn

    return decorator


PartialType = Union[Type[pydantic.BaseModel], Type[dict]]

PydanticStreamingActionFunctionSync = Callable[
    ..., Generator[Tuple[Union[pydantic.BaseModel, dict], Optional[pydantic.BaseModel]], None, None]
]

PydanticStreamingActionFunctionAsync = Callable[
    ..., AsyncGenerator[Tuple[Union[pydantic.BaseModel, dict], Optional[pydantic.BaseModel]], None]
]

PydanticStreamingActionFunction = Union[
    PydanticStreamingActionFunctionSync, PydanticStreamingActionFunctionAsync
]

PydanticStreamingActionFunctionVar = TypeVar(
    "PydanticStreamingActionFunctionVar", bound=PydanticStreamingActionFunction
)


def _validate_and_extract_signature_types_streaming(
    fn: PydanticStreamingActionFunction,
    stream_type: Optional[Union[Type[pydantic.BaseModel], Type[dict]]],
    state_input_type: Optional[Type[pydantic.BaseModel]] = None,
    state_output_type: Optional[Type[pydantic.BaseModel]] = None,
) -> Tuple[
    Type[pydantic.BaseModel], Type[pydantic.BaseModel], Union[Type[dict], Type[pydantic.BaseModel]]
]:
    if stream_type is None:
        # TODO -- derive from the signature
        raise ValueError(f"stream_type is required for function: {fn.__qualname__}")
    if state_input_type is None:
        # TODO -- derive from the signature
        raise ValueError(f"state_input_type is required for function: {fn.__qualname__}")
    if state_output_type is None:
        # TODO -- derive from the signature
        raise ValueError(f"state_output_type is required for function: {fn.__qualname__}")
    return state_input_type, state_output_type, stream_type


def pydantic_streaming_action(
    reads: List[str],
    writes: List[str],
    state_input_type: Type[pydantic.BaseModel],
    state_output_type: Type[pydantic.BaseModel],
    stream_type: PartialType,
    tags: Optional[List[str]] = None,
) -> Callable[[PydanticStreamingActionFunction], PydanticStreamingActionFunction]:
    """See docstring for @streaming_action.pydantic"""

    def decorator(fn: PydanticStreamingActionFunctionVar) -> PydanticStreamingActionFunctionVar:
        itype, otype, stream_type_processed = _validate_and_extract_signature_types_streaming(
            fn, stream_type, state_input_type=state_input_type, state_output_type=state_output_type
        )
        _validate_keys(model=itype, keys=reads, fn=fn)
        _validate_keys(model=otype, keys=writes, fn=fn)
        SubsetInputType = subset_model(
            model=itype,
            fields=reads,
            force_optional_fields=[item for item in writes if item not in reads],
            model_name_suffix=f"{fn.__name__}_input",
        )
        SubsetOutputType = subset_model(
            model=otype,
            fields=writes,
            force_optional_fields=[],
            model_name_suffix=f"{fn.__name__}_input",
        )
        # PartialModelType = stream_type_processed  # TODO -- attach to action
        # We don't currently use this, but we will be passing to the action to validate

        def action_generator(
            state: State, **kwargs
        ) -> Generator[tuple[PartialType, Optional[State]], None, None]:
            model_to_use = model_from_state(model=SubsetInputType, state=state)
            for partial, state_update in fn(state=model_to_use, **kwargs):
                if state_update is None:
                    yield partial, None
                else:
                    output = SubsetOutputType(**model_to_dict(state_update, include=writes))
                    yield partial, merge_to_state(model=output, write_keys=writes, state=state)

        async def async_action_generator(
            state: State, **kwargs
        ) -> AsyncGenerator[tuple[dict, Optional[State]], None]:
            model_to_use = model_from_state(model=SubsetInputType, state=state)
            async for partial, state_update in fn(state=model_to_use, **kwargs):
                if state_update is None:
                    yield partial, None
                else:
                    output = SubsetOutputType(**model_to_dict(state_update, include=writes))
                    yield partial, merge_to_state(model=output, write_keys=writes, state=state)

        is_async = inspect.isasyncgenfunction(fn)
        # This recreates the @streaming_action decorator
        # TODO -- use the @streaming_action decorator directly
        setattr(
            fn,
            FunctionBasedAction.ACTION_FUNCTION,
            FunctionBasedStreamingAction(
                async_action_generator if is_async else action_generator,
                reads,
                writes,
                input_spec=derive_inputs_from_fn({}, fn),
                originating_fn=fn,
                schema=PydanticActionSchema(
                    input_type=SubsetInputType,
                    output_type=SubsetOutputType,
                    intermediate_result_type=stream_type_processed,
                ),
                tags=tags,
            ),
        )
        setattr(fn, "bind", types.MethodType(bind, fn))
        return fn

    return decorator


StateModel = TypeVar("StateModel", bound=pydantic.BaseModel)


class PydanticTypingSystem(TypingSystem[StateModel]):
    """Typing system for pydantic models.

    :param TypingSystem: Parameterized on the state model type.
    """

    def __init__(self, model_type: Type[StateModel]):
        self.model_type = model_type

    def state_type(self) -> Type[StateModel]:
        return self.model_type

    def state_pre_action_run_type(self, action: Action, graph: Graph) -> Type[pydantic.BaseModel]:
        raise NotImplementedError(
            "TODO -- crawl through"
            "the graph to figure out what can possibly be optional and what can't..."
            "First get all "
        )

    def state_post_action_run_type(self, action: Action, graph: Graph) -> Type[pydantic.BaseModel]:
        raise NotImplementedError(
            "TODO -- crawl through"
            "the graph to figure out what can possibly be optional and what can't..."
            "First get all "
        )

    def construct_data(self, state: State) -> StateModel:
        return model_from_state(model=self.model_type, state=state)

    def construct_state(self, data: StateModel) -> State:
        return State(model_to_dict(data))



---
File: /burr/burr/integrations/ray.py
---

import concurrent.futures

import ray


class RayExecutor(concurrent.futures.Executor):
    """Ray parallel executor -- implementation of concurrent.futures.Executor.
    Currently experimental"""

    def __init__(self, shutdown_on_end: bool = False):
        """Creates a Ray executor -- remember to call ray.init() before running anything!"""
        self.shutdown_on_end = shutdown_on_end

    def submit(self, fn, *args, **kwargs):
        """Submits to ray -- creates a python future by calling ray.remote

        :param fn: Function to submit
        :param args: Args for the fn
        :param kwargs: Kwargs for the fn
        :return: The future for the fn
        """
        if not ray.is_initialized():
            raise RuntimeError("Ray is not initialized. Call ray.init() before running anything!")
        ray_fn = ray.remote(fn)
        object_ref = ray_fn.remote(*args, **kwargs)
        future = object_ref.future()

        return future

    def shutdown(self, wait=True, **kwargs):
        """Shuts down the executor by shutting down ray

        :param wait: Whether to wait -- required for hte API but not respected (yet)
        :param kwargs: Keyword arguments -- not used yet
        """
        if self.shutdown_on_end:
            if ray.is_initialized():
                ray.shutdown()



---
File: /burr/burr/integrations/streamlit.py
---

import colorsys
import dataclasses
import inspect
import json
from typing import List, Optional

from burr.core import Application
from burr.core.action import FunctionBasedAction
from burr.integrations.base import require_plugin
from burr.integrations.hamilton import Hamilton, StateSource

try:
    import graphviz
    import matplotlib.colors as mc
    import streamlit as st
except ImportError as e:
    require_plugin(
        e,
        "streamlit",
    )


@dataclasses.dataclass
class Record:
    state: dict
    action: str
    result: dict


@dataclasses.dataclass
class AppState:
    display_index: Optional[int]  # index in the state/results dict
    history: list[Record]
    app: Application  # we have to have this for registering the state machine --
    num_prior_nodes: int = 5  # view last 5

    @property
    def current_action(self) -> Optional[str]:
        if self.display_index is None or len(self.history) == 0:
            return None
        return self.history[self.display_index].action

    @property
    def prior_actions(self) -> List[str]:
        if self.display_index is None or len(self.history) == 0:
            return []
        out = [
            record.action
            for record in self.history[
                max(self.display_index - self.num_prior_nodes, 0) : self.display_index
            ]
        ]
        return out

    @property
    def next_action(self) -> Optional[str]:
        if self.display_index < len(self.history) - 1:
            return self.history[self.display_index + 1].action
        if self.app.get_next_action() is None:
            return None
        return self.app.get_next_action().name  # return the future one

    @property
    def max_index(self) -> int:
        return len(self.history) - 1

    @classmethod
    def from_empty(cls, app: Application) -> "AppState":
        return AppState(display_index=0, history=[], app=app)


def load_state_from_log_file(jsonl_log_file: str, app: Application) -> AppState:
    """Initializes the state from a log file. This must have been logged using StateAndResultFullLogger.
    Note that, currently, you must pass in an Application object (although that will be optoinal in the future).

    :param jsonl_log_file: Log file to load
    :param app: Application object
    :return: AppState
    """
    out = []
    for i, line in enumerate(open(jsonl_log_file)):
        json_line = json.loads(line)
        record = Record(
            state=json_line["state"],
            action=json_line["action"],
            result=json_line["result"]
            # TODO -- add start time, end time
        )
        out.append(record)
    return AppState(display_index=len(out) - 1, history=out, app=app)


def update_state(new_state: AppState):
    st.session_state.burr_state = new_state


def get_state():
    if "burr_state" not in st.session_state:
        raise ValueError(
            "No state found in streamlit session state. To initialize the state, call "
            "initialize_state() as the first line from streamlit -- it will do nothing if the state is already initialized."
        )
    return st.session_state.get("burr_state")


def _modify_state_machine_digraph(
    digraph: graphviz.Digraph,
    current_node: str = None,
    prior_nodes: list = [],
    next_node: str = None,
):
    def lighten_color(color, amount=0.5):
        if amount > 1:
            amount = 1
        if amount < 0:
            amount = 0
        try:
            c = mc.cnames[color]
        except KeyError:
            c = color
        c = colorsys.rgb_to_hls(*mc.to_rgb(c))
        lightened_color = colorsys.hls_to_rgb(c[0], 1 - amount * (1 - c[1]), c[2])
        return mc.to_hex(lightened_color)

    seen = {current_node}
    digraph.node(current_node, fillcolor="darkgreen", style="rounded,filled", fontcolor="white")
    if next_node is not None:
        digraph.node(next_node, fillcolor="blue", style="rounded,filled", fontcolor="white")
        seen.add(next_node)
    base_color = "lightblue"
    for i, node in enumerate(prior_nodes):
        if node not in seen:
            seen.add(node)
            lighter_color = lighten_color(base_color, amount=1 - ((i + 1) * 0.1))
            digraph.node(node, fillcolor=lighter_color, style="rounded,filled", fontcolor="black")
        else:
            continue

    digraph.attr(bgcolor="transparent")


def render_state_machine(state: AppState):
    """Visualization of the current state machine. Highlights:
        1. Current node in blue (with white backgorund)
        2. Prior nodes in progressively lighter shades of blue

    Use this individually, or within the "render_explorer" view

    :param state:
    :return:
    """
    prior_nodes = state.prior_actions  # grab the prior nodes
    current_node = state.current_action
    next_node = state.next_action
    app = state.app
    visualized = app.visualize(None, include_conditions=False, include_state=False)
    if current_node is not None:
        _modify_state_machine_digraph(
            visualized, current_node=current_node, prior_nodes=prior_nodes, next_node=next_node
        )
    st.graphviz_chart(visualized, use_container_width=True)


def render_action(state: AppState):
    """Renders the current action, including the reads, writes, and the code for the action.
    With Hamilton actions, it will also show the visualization of the action.

    This can be used individually (with a state object) or within the "render_explorer" view.

    :param state:
    :return:
    """
    app: Application = state.app
    current_node = state.current_action
    actions = {action.name: action for action in app.graph.actions}
    if current_node is None:
        st.markdown("No current action.")
        return
    st.header(f"`{current_node}`")
    action_object = actions[current_node]
    is_hamilton = isinstance(action_object, Hamilton)
    is_function_api = isinstance(action_object, FunctionBasedAction)

    def format_read(var):
        out = f"- `{var}`"
        if is_hamilton:
            inputs = action_object._inputs  # TODO -- don't use private variables
            corresponding_input = {
                k: v for k, v in inputs.items() if isinstance(v, StateSource) and v.state_key == var
            }
            if corresponding_input:
                return f"- `state['{var}']` â†’ `{list(corresponding_input.keys())[0]}`"
        return out

    def format_write(var):
        out = f"- `{var}`"
        if is_hamilton:
            outputs = action_object._outputs
            corresponding_output = {k: v for k, v in outputs.items() if v.key == var}
            k, v = list(corresponding_output.items())[0]
            if corresponding_output:
                out = f"- `state['{var}']`"
                if v.mode == "update":
                    out += f"â† `{k}`"
                if v.mode == "append":
                    out += f" â† `state['{var}'].append({k})`"
                    # out += " (`.append()`)"
        return out

    reads = "\n".join([format_read(var) for var in action_object.reads])
    writes = "\n".join([format_write(var) for var in action_object.writes])
    st.markdown(f"#### Reads: \n {reads}")
    st.markdown(f"#### Writes: \n {writes}")
    if is_hamilton:
        digraph = action_object.visualize_step(show_legend=False)
        st.graphviz_chart(digraph, use_container_width=False)
    elif is_function_api:
        code = inspect.getsource(action_object.fn)
        st.code(code, language="python")
    else:
        code = inspect.getsource(action_object.__class__)
        st.code(code, language="python")


def render_state_results(state: AppState):
    """Render the state and results for the current state. This includes the state and the result of the action.
    This can be used individually (with a state object) or within the "render_explorer" view.

    :param state: State object
    :return: None
    """
    if len(state.history) == 0:  # empty, have not yet started
        return
    current_node = state.current_action
    st.header(f"`{current_node}`")
    state_to_render = state.history[state.display_index].state
    result_to_render = state.history[state.display_index].result
    # if "chat_history" in state_to_render:
    #     del state_to_render["chat_history"]
    st.header("State")
    st.json(state_to_render, expanded=True)
    st.header("Result")
    st.json(result_to_render, expanded=True)


def set_slider_to_current():
    st.session_state.index_slider = get_state().max_index


def render_explorer(app_state: AppState):
    """Renders the entire explorer, including the state machine, the action, and the state/results.

    :param app_state: State of the app
    :return: None
    """
    total_state_length = len(app_state.history)
    placeholder = st.empty()
    placeholder.markdown("`   `")
    if total_state_length > 1:
        slider_values = list(range(total_state_length))
        slider_strings = [record.action for record in app_state.history]

        def stringify(i):
            return slider_strings[i]

        slider = st.select_slider(
            "index",
            options=slider_values,
            label_visibility="hidden",
            format_func=stringify,
            key="index_slider",
        )
        current_node_index = slider
        # TODO -- consider a callback here instead
        app_state.display_index = current_node_index

    with placeholder.container(height=900):
        state_machine_view, step_view, data_view = st.tabs(
            ["Application", "Action", "State/Results"]
        )
        with st.container():
            with state_machine_view:
                render_state_machine(app_state)
            with step_view:
                render_action(app_state)
            with data_view:
                render_state_results(app_state)
    update_state(app_state)



---
File: /burr/burr/lifecycle/__init__.py
---

from burr.lifecycle.base import (
    LifecycleAdapter,
    PostApplicationCreateHook,
    PostApplicationExecuteCallHook,
    PostApplicationExecuteCallHookAsync,
    PostEndSpanHook,
    PostRunStepHook,
    PostRunStepHookAsync,
    PreApplicationExecuteCallHook,
    PreApplicationExecuteCallHookAsync,
    PreRunStepHook,
    PreRunStepHookAsync,
    PreStartSpanHook,
)
from burr.lifecycle.default import StateAndResultsFullLogger

__all__ = [
    "PreRunStepHook",
    "PreRunStepHookAsync",
    "PostRunStepHook",
    "PostRunStepHookAsync",
    "PreApplicationExecuteCallHook",
    "PreApplicationExecuteCallHookAsync",
    "PostApplicationExecuteCallHook",
    "PostApplicationExecuteCallHookAsync",
    "LifecycleAdapter",
    "StateAndResultsFullLogger",
    "PostApplicationCreateHook",
    "PostEndSpanHook",
    "PreStartSpanHook",
]



---
File: /burr/burr/lifecycle/base.py
---

import abc
import datetime
import enum
from typing import TYPE_CHECKING, Any, Dict, Optional, Union

import burr.common.types as burr_types

if TYPE_CHECKING:
    # type-checking-only for a circular import
    from burr.core import State, Action, ApplicationGraph
    from burr.visibility import ActionSpan

from burr.lifecycle.internal import lifecycle


@lifecycle.base_hook("pre_run_step")
class PreRunStepHook(abc.ABC):
    """Hook that runs before a step is executed"""

    @abc.abstractmethod
    def pre_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        inputs: Dict[str, Any],
        **future_kwargs: Any,
    ):
        """Run before a step is executed.

        :param state: State prior to step execution
        :param action: Action to be executed
        :param inputs: Inputs to the action
        :param sequence_id: Sequence ID of the action
        :param future_kwargs: Future keyword arguments
        """
        pass


@lifecycle.base_hook("pre_run_step")
class PreRunStepHookAsync(abc.ABC):
    """Async hook that runs before a step is executed"""

    @abc.abstractmethod
    async def pre_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        inputs: Dict[str, Any],
        **future_kwargs: Any,
    ):
        """Async run before a step is executed.

        :param state: State prior to step execution
        :param action: Action to be executed
        :param inputs: Inputs to the action
        :param sequence_id: Sequence ID of the action
        :param future_kwargs: Future keyword arguments
        """
        pass


@lifecycle.base_hook("post_run_step")
class PostRunStepHook(abc.ABC):
    """Hook that runs after a step is executed"""

    @abc.abstractmethod
    def post_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        result: Optional[Dict[str, Any]],
        exception: Exception,
        **future_kwargs: Any,
    ):
        """Run after a step is executed.

        :param state: State after step execution
        :param action: Action that was executed
        :param result: Result of the action
        :param sequence_id: Sequence ID of the action
        :param exception: Exception that was raised
        :param future_kwargs: Future keyword arguments
        """
        pass


@lifecycle.base_hook("post_run_step")
class PostRunStepHookAsync(abc.ABC):
    """Async hook that runs after a step is executed"""

    @abc.abstractmethod
    async def post_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        result: Optional[dict],
        exception: Exception,
        **future_kwargs: Any,
    ):
        """Async run after a step is executed

        :param state: State after step execution
        :param action: Action that was executed
        :param result: Result of the action
        :param sequence_id: Sequence ID of the action
        :param exception: Exception that was raised
        :param future_kwargs: Future keyword arguments
        """
        pass


@lifecycle.base_hook("post_application_create")
class PostApplicationCreateHook(abc.ABC):
    """Synchronous hook that runs post instantiation of an ``Application``
    object (after ``.build()`` is called on the ``ApplicationBuilder`` object.)"""

    @abc.abstractmethod
    def post_application_create(
        self,
        *,
        app_id: str,
        partition_key: Optional[str],
        state: "State",
        application_graph: "ApplicationGraph",
        parent_pointer: Optional[burr_types.ParentPointer],
        spawning_parent_pointer: Optional[burr_types.ParentPointer],
        **future_kwargs: Any,
    ):
        """Runs after an "application" object is instantiated. This is run by the Application, in its constructor,
        as the last step.

        :param app_id: Application ID
        :param partition_key: Partition key of application
        :param state: Current state of the application
        :param application_graph: Application graph of the application, representing the state machine
        :param parent_pointer: Forking parent pointer of the application (application that it copied from)
        :param spawning_parent_pointer: Spawning parent pointer of the application (application that it was launched from)
        :param future_kwargs: Future keyword arguments for backwards compatibility
        """
        pass


@lifecycle.base_hook("pre_start_span")
class PreStartSpanHook(abc.ABC):
    """Hook that runs before a span is started in the tracing API.
    This can be either a context manager or a logger of sorts."""

    @abc.abstractmethod
    def pre_start_span(
        self,
        *,
        action: str,
        action_sequence_id: int,
        span: "ActionSpan",
        span_dependencies: list[str],
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


@lifecycle.base_hook("pre_start_span")
class PreStartSpanHookAsync(abc.ABC):
    @abc.abstractmethod
    async def pre_start_span(
        self,
        *,
        action: str,
        action_sequence_id: int,
        span: "ActionSpan",
        span_dependencies: list[str],
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


@lifecycle.base_hook("do_log_attributes")
class DoLogAttributeHook(abc.ABC):
    """Hook that is responsible for logging attributes,
    called by the tracer."""

    @abc.abstractmethod
    def do_log_attributes(
        self,
        *,
        attributes: Dict[str, Any],
        action: str,
        action_sequence_id: int,
        span: Optional["ActionSpan"],
        tags: dict,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


@lifecycle.base_hook("do_log_attributes")
class DoLogAttributeHookAsync(abc.ABC):
    """Hook that runs after a span is ended in the tracing API.
    This can be either a context manager or a logger."""

    @abc.abstractmethod
    async def do_log_attributes(
        self,
        *,
        attributes: Dict[str, Any],
        action: str,
        action_sequence_id: int,
        span: "ActionSpan",
        tags: dict,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


@lifecycle.base_hook("post_end_span")
class PostEndSpanHook(abc.ABC):
    """Hook that runs after a span is ended in the tracing API.
    This can be either a context manager or a logger."""

    @abc.abstractmethod
    def post_end_span(
        self,
        *,
        action: str,
        action_sequence_id: int,
        span: "ActionSpan",
        span_dependencies: list[str],
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


@lifecycle.base_hook("post_end_span")
class PostEndSpanHookAsync(abc.ABC):
    """Hook that runs at the end of an async span"""

    @abc.abstractmethod
    async def post_end_span(
        self,
        *,
        action: str,
        action_sequence_id: int,
        span: "ActionSpan",
        span_dependencies: list[str],
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


class ExecuteMethod(enum.Enum):
    """A set of the application methods the user can call.
    These correspond to interface methods in application.py, and
    allow us to say *which* method is being called for the following hooks."""

    step = "step"
    astep = "astep"
    iterate = "iterate"
    aiterate = "aiterate"
    run = "run"
    arun = "arun"
    stream_result = "stream_result"
    astream_result = "astream_result"
    stream_iterate = "stream_iterate"
    astream_iterate = "astream_iterate"


@lifecycle.base_hook("pre_run_execute_call")
class PreApplicationExecuteCallHook(abc.ABC):
    """Hook that runs before an application method (step/iterate/run/stream...) is called."""

    @abc.abstractmethod
    def pre_run_execute_call(
        self,
        *,
        app_id: str,
        partition_key: str,
        state: "State",
        method: ExecuteMethod,
        **future_kwargs: Any,
    ):
        pass


@lifecycle.base_hook("pre_run_execute_call")
class PreApplicationExecuteCallHookAsync(abc.ABC):
    """Hook that runs before an async application method (step/iterate/run/stream...) is called."""

    @abc.abstractmethod
    async def pre_run_execute_call(
        self,
        *,
        app_id: str,
        partition_key: str,
        state: "State",
        method: ExecuteMethod,
        **future_kwargs,
    ):
        pass


@lifecycle.base_hook("post_run_execute_call")
class PostApplicationExecuteCallHook(abc.ABC):
    """Hook that runs after an application method (step/iterate/run/stream...) is called."""

    @abc.abstractmethod
    def post_run_execute_call(
        self,
        *,
        app_id: str,
        partition_key: str,
        state: "State",
        method: ExecuteMethod,
        exception: Optional[Exception],
        **future_kwargs,
    ):
        pass


@lifecycle.base_hook("post_run_execute_call")
class PostApplicationExecuteCallHookAsync(abc.ABC):
    """Hook that runs after an async application method (step/iterate/run/stream...) is called."""

    @abc.abstractmethod
    async def post_run_execute_call(
        self,
        *,
        app_id: str,
        partition_key: str,
        state: "State",
        method: ExecuteMethod,
        exception: Optional[Exception],
        **future_kwargs,
    ):
        pass


@lifecycle.base_hook("pre_start_stream")
class PreStartStreamHook(abc.ABC):
    """Hook that runs after a stream is started.
    If you have a generator, this gets run directly when the generator is called.
    """

    @abc.abstractmethod
    def pre_start_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


@lifecycle.base_hook("pre_start_stream")
class PreStartStreamHookAsync(abc.ABC):
    """Hook that runs after a stream is started.
    If you have a generator, this gets run directly when the generator is called.
    """

    @abc.abstractmethod
    async def pre_start_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


@lifecycle.base_hook("post_stream_item")
class PostStreamItemHook(abc.ABC):
    """Hook that runs after a stream item is yielded"""

    @abc.abstractmethod
    def post_stream_item(
        self,
        *,
        item: Any,
        item_index: int,
        stream_initialize_time: datetime.datetime,
        first_stream_item_start_time: datetime.datetime,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


@lifecycle.base_hook("post_stream_item")
class PostStreamItemHookAsync(abc.ABC):
    """Hook that runs after a stream item is yielded"""

    @abc.abstractmethod
    async def post_stream_item(
        self,
        *,
        item: Any,
        item_index: int,
        stream_initialize_time: datetime.datetime,
        first_stream_item_start_time: datetime.datetime,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


@lifecycle.base_hook("post_end_stream")
class PostEndStreamHook(abc.ABC):
    """Hook that runs after a stream is ended"""

    @abc.abstractmethod
    def post_end_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


@lifecycle.base_hook("post_end_stream")
class PostEndStreamHookAsync(abc.ABC):
    """Hook that runs after a stream is ended"""

    @abc.abstractmethod
    async def post_end_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


# strictly for typing -- this conflicts a bit with the lifecycle decorator above, but its fine for now
# This makes IDE completion/type-hinting easier
LifecycleAdapter = Union[
    DoLogAttributeHook,
    PreRunStepHook,
    PreRunStepHookAsync,
    PostRunStepHook,
    PostRunStepHookAsync,
    PreApplicationExecuteCallHook,
    PreApplicationExecuteCallHookAsync,
    PostApplicationExecuteCallHook,
    PostApplicationExecuteCallHookAsync,
    PostApplicationCreateHook,
    PreStartSpanHook,
    PreStartSpanHookAsync,
    PostEndSpanHook,
    PostEndSpanHookAsync,
    PreStartStreamHook,
    PostStreamItemHook,
    PostEndStreamHook,
    PreStartStreamHookAsync,
    PostStreamItemHookAsync,
    PostEndStreamHookAsync,
]



---
File: /burr/burr/lifecycle/default.py
---

import datetime
import json
import time
from typing import TYPE_CHECKING, Any, Callable, Literal, Optional

if TYPE_CHECKING:
    from burr.core import State, Action

from burr.lifecycle.base import PostRunStepHook, PreRunStepHook


def safe_json(obj: Any) -> str:
    return json.dumps(obj, default=str)


class StateAndResultsFullLogger(PostRunStepHook, PreRunStepHook):
    """Logs the state and results of the action in a jsonl file."""

    DONT_INCLUDE = object()  # sentinel value

    def __init__(
        self,
        jsonl_path: str,
        mode: Literal["append", "w"] = "append",
        json_dump: Callable[[dict], str] = safe_json,
    ):
        """Initializes the logger.

        :param jsonl_path: Path to the jsonl file
        :param mode: Mode to open the file in. Either "append" or "w"
        :param json_dump: Function to use to dump the json. Default is safe_json
        """
        if not jsonl_path.endswith(".jsonl"):
            raise ValueError(f"jsonl_path must end with .jsonl. Got: {jsonl_path}")
        self.jsonl_path = jsonl_path
        open_mode = "a" if mode == "append" else "w"
        self.f = open(jsonl_path, mode=open_mode)  # open in append mode
        self.tracker = []  # tracker to keep track of timing/whatnot
        self.json_dump = json_dump

    def pre_run_step(self, **future_kwargs: Any):
        self.tracker.append({"time": datetime.datetime.now()})

    def post_run_step(
        self,
        *,
        state: "State",
        action: "Action",
        result: Optional[dict],
        exception: Exception,
        **future_kwargs: Any,
    ):
        state_and_result = {
            "state": state.get_all(),
            "action": action.name,
            "result": result,
            "exception": str(exception),
            "start_time": self.tracker[-1]["time"].isoformat(),
            "end_time": datetime.datetime.now().isoformat(),
        }
        self.f.writelines([self.json_dump(state_and_result) + "\n"])

    def __del__(self):
        if hasattr(self, "f"):
            # possible something fails beforehand
            self.f.close()


class SlowDownHook(PostRunStepHook, PreRunStepHook):
    """Slows down execution. You'll only want to use this for debugging/visualizing."""

    def __init__(self, pre_sleep_time: float = 0.5, post_sleep_time: float = 0.5):
        """Initializes the hook.

        :param pre_sleep_time: Time to sleep before the step
        :param post_sleep_time: Time to sleep after the step
        """
        self.post_sleep_time = post_sleep_time
        self.pre_sleep_time = pre_sleep_time

    def post_run_step(
        self,
        *,
        state: "State",
        action: "Action",
        result: Optional[dict],
        exception: Exception,
        **future_kwargs: Any,
    ):
        time.sleep(self.post_sleep_time)

    def pre_run_step(self, *, state: "State", action: "Action", **future_kwargs: Any):
        time.sleep(self.pre_sleep_time)



---
File: /burr/burr/lifecycle/internal.py
---

"""Base tooling, internal-facing, for lifecycle hooks. This is stolen from the
hamilton implementation, but significantly simplified."""
import asyncio
import collections
import inspect
from typing import TYPE_CHECKING, Callable, Dict, List, Set, Tuple

if TYPE_CHECKING:
    # type-checking-only for a circular import
    from burr.lifecycle.base import LifecycleAdapter

SYNC_HOOK = "hooks"
ASYNC_HOOK = "async_hooks"

REGISTERED_SYNC_HOOKS: Set[str] = set()
REGISTERED_ASYNC_HOOKS: Set[str] = set()


class InvalidLifecycleHook(Exception):
    """Container exception to indicate that a lifecycle adapter is invalid."""

    pass


def validate_hook_fn(fn: Callable):
    """Validates that a function forms a valid hook. This means:
    1. Function returns nothing
    2. Function must consist of only kwarg-only arguments

    :param fn: The function to validate
    :raises InvalidLifecycleAdapter: If the function is not a valid hook
    """
    sig = inspect.signature(fn)
    if (
        "future_kwargs" not in sig.parameters
        or sig.parameters["future_kwargs"].kind != inspect.Parameter.VAR_KEYWORD
    ):
        raise InvalidLifecycleHook(
            f"Lifecycle hooks must have a `**future_kwargs` argument. Method/hook {fn} does not."
        )
    for param in sig.parameters.values():
        if param.name != "future_kwargs":
            if param.kind != inspect.Parameter.KEYWORD_ONLY and param.name != "self":
                raise InvalidLifecycleHook(
                    f"Lifecycle hooks can only have keyword-only arguments. "
                    f"Method/hook {fn} has argument {param} that is not keyword-only."
                )


class lifecycle:
    """Container class for decorators to register hooks.
    This is just a container so it looks clean (`@lifecycle.base_hook(...)`), but we could easily move it out.
    What do these decorators do?
      1. We decorate a class with a method/hook/validator call
      2. This implies that there exists a function by that name
      3. We validate that that function has an appropriate signature
      4. We store this in the appropriate registry (see the constants above)
    Then, when we want to perform a hook/method/validator, we can ask the AdapterLifecycleSet to do so.
    It crawls up the MRO, looking to see which classes are in the registry, then elects which functions to run.
    See LifecycleAdapterSet for more information.
    """

    @classmethod
    def base_hook(cls, fn_name: str):
        """Hooks get called at distinct stages of Hamilton's execution.
        These can be layered together, and potentially coupled to other hooks.

        :param fn_name: Name of the function that will reside in the class we're decorating
        """

        def decorator(clazz):
            fn = getattr(clazz, fn_name, None)
            if fn is None:
                raise ValueError(
                    f"Class {clazz} does not have a method {fn_name}, but is "
                    f'decorated with @lifecycle.base_hook("{fn_name}"). The parameter '
                    f"to @lifecycle.base_hook must be the name "
                    f"of a method on the class."
                )
            validate_hook_fn(fn)
            if inspect.iscoroutinefunction(fn):
                setattr(clazz, ASYNC_HOOK, fn_name)
                REGISTERED_ASYNC_HOOKS.add(fn_name)
            else:
                setattr(clazz, SYNC_HOOK, fn_name)
                REGISTERED_SYNC_HOOKS.add(fn_name)
            return clazz

        return decorator


class LifecycleAdapterSet:
    """An internal class that groups together all the lifecycle adapters.
    This allows us to call methods through a delegation pattern, enabling us to add
    whatever callbacks, logging, error-handling, etc... we need globally. While this
    does increase the stack trace in an error, it should be pretty easy to figure out what's going on.
    """

    def __init__(self, *adapters: "LifecycleAdapter"):
        """Initializes the adapter set.

        :param adapters: Adapters to group together
        """
        self._adapters = list(adapters)
        self.sync_hooks, self.async_hooks = self._get_lifecycle_hooks()

    def with_new_adapters(self, *adapters: "LifecycleAdapter") -> "LifecycleAdapterSet":
        """Adds new adapters to the set.

        :param adapters: Adapters to add
        :return: A new adapter set with the new adapters
        """
        return LifecycleAdapterSet(*self.adapters, *adapters)

    def _get_lifecycle_hooks(
        self,
    ) -> Tuple[Dict[str, List["LifecycleAdapter"]], Dict[str, List["LifecycleAdapter"]]]:
        sync_hooks = collections.defaultdict(list)
        async_hooks = collections.defaultdict(list)
        for adapter in self.adapters:
            for cls in inspect.getmro(adapter.__class__):
                sync_hook = getattr(cls, SYNC_HOOK, None)
                if sync_hook is not None:
                    if adapter not in sync_hooks[sync_hook]:
                        sync_hooks[sync_hook].append(adapter)
                async_hook = getattr(cls, ASYNC_HOOK, None)
                if async_hook is not None:
                    if adapter not in async_hooks[async_hook]:
                        async_hooks[async_hook].append(adapter)
        return (
            {hook: adapters for hook, adapters in sync_hooks.items()},
            {hook: adapters for hook, adapters in async_hooks.items()},
        )

    def _does_hook(self, hook_name: str, is_async: bool) -> bool:
        """Whether or not a hook is implemented by any of the adapters in this group.
        If this hook is not registered, this will raise a ValueError.

        :param hook_name: Name of the hook
        :param is_async: Whether you want the async version or not
        :return: True if this adapter set does this hook, False otherwise
        """
        if is_async and hook_name not in REGISTERED_ASYNC_HOOKS:
            raise ValueError(
                f"Hook {hook_name} is not registered as an asynchronous lifecycle hook. "
                f"Registered hooks are {REGISTERED_ASYNC_HOOKS}"
            )
        if not is_async and hook_name not in REGISTERED_SYNC_HOOKS:
            raise ValueError(
                f"Hook {hook_name} is not registered as a synchronous lifecycle hook. "
                f"Registered hooks are {REGISTERED_SYNC_HOOKS}"
            )
        if not is_async:
            return hook_name in self.sync_hooks
        return hook_name in self.async_hooks

    def call_all_lifecycle_hooks_sync(self, hook_name: str, **kwargs):
        """Calls all the lifecycle hooks in this group, by hook name (stage)

        :param hook_name: Name of the hooks to call
        :param kwargs: Keyword arguments to pass into the hook
        """
        if not self._does_hook(hook_name, False):
            return
        for adapter in self.sync_hooks[hook_name]:
            getattr(adapter, hook_name)(**kwargs)

    async def call_all_lifecycle_hooks_async(self, hook_name: str, **kwargs):
        """Calls all the lifecycle hooks in this group, by hook name (stage).

        :param hook_name: Name of the hook
        :param kwargs: Keyword arguments to pass into the hook
        """
        if not self._does_hook(hook_name, True):
            return
        futures = []
        for adapter in self.async_hooks[hook_name]:
            futures.append(getattr(adapter, hook_name)(**kwargs))
        if len(futures) == 0:
            return  # No async hooks to call
        await asyncio.gather(*futures)

    async def call_all_lifecycle_hooks_sync_and_async(self, hook_name: str, **kwargs):
        """Calls all the lifecycle hooks in this group, by hook name (stage).

        :param hook_name: Name of the hook
        """
        self.call_all_lifecycle_hooks_sync(hook_name, **kwargs)
        await self.call_all_lifecycle_hooks_async(hook_name, **kwargs)

    @property
    def adapters(self) -> List["LifecycleAdapter"]:
        """Gives the adapters in this group

        :return: A list of adapters
        """
        return self._adapters



---
File: /burr/burr/testing/__init__.py
---

"""
Module to help with testing.
"""

import json


# one way to parameterize tests is to store the serialized state in a json file
# and then load it for the test.
def load_test_cases(file_name: str) -> tuple:
    """Load test cases from a json file."""
    with open(file_name, "r") as f:
        json_test_cases = json.load(f)
        test_cases = [(tc.get("input_state"), tc.get("expected_state")) for tc in json_test_cases]
        test_ids = [
            f"{tc.get('action', 'ACTION_MISSING')}-{tc.get('name', 'NAME_MISSING')}"
            for tc in json_test_cases
        ]
    return test_cases, test_ids


# Define the pytest_generate_tests hook to generate test cases dynamically based on the
# contents of a file
def pytest_generate_tests(metafunc):
    """This function dynamically generates test cases based on the contents of a file.

    Use this with PyTest.
    """

    # for each function that has the 'input_state' and 'expected_state' arguments:
    if "input_state" in metafunc.fixturenames and "expected_state" in metafunc.fixturenames:
        # gets all filenames from the 'file_name' marker
        file_names = [
            file_arg
            for m in metafunc.definition.own_markers
            if m.name == "file_name"
            for file_arg in m.args
        ]
        all_test_cases = []
        all_test_ids = []
        for file_name in file_names:
            test_cases, test_case_ids = load_test_cases(file_name)
            all_test_cases.extend(test_cases)
            all_test_ids.extend(test_case_ids)
        if not all_test_cases:
            raise ValueError(
                f"No test cases could be created for test {metafunc.definition.originalname}."
            )
        # Generate test cases based on the test_data list
        metafunc.parametrize("input_state,expected_state", all_test_cases, ids=all_test_ids)



---
File: /burr/burr/tracking/common/__init__.py
---




---
File: /burr/burr/tracking/common/models.py
---

import datetime
from typing import Any, Dict, List, Literal, Optional, Union

from burr.common import types as burr_types
from burr.core import Action
from burr.core.application import ApplicationGraph
from burr.core.graph import Transition
from burr.integrations.base import require_plugin

try:
    import pydantic
except ImportError as e:
    require_plugin(
        e,
        "tracking",
    )


class IdentifyingModel(pydantic.BaseModel):
    type: str


class ActionModel(IdentifyingModel):
    """Pydantic model that represents an action for storing/visualization in the UI"""

    name: str
    reads: list[str]
    writes: list[str]
    code: str
    type: str = "action"
    inputs: List[str] = pydantic.Field(default_factory=list)
    optional_inputs: List[str] = pydantic.Field(default_factory=list)

    @staticmethod
    def from_action(action: Action) -> "ActionModel":
        """Creates an ActionModel from an action.

        :param action: Action to create the model from
        :return:
        """
        code = action.get_source()  # delegate to the action
        optional_inputs, required_inputs = action.optional_and_required_inputs
        return ActionModel(
            name=action.name,
            reads=list(action.reads),
            writes=list(action.writes),
            code=code,
            inputs=list(required_inputs),
            optional_inputs=list(optional_inputs),
        )


class TransitionModel(IdentifyingModel):
    """Pydantic model that represents a transition for storing/visualization in the UI"""

    from_: str
    to: str
    condition: str
    type: str = "transition"

    @staticmethod
    def from_transition(transition: Transition) -> "TransitionModel":
        return TransitionModel(
            from_=transition.from_.name, to=transition.to.name, condition=transition.condition.name
        )


class PointerModel(IdentifyingModel):
    """Stores pointers to unique identifiers for an application.
    This is used by a few different places to, say, store parent references
    bewteen application instances.
    """

    app_id: str
    sequence_id: Optional[int]
    partition_key: Optional[str]
    type: str = "pointer_data"

    @staticmethod
    def from_pointer(pointer: Optional[burr_types.ParentPointer]) -> Optional["PointerModel"]:
        return (
            PointerModel(
                app_id=pointer.app_id,
                sequence_id=pointer.sequence_id,
                partition_key=pointer.partition_key,
            )
            if pointer is not None
            else None
        )


class ChildApplicationModel(IdentifyingModel):
    """Stores data about a child application (either a fork or a spawned application).
    This allows us to link from parent -> child in the UI."""

    child: PointerModel
    event_time: datetime.datetime
    event_type: Literal[
        "fork", "spawn_start"
    ]  # TODO -- get spawn_end working when we have interaction hooks (E.G. on app fn calls)
    sequence_id: Optional[int]
    type: str = "child_application_data"


class ApplicationModel(IdentifyingModel):
    """Pydantic model that represents an application for storing/visualization in the UI"""

    entrypoint: str
    actions: list[ActionModel]
    transitions: list[TransitionModel]
    type: str = "application"

    @staticmethod
    def from_application_graph(
        application_graph: ApplicationGraph,
    ) -> "ApplicationModel":
        return ApplicationModel(
            entrypoint=application_graph.entrypoint.name,
            actions=[ActionModel.from_action(action) for action in application_graph.actions],
            transitions=[
                TransitionModel.from_transition(transition)
                for transition in application_graph.transitions
            ],
        )


class ApplicationMetadataModel(IdentifyingModel):
    """Pydantic model that represents metadata for an application.
    We will want to add tags here when we have them."""

    partition_key: Optional[str] = None
    parent_pointer: Optional[PointerModel] = None  # pointer to parent data
    spawning_parent_pointer: Optional[PointerModel] = None  # pointer to spawning parent data
    type: str = "application_metadata"


INPUT_FILTERLIST = {"__tracer", "__context"}


def _filter_inputs(d: dict) -> dict:
    return {k: v for k, v in d.items() if k not in INPUT_FILTERLIST}


class BeginEntryModel(IdentifyingModel):
    """Pydantic model that represents an entry for the beginning of a step"""

    start_time: datetime.datetime
    action: str
    inputs: Dict[str, Any]
    sequence_id: int
    type: str = "begin_entry"


class EndEntryModel(IdentifyingModel):
    """Pydantic model that represents an entry for the end of a step"""

    end_time: datetime.datetime
    action: str
    result: Optional[dict]
    exception: Optional[str]
    state: Dict[str, Any]  # TODO -- consider logging updates to the state so we can recreate
    sequence_id: int
    type: str = "end_entry"


class BeginSpanModel(IdentifyingModel):
    """Pydantic model that represents an entry for the beginning of a span"""

    start_time: datetime.datetime
    action_sequence_id: int
    span_id: str  # unique among the application
    span_name: str
    parent_span_id: Optional[str]
    span_dependencies: list[str]
    type: str = "begin_span"

    @property
    def sequence_id(self) -> int:
        return self.action_sequence_id


class EndSpanModel(IdentifyingModel):
    """Pydantic model that represents an entry for the end of a span"""

    end_time: datetime.datetime
    action_sequence_id: int
    span_id: str  # unique among the application
    type: str = "end_span"

    @property
    def sequence_id(self) -> int:
        # so we have full backwards compatibility
        # the server likes them all to be called sequence_id
        return self.action_sequence_id


class AttributeModel(IdentifyingModel):
    """Represents a logged artifact"""

    key: str
    action_sequence_id: int
    span_id: Optional[
        str
    ]  # It doesn't have to relate to a span, it can be at the level of an action as well
    value: Union[dict, str, int, float, bool, list, None]
    tags: Dict[str, str]
    time_logged: Optional[datetime.datetime] = None
    type: str = "attribute"

    @property
    def sequence_id(self) -> int:
        # Ditto with the above
        return self.action_sequence_id


class InitializeStreamModel(IdentifyingModel):
    """Pydantic model that represents an entry for the beginning of a stream"""

    action_sequence_id: int
    span_id: Optional[
        str
    ]  # It doesn't have to relate to a span, but if it was started in a span, this is the span_id
    stream_init_time: datetime.datetime
    type: str = "begin_stream"

    @property
    def sequence_id(self) -> int:
        return self.action_sequence_id


class FirstItemStreamModel(IdentifyingModel):
    """Pydantic model that represents an entry for the first item of a stream"""

    action_sequence_id: int
    span_id: Optional[
        str
    ]  # It doesn't have to relate to a span, but if it was started in a span, this is the span_id
    first_item_time: datetime.datetime
    type: str = "first_item_stream"

    @property
    def sequence_id(self) -> int:
        return self.action_sequence_id


class EndStreamModel(IdentifyingModel):
    """Pydantic model that represents an entry for the first item of a stream"""

    action_sequence_id: int
    span_id: Optional[
        str
    ]  # It doesn't have to relate to a span, but if it was started in a span, this is the span_id
    end_time: datetime.datetime
    items_streamed: int
    type: str = "end_stream"

    @property
    def sequence_id(self) -> int:
        return self.action_sequence_id



---
File: /burr/burr/tracking/server/s3/__init__.py
---




---
File: /burr/burr/tracking/server/s3/backend.py
---

import dataclasses
import datetime
import functools
import itertools
import json
import logging
import operator
import os.path
import uuid
from collections import Counter
from typing import List, Literal, Optional, Sequence, Tuple, Type, TypeVar, Union

import fastapi
import pydantic
from aiobotocore import session
from fastapi import FastAPI
from pydantic_settings import BaseSettings
from tortoise import functions, transactions
from tortoise.contrib.fastapi import RegisterTortoise
from tortoise.expressions import Q

from burr import system
from burr.tracking.common.models import ApplicationModel
from burr.tracking.server import schema
from burr.tracking.server.backend import (
    BackendBase,
    BurrSettings,
    IndexingBackendMixin,
    SnapshottingBackendMixin,
)
from burr.tracking.server.s3 import settings, utils
from burr.tracking.server.s3.models import (
    Application,
    IndexingJob,
    IndexingJobStatus,
    IndexStatus,
    LogFile,
    Project,
)
from burr.tracking.server.schema import ApplicationLogs, Step

logger = logging.getLogger(__name__)

FileType = Literal["log", "metadata", "graph"]

ContentsModel = TypeVar("ContentsModel", bound=pydantic.BaseModel)


async def _query_s3_file(
    bucket: str,
    key: str,
    client: session.AioBaseClient,
) -> Union[ContentsModel, List[ContentsModel]]:
    response = await client.get_object(Bucket=bucket, Key=key)
    body = await response["Body"].read()
    return body


@dataclasses.dataclass
class DataFile:
    """Generic data file object meant to represent a file in the s3 bucket. This has a few possible roles (log, metadata, and graph file)"""

    prefix: str
    yyyy: str
    mm: str
    dd: str
    hh: str
    minutes_string: str
    partition_key: str
    application_id: str
    file_type: FileType
    path: str
    created_date: datetime.datetime

    @classmethod
    def from_path(cls, path: str, created_date: datetime.datetime) -> "DataFile":
        parts = path.split("/")

        # Validate that there are enough parts to extract the needed fields
        if len(parts) < 9:
            raise ValueError(f"Path '{path}' is not valid")

        prefix = "/".join(parts[:-8])  # Everything before the year part
        yyyy = parts[2]
        mm = parts[3]
        dd = parts[4]
        hh = parts[5]
        minutes_string = parts[6]
        application_id = parts[8]
        partition_key = parts[7]
        filename = parts[9]
        file_type = (
            "graph"
            if filename.endswith("graph.json")
            else "metadata"
            if filename.endswith("_metadata.json")
            else "log"
        )

        # # Validate the date parts
        # if not (yyyy.isdigit() and mm.isdigit() and dd.isdigit() and hh.isdigit()):
        #     raise ValueError(f"Date components in the path '{path}' are not valid")

        return cls(
            prefix=prefix,
            yyyy=yyyy,
            mm=mm,
            dd=dd,
            hh=hh,
            minutes_string=minutes_string,
            application_id=application_id,
            partition_key=partition_key,
            file_type=file_type,
            path=path,
            created_date=created_date,
        )


class S3Settings(BurrSettings):
    s3_bucket: str
    update_interval_milliseconds: int = 120_000
    aws_max_concurrency: int = 100
    snapshot_interval_milliseconds: int = 3_600_000
    load_snapshot_on_start: bool = True
    prior_snapshots_to_keep: int = 5


def timestamp_to_reverse_alphabetical(timestamp: datetime) -> str:
    # Get the inverse of the timestamp
    epoch = datetime.datetime(1970, 1, 1, tzinfo=system.utc)
    total_seconds = int((timestamp - epoch).total_seconds())

    # Invert the seconds (latest timestamps become smallest values)
    inverted_seconds = 2**32 - total_seconds

    # Convert the inverted seconds to a zero-padded string
    inverted_str = str(inverted_seconds).zfill(10)

    return inverted_str + "-" + timestamp.isoformat()


class SQLiteS3Backend(BackendBase, IndexingBackendMixin, SnapshottingBackendMixin):
    def __init__(
        self,
        s3_bucket: str,
        update_interval_milliseconds: int,
        aws_max_concurrency: int,
        snapshot_interval_milliseconds: int,
        load_snapshot_on_start: bool,
        prior_snapshots_to_keep: int,
    ):
        self._backend_id = system.now().isoformat() + str(uuid.uuid4())
        self._bucket = s3_bucket
        self._session = session.get_session()
        self._update_interval_milliseconds = update_interval_milliseconds
        self._aws_max_concurrency = aws_max_concurrency
        self._snapshot_interval_milliseconds = snapshot_interval_milliseconds
        self._data_prefix = "data"
        self._snapshot_prefix = "snapshots"
        self._load_snapshot_on_start = load_snapshot_on_start
        self._snapshot_key_history = []
        self._prior_snapshots_to_keep = prior_snapshots_to_keep

    async def load_snapshot(self):
        if not self._load_snapshot_on_start:
            return
        path = settings.DB_PATH
        # if it already exists then return
        if os.path.exists(path):
            return
        if not os.path.exists(os.path.dirname(path)):
            os.makedirs(os.path.dirname(path))
        async with self._session.create_client("s3") as client:
            objects = await client.list_objects_v2(
                Bucket=self._bucket, Prefix=self._snapshot_prefix, MaxKeys=1
            )
            if objects["KeyCount"] == 0:
                return
            # get the latest snapshot -- it's organized by alphabetical order
            latest_snapshot = objects["Contents"][0]
            # download the snapshot
            response = await client.get_object(Bucket=self._bucket, Key=latest_snapshot["Key"])
            async with response["Body"] as stream:
                with open(path, "wb") as file:
                    file.write(await stream.read())

    def snapshot_interval_milliseconds(self) -> Optional[int]:
        return self._snapshot_interval_milliseconds

    @classmethod
    def settings_model(cls) -> Type[BaseSettings]:
        return S3Settings

    async def snapshot(self):
        path = settings.DB_PATH
        timestamp = timestamp_to_reverse_alphabetical(system.now())
        # latest
        s3_key = f"{self._snapshot_prefix}/{timestamp}/{self._backend_id}/snapshot.db"
        # TODO -- copy the path at snapshot_path to s3 using aiobotocore
        session = self._session
        logger.info(f"Saving db snapshot at: {s3_key}")
        async with session.create_client("s3") as s3_client:
            with open(path, "rb") as file_data:
                await s3_client.put_object(Bucket=self._bucket, Key=s3_key, Body=file_data)

        self._snapshot_key_history.append(s3_key)
        if len(self._snapshot_key_history) > 5:
            old_snapshot_to_remove = self._snapshot_key_history.pop(0)
            logger.info(f"Removing old snapshot: {old_snapshot_to_remove}")
            async with session.create_client("s3") as s3_client:
                await s3_client.delete_object(Bucket=self._bucket, Key=old_snapshot_to_remove)

    def update_interval_milliseconds(self) -> Optional[int]:
        return self._update_interval_milliseconds

    async def _s3_get_first_write_date(self, project_id: str):
        async with self._session.create_client("s3") as client:
            paginator = client.get_paginator("list_objects_v2")
            async for result in paginator.paginate(
                Bucket=self._bucket,
                Prefix=f"{self._data_prefix}/{project_id}/",
                Delimiter="/",
                MaxKeys=1,
            ):
                if "Contents" in result:
                    first_object = result["Contents"][0]
                    return first_object["LastModified"]
        return system.now()  # This should never be hit unless someone is concurrently deleting...

    async def _update_projects(self):
        current_projects = await Project.all()
        project_names = {project.name for project in current_projects}
        logger.info(f"Current projects: {project_names}")
        async with self._session.create_client("s3") as client:
            paginator = client.get_paginator("list_objects_v2")
            async for result in paginator.paginate(
                Bucket=self._bucket, Prefix=f"{self._data_prefix}/", Delimiter="/"
            ):
                for prefix in result.get("CommonPrefixes", []):
                    project_name = prefix.get("Prefix").split("/")[-2]
                    if project_name not in project_names:
                        now = system.now()
                        logger.info(f"Creating project: {project_name}")
                        await Project.create(
                            name=project_name,
                            uri=None,
                            created_at=await self._s3_get_first_write_date(project_id=project_name),
                            indexed_at=now,
                            updated_at=now,
                        )

    async def query_applications_by_key(
        self, application_keys: Sequence[tuple[str, Optional[str]]]
    ):
        conditions = [
            Q(name=app_id, partition_key=partition_key)
            for app_id, partition_key in application_keys
        ]

        # Combine the conditions with an OR operation
        query = Application.filter(functools.reduce(operator.or_, conditions))

        # Execute the query
        applications = await query.all()
        return applications

    async def _gather_metadata_files(
        self,
        metadata_files: List[DataFile],
    ) -> Sequence[dict]:
        """Gives a list of metadata files so we can update the application"""

        async def _query_metadata_file(metadata_file: DataFile) -> dict:
            async with self._session.create_client("s3") as client:
                response = await client.head_object(
                    Bucket=self._bucket,
                    Key=metadata_file.path,
                )
                # metadata = await response['Body'].read()
                parent_pointer_raw = response["Metadata"].get("parent_pointer")
                spawning_parent_pointer_raw = response["Metadata"].get("spawning_parent_pointer")
                return dict(
                    partition_key=metadata_file.partition_key,
                    parent_pointer=json.loads(parent_pointer_raw)
                    if parent_pointer_raw != "None"
                    else None,
                    spawning_parent_pointer=json.loads(spawning_parent_pointer_raw)
                    if spawning_parent_pointer_raw != "None"
                    else None,
                )

        out = await utils.gather_with_concurrency(
            self._aws_max_concurrency,
            *[_query_metadata_file(metadata_file) for metadata_file in metadata_files],
        )
        return out

    async def _gather_log_file_data(self, log_files: List[DataFile]) -> Sequence[dict]:
        """Gives a list of log files so we can update the application"""

        async def _query_log_file(log_file: DataFile) -> dict:
            async with self._session.create_client("s3") as client:
                response = await client.head_object(
                    Bucket=self._bucket,
                    Key=log_file.path,
                )
                # TODO -- consider the default cases, we should not have them and instead mark this as failed
                return {
                    "min_sequence_id": response["Metadata"].get("min_sequence_id", 0),
                    "max_sequence_id": response["Metadata"].get("max_sequence_id", 0),
                    "tracker_id": response["Metadata"].get("tracker_id", "unknown"),
                }

        out = await utils.gather_with_concurrency(
            self._aws_max_concurrency, *[_query_log_file(log_file) for log_file in log_files]
        )
        return out

    async def _gather_paths_to_update(
        self,
        project: Project,
        high_watermark_s3_path: str,
        max_paths=200,
    ) -> Sequence[DataFile]:
        """Gathers all paths to update in s3 -- we store file pointers in the db for these.
        This allows us to periodically scan for more files to index.

        :return: list of paths to update
        """
        logger.info(f"Scanning db with highwatermark: {high_watermark_s3_path}")
        paths_to_update = []
        logger.info(f"Scanning log data for project: {project.name}")
        async with self._session.create_client("s3") as client:
            paginator = client.get_paginator("list_objects_v2")
            async for result in paginator.paginate(
                Bucket=self._bucket,
                Prefix=f"{self._data_prefix}/{project.name}/",
                StartAfter=high_watermark_s3_path,
            ):
                for content in result.get("Contents", []):
                    key = content["Key"]
                    last_modified = content["LastModified"]
                    # Created == last_modified as we have an immutable data model
                    logger.info(f"Found new file: {key}")
                    paths_to_update.append(DataFile.from_path(key, created_date=last_modified))
                    if len(paths_to_update) >= max_paths:
                        break
        logger.info(f"Found {len(paths_to_update)} new files to index")
        return paths_to_update

    async def _ensure_applications_exist(
        self, paths_to_update: Sequence[DataFile], project: Project
    ):
        """Given the paths to update, ensure that all corresponding applications exist in the database.

        :param paths_to_update:
        :param project:
        :return:
        """
        all_application_keys = sorted(
            {(path.application_id, path.partition_key) for path in paths_to_update}
        )
        counter = Counter([path.file_type for path in paths_to_update])
        logger.info(
            f"Found {len(all_application_keys)} applications in the scan, "
            f"including: {counter['log']} log files, "
            f"{counter['metadata']} metadata files, and {counter['graph']} graph files, "
            f"and {len(paths_to_update) - len(all_application_keys)} other files."
        )

        # First, let's create all applications, ignoring them if they exist

        # first let's create all the applications if they don't exist
        existing_applications = {
            (app.name, app.partition_key): app
            for app in await self.query_applications_by_key(all_application_keys)
        }
        # all_applications = await Application.all()

        apps_to_create = [
            Application(
                name=app_id,
                partition_key=pk,
                project=project,
                created_at=system.now(),
            )
            for app_id, pk in all_application_keys
            if (app_id, pk) not in existing_applications
        ]

        logger.info(
            f"Creating {len(apps_to_create)} new applications, with keys: {[(app.name, app.partition_key) for app in apps_to_create]}"
        )
        await Application.bulk_create(apps_to_create)
        all_applications = await self.query_applications_by_key(all_application_keys)
        return all_applications

    async def _update_all_applications(
        self, all_applications: Sequence[Application], paths_to_update: Sequence[DataFile]
    ) -> Sequence[Application]:
        """Updates all application with associate metadata and graph files

        :param all_applications: All applications that are relevant
        :param paths_to_update: All paths to update
        :return:
        """
        logger.info(f"found: {len(all_applications)} applications to update in the db")
        metadata_data = [path for path in paths_to_update if path.file_type == "metadata"]
        graph_data = [path for path in paths_to_update if path.file_type == "graph"]
        metadata_objects = await self._gather_metadata_files(metadata_data)
        key_to_application_map = {(app.name, app.partition_key): app for app in all_applications}
        # For every metadata file we want to add the metadata file
        for metadata, datafile in zip(metadata_objects, metadata_data):
            key = (datafile.application_id, datafile.partition_key)
            app = key_to_application_map[key]
            app.metadata_file_pointer = datafile.path

            # TODO -- download the metadata file and update the application

        # for every graph file, we want to add the pointer
        for graph_file in graph_data:
            key = (graph_file.application_id, graph_file.partition_key)
            app = key_to_application_map[key]
            app.graph_file_pointer = graph_file.path
        # Go through every application and save them
        async with transactions.in_transaction():
            # TODO -- look at bulk saving, instead of transactions
            for app in all_applications:
                await app.save()
        return all_applications

    async def update_log_files(
        self, paths_to_update: Sequence[DataFile], all_applications: Sequence[Application]
    ):
        log_data = [path for path in paths_to_update if path.file_type == "log"]
        logfile_objects = await self._gather_log_file_data(log_data)
        key_to_application_map = {(app.name, app.partition_key): app for app in all_applications}

        # TODO -- gather referenced apps (parent pointers) and get the map of IDs to names

        # Go through every log file we've stored and update the appropriate item in the db
        logfiles_to_save = []
        for logfile, datafile in zip(logfile_objects, log_data):
            # get the application for the log file
            app = key_to_application_map[(datafile.application_id, datafile.partition_key)]
            # create the log file object
            logfiles_to_save.append(
                LogFile(
                    s3_path=datafile.path,
                    application=app,
                    tracker_id=logfile["tracker_id"],
                    min_sequence_id=logfile["min_sequence_id"],
                    max_sequence_id=logfile["max_sequence_id"],
                    created_at=datafile.created_date,
                )
            )
        # Save all the log files
        await LogFile.bulk_create(logfiles_to_save)

    async def _update_high_watermark(
        self, paths_to_update: Sequence[DataFile], project: Project, indexing_job: IndexingJob
    ):
        new_high_watermark = max(paths_to_update, key=lambda x: x.path).path
        next_status = IndexStatus(s3_highwatermark=new_high_watermark, project=project)
        await next_status.save()
        return next_status

    async def _scan_and_update_db_for_project(
        self, project: Project, indexing_job: IndexingJob
    ) -> Tuple[IndexStatus, int]:
        """Scans and updates the database for a project.

        TODO -- break this up into functions

        :param project: Project to scan/update
        :param max_length: Maximum length of the scan -- will pause and return after this. This is so we don't block for too long.
        :return: tuple of index status/num files processed
        """
        # get the current status
        current_status = (
            await IndexStatus.filter(project=project).order_by("-captured_time").first()
        )
        # This way we can sort by the latest captured time
        high_watermark = current_status.s3_highwatermark if current_status is not None else ""
        logger.info(f"Scanning db with highwatermark: {high_watermark}")
        paths_to_update = await self._gather_paths_to_update(
            project=project, high_watermark_s3_path=high_watermark
        )
        # Nothing new to see here
        if len(paths_to_update) == 0:
            return current_status, 0

        all_applications = await self._ensure_applications_exist(paths_to_update, project)
        await self._update_all_applications(all_applications, paths_to_update)
        await self.update_log_files(paths_to_update, all_applications)
        next_status = await self._update_high_watermark(paths_to_update, project, indexing_job)
        return next_status, len(paths_to_update)

    async def _scan_and_update_db(self):
        for project in await Project.all():
            indexing_job = IndexingJob(
                records_processed=0,  # start with zero
                end_time=None,
                status=IndexingJobStatus.RUNNING,
            )
            await indexing_job.save()

            # TODO -- add error catching
            status, num_files = await self._scan_and_update_db_for_project(project, indexing_job)
            logger.info(f"Scanned: {num_files} files with status stored at ID={status.id}")

            indexing_job.records_processed = num_files
            indexing_job.end_time = system.now()
            # TODO -- handle failure
            indexing_job.status = IndexingJobStatus.SUCCESS
            indexing_job.index_status = status
            await indexing_job.save()

    async def update(self):
        await self._update_projects()
        await self._scan_and_update_db()

    async def lifespan(self, app: FastAPI):
        if not os.path.exists(dirname := os.path.dirname(settings.DB_PATH)):
            os.mkdir(dirname)
        async with RegisterTortoise(app, config=settings.TORTOISE_ORM, add_exception_handlers=True):
            yield

    async def list_projects(self, request: fastapi.Request) -> Sequence[schema.Project]:
        project_query = await Project.all()
        out = []
        for project in project_query:
            latest_logfile = (
                await LogFile.filter(application__project=project).order_by("-created_at").first()
            )
            out.append(
                schema.Project(
                    name=project.name,
                    id=project.name,
                    uri=project.uri if project.uri is not None else "TODO",
                    last_written=latest_logfile.created_at
                    if latest_logfile is not None
                    else project.created_at,
                    created=project.created_at,
                    num_apps=await Application.filter(project=project).count(),
                )
            )
        return out

    async def list_apps(
        self,
        request: fastapi.Request,
        project_id: str,
        partition_key: Optional[str],
        limit: int = 100,
        offset: int = 0,
    ) -> Tuple[Sequence[schema.ApplicationSummary], int]:
        # TODO -- distinctify between project name and project ID
        # Currently they're the same in the UI but we'll want to have them decoupled
        app_query = (
            Application.filter(project__name=project_id).order_by("-updated_at")
            if partition_key is None
            else Application.filter(project__name=project_id, partition_key=partition_key)
        )

        applications = (
            await app_query.annotate(
                # TODO -- figure out why created_at is not giving the true latest date
                # For now we're using updated_at, but this is troubling...
                latest_logfile_created_at=functions.Max("log_files__created_at"),
                latest_logfile_updated_at=functions.Max("log_files__updated_at"),
                logfile_count=functions.Max("log_files__max_sequence_id"),
            )
            .offset(offset)
            .limit(limit)
            .prefetch_related("log_files", "project")
        )
        out = []
        for application in applications:
            last_written = (
                datetime.datetime.fromisoformat(application.latest_logfile_updated_at)
                if (application.latest_logfile_updated_at is not None)
                else application.created_at
            )
            out.append(
                schema.ApplicationSummary(
                    app_id=application.name,
                    partition_key=application.partition_key,
                    first_written=application.created_at,
                    last_written=last_written,
                    num_steps=application.logfile_count
                    if application.logfile_count is not None
                    else 0,
                    tags={},
                )
            )
        total_app_count = await app_query.count()
        return out, total_app_count

    async def get_application_logs(
        self, request: fastapi.Request, project_id: str, app_id: str, partition_key: str
    ) -> ApplicationLogs:
        # TODO -- handle partition keys
        query = (
            Application.filter(name=app_id, project__name=project_id)
            if partition_key is None
            else Application.filter(
                name=app_id, project__name=project_id, partition_key=partition_key
            )
        )
        applications = await query.all()
        application = applications[0]
        application_logs = await LogFile.filter(application__id=application.id).order_by(
            "-created_at"
        )
        async with self._session.create_client("s3") as client:
            # Get all the files
            files = await utils.gather_with_concurrency(
                1,
                _query_s3_file(self._bucket, application.graph_file_pointer, client),
                # _query_s3_files(self.bucket, application.metadata_file_pointer, client),
                *itertools.chain(
                    _query_s3_file(self._bucket, log_file.s3_path, client)
                    for log_file in application_logs
                ),
            )
        graph_data = ApplicationModel.parse_raw(files[0])
        # TODO -- deal with what happens if the application is None
        # TODO -- handle metadata
        # metadata = ApplicationMetadataModel.parse_raw(files[1])
        steps = Step.from_logs(list(itertools.chain(*[f.splitlines() for f in files[1:]])))

        return ApplicationLogs(
            children=[],
            steps=steps,
            # TODO -- get this in
            parent_pointer=None,
            spawning_parent_pointer=None,
            application=graph_data,
        )

    async def indexing_jobs(
        self, offset: int = 0, limit: int = 100, filter_empty: bool = True
    ) -> Sequence[schema.IndexingJob]:
        indexing_jobs_query = (
            IndexingJob.all().order_by("-start_time").prefetch_related("index_status__project")
        )

        # Apply filter conditionally
        if filter_empty:
            indexing_jobs_query = indexing_jobs_query.filter(records_processed__gt=0)
        indexing_jobs = await indexing_jobs_query.offset(offset).limit(limit)
        out = []
        for indexing_job in indexing_jobs:
            out.append(
                schema.IndexingJob(
                    id=indexing_job.id,
                    start_time=indexing_job.start_time,
                    end_time=indexing_job.end_time,
                    status=indexing_job.status,
                    records_processed=indexing_job.records_processed,
                    metadata={
                        "project": indexing_job.index_status.project.name
                        if indexing_job.index_status
                        else "unknown",
                        "s3_highwatermark": indexing_job.index_status.s3_highwatermark
                        if indexing_job.index_status
                        else "unknown",
                    },
                )
            )
        return out


if __name__ == "__main__":
    os.environ["BURR_LOAD_SNAPSHOT_ON_START"] = "True"
    import asyncio

    be = SQLiteS3Backend.from_settings(S3Settings())
    # coro = be.snapshot()  # save to s3
    # asyncio.run(coro)
    coro = be.load_snapshot()  # load from s3
    asyncio.run(coro)



---
File: /burr/burr/tracking/server/s3/initialize_db.py
---

import os
from pathlib import Path

from tortoise import Tortoise, run_async

from burr.tracking.server.s3 import settings

DB_PATH = Path("~/.burr_server/db.sqlite3").expanduser()


async def connect():
    if not os.path.exists(DB_PATH):
        os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    await Tortoise.init(
        config=settings.TORTOISE_ORM,
    )


#
async def first_time_init():
    await connect()
    # Generate the schema
    await Tortoise.generate_schemas()


if __name__ == "__main__":
    # db_path = sys.argv[1]
    run_async(first_time_init())



---
File: /burr/burr/tracking/server/s3/models.py
---

import enum

from tortoise import fields
from tortoise.models import Model


class IndexingJobStatus(enum.Enum):
    SUCCESS = "SUCCESS"
    FAILURE = "FAILURE"
    RUNNING = "RUNNING"


class IndexedModel(Model):
    """Base model for all models that are indexed in s3. Contains data on creating/updating"""

    created_at = fields.DatetimeField(null=False)
    indexed_at = fields.DatetimeField(null=True, auto_now_add=True)
    updated_at = fields.DatetimeField(null=False, auto_now=True)

    class Meta:
        abstract = True


class IndexingJob(Model):
    """Job for indexing data in s3. Records only if there's something to index"""

    id = fields.IntField(pk=True)
    start_time = fields.DatetimeField(auto_now_add=True)
    records_processed = fields.IntField()
    end_time = fields.DatetimeField(null=True)
    status = fields.CharEnumField(IndexingJobStatus)
    index_status = fields.ForeignKeyField(
        "models.IndexStatus", related_name="index_status", null=True
    )

    def __str__(self):
        return f"{self.start_time} - {self.end_time}"


class IndexStatus(Model):
    """Status to index. These are per-project and the latest is chosen"""

    id = fields.IntField(pk=True)
    s3_highwatermark = fields.CharField(max_length=1023)
    captured_time = fields.DatetimeField(index=True, auto_now_add=True)
    project = fields.ForeignKeyField("models.Project", related_name="project", index=True)

    def __str__(self):
        return f"{self.project} - {self.captured_time}"


class Project(IndexedModel):
    """Static model representing a project"""

    id = fields.IntField(pk=True)
    name = fields.CharField(index=True, max_length=255, unique=True)
    uri = fields.CharField(max_length=255, null=True)

    def __str__(self):
        return self.name


class Application(IndexedModel):
    id = fields.IntField(pk=True)
    name = fields.CharField(index=True, max_length=255)
    partition_key = fields.CharField(max_length=255, index=True, null=False)
    project = fields.ForeignKeyField("models.Project", related_name="applications", index=True)
    graph_file_pointer = fields.CharField(max_length=255, null=True)
    metadata_file_pointer = fields.CharField(max_length=255, null=True)
    fork_parent = fields.ForeignKeyField("models.Application", related_name="forks", null=True)
    spawning_parent = fields.ForeignKeyField("models.Application", related_name="spawns", null=True)

    class Meta:
        # App name is unique together
        unique_together = (("name", "partition_key"),)

    def graph_file_indexed(self) -> bool:
        return self.graph_file_pointer is not None

    def metadata_file_indexed(self) -> bool:
        return self.metadata_file_pointer is not None


class LogFile(IndexedModel):
    # s3 path is named
    # <tracker_id>-<logfile_sequence>-<min_sequence_id>-<max_sequence_id>.jsonl
    id = fields.IntField(pk=True)
    s3_path = fields.CharField(max_length=1024)
    application = fields.ForeignKeyField(
        "models.Application",
        related_name="log_files",
        index=True,
    )
    tracker_id = fields.CharField(max_length=255)
    min_sequence_id = fields.IntField()
    max_sequence_id = fields.IntField()



---
File: /burr/burr/tracking/server/s3/settings.py
---

import os

BURR_SERVER_ROOT = os.environ.get("BURR_SERVER_ROOT", os.path.expanduser("~/.burr_server"))
BURR_DB_FILENAME = os.environ.get("BURR_DB_FILENAME", "db.sqlite3")

DB_PATH = os.path.join(
    BURR_SERVER_ROOT,
    BURR_DB_FILENAME,
)
TORTOISE_ORM = {
    "connections": {"default": f"sqlite:///{DB_PATH}"},
    "apps": {
        "models": {
            "models": ["burr.tracking.server.s3.models", "aerich.models"],
            "default_connection": "default",
        },
    },
}



---
File: /burr/burr/tracking/server/s3/utils.py
---

import asyncio
from typing import Awaitable, TypeVar

AwaitableType = TypeVar("AwaitableType")


async def gather_with_concurrency(n, *coros: Awaitable[AwaitableType]) -> tuple[AwaitableType, ...]:
    semaphore = asyncio.Semaphore(n)

    async def sem_coro(coro: Awaitable[AwaitableType]) -> AwaitableType:
        async with semaphore:
            return await coro

    return await asyncio.gather(*(sem_coro(c) for c in coros))



---
File: /burr/burr/tracking/server/backend.py
---

import abc
import collections
import importlib
import json
import os.path
import sys
from datetime import datetime
from typing import Any, Optional, Sequence, Tuple, Type, TypeVar

import aiofiles
import aiofiles.os as aiofilesos
import fastapi
from fastapi import FastAPI
from pydantic_settings import BaseSettings, SettingsConfigDict

from burr.tracking.common import models
from burr.tracking.common.models import ChildApplicationModel
from burr.tracking.server import schema
from burr.tracking.server.schema import (
    AnnotationCreate,
    AnnotationOut,
    AnnotationUpdate,
    ApplicationLogs,
    ApplicationSummary,
    Step,
)

T = TypeVar("T")

# The following is a backend for the server.
# Note this is not a fixed API yet, and thus not documented (in Burr's documentation)
# Specifically, this does not have:
# - Streaming returns (just log tails)
# - Pagination
# - Authentication/Authorization


if sys.version_info <= (3, 11):
    Self = Any
else:
    from typing import Self


class BurrSettings(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="burr_")


class IndexingBackendMixin(abc.ABC):
    """Base mixin for an indexing backend -- one that index from
    logs (E.G. s3)"""

    @abc.abstractmethod
    async def update(self):
        """Updates the index"""
        pass

    @abc.abstractmethod
    async def update_interval_milliseconds(self) -> Optional[int]:
        """Returns the update interval in milliseconds"""
        pass

    @abc.abstractmethod
    async def indexing_jobs(
        self, offset: int = 0, limit: int = 100, filter_empty: bool = True
    ) -> Sequence[schema.IndexingJob]:
        """Returns the indexing jobs"""
        pass


class AnnotationsBackendMixin(abc.ABC):
    @abc.abstractmethod
    async def create_annotation(
        self,
        annotation: AnnotationCreate,
        project_id: str,
        partition_key: Optional[str],
        app_id: str,
        step_sequence_id: int,
    ) -> AnnotationOut:
        """Createse an annotation -- annotation has annotation data, the other pointers are given in the parameters.

        :param annotation: Annotation object to create
        :param partition_key: Partition key to associate with
        :param project_id: Project ID to associate with
        :param app_id: App ID to associate with
        :param step_sequence_id: Step sequence ID to associate with
        :return:
        """

    @abc.abstractmethod
    async def update_annotation(
        self,
        annotation: AnnotationUpdate,
        project_id: str,
        annotation_id: int,
    ) -> AnnotationOut:
        """Updates an annotation -- annotation has annotation data, the other pointers are given in the parameters.

        :param annotation: Annotation object to update
        :param project_id: Project ID to associate with
        :param annotation_id: Annotation ID to update. We include this as we may have multiple...
        :return: Updated annotation
        """

    @abc.abstractmethod
    async def get_annotations(
        self,
        project_id: str,
        partition_key: Optional[str] = None,
        app_id: Optional[str] = None,
        step_sequence_id: Optional[int] = None,
    ) -> Sequence[AnnotationOut]:
        """Returns annotations for a given project, partition_key, app_id, and step sequence ID.
        If these are None it does not filter by them.

        :param project_id: Project ID to query for
        :param partition_key: Partition key to query for
        :param app_id: App ID to query for
        :param step_sequence_id: Step sequence ID to query for
        :return: Annotations
        """
        pass


class SnapshottingBackendMixin(abc.ABC):
    """Mixin for backend that conducts snapshotting -- e.g. saves
    the data to a file or database."""

    @abc.abstractmethod
    async def load_snapshot(self):
        """Loads the snapshot if it exists.

        :return:
        """
        pass

    @abc.abstractmethod
    async def snapshot(self):
        """Snapshots the data"""
        pass

    @abc.abstractmethod
    def snapshot_interval_milliseconds(self) -> Optional[int]:
        """Returns the snapshot interval in milliseconds"""
        pass


class BackendBase(abc.ABC):
    async def lifespan(self, app: FastAPI):
        """Quick tool to allow plugin to the app's lifecycle.
        This is fine given that it's an internal API, but if we open it up more
        we should make this less flexible. For now this allows us to do clever
        initializations in the right order."""
        yield

    @abc.abstractmethod
    async def list_projects(self, request: fastapi.Request) -> Sequence[schema.Project]:
        """Lists out all projects -- this relies on the paginate function to work properly.

        :param request: The request object, used for authentication/authorization if needed
        :return: the next page
        """
        pass

    @abc.abstractmethod
    async def list_apps(
        self,
        request: fastapi.Request,
        project_id: str,
        partition_key: Optional[str],
        limit: int,
        offset: int,
    ) -> Tuple[Sequence[schema.ApplicationSummary], int]:
        """Lists out all apps (continual state machine runs with shared state) for a given project.

        :param request: The request object, used for authentication/authorization if needed
        :param project_id: filter by project id
        :param partition_key: filter by partition key
        :return: A list of apps and the total number of apps (given that we paginate)
        """
        pass

    @abc.abstractmethod
    async def get_application_logs(
        self, request: fastapi.Request, project_id: str, app_id: str, partition_key: Optional[str]
    ) -> ApplicationLogs:
        """Lists out all steps for a given app.

        :param request: The request object, used for authentication/authorization if needed
        :param app_id:
        :return:
        """
        pass

    @classmethod
    @abc.abstractmethod
    def settings_model(cls) -> Type[BaseSettings]:
        """Gives a settings model that tells us how to configure the backend.
        This is a class of pydantic BaseSettings type

        :return:  the settings model
        """
        pass

    @classmethod
    def from_settings(cls, settings_model: BaseSettings) -> Self:
        """Creates a backend from settings, of the type of settings_model above
        This defaults to assuming the constructor takes in settings parameters

        :param settings_model:
        :return:
        """
        return cls(**settings_model.dict())

    @classmethod
    def create_from_env(cls, dotenv_path: Optional[str] = None) -> Self:
        cls_path = os.environ.get("BURR_BACKEND_IMPL", "burr.tracking.server.backend.LocalBackend")
        mod_path = ".".join(cls_path.split(".")[:-1])
        mod = importlib.import_module(mod_path)
        cls_name = cls_path.split(".")[-1]
        if not hasattr(mod, cls_name):
            raise ValueError(f"Could not find {cls_name} in {mod_path}")
        cls = getattr(mod, cls_name)
        return cls.from_settings(
            cls.settings_model()(_env_file=dotenv_path, _env_file_encoding="utf-8")
        )

    def supports_demos(self) -> bool:
        """Whether this supports demos. Really we should abstract this out into a new mixin
        but for now this is OK.
        """
        return False


def safe_json_load(line: bytes):
    # Every once in a while we'll hit a non-utf-8 character
    # In this case we replace it and hope for the best
    return json.loads(line.decode("utf-8", errors="replace"))


def get_uri(project_id: str) -> str:
    project_id_map = {
        "demo_counter": "https://github.com/DAGWorks-Inc/burr/tree/main/examples/hello-world-counter",
        "demo_tracing": "https://github.com/DAGWorks-Inc/burr/tree/main/examples/tracing-and-spans/application.py",
        "demo_chatbot": "https://github.com/DAGWorks-Inc/burr/tree/main/examples/multi-modal-chatbot",
        "demo_conversational-rag": "https://github.com/DAGWorks-Inc/burr/tree/main/examples/conversational-rag",
    }
    return project_id_map.get(project_id, "")


DEFAULT_PATH = os.path.expanduser("~/.burr")


class LocalBackend(BackendBase, AnnotationsBackendMixin):
    """Quick implementation of a local backend for testing purposes. This is not a production backend.

    To override the path, set a `burr_path` environment variable to the path you want to use.
    """

    def __init__(self, path: str = DEFAULT_PATH):
        self.path = path

    def _get_annotation_path(self, project_id: str) -> str:
        return os.path.join(self.path, project_id, "annotations.jsonl")

    async def _load_project_annotations(self, project_id: str):
        annotations_path = self._get_annotation_path(project_id)
        annotations = []
        if os.path.exists(annotations_path):
            async with aiofiles.open(annotations_path) as f:
                for line in await f.readlines():
                    annotations.append(AnnotationOut.parse_raw(line))
        return annotations

    async def create_annotation(
        self,
        annotation: AnnotationCreate,
        project_id: str,
        partition_key: Optional[str],
        app_id: str,
        step_sequence_id: int,
    ) -> AnnotationOut:
        """Creates an annotation by loading all annotations, finding the max ID, and then appending the new annotation.
        This is not efficient but it's OK -- this is the local version and the number of annotations will be unlikely to be
        huge.

        :param annotation: Annotation to create
        :param project_id: ID of the associated project
        :param partition_key: Partition key to associate with
        :param app_id: App ID to associate with
        :param step_sequence_id: Step sequence ID to associate with
        :return: The created annotation, complete with an ID + timestamps
        """
        all_annotations = await self._load_project_annotations(project_id)
        annotation_id = (
            max([a.id for a in all_annotations], default=-1) + 1
        )  # get the ID, increment
        annotation_out = AnnotationOut(
            id=annotation_id,
            project_id=project_id,
            app_id=app_id,
            partition_key=partition_key,
            step_sequence_id=step_sequence_id,
            created=datetime.now(),
            updated=datetime.now(),
            **annotation.dict(),
        )
        annotations_path = self._get_annotation_path(project_id)
        async with aiofiles.open(annotations_path, "a") as f:
            await f.write(annotation_out.json() + "\n")
        return annotation_out

    async def update_annotation(
        self,
        annotation: AnnotationUpdate,
        project_id: str,
        annotation_id: int,
    ) -> AnnotationOut:
        """Updates an annotation by loading all annotations, finding the annotation, updating it, and then writing it back.
        Again, inefficient, but this is the local backend and we don't expect huge numbers of annotations.

        :param annotation: Annotation to update -- this is just the update fields to the full annotation
        :param project_id: ID of the associated project
        :param annotation_id: ID of the associated annotation, created by the backend
        :return: The updated annotation, complete with an ID + timestamps
        """
        all_annotations = await self._load_project_annotations(project_id)
        annotation_out = None
        for idx, a in enumerate(all_annotations):
            if a.id == annotation_id:
                annotation_out = a
                all_annotations[idx] = annotation_out.copy(
                    update={**annotation.dict(), "updated": datetime.now()}
                )
                break
        if annotation_out is None:
            raise fastapi.HTTPException(
                status_code=404,
                detail=f"Annotation: {annotation_id} from project: {project_id} not found",
            )
        annotations_path = self._get_annotation_path(project_id)
        async with aiofiles.open(annotations_path, "w") as f:
            for a in all_annotations:
                await f.write(a.json() + "\n")
        return annotation_out

    async def get_annotations(
        self,
        project_id: str,
        partition_key: Optional[str] = None,
        app_id: Optional[str] = None,
        step_sequence_id: Optional[int] = None,
    ) -> Sequence[AnnotationOut]:
        """Gets the annotation by loading all annotations and filtering by the parameters. Will return all annotations
        that match. Only project is required.


        :param project_id:
        :param partition_key:
        :param app_id:
        :param step_sequence_id:
        :return:
        """
        annotation_path = self._get_annotation_path(project_id)
        if not os.path.exists(annotation_path):
            return []
        annotations = []
        async with aiofiles.open(annotation_path) as f:
            for line in await f.readlines():
                parsed = AnnotationOut.parse_raw(line)
                if (
                    (partition_key is None or parsed.partition_key == partition_key)
                    and (app_id is None or parsed.app_id == app_id)
                    and (step_sequence_id is None or parsed.step_sequence_id == step_sequence_id)
                ):
                    annotations.append(parsed)
        return annotations

    async def list_projects(self, request: fastapi.Request) -> Sequence[schema.Project]:
        out = []
        if not os.path.exists(self.path):
            return out
        for entry in await aiofilesos.listdir(self.path):
            full_path = os.path.join(self.path, entry)
            if os.path.isdir(full_path):
                out.append(
                    schema.Project(
                        name=entry,
                        id=entry,
                        uri=get_uri(entry),
                        last_written=await aiofilesos.path.getmtime(full_path),
                        created=await aiofilesos.path.getctime(full_path),
                        num_apps=len(await aiofilesos.listdir(full_path)),
                    )
                )
        return out

    async def get_number_of_steps(self, file_path: str) -> int:
        """Quick tool to get the latest sequence ID from a log file.
        This is not efficient and should be replaced."""
        async with aiofiles.open(file_path, "rb") as f:
            for line in reversed(await f.readlines()):
                line_data = safe_json_load(line)
                if "sequence_id" in line_data:
                    # Just return the latest we can determine for now
                    # We add one as it is the count, not the index
                    return line_data["sequence_id"] + 1
        return 0

    async def _load_metadata(self, metadata_path: str) -> models.ApplicationMetadataModel:
        if os.path.exists(metadata_path):
            async with aiofiles.open(metadata_path, "rb") as f:
                raw = await f.read()
                return models.ApplicationMetadataModel.parse_obj(safe_json_load(raw))
        return models.ApplicationMetadataModel()

    async def list_apps(
        self,
        request: fastapi.Request,
        project_id: str,
        partition_key: Optional[str],
        limit: int = 100,
        offset: int = 0,
    ) -> Tuple[Sequence[ApplicationSummary], int]:
        project_filepath = os.path.join(self.path, project_id)
        if not os.path.exists(project_filepath):
            return [], 0
            # raise fastapi.HTTPException(status_code=404, detail=f"Project: {project_id} not found")
        out = []
        for entry in await aiofilesos.listdir(project_filepath):
            if entry.startswith("."):
                # skip hidden files/directories
                continue
            full_path = os.path.join(project_filepath, entry)
            metadata_path = os.path.join(full_path, "metadata.json")
            log_path = os.path.join(full_path, "log.jsonl")
            if os.path.isdir(full_path):
                metadata = await self._load_metadata(metadata_path)
                app_partition_key = metadata.partition_key
                # quick, hacky way to do it -- we should really have this be part of the path
                # But we load it up anyway. TODO -- add partition key to the path
                # If this is slow you'll want to use the s3-based storage system
                # Which has an actual index
                if partition_key is not None and partition_key != app_partition_key:
                    continue
                out.append(
                    schema.ApplicationSummary(
                        app_id=entry,
                        partition_key=metadata.partition_key,
                        first_written=await aiofilesos.path.getctime(full_path),
                        last_written=await aiofilesos.path.getmtime(full_path),
                        num_steps=await self.get_number_of_steps(log_path),
                        tags={},
                        parent_pointer=metadata.parent_pointer,
                        spawning_parent_pointer=metadata.spawning_parent_pointer,
                    )
                )
        out = sorted(out, key=lambda x: x.last_written, reverse=True)
        # TODO -- actually only get the most recent ones rather than reading everything, this is inefficient
        return out[offset : offset + limit], len(out)

    async def get_application_logs(
        self, request: fastapi.Request, project_id: str, app_id: str, partition_key: Optional[str]
    ) -> ApplicationLogs:
        # TODO -- handle partition key here
        # This currently assumes uniqueness
        app_filepath = os.path.join(self.path, project_id, app_id)
        if not os.path.exists(app_filepath):
            raise fastapi.HTTPException(
                status_code=404, detail=f"App: {app_id} from project: {project_id} not found"
            )
        log_file = os.path.join(app_filepath, "log.jsonl")
        graph_file = os.path.join(app_filepath, "graph.json")
        metadata_file = os.path.join(app_filepath, "metadata.json")
        children_file = os.path.join(app_filepath, "children.jsonl")
        metadata = await self._load_metadata(metadata_file)
        if not os.path.exists(graph_file):
            raise fastapi.HTTPException(
                status_code=404,
                detail=f"Graph file for app: {app_id} from project: {project_id} not found",
            )
        async with aiofiles.open(graph_file) as f:
            str_graph = await f.read()
        collections.defaultdict(list)
        if os.path.exists(log_file):
            async with aiofiles.open(log_file, "rb") as f:
                lines = await f.readlines()
                steps = Step.from_logs(lines)
        children = []
        if os.path.exists(children_file):
            async with aiofiles.open(children_file) as f:
                str_children = await f.readlines()
                children = [
                    ChildApplicationModel.parse_obj(json.loads(item)) for item in str_children
                ]

        return ApplicationLogs(
            application=schema.ApplicationModel.parse_raw(str_graph),
            steps=steps,
            parent_pointer=metadata.parent_pointer,
            spawning_parent_pointer=metadata.spawning_parent_pointer,
            children=children,
        )

    def supports_demos(self) -> bool:
        return True

    class BackendSettings(BurrSettings):
        path: str = DEFAULT_PATH

    @classmethod
    def settings_model(cls) -> Type[BurrSettings]:
        return cls.BackendSettings

    @classmethod
    def from_settings(cls, settings_model: BurrSettings) -> Self:
        return cls(**settings_model.dict())



---
File: /burr/burr/tracking/server/run.py
---

import importlib
import logging
import os
from contextlib import asynccontextmanager
from importlib.resources import files
from typing import Optional, Sequence

from starlette import status

# TODO -- remove this, just for testing
from burr.log_setup import setup_logging
from burr.tracking.server.backend import (
    AnnotationsBackendMixin,
    BackendBase,
    IndexingBackendMixin,
    SnapshottingBackendMixin,
)

setup_logging(logging.INFO)

logger = logging.getLogger(__name__)

try:
    import uvicorn
    from fastapi import FastAPI, HTTPException, Request
    from fastapi.staticfiles import StaticFiles
    from fastapi_utils.tasks import repeat_every
    from starlette.templating import Jinja2Templates

    from burr.tracking.server import schema
    from burr.tracking.server.schema import (  # AnnotationUpdate,
        AnnotationCreate,
        AnnotationOut,
        AnnotationUpdate,
        ApplicationLogs,
        ApplicationPage,
        BackendSpec,
        IndexingJob,
    )

    # dynamic importing due to the dashes (which make reading the examples on github easier)
    email_assistant = importlib.import_module("burr.examples.email-assistant.server")
    chatbot = importlib.import_module("burr.examples.multi-modal-chatbot.server")
    streaming_chatbot = importlib.import_module("burr.examples.streaming-fastapi.server")
    deep_researcher = importlib.import_module("burr.examples.deep-researcher.server")

except ImportError as e:
    raise e
    # require_plugin(
    #     e,
    #     [
    #         "click",
    #         "fastapi",
    #         "uvicorn",
    #         "pydantic",
    #         "fastapi-pagination",
    #         "aiofiles",
    #         "requests",
    #         "jinja2",
    #     ],
    #     "tracking",
    # )

SERVE_STATIC = os.getenv("BURR_SERVE_STATIC", "true").lower() == "true"

SENTINEL_PARTITION_KEY = "__none__"

backend = BackendBase.create_from_env()


# if it is an indexing backend we want to expose a few endpoints


# TODO -- add a health check for intialization


async def sync_index():
    if app_spec.indexing:
        logger.info("Updating backend index...")
        await backend.update()
        logger.info("Updated backend index...")


async def download_snapshot():
    if app_spec.snapshotting:
        logger.info("Downloading snapshot of DB for backend to use")
        await backend.load_snapshot()
        logger.info("Downloaded snapshot of DB for backend to use")


first_snapshot = True


async def save_snapshot():
    # is_first is due to the weirdness of the repeat_every decorator
    # It has to be called but we don't want this to run every time
    # So we just skip the first
    global first_snapshot
    if first_snapshot:
        first_snapshot = False
        return
    if app_spec.snapshotting:
        logger.info("Saving snapshot of DB for recovery")
        await backend.snapshot()
        logger.info("Saved snapshot of DB for recovery")


initialized = False


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Download if it does it
    # For now we do this before the lifespan
    await download_snapshot()
    # No yield from allowed
    await backend.lifespan(app).__anext__()
    await sync_index()  # this will trigger the repeat every N seconds
    await save_snapshot()  # this will trigger the repeat every N seconds
    global initialized
    initialized = True
    yield
    await backend.lifespan(app).__anext__()


app = FastAPI(lifespan=lifespan)


@app.get("/ready")
def is_ready():
    if not initialized:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail="Backend is not ready yet."
        )
    return {"ready": True}


@app.get("/api/v0/metadata/app_spec", response_model=BackendSpec)
def get_app_spec():
    is_indexing_backend = isinstance(backend, IndexingBackendMixin)
    is_snapshotting_backend = isinstance(backend, SnapshottingBackendMixin)
    is_annotations_backend = isinstance(backend, AnnotationsBackendMixin)
    supports_demos = backend.supports_demos()
    return BackendSpec(
        indexing=is_indexing_backend,
        snapshotting=is_snapshotting_backend,
        supports_demos=supports_demos,
        supports_annotations=is_annotations_backend,
    )


app_spec = get_app_spec()

logger = logging.getLogger(__name__)

if app_spec.indexing:
    update_interval = backend.update_interval_milliseconds() / 1000 if app_spec.indexing else None
    sync_index = repeat_every(
        seconds=backend.update_interval_milliseconds() / 1000,
        wait_first=True,
        logger=logger,
    )(sync_index)

if app_spec.snapshotting:
    snapshot_interval = (
        backend.snapshot_interval_milliseconds() / 1000 if app_spec.snapshotting else None
    )
    save_snapshot = repeat_every(
        seconds=backend.snapshot_interval_milliseconds() / 1000,
        wait_first=True,
        logger=logger,
    )(save_snapshot)


@app.get("/api/v0/projects", response_model=Sequence[schema.Project])
async def get_projects(request: Request) -> Sequence[schema.Project]:
    """Gets all projects visible by the user.

    :param request: FastAPI request
    :return:  a list of projects visible by the user
    """
    return await backend.list_projects(request)


@app.get("/api/v0/{project_id}/{partition_key}/apps", response_model=ApplicationPage)
async def get_apps(
    request: Request,
    project_id: str,
    partition_key: str,
    limit: int = 100,
    offset: int = 0,
) -> ApplicationPage:
    """Gets all apps visible by the user

    :param request: FastAPI request
    :param project_id: project name
    :return: a list of projects visible by the user
    """
    if partition_key == SENTINEL_PARTITION_KEY:
        partition_key = None
    applications, total_count = await backend.list_apps(
        request, project_id, partition_key=partition_key, limit=limit, offset=offset
    )
    return ApplicationPage(
        applications=list(applications),
        total=total_count,
        has_another_page=total_count > offset + limit,
    )


@app.get("/api/v0/{project_id}/{app_id}/{partition_key}/apps")
async def get_application_logs(
    request: Request, project_id: str, app_id: str, partition_key: str
) -> ApplicationLogs:
    """Lists steps for a given App.
    TODO: add streaming capabilities for bi-directional communication
    TODO: add pagination for quicker loading

    :param request: FastAPI
    :param project_id: ID of the project
    :param app_id: ID of the assIndociated application
    :return: A list of steps with all associated step data
    """
    if partition_key == SENTINEL_PARTITION_KEY:
        partition_key = None
    return await backend.get_application_logs(
        request, project_id=project_id, app_id=app_id, partition_key=partition_key
    )


@app.post(
    "/api/v0/{project_id}/{app_id}/{partition_key}/{sequence_id}/annotations",
    response_model=AnnotationOut,
)
async def create_annotation(
    request: Request,
    project_id: str,
    app_id: str,
    partition_key: str,
    sequence_id: int,
    annotation: AnnotationCreate,
):
    if partition_key == SENTINEL_PARTITION_KEY:
        partition_key = None
    spec = get_app_spec()
    if not spec.supports_annotations:
        return []  # empty default -- the case that we don't support annotations
    return await backend.create_annotation(
        annotation, project_id, partition_key, app_id, sequence_id
    )


#
# # TODO -- take out these parameters cause we have the annotation ID
@app.put(
    "/api/v0/{project_id}/{annotation_id}/update_annotations",
    response_model=AnnotationOut,
)
async def update_annotation(
    request: Request,
    project_id: str,
    annotation_id: int,
    annotation: AnnotationUpdate,
):
    return await backend.update_annotation(
        annotation_id=annotation_id, annotation=annotation, project_id=project_id
    )


@app.get("/api/v0/{project_id}/annotations", response_model=Sequence[AnnotationOut])
async def get_annotations(
    request: Request,
    project_id: str,
    app_id: Optional[str] = None,
    partition_key: Optional[str] = None,
    step_sequence_id: Optional[int] = None,
):
    # Handle the sentinel value for partition_key
    if partition_key == SENTINEL_PARTITION_KEY:
        partition_key = None
    backend_spec = get_app_spec()

    if not backend_spec.supports_annotations:
        # makes it easier to wire through to the FE
        return []

    # Logic to retrieve the annotations
    return await backend.get_annotations(project_id, partition_key, app_id, step_sequence_id)


@app.get("/api/v0/ready")
async def ready() -> bool:
    return True


@app.get("/api/v0/indexing_jobs", response_model=Sequence[IndexingJob])
async def get_indexing_jobs(
    offset: int = 0, limit: int = 100, filter_empty: bool = True
) -> Sequence[IndexingJob]:
    if not app_spec.indexing:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="This backend does not support indexing jobs.",
        )
    return await backend.indexing_jobs(offset=offset, limit=limit, filter_empty=filter_empty)


@app.get("/api/v0/version")
async def version() -> dict:
    """Returns the burr version"""
    import pkg_resources

    try:
        version = pkg_resources.get_distribution("burr").version
    except pkg_resources.DistributionNotFound:
        version = "unknown"
    return {"version": version}


# Examples -- todo -- put them behind `if` statements
app.include_router(chatbot.router, prefix="/api/v0/chatbot")
app.include_router(email_assistant.router, prefix="/api/v0/email_assistant")
app.include_router(streaming_chatbot.router, prefix="/api/v0/streaming_chatbot")
app.include_router(deep_researcher.router, prefix="/api/v0/deep_researcher")

if SERVE_STATIC:
    BASE_ASSET_DIRECTORY = str(files("burr").joinpath("tracking/server/build"))

    templates = Jinja2Templates(directory=BASE_ASSET_DIRECTORY)
    app.mount(
        "/static", StaticFiles(directory=os.path.join(BASE_ASSET_DIRECTORY, "static")), "/static"
    )
    # public assets in create react app don't get put under build/static, we need to route them over
    app.mount("/public", StaticFiles(directory=BASE_ASSET_DIRECTORY, html=True), "/public")

    @app.get("/{rest_of_path:path}")
    async def react_app(req: Request, rest_of_path: str):
        """Quick trick to server the react app
        Thanks to https://github.com/hop-along-polly/fastapi-webapp-react for the example/demo
        """
        return templates.TemplateResponse("index.html", {"request": req})


if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))  # Default to 8000 if no PORT environment variable is set
    uvicorn.run(app, host="0.0.0.0", port=port)



---
File: /burr/burr/tracking/server/schema.py
---

import collections
import datetime
from typing import Any, Dict, List, Literal, Optional, Union

import pydantic
from pydantic import fields

from burr.tracking.common.models import (
    ApplicationModel,
    AttributeModel,
    BeginEntryModel,
    BeginSpanModel,
    ChildApplicationModel,
    EndEntryModel,
    EndSpanModel,
    EndStreamModel,
    FirstItemStreamModel,
    InitializeStreamModel,
    PointerModel,
)
from burr.tracking.utils import safe_json_load


class Project(pydantic.BaseModel):
    name: str
    id: str  # defaults to name for local, not for remote
    last_written: datetime.datetime
    created: datetime.datetime
    num_apps: int
    uri: str


class ApplicationSummary(pydantic.BaseModel):
    app_id: str
    partition_key: Optional[str]
    first_written: datetime.datetime
    last_written: datetime.datetime
    num_steps: int
    tags: Dict[str, str]
    parent_pointer: Optional[PointerModel] = None
    spawning_parent_pointer: Optional[PointerModel] = None


class ApplicationPage(pydantic.BaseModel):
    applications: List[ApplicationSummary]
    total: int
    has_another_page: bool


class ApplicationModelWithChildren(pydantic.BaseModel):
    application: ApplicationModel
    children: List[PointerModel]
    type: str = "application_with_children"


class PartialSpan(pydantic.BaseModel):
    begin_entry: Optional[BeginSpanModel] = fields.Field(default_factory=lambda: None)
    end_entry: Optional[EndSpanModel] = fields.Field(default_factory=lambda: None)


class Span(pydantic.BaseModel):
    """Represents a span. These have action sequence IDs associated with
    them to put them in order."""

    begin_entry: BeginSpanModel
    end_entry: Optional[EndSpanModel]


class PartialStep(pydantic.BaseModel):
    step_start_log: Optional[BeginEntryModel] = fields.Field(default_factory=lambda: None)
    step_end_log: Optional[EndEntryModel] = fields.Field(default_factory=lambda: None)
    spans: List[Span] = fields.Field(default_factory=list)
    streaming_events: List[
        Union[InitializeStreamModel, FirstItemStreamModel, EndStreamModel]
    ] = fields.Field(default_factory=list)


class Step(pydantic.BaseModel):
    """Log of  astep -- has a start and an end."""

    step_start_log: BeginEntryModel
    step_end_log: Optional[EndEntryModel]
    spans: List[Span]
    attributes: List[AttributeModel]
    streaming_events: List[Union[InitializeStreamModel, FirstItemStreamModel, EndStreamModel]]

    @staticmethod
    def from_logs(log_lines: List[bytes]) -> List["Step"]:
        steps_by_sequence_id = collections.defaultdict(PartialStep)
        spans_by_id = collections.defaultdict(PartialSpan)
        attributes_by_step: dict[int, List[AttributeModel]] = collections.defaultdict(list)
        for line in log_lines:
            json_line = safe_json_load(line)
            # TODO -- make these into constants
            if json_line["type"] == "begin_entry":
                begin_step = BeginEntryModel.model_validate(json_line)
                steps_by_sequence_id[begin_step.sequence_id].step_start_log = begin_step
            elif json_line["type"] == "end_entry":
                step_end_log = EndEntryModel.model_validate(json_line)
                steps_by_sequence_id[step_end_log.sequence_id].step_end_log = step_end_log
            elif json_line["type"] == "begin_span":
                span = BeginSpanModel.model_validate(json_line)
                spans_by_id[span.span_id] = PartialSpan(
                    begin_entry=span,
                    end_entry=None,
                )
            elif json_line["type"] == "end_span":
                end_span = EndSpanModel.model_validate(json_line)
                span = spans_by_id[end_span.span_id]
                span.end_entry = end_span
            elif json_line["type"] == "attribute":
                attribute = AttributeModel.model_validate(json_line)
                attributes_by_step[attribute.action_sequence_id].append(attribute)
            elif json_line["type"] in ["begin_stream", "first_item_stream", "end_stream"]:
                streaming_event = {
                    "begin_stream": InitializeStreamModel,
                    "first_item_stream": FirstItemStreamModel,
                    "end_stream": EndStreamModel,
                }[json_line["type"]].model_validate(json_line)
                steps_by_sequence_id[streaming_event.sequence_id].streaming_events.append(
                    streaming_event
                )
        for span in spans_by_id.values():
            sequence_id = (
                span.begin_entry.action_sequence_id
                if span.begin_entry
                else span.end_entry.action_sequence_id
            )
            step = (
                steps_by_sequence_id[sequence_id] if sequence_id in steps_by_sequence_id else None
            )
            if step is not None:
                if span.begin_entry is not None:
                    full_span = Span(
                        begin_entry=span.begin_entry,
                        end_entry=span.end_entry,
                    )
                    step.spans.append(full_span)
        # filter out all the non-null start steps
        return [
            Step(
                step_start_log=value.step_start_log,
                step_end_log=value.step_end_log,
                spans=[Span(**span.dict()) for span in value.spans if span.begin_entry is not None],
                attributes=attributes_by_step[key],
                streaming_events=value.streaming_events,
            )
            for key, value in sorted(steps_by_sequence_id.items())
            if value.step_start_log is not None
        ]


class StepWithMinimalData(Step):
    step_start_log: Optional[BeginEntryModel]


class ApplicationLogs(pydantic.BaseModel):
    """Application logs are purely flat --
    we will likely be rethinking this but for now this provides for easy parsing."""

    application: ApplicationModel
    children: List[ChildApplicationModel]
    steps: List[Step]
    parent_pointer: Optional[PointerModel] = None
    spawning_parent_pointer: Optional[PointerModel] = None


class IndexingJob(pydantic.BaseModel):
    """Generic link for indexing job -- can be exposed in 'admin mode' in the UI"""

    id: int
    start_time: datetime.datetime
    end_time: Optional[datetime.datetime]
    status: str
    records_processed: int
    metadata: Dict[str, Any]


class BackendSpec(pydantic.BaseModel):
    """Generic link for indexing job -- can be exposed in 'admin mode' in the UI"""

    indexing: bool
    snapshotting: bool
    supports_demos: bool
    supports_annotations: bool


class AnnotationDataPointer(pydantic.BaseModel):
    type: Literal["state_field", "attribute"]
    field_name: str  # key of attribute/state field
    span_id: Optional[
        str
    ]  # span_id if it's associated with a span, otherwise it's associated with an action


AllowedDataField = Literal["note", "ground_truth"]


class AnnotationObservation(pydantic.BaseModel):
    data_fields: dict[str, Any]
    thumbs_up_thumbs_down: Optional[bool]
    data_pointers: List[AnnotationDataPointer]


class AnnotationCreate(pydantic.BaseModel):
    """Generic link for indexing job -- can be exposed in 'admin mode' in the UI"""

    span_id: Optional[str]
    step_name: str  # Should be able to look it up but including for now
    tags: List[str]
    observations: List[AnnotationObservation]


class AnnotationUpdate(AnnotationCreate):
    """Generic link for indexing job -- can be exposed in 'admin mode' in the UI"""

    # Identification for association
    span_id: Optional[str] = None
    tags: Optional[List[str]] = []
    observations: List[AnnotationObservation]


class AnnotationOut(AnnotationCreate):
    """Generic link for indexing job -- can be exposed in 'admin mode' in the UI"""

    id: int
    # Identification for association
    project_id: str  # associated project ID
    app_id: str
    partition_key: Optional[str]
    step_sequence_id: int
    created: datetime.datetime
    updated: datetime.datetime



---
File: /burr/burr/tracking/__init__.py
---

from .client import LocalTrackingClient

__all__ = ["LocalTrackingClient"]



---
File: /burr/burr/tracking/base.py
---

import abc

from burr.lifecycle import (
    PostApplicationCreateHook,
    PostEndSpanHook,
    PostRunStepHook,
    PreRunStepHook,
    PreStartSpanHook,
)
from burr.lifecycle.base import (
    DoLogAttributeHook,
    PostEndStreamHook,
    PostStreamItemHook,
    PreStartStreamHook,
)


class SyncTrackingClient(
    PostApplicationCreateHook,
    PreRunStepHook,
    PostRunStepHook,
    PreStartSpanHook,
    PostEndSpanHook,
    DoLogAttributeHook,
    PreStartStreamHook,
    PostStreamItemHook,
    PostEndStreamHook,
    abc.ABC,
):
    """Base class for synchronous tracking clients. All tracking clients must implement from this
    TODO -- create an async tracking client"""

    @abc.abstractmethod
    def copy(self):
        pass


TrackingClient = SyncTrackingClient



---
File: /burr/burr/tracking/client.py
---

import abc
import dataclasses
import datetime

from burr.common.types import BaseCopyable
from burr.lifecycle.base import (
    DoLogAttributeHook,
    PostEndStreamHook,
    PostStreamItemHook,
    PreStartStreamHook,
)

# this is a quick hack to get it to work on windows
# we'll have to implement a proper lock later
# but its better that it works than breaks on import
try:
    import fcntl
except ImportError:

    class fcntl:
        @staticmethod
        def flock(*args, **kwargs):
            return

        LOCK_EX = 0
        LOCK_UN = 0


import json
import logging
import os
import re
import traceback
from abc import ABC
from typing import Any, Dict, Optional, Tuple

try:
    from typing import Self
except ImportError:
    Self = "Self"

from burr import system
from burr.common import types as burr_types
from burr.core import Action, ApplicationGraph, State, serde
from burr.core.persistence import BaseStateLoader, PersistedStateData
from burr.integrations.base import require_plugin
from burr.lifecycle import (
    PostApplicationCreateHook,
    PostEndSpanHook,
    PostRunStepHook,
    PreRunStepHook,
    PreStartSpanHook,
)
from burr.tracking.common.models import (
    ApplicationMetadataModel,
    ApplicationModel,
    AttributeModel,
    BeginEntryModel,
    BeginSpanModel,
    ChildApplicationModel,
    EndEntryModel,
    EndSpanModel,
    EndStreamModel,
    FirstItemStreamModel,
    InitializeStreamModel,
    PointerModel,
)
from burr.visibility import ActionSpan

logger = logging.getLogger(__name__)

try:
    import pydantic
except ImportError as e:
    require_plugin(
        e,
        "tracking-client",
    )


def _format_exception(exception: Exception) -> Optional[str]:
    if exception is None:
        return None
    return "".join(traceback.format_exception(type(exception), exception, exception.__traceback__))


INPUT_FILTERLIST = {"__tracer"}


def _filter_inputs(d: dict) -> dict:
    return {k: v for k, v in d.items() if k not in INPUT_FILTERLIST}


def _allowed_project_name(project_name: str, on_windows: bool) -> bool:
    allowed_chars = r"a-zA-Z0-9_\-"
    if not on_windows:
        allowed_chars += ":"
    pattern = f"^[{allowed_chars}]+$"

    # Use regular expression to check if the string is valid
    return bool(re.match(pattern, project_name))


@dataclasses.dataclass
class StreamState:
    stream_init_time: datetime.datetime
    count: Optional[int]


StateKey = Tuple[str, str, Optional[str]]


class SyncTrackingClient(
    PostApplicationCreateHook,
    PreRunStepHook,
    PostRunStepHook,
    PreStartSpanHook,
    PostEndSpanHook,
    DoLogAttributeHook,
    PreStartStreamHook,
    PostStreamItemHook,
    PostEndStreamHook,
    BaseCopyable,
    ABC,
):
    @abc.abstractmethod
    def copy(self) -> Self:
        """Clones the tracking client. This is useful for forking applications.
        Note we have to copy the tracking client as it is stateful for tracking.
        We may make this called internally if it has started already,
        or make it so it carries multiple states at once.

        :return: a copy of the self.
        """
        pass


class LocalTrackingClient(
    SyncTrackingClient,
    BaseStateLoader,
):
    """Tracker to track locally -- goes along with the Burr UI. Writes
    down the following:
    #. The whole application + debugging information (e.g. source code) to a file
    #. A line for the start/end of each step
    """

    GRAPH_FILENAME = "graph.json"
    METADATA_FILENAME = "metadata.json"
    LOG_FILENAME = "log.jsonl"
    CHILDREN_FILENAME = (
        "children.jsonl"  # any applications that are spawned or forked will show up here
    )
    # This is purely an optimization for bi-directional relationships using filesystems (denormalized data)
    DEFAULT_STORAGE_DIR = "~/.burr"

    def __init__(
        self,
        project: str,
        storage_dir: str = DEFAULT_STORAGE_DIR,
        serde_kwargs: Optional[Dict[str, Any]] = None,
    ):
        """Instantiates a local tracking client. This will create the following directories, if they don't exist:
        #. The base directory (defaults to ~/.burr)
        #. The project directory (defaults to ~/.burr/<project>)
        #. The application directory (defaults to ~/.burr/<project>/<app_id>) on each

        On application create, it will write the state machine to the application directory.
        On pre/post run step, it will write the start/end of each step to the application directory.

        :param project: Project name -- if this already exists it will be used, otherwise it will be created.
        :param storage_dir: Storage directory
        """

        self.f = None
        if not _allowed_project_name(project, on_windows=system.IS_WINDOWS):
            raise ValueError(
                f"Project: {project} is not valid. Project name cannot contain non-alphanumeric (except _ and -) characters."
                "We will be relaxing this restriction later but for now please rename your project!"
            )
        self.raw_storage_dir = storage_dir
        self.storage_dir = LocalTrackingClient.get_storage_path(project, storage_dir)
        self.project_id = project
        self.serde_kwargs = serde_kwargs if serde_kwargs is not None else {}
        # app_id, action, partition_key  -> stream data so we can track
        self.stream_state: Dict[StateKey, StreamState] = dict()

    def _log_child_relationships(
        self,
        fork_parent_pointer_model: Optional[burr_types.ParentPointer],
        spawn_parent_pointer_model: Optional[burr_types.ParentPointer],
        app_id: str,
    ):
        """Logs a child relationship. This is special as it does not log to the main log file. Rather
        it logs within the parent directory. Note this only exists to maintain (denormalized) bidirectional
        pointers, as the filesystem is not a database, so querying is very inefficient.

        This uses fctl.flock to ensure that the file is not written to by multiple processes at the same time.
        Read may be corrupted (by the server), as it does not use the lock, but that will retry.
        """
        parent_relationships = []
        if fork_parent_pointer_model is not None:
            parent_relationships.append(
                # This effectively inverts the pointers
                # The logging call is being made from the child, so we're logging to the parent
                # We do it once for the fork_parent (if it exists), and once for the spawn_parent (if it exists)
                (
                    fork_parent_pointer_model.app_id,
                    ChildApplicationModel(
                        child=PointerModel(
                            app_id=app_id,
                            sequence_id=None,
                            partition_key=None,  # TODO -- get partition key
                        ),
                        event_time=datetime.datetime.now(),
                        event_type="fork",
                        # this is the sequence ID at which this link occurred (from the parent)
                        sequence_id=fork_parent_pointer_model.sequence_id,
                    ),
                )
            )
        if spawn_parent_pointer_model is not None:
            # See the notes above
            parent_relationships.append(
                (
                    spawn_parent_pointer_model.app_id,
                    ChildApplicationModel(
                        child=PointerModel(
                            app_id=app_id,
                            sequence_id=None,
                            partition_key=None,  # TODO -- get partition key
                        ),
                        event_time=datetime.datetime.now(),
                        event_type="spawn_start",
                        sequence_id=spawn_parent_pointer_model.sequence_id,
                    ),
                )
            )
        for parent_id, child_of in parent_relationships:
            parent_path = os.path.join(self.storage_dir, parent_id)
            if not os.path.exists(parent_path):
                # This currently makes the parent directory so that it does not fail
                # If the parent directory exists we'll just use that
                os.makedirs(parent_path)
            parent_children_list_path = os.path.join(
                parent_path, LocalTrackingClient.CHILDREN_FILENAME
            )
            # Order should not matter here
            # currently we write start events, so it really won't matter
            # but in the future we'll write end events, but we'll parse it in a
            # way that allows them to be interwoven
            with open(parent_children_list_path, "a", errors="replace", encoding="utf-8") as f:
                fileno = f.fileno()
                try:
                    fcntl.flock(fileno, fcntl.LOCK_EX)
                    f.write(child_of.model_dump_json() + "\n")
                finally:
                    # we always want to release the lock so its not held indefinitely
                    fcntl.flock(fileno, fcntl.LOCK_UN)

    def copy(self) -> "LocalTrackingClient":
        return LocalTrackingClient(
            project=self.project_id,
            storage_dir=self.raw_storage_dir,
            serde_kwargs=self.serde_kwargs,
        )

    @classmethod
    def get_storage_path(cls, project, storage_dir) -> str:
        return str(os.path.join(os.path.expanduser(storage_dir), project))

    @classmethod
    def app_log_exists(
        cls,
        project: str,
        app_id: str,
        storage_dir: str = DEFAULT_STORAGE_DIR,
    ) -> bool:
        """Function to check if state exists for a given project and app_id.

        :param project: the name of the project
        :param app_id: the application instance id
        :param storage_dir: the storage directory.
        :return: True if state exists, False otherwise.
        """
        path = os.path.join(cls.get_storage_path(project, storage_dir), app_id, cls.LOG_FILENAME)
        if not os.path.exists(path):
            return False
        lines = open(path, "r", errors="replace", encoding="utf-8").readlines()
        if len(lines) == 0:
            return False
        return True

    @classmethod
    def load_state(
        cls,
        project: str,
        app_id: str,
        sequence_id: int = -1,
        storage_dir: str = DEFAULT_STORAGE_DIR,
    ) -> tuple[dict, str]:
        """This is deprecated and will be removed when we migrate over demos. Do not use! Instead use
        the persistence API :py:class:`initialize_from <burr.core.application.ApplicationBuilder.initialize_from>`
        to load state.

        It defaults to loading the last state, but you can supply a sequence number.

        This is a temporary solution -- not particularly ergonomic, and makes assumptions (particularly that
        all logging is in order), which is fine for now. We will be improving this and making it a first-class
        citizen.


        :param project: the name of the project
        :param app_id: the application instance id
        :param sequence_id: the sequence number of the state to load. Defaults to last index (i.e. -1).
        :param storage_dir: the storage directory.
        :return: the state as a dictionary, and the entry point as a string.
        """
        if sequence_id is None:
            sequence_id = -1  # get the last one
        path = os.path.join(cls.get_storage_path(project, storage_dir), app_id, cls.LOG_FILENAME)
        if not os.path.exists(path):
            raise ValueError(f"No logs found for {project}/{app_id} under {storage_dir}")
        with open(path, "r", errors="replace", encoding="utf-8") as f:
            json_lines = f.readlines()
        # load as JSON
        json_lines = [json.loads(js_line) for js_line in json_lines]
        # filter to only end_entry
        json_lines = [js_line for js_line in json_lines if js_line["type"] == "end_entry"]
        try:
            line = json_lines[sequence_id]
        except IndexError:
            raise ValueError(f"Sequence number {sequence_id} not found for {project}/{app_id}.")
        # check sequence number matches if non-negative; will break if either is None.
        line_seq = int(line["sequence_id"])
        if -1 < sequence_id != line_seq:
            logger.warning(
                f"Sequence number mismatch. For {project}/{app_id}: "
                f"actual:{line_seq} != expected:{sequence_id}"
            )
        # get the prior state
        prior_state = line["state"]
        entry_point = line["action"]
        # delete internally stuff. We can't loop over the keys and delete them in the same loop
        to_delete = []
        for key in prior_state.keys():
            # remove any internal "__" state
            if key.startswith("__"):
                to_delete.append(key)
        for key in to_delete:
            del prior_state[key]
        prior_state["__SEQUENCE_ID"] = line_seq  # add the sequence id back
        return prior_state, entry_point

    def _ensure_dir_structure(self, app_id: str):
        if not os.path.exists(self.storage_dir):
            logger.info(f"Creating storage directory: {self.storage_dir}")
            os.makedirs(self.storage_dir)
        application_path = os.path.join(self.storage_dir, app_id)
        if not os.path.exists(application_path):
            logger.info(f"Creating application directory: {application_path}")
            os.makedirs(application_path)

    def __setstate__(self, state):
        self.__dict__.update(state)

    def __getstate__(self):
        out = {
            key: value for key, value in self.__dict__.items() if key != "f"
        }  # the file we don't want to serialize
        # Note that this will only work if we also call post_application_create
        # For now that's OK as that's the only reason we'll add it -- if we want more distribution later we'll have to serialize the file
        out["f"] = None
        return out

    def post_application_create(
        self,
        *,
        app_id: str,
        partition_key: Optional[str],
        state: "State",
        application_graph: "ApplicationGraph",
        parent_pointer: Optional[burr_types.ParentPointer],
        spawning_parent_pointer: Optional[burr_types.ParentPointer],
        **future_kwargs: Any,
    ):
        self._ensure_dir_structure(app_id)
        self.f = open(
            os.path.join(self.storage_dir, app_id, self.LOG_FILENAME),
            "a",
            encoding="utf-8",
            errors="replace",
        )

        graph_path = os.path.join(self.storage_dir, app_id, self.GRAPH_FILENAME)
        if os.path.exists(graph_path):
            logger.info(f"Graph already exists at {graph_path}. Not overwriting.")
            return
        graph = ApplicationModel.from_application_graph(
            application_graph,
        ).model_dump()
        with open(graph_path, "w", encoding="utf-8", errors="replace") as f:
            json.dump(graph, f)

        metadata_path = os.path.join(self.storage_dir, app_id, self.METADATA_FILENAME)
        if os.path.exists(metadata_path):
            logger.info(f"Metadata already exists at {metadata_path}. Not overwriting.")
            return
        metadata = ApplicationMetadataModel(
            partition_key=partition_key,
            parent_pointer=PointerModel.from_pointer(parent_pointer),
            spawning_parent_pointer=PointerModel.from_pointer(spawning_parent_pointer),
        ).model_dump()
        with open(metadata_path, "w", errors="replace", encoding="utf-8") as f:
            json.dump(metadata, f)

        # Append to the parents of this the pointer to this, now
        self._log_child_relationships(
            parent_pointer,
            spawning_parent_pointer,
            app_id,
        )

    def _append_write_line(self, model: pydantic.BaseModel):
        self.f.write(model.model_dump_json() + "\n")
        self.f.flush()

    def pre_run_step(
        self,
        state: State,
        action: Action,
        inputs: Dict[str, Any],
        sequence_id: int,
        **future_kwargs: Any,
    ):
        _filtered_inputs = _filter_inputs(inputs)
        pre_run_entry = BeginEntryModel(
            start_time=datetime.datetime.now(),
            action=action.name,
            inputs=serde.serialize(_filtered_inputs, **self.serde_kwargs),
            sequence_id=sequence_id,
        )
        self._append_write_line(pre_run_entry)

    def post_run_step(
        self,
        state: State,
        action: Action,
        result: Optional[dict],
        sequence_id: int,
        exception: Exception,
        **future_kwargs: Any,
    ):
        post_run_entry = EndEntryModel(
            end_time=datetime.datetime.now(),
            action=action.name,
            result=serde.serialize(result, **self.serde_kwargs),
            sequence_id=sequence_id,
            exception=_format_exception(exception),
            state=state.serialize(**self.serde_kwargs),
        )
        self._append_write_line(post_run_entry)

    def pre_start_span(
        self,
        *,
        action: str,
        action_sequence_id: int,
        span: ActionSpan,
        span_dependencies: list[str],
        **future_kwargs: Any,
    ):
        begin_span_model = BeginSpanModel(
            start_time=datetime.datetime.now(),
            action_sequence_id=action_sequence_id,
            span_id=span.uid,
            parent_span_id=span.parent.uid if span.parent else None,
            span_dependencies=span_dependencies,
            span_name=span.name,
        )
        self._append_write_line(begin_span_model)

    def post_end_span(
        self,
        *,
        action: str,
        action_sequence_id: int,
        span: ActionSpan,
        span_dependencies: list[str],
        **future_kwargs: Any,
    ):
        end_span_model = EndSpanModel(
            end_time=datetime.datetime.now(),
            action_sequence_id=action_sequence_id,
            span_id=span.uid,
        )
        self._append_write_line(end_span_model)

    def do_log_attributes(
        self,
        *,
        attributes: Dict[str, Any],
        tags: dict,
        action: str,
        action_sequence_id: int,
        span: Optional["ActionSpan"],
        **future_kwargs: Any,
    ):
        for attribute_name, attribute in attributes.items():
            attribute_model = AttributeModel(
                key=attribute_name,
                action_sequence_id=action_sequence_id,
                span_id=span.uid if span is not None else None,
                value=serde.serialize(attribute, **self.serde_kwargs),
                tags=tags,
                time_logged=system.now(),
            )
            self._append_write_line(attribute_model)

    def pre_start_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        self._append_write_line(
            InitializeStreamModel(
                action_sequence_id=sequence_id,
                span_id=None,
                stream_init_time=system.now(),
            )
        )
        self.stream_state[app_id, action, partition_key] = StreamState(
            stream_init_time=system.now(),
            count=0,
        )

    def post_stream_item(
        self,
        *,
        item: Any,
        item_index: int,
        stream_initialize_time: datetime.datetime,
        first_stream_item_start_time: datetime.datetime,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        stream_state = self.stream_state[app_id, action, partition_key]
        if stream_state.count == 0:
            stream_state.count += 1
            self._append_write_line(
                FirstItemStreamModel(
                    action_sequence_id=sequence_id,
                    span_id=None,
                    first_item_time=system.now(),
                )
            )
        else:
            stream_state.count += 1

    def post_end_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        stream_state = self.stream_state[app_id, action, partition_key]
        self._append_write_line(
            EndStreamModel(
                action_sequence_id=sequence_id,
                span_id=None,
                end_time=system.now(),
                items_streamed=stream_state.count,
            )
        )
        del self.stream_state[app_id, action, partition_key]

    def __del__(self):
        if self.f is not None:
            self.f.close()

    def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        # TODO:
        return []

    def load(
        self, partition_key: str, app_id: Optional[str], sequence_id: Optional[int] = None, **kwargs
    ) -> Optional[PersistedStateData]:
        # TODO:
        if app_id is None:
            return  # no application ID
        path = os.path.join(self.storage_dir, app_id, self.LOG_FILENAME)
        if not os.path.exists(path):
            return None
        with open(path, "r", errors="replace", encoding="utf-8") as f:
            json_lines = f.readlines()
        if len(json_lines) == 0:
            return None  # in this case we have not logged anything yet
        # load as JSON
        json_lines = [json.loads(js_line) for js_line in json_lines]
        # filter to only end_entry
        line = None
        if sequence_id is None:
            # get the last one, we want to start at the end
            for _line in reversed(json_lines):
                if _line["type"] == "end_entry":
                    sequence_id = _line["sequence_id"]
                    line = _line
                    break
        else:
            for js_line in json_lines:
                if js_line["type"] == "end_entry":
                    if js_line["sequence_id"] == sequence_id:
                        line = js_line

        if line is None:
            raise ValueError(
                f"Sequence number {sequence_id} not found for {self.project_id}/{app_id}."
            )
        prior_state = line["state"]
        position = line["action"]
        # delete internally stuff. We can't loop over the keys and delete them in the same loop
        to_delete = []
        for key in prior_state.keys():
            # remove any internal "__" state
            if key.startswith("__"):
                to_delete.append(key)
        for key in to_delete:
            del prior_state[key]
        prior_state["__SEQUENCE_ID"] = sequence_id  # add the sequence id back
        return PersistedStateData(
            partition_key=partition_key,
            app_id=app_id,
            sequence_id=sequence_id,
            position=position,
            state=State.deserialize(prior_state, **self.serde_kwargs),
            created_at=datetime.datetime.fromtimestamp(os.path.getctime(path)).isoformat(),
            status="completed" if line["exception"] is None else "failed",
        )


# TODO -- implement async version
# class AsyncTrackingClient(PreRunStepHookAsync, PostRunStepHookAsync, PostApplicationCreateHook):
#     def post_application_create(self, *, state: State, state_graph: ApplicationGraph, **future_kwargs: Any):
#         pass
#
#     async def pre_run_step(self, *, state: State, action: Action, **future_kwargs: Any):
#         raise NotImplementedError(f"TODO: {self.__class__.__name__}.pre_run_step")
#
#     async def post_run_step(self, *, state: State, action: Action, result: Optional[dict], exception: Exception, **future_kwargs: Any):
#         raise NotImplementedError(f"TODO: {self.__class__.__name__}.pre_run_step")
#



---
File: /burr/burr/tracking/s3client.py
---

import dataclasses
import datetime
import json
import logging
import queue
import re
import threading
import time
import traceback
import uuid
from typing import Any, Dict, List, Optional, Tuple, Union

import pydantic

from burr import system
from burr.common import types as burr_types
from burr.core import Action, ApplicationGraph, State, serde
from burr.integrations.base import require_plugin
from burr.tracking.base import SyncTrackingClient
from burr.tracking.client import StateKey, StreamState
from burr.tracking.common.models import (
    ApplicationMetadataModel,
    ApplicationModel,
    AttributeModel,
    BeginEntryModel,
    BeginSpanModel,
    EndEntryModel,
    EndSpanModel,
    EndStreamModel,
    FirstItemStreamModel,
    InitializeStreamModel,
    PointerModel,
)
from burr.visibility import ActionSpan

logger = logging.getLogger(__name__)

try:
    import boto3
except ImportError as e:
    require_plugin(
        e,
        "tracking-s3",
    )


def fire_and_forget(func):
    def wrapper(self, *args, **kwargs):
        if self.non_blocking:  # must be used with the S3TrackingClient

            def run():
                try:
                    func(self, *args, **kwargs)
                except Exception:
                    logger.exception(
                        "Exception occurred in fire-and-forget function: %s", func.__name__
                    )

            threading.Thread(target=run).start()
        return func(self, *args, **kwargs)

    return wrapper


# TODO -- move to common and share with client.py

INPUT_FILTERLIST = {"__tracer"}


def _format_exception(exception: Exception) -> Optional[str]:
    if exception is None:
        return None
    return "".join(traceback.format_exception(type(exception), exception, exception.__traceback__))


INPUT_FILTERLIST = {"__tracer"}


def _filter_inputs(d: dict) -> dict:
    return {k: v for k, v in d.items() if k not in INPUT_FILTERLIST}


def _allowed_project_name(project_name: str, on_windows: bool) -> bool:
    allowed_chars = "a-zA-Z0-9_\-"
    if not on_windows:
        allowed_chars += ":"
    pattern = f"^[{allowed_chars}]+$"

    # Use regular expression to check if the string is valid
    return bool(re.match(pattern, project_name))


EventType = Union[
    BeginEntryModel,
    EndEntryModel,
    BeginSpanModel,
    EndSpanModel,
    AttributeModel,
    InitializeStreamModel,
    FirstItemStreamModel,
    EndStreamModel,
]


def unique_ordered_prefix() -> str:
    return datetime.datetime.now().isoformat() + str(uuid.uuid4())


def str_partition_key(partition_key: Optional[str]) -> str:
    return partition_key or "__none__"


class S3TrackingClient(SyncTrackingClient):
    """Synchronous tracking client that logs to S3. Experimental. Errs on the side of writing many little files.
    General schema is:
    - bucket
        - data/
            - project_name_1
                - YYYY/MM/DD/HH
                    - application.json (optional, will be on the first write from this tracker object)
                    - metadata.json (optional, will be on the first write from this tracker object)
                    - log_<timestamp>.jsonl
                    - log_<timestamp>.jsonl
                - YYYY/MM/DD/HH
                    ...
            ...

    This is designed to be fast to write, generally slow(ish) to read, but doable, and require no db.
    This also has a non-blocking mode that just launches a thread (expensive but doable solution)
    TODO -- get working with aiobotocore and an async tracker
    """

    def do_log_attributes(
        self,
        *,
        attributes: Dict[str, Any],
        action: str,
        action_sequence_id: int,
        span: Optional["ActionSpan"],
        app_id: str,
        partition_key: Optional[str],
        tags: dict,
        **future_kwargs: Any,
    ):
        # TODO -- log attributes to s3 as well
        # Coming up shortly
        for attribute_name, attribute in attributes.items():
            attribute_model = AttributeModel(
                key=attribute_name,
                action_sequence_id=action_sequence_id,
                span_id=span.uid if span is not None else None,
                value=serde.serialize(attribute, **self.serde_kwargs),
                tags=tags,
                time_logged=system.now(),
            )
            self.submit_log_event(attribute_model, app_id=app_id, partition_key=partition_key)

    def __init__(
        self,
        project: str,
        bucket: str,
        region: str = None,
        endpoint_url: Optional[str] = None,
        non_blocking: bool = False,
        serde_kwargs: Optional[dict] = None,
        unique_tracker_id: str = None,
        flush_interval: int = 5,
    ):
        self.bucket = bucket
        self.project = project
        self.region = region
        self.endpoint_url = endpoint_url
        self.non_blocking = non_blocking
        self.s3 = boto3.client("s3", region_name=region, endpoint_url=endpoint_url)
        self.serde_kwargs = serde_kwargs or {}
        self.unique_tracker_id = (
            unique_tracker_id
            if unique_tracker_id is not None
            else datetime.datetime.now().isoformat() + "-" + str(uuid.uuid4())
        )
        self.log_queue = queue.Queue()  # Tuple[app_id, EventType]
        self.flush_interval = flush_interval
        self.max_batch_size = 10000  # rather large batch size -- why not? It'll flush every 5 seconds otherwise and we don't want to spam s3 with files
        self.initialized = False
        self.running = True
        self.init()
        self.stream_state: Dict[StateKey, StreamState] = dict()

    def _get_time_partition(self):
        time = datetime.datetime.utcnow().isoformat()
        return [time[:4], time[5:7], time[8:10], time[11:13], time[14:]]

    def get_prefix(self):
        return [
            "data",
            self.project,
            *self._get_time_partition(),
        ]

    def init(self):
        if not self.initialized:
            logger.info("Initializing S3TrackingClient with flushing thread")
            thread = threading.Thread(target=self.thread)
            # This will quit when the main thread is ready to, and gracefully
            # But it will gracefully exit due to the "None" on the queue
            threading.Thread(
                target=lambda: threading.main_thread().join() or self.log_queue.put(None)
            ).start()
            thread.start()
            self.initialized = True

    def thread(self):
        batch = []
        last_flush_time = time.time()

        while self.running:
            try:
                logger.debug(f"Checking for new data to flush -- s3 batch is of size: {len(batch)}")
                # Wait up to flush_interval for new data
                item = self.log_queue.get(timeout=self.flush_interval)
                # signal that we're done
                if item is None:
                    self.log_events(batch)
                    self.running = False
                    break
                batch.append(item)
                # Check if batch is full or flush interval has passed
                if (
                    len(batch) >= self.max_batch_size
                    or (time.time() - last_flush_time) >= self.flush_interval
                ):
                    logger.debug(f"Flushing s3 batch with {len(batch)} events")
                    self.log_events(batch)
                    batch = []
                    last_flush_time = time.time()
            except queue.Empty:
                # Flush on timeout if there's any data
                if batch:
                    logger.debug(f"Flushing s3 batch on queue empty with {len(batch)} events")
                    self.log_events(batch)
                    batch = []
                    last_flush_time = time.time()

    def stop(self, thread: threading.Thread):
        self.running = False  # will stop the thread
        thread.join()  # then wait for it to be done
        events = []
        # Flush any remaining events
        while self.log_queue.qsize() > 0:
            events.append(self.log_queue.get())
        self.log_events(events)

    def submit_log_event(self, event: EventType, app_id: str, partition_key: str):
        self.log_queue.put((app_id, partition_key, event))

    def log_events(self, events: List[Tuple[str, EventType]]):
        events_by_app_id = {}
        for app_id, partition_key, event in events:
            if (app_id, partition_key) not in events_by_app_id:
                events_by_app_id[(app_id, partition_key)] = []
            events_by_app_id[(app_id, partition_key)].append(event)
        for (app_id, partition_key), app_events in events_by_app_id.items():
            logger.debug(f"Logging {len(app_events)} events for app {app_id}")
            min_sequence_id = min([e.sequence_id for e in app_events])
            max_sequence_id = max([e.sequence_id for e in app_events])
            path = [
                str_partition_key(partition_key),
                app_id,
                str(uuid.uuid4())
                + "__log.jsonl",  # in case we happen to have multiple at the same time....
            ]
            self.log_object(
                *path,
                data=app_events,
                metadata={
                    "min_sequence_id": str(min_sequence_id),
                    "max_sequence_id": str(max_sequence_id),
                },
            )

    def log_object(
        self,
        *path_within_project: str,
        data: Union[pydantic.BaseModel, List[pydantic.BaseModel]],
        metadata: Dict[str, str] = None,
    ):
        if metadata is None:
            metadata = {}
        metadata = {**metadata, "tracker_id": self.unique_tracker_id}
        full_path = self.get_prefix() + list(path_within_project)
        key = "/".join(full_path)
        if isinstance(data, list):
            body = "\n".join([d.model_dump_json() for d in data])
        else:
            body = data.model_dump_json()
        self.s3.put_object(Bucket=self.bucket, Key=key, Body=body, Metadata=metadata)

    @fire_and_forget
    def post_application_create(
        self,
        *,
        app_id: str,
        partition_key: Optional[str],
        state: "State",
        application_graph: "ApplicationGraph",
        parent_pointer: Optional[burr_types.ParentPointer],
        spawning_parent_pointer: Optional[burr_types.ParentPointer],
        **future_kwargs: Any,
    ):
        graph = ApplicationModel.from_application_graph(
            application_graph,
        )
        graph_path = [str_partition_key(partition_key), app_id, "graph.json"]
        self.log_object(*graph_path, data=graph)
        metadata = ApplicationMetadataModel(
            partition_key=partition_key,
            parent_pointer=PointerModel.from_pointer(parent_pointer),
            spawning_parent_pointer=PointerModel.from_pointer(spawning_parent_pointer),
        )
        metadata_path = [str_partition_key(partition_key), app_id, "metadata.json"]
        # we put these here to allow for quicker retrieval on the server side
        # It's a bit of a hack to put it all into metadata, but it helps with ingestion
        self.log_object(
            *metadata_path,
            data=metadata,
            metadata={
                "parent_pointer": json.dumps(dataclasses.asdict(parent_pointer))
                if parent_pointer is not None
                else "None",
                "spawning_parent_pointer": json.dumps(dataclasses.asdict(spawning_parent_pointer))
                if spawning_parent_pointer is not None
                else "None",
            },
        )

    def pre_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        inputs: Dict[str, Any],
        **future_kwargs: Any,
    ):
        _filtered_inputs = _filter_inputs(inputs)
        pre_run_entry = BeginEntryModel(
            start_time=datetime.datetime.now(),
            action=action.name,
            inputs=serde.serialize(_filtered_inputs, **self.serde_kwargs),
            sequence_id=sequence_id,
        )
        self.submit_log_event(pre_run_entry, app_id, partition_key)

    def post_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        result: Optional[Dict[str, Any]],
        exception: Exception,
        **future_kwargs: Any,
    ):
        post_run_entry = EndEntryModel(
            end_time=datetime.datetime.now(),
            action=action.name,
            result=serde.serialize(result, **self.serde_kwargs),
            sequence_id=sequence_id,
            exception=_format_exception(exception),
            state=state.serialize(),
        )
        self.submit_log_event(post_run_entry, app_id, partition_key)

    def pre_start_span(
        self,
        *,
        action_sequence_id: int,
        partition_key: str,
        app_id: str,
        span: ActionSpan,
        span_dependencies: list[str],
        **future_kwargs: Any,
    ):
        begin_span_model = BeginSpanModel(
            start_time=datetime.datetime.now(),
            action_sequence_id=action_sequence_id,
            span_id=span.uid,
            parent_span_id=span.parent.uid if span.parent else None,
            span_dependencies=span_dependencies,
            span_name=span.name,
        )
        self.submit_log_event(begin_span_model, app_id, partition_key)

    def post_end_span(
        self,
        *,
        action: str,
        action_sequence_id: int,
        span: ActionSpan,
        span_dependencies: list[str],
        app_id: str,
        partition_key: str,
        **future_kwargs: Any,
    ):  # TODO -- implemenet
        end_span_model = EndSpanModel(
            end_time=datetime.datetime.now(),
            action_sequence_id=action_sequence_id,
            span_id=span.uid,
        )
        self.submit_log_event(end_span_model, app_id, partition_key)

    def copy(self):
        return S3TrackingClient(
            project=self.project,
            bucket=self.bucket,
            region=self.region,
            endpoint_url=self.endpoint_url,
            non_blocking=self.non_blocking,
            serde_kwargs=self.serde_kwargs,
            unique_tracker_id=self.unique_tracker_id,
            flush_interval=self.flush_interval,
        )

    def pre_start_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        initialize_stream_model = InitializeStreamModel(
            action_sequence_id=sequence_id,
            span_id=None,
            stream_init_time=system.now(),
        )
        self.submit_log_event(initialize_stream_model, app_id, partition_key)
        self.stream_state[app_id, action, partition_key] = StreamState(
            stream_init_time=system.now(),
            count=0,
        )

    def post_stream_item(
        self,
        *,
        item: Any,
        item_index: int,
        stream_initialize_time: datetime.datetime,
        first_stream_item_start_time: datetime.datetime,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        stream_state = self.stream_state[app_id, action, partition_key]
        if stream_state.count == 0:
            stream_state.count += 1
            self.submit_log_event(
                FirstItemStreamModel(
                    action_sequence_id=sequence_id,
                    span_id=None,
                    first_item_time=system.now(),
                ),
                app_id,
                partition_key,
            )
        else:
            stream_state.count += 1

    def post_end_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        stream_state = self.stream_state[app_id, action, partition_key]
        self.submit_log_event(
            EndStreamModel(
                action_sequence_id=sequence_id,
                span_id=None,
                end_time=system.now(),
                items_streamed=stream_state.count,
            ),
            app_id,
            partition_key,
        )
        del self.stream_state[app_id, action, partition_key]



---
File: /burr/burr/tracking/utils.py
---

import json


def safe_json_load(line: bytes):
    # Every once in a while we'll hit a non-utf-8 character
    # In this case we replace it and hope for the best
    return json.loads(line.decode("utf-8", errors="replace"))



---
File: /burr/burr/visibility/__init__.py
---

from burr.visibility.tracing import ActionSpan, ActionSpanTracer, TracerFactory, trace

__all__ = ["TracerFactory", "ActionSpan", "ActionSpanTracer", "trace"]



---
File: /burr/burr/visibility/tracing.py
---

import dataclasses
import functools
import inspect
import logging
import uuid
from contextlib import AbstractAsyncContextManager, AbstractContextManager
from contextvars import ContextVar
from hashlib import sha256
from typing import Any, Callable, Dict, List, Optional, Type, TypeVar

from burr.lifecycle.internal import LifecycleAdapterSet

logger = logging.getLogger(__name__)


@dataclasses.dataclass
class ActionSpan:
    action: str
    action_sequence_id: int
    name: str
    parent: Optional["ActionSpan"]
    sequence_id: int = 0
    child_count: int = 0

    def spawn(self, name: str) -> "ActionSpan":
        self.child_count += 1
        return ActionSpan(
            action=self.action,
            action_sequence_id=self.action_sequence_id,
            name=name,
            parent=self,
            sequence_id=self.child_count - 1,  # sequence id is 0 indexed
            child_count=0,  # start the child count at 0
        )

    @property
    def uid(self) -> str:
        action_sequence_id = self.action_sequence_id
        parent_ids = [self.sequence_id]
        current = self
        while (current := current.parent) is not None:
            parent_ids.append(current.sequence_id)

        return f"{action_sequence_id}:{'.'.join(reversed([str(x) for x in parent_ids]))}"

    @classmethod
    def create_initial(
        cls, action: str, name: str, sequence_id: int, action_sequence_id: int
    ) -> "ActionSpan":
        """Creates the initial action span for an action. This should be only called if the current
        action span is none.

        :param action:
        :param name:
        :param sequence_id:
        :return:
        """
        return ActionSpan(
            action=action,
            name=name,
            sequence_id=sequence_id,
            parent=None,
            action_sequence_id=action_sequence_id,
        )


execution_context_var: ContextVar[Optional[ActionSpan]] = ContextVar(
    "execution_context",
    default=None,
)


def create_span_id() -> str:
    """Creates a unique

    :return:
    """
    return f"span_{str(uuid.uuid4())}"


class ActionSpanTracer(AbstractContextManager, AbstractAsyncContextManager):
    """Context manager for use within tracing actions. This has the role solely of delegating
    to hooks -- it does not do anything except manage context and pass those to the hooks.

    Note that a new instance of this will be passed to every action that is traced. This allows
    us to reset this based on state. This can handle both synchronous and asynchronous contexts.

    You will be using this through the action API. When you declare ``__tracer`` as a parameter,
    it gives you a callable that, when called with a span name, returns an `ActionSpanTracer`

    .. code-block:: python

        @action(reads=[...], writes=[...])
        def my_action(state: State, __tracer: TracerFactory) -> tuple[dict, State]:
            context_manager: ActionSpanTracer = __tracer("my_span_name")
            with context_manager:
                ...


    The following hooks are respected:

    - :py:class:`pre_span_start <burr.lifecycle.base.PreSpanStart>` and :py:class:`async pre_span_start <burr.lifecycle.base.PreSpanStartAsync>`
    - :py:class:`post_span_end <burr.lifecycle.base.PostSpanEnd>` and :py:class:`async post_span_end <burr.lifecycle.base.PostSpanEndAsync>`
    """

    def __init__(
        self,
        action: str,
        action_sequence_id: int,
        span_name: str,
        lifecycle_adapters: LifecycleAdapterSet,
        app_id: str,
        partition_key: Optional[str],
        span_dependencies: List[str],
        top_level_span_count: int = 0,
        context_var=execution_context_var,
    ):
        """Initialiezs the ActionSpanTracer.

        :param action:
        :param span_name:
        :param lifecycle_adapters:
        :param span_dependencies:
        :param top_level_span_count:
        """
        self.action = action
        self.action_sequence_id = action_sequence_id
        self.lifecycle_adapters = lifecycle_adapters
        self.span_name = span_name
        self.span_dependencies = span_dependencies
        self.top_level_span_count = top_level_span_count
        self.context_var = context_var
        self.app_id = app_id
        self.partition_key = partition_key

    def _sync_hooks_enter(self, context: ActionSpan):
        self.lifecycle_adapters.call_all_lifecycle_hooks_sync(
            "pre_start_span",
            action=self.action,
            span=context,
            span_dependencies=self.span_dependencies,
            action_sequence_id=self.action_sequence_id,
            app_id=self.app_id,
            partition_key=self.partition_key,
        )

    async def _async_hooks_enter(self, context: ActionSpan):
        await self.lifecycle_adapters.call_all_lifecycle_hooks_async(
            "pre_start_span",
            action=self.action,
            span=context,
            span_dependencies=self.span_dependencies,
            action_sequence_id=self.action_sequence_id,
            app_id=self.app_id,
            partition_key=self.partition_key,
        )

    async def _async_hooks_exit(self, context: ActionSpan):
        await self.lifecycle_adapters.call_all_lifecycle_hooks_async(
            "post_end_span",
            action=self.action,
            span=context,
            span_dependencies=self.span_dependencies,
            action_sequence_id=self.action_sequence_id,
            app_id=self.app_id,
            partition_key=self.partition_key,
        )

    def _enter(self):
        current_execution_context = self.context_var.get()
        if current_execution_context is None:
            # create an initial one if we're at the top level
            self.context_var.set(
                ActionSpan.create_initial(
                    self.action,
                    self.span_name,
                    self.top_level_span_count - 1,
                    action_sequence_id=self.action_sequence_id,
                )
            )
        else:
            self.context_var.set(current_execution_context.spawn(self.span_name))
        return self.context_var.get()

    def _exit(self):
        current_execution_context = self.context_var.get()
        self.context_var.set(current_execution_context.parent)
        return current_execution_context

    def _sync_hooks_exit(self, context: ActionSpan):
        self.lifecycle_adapters.call_all_lifecycle_hooks_sync(
            "post_end_span",
            action=self.action,
            span=context,
            span_dependencies=self.span_dependencies,
            action_sequence_id=self.action_sequence_id,
            app_id=self.app_id,
            partition_key=self.partition_key,
        )

    def __enter__(self):
        """Enters the context manager"""
        enter_context = self._enter()
        self._sync_hooks_enter(enter_context)
        return self

    def __exit__(
        self,
        __exc_type: Type[BaseException],
        __exc_value: Optional[Type[BaseException]],
        __traceback: Optional[Type[BaseException]],
    ) -> Optional[bool]:
        """Exits the context manager."""
        prior_execution_context = self._exit()
        self._sync_hooks_exit(prior_execution_context)
        return None

    def log_attribute(self, key: str, value: Any):
        """Logs a single attribute to the UI. Note that this must
        be paired with a tracker or a tracking hook to be useful, otherwise it
        will be a no-op.

        :param key: Name of the attribute (must be unique per action/span)
        :param value: Value of the attribute.
        """
        self.log_attributes(**{key: value})

    def log_attributes(self, **attributes):
        """Logs a set of attributes to the UI. Note that this must
        be paired with a tracker or a tracking hook to be useful, otherwise it
        will be a no-op.

        :param attributes: Attributes to log
        """
        self.lifecycle_adapters.call_all_lifecycle_hooks_sync(
            "do_log_attributes",
            attributes=attributes,
            action=self.action,
            action_sequence_id=self.action_sequence_id,
            span=self.context_var.get(),
            app_id=self.app_id,
            partition_key=self.partition_key,
            tags={},  # TODO -- add tags
        )

    async def __aenter__(self) -> "ActionSpanTracer":
        """Enters the context manager, async mode"""
        enter_context = self._enter()
        self._sync_hooks_enter(enter_context)
        await self._async_hooks_enter(self.context_var.get())
        return self

    async def __aexit__(
        self,
        __exc_type: Type[BaseException],
        __exc_value: Optional[Type[BaseException]],
        __traceback: Optional[Type[BaseException]],
    ) -> Optional[bool]:
        """Exits the context manager, async mode"""
        prior_execution_context = self._exit()
        self._sync_hooks_exit(prior_execution_context)
        await self._async_hooks_exit(prior_execution_context)
        return None


class TracerFactory(ActionSpanTracer):
    """Represents a tracer factory to create tracer instances. User never instantiates this
    directly. Rather, this gets injected by the application. This gives a new span.

    Note this carries state -- the top level span count. This is important for the sequence id
    at the root level.

    You will only ever see a tracer factory in the context of an action, passed through the `__tracer`
    parameter.

     .. code-block:: python

        @action(reads=[...], writes=[...])
        def my_action(state: State, __tracer: TracerFactory) -> tuple[dict, State]:
            context_manager: ActionSpanTracer = __tracer("my_span_name")
            with context_manager:
                ...
    """

    def __init__(
        self,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        lifecycle_adapters: LifecycleAdapterSet,
        _context_var: ContextVar[Optional[ActionSpan]] = execution_context_var,
    ):
        """Instantiates a tracer factory.

        :param action: Action name
        :param lifecycle_adapters: Lifecycle adapters
        :param _context_var: Context var to use for the action span
        """
        super(TracerFactory, self).__init__(
            action=action,
            action_sequence_id=sequence_id,
            span_name="root",
            lifecycle_adapters=lifecycle_adapters,
            app_id=app_id,
            partition_key=partition_key,
            span_dependencies=[],
            top_level_span_count=0,
            context_var=_context_var,
        )

    def __call__(
        self, span_name: str, span_dependencies: Optional[List[str]] = None
    ) -> ActionSpanTracer:
        if self.context_var.get() is None:
            self.top_level_span_count += 1
        if span_dependencies is None:
            span_dependencies = []
        return ActionSpanTracer(
            action=self.action,
            action_sequence_id=self.action_sequence_id,
            span_name=span_name,
            lifecycle_adapters=self.lifecycle_adapters,
            span_dependencies=span_dependencies,
            context_var=self.context_var,
            top_level_span_count=self.top_level_span_count,
            app_id=self.app_id,
            partition_key=self.partition_key,
        )


FNType = TypeVar("FNType", bound=Callable)

# This is specifically meant to set anything that is "out of band" -- I.E.
# Through a decorator that does not have access to the parameter
# This is an internal API that we are liable to change over time
# This is set by the application on every `step` call

tracer_factory_context_var: ContextVar[Optional[TracerFactory]] = ContextVar(
    "tracer_context",
    default=None,
)


class trace:
    def __init__(
        self,
        capture_inputs: bool = True,
        capture_outputs: bool = True,
        input_filterlist: Optional[List[str]] = None,
        span_name: Optional[str] = None,
        _context_var: ContextVar[Optional[TracerFactory]] = tracer_factory_context_var,
    ):
        """trace() can wrap any function and uses the tracer to create a span and log
        attributes. This also (by default) logs the inputs/outputs of a function as attributes.
        Be careful not to include sensitive data in the inputs/outputs, but if you do, you have the
        input_filterlist to exclude it.

        This works with sync/async

        Take the following code:

        .. code-block:: python

            from burr.visibility import trace

            @trace()
            def call_llm(prompt):
                return _query(...)

            @trace()
            def generate_text(prompt: str) -> str:
                result = call_llm(prompt)
                return f"<p>{result}</p>"

            @action(reads=["prompt"], writes=["response"])
            def prompt_action(state: State) -> State:
                response = generate_text(state["prompt"])
                return state.update(response=response)

        Every time `prompt_action` is called (within the context of prompt_action), it will generate a trace that looks like the following:

        ------ prompt action --------------------------------------------
            ----- generate_text -----------------------------------
                ----- call_llm -----------------------------------

        If it is called outside the context of a Burr action, it will be effectively a no-op.


        :param capture_inputs: Whether to capture inputs as attributes (defaults to True).
            Note that this only works with keyword-argument bindable functions.
        :param capture_outputs: Whether to capture outputs as attributes (defaults to True)
        :param input_filterlist: A list of inputs to filter out (defaults to filtering nothing. Use if you have sensitive data)
        :param span_name: Name of the span, will default to the function name
        :param _context_var: Context var to use for the tracer factory, used purely for internal testing
        """
        self.capture_inputs = capture_inputs
        self.capture_outputs = capture_outputs
        self.input_filterlist = input_filterlist or []
        self.span_name = span_name
        self.context_var = _context_var

    def _ensure_bind(self, fn, *args, **kwargs) -> Dict[str, Any]:
        try:
            bound_params = inspect.signature(fn).bind(*args, **kwargs)
        except TypeError as e:
            func_name = fn.__name__
            raise TypeError(f"{func_name}: {e.args[0]}") from None

        bound_kwargs = bound_params.arguments
        return bound_kwargs

    def _get_auto_attributes(self, fn: FNType):
        qualname = fn.__qualname__
        try:
            code, *_ = inspect.getsourcelines(fn)
            code_hash = sha256("".join(code).encode()).hexdigest()
        except OSError:
            code_hash = "Unable to retrieve source code for hash"
        return {"__burr_function": qualname, "__burr_function_hash": code_hash}

    def _log_parameters(self, __action_span_tracer: ActionSpanTracer, bound_aprams):
        # TODO -- ensure we can serialize this
        filtered_params = {k: v for k, v in bound_aprams.items() if k not in self.input_filterlist}
        __action_span_tracer.log_attributes(**filtered_params)

    def _get_span_name(self, fn: FNType) -> str:
        return self.span_name if self.span_name else fn.__name__

    def __call__(self, fn: FNType) -> FNType:
        @functools.wraps(fn)
        def wrapped_fn(*args, **kwargs):
            tracer_factory = self.context_var.get()
            if tracer_factory is not None:
                bound_params = self._ensure_bind(fn, *args, **kwargs)
                with tracer_factory(self._get_span_name(fn)) as action_span_tracer:
                    if self.capture_inputs:
                        self._log_parameters(action_span_tracer, bound_params)
                    additional_attributes = self._get_auto_attributes(fn)
                    action_span_tracer.log_attributes(**additional_attributes)
                    output = fn(*args, **kwargs)
                    if self.capture_outputs:
                        action_span_tracer.log_attributes(**{"return": output})
                    return output
            logger.debug(
                f"Function: {fn.__name__} is decorated with @trace but is not being executed "
                f"in the context of a Burr action. No tracing will occur."
            )
            return fn(*args, **kwargs)

        @functools.wraps(fn)
        async def awrapped_fn(*args, **kwargs):
            tracer_factory = self.context_var.get()
            if tracer_factory is not None:
                bound_params = self._ensure_bind(fn, *args, **kwargs)
                with tracer_factory(self._get_span_name(fn)) as action_span_tracer:
                    if self.capture_inputs:
                        self._log_parameters(action_span_tracer, bound_aprams=bound_params)
                    additional_attributes = self._get_auto_attributes(fn)
                    action_span_tracer.log_attributes(**additional_attributes)
                    output = await fn(*args, **kwargs)
                    if self.capture_outputs:
                        action_span_tracer.log_attributes(**{"return": output})
                    return output
            logger.debug(
                f"Function: {fn.__name__} is decorated with @trace but is not being executed "
                f"in the context of a Burr action. No tracing will occur."
            )
            return await fn(*args, **kwargs)

        if inspect.iscoroutinefunction(fn):
            return awrapped_fn
        return wrapped_fn



---
File: /burr/burr/__init__.py
---




---
File: /burr/burr/log_setup.py
---

import logging
import sys

LOG_LEVELS = {
    "CRITICAL": logging.CRITICAL,
    "ERROR": logging.ERROR,
    "WARNING": logging.WARNING,
    "INFO": logging.INFO,
    "DEBUG": logging.DEBUG,
}


# this is suboptimal but python has no public mapping of log names to levels


def setup_logging(log_level: int = logging.INFO):
    """Helper function to setup logging to console.
    :param log_level: Log level to use when logging
    """
    root_logger = logging.getLogger("")  # root logger
    formatter = logging.Formatter("[%(levelname)s] %(asctime)s %(name)s(%(lineno)s): %(message)s")
    stream_handler = logging.StreamHandler(sys.stdout)
    stream_handler.setFormatter(formatter)
    if not len(root_logger.handlers):
        # assumes we have already been set up.
        root_logger.addHandler(stream_handler)
        root_logger.setLevel(log_level)



---
File: /burr/burr/system.py
---

import datetime
import os
import sys

IS_WINDOWS = os.name == "nt"

if sys.version_info >= (3, 11):
    utc = datetime.UTC
else:
    utc = datetime.timezone.utc


def now():
    return datetime.datetime.now(utc)



---
File: /burr/burr/telemetry.py
---

"""
This module contains code that relates to sending Burr usage telemetry.

To disable sending telemetry there are three ways:

1. Set it to false programmatically in your driver:
  >>> from burr import telemetry
  >>> telemetry.disable_telemetry()
2. Set it to `false` in ~/.burr.conf under `DEFAULT`
  [DEFAULT]
  telemetry_enabled = False
3. Set BURR_TELEMETRY_ENABLED=false as an environment variable:
  BURR_TELEMETRY_ENABLED=false python run.py
  or:
  export BURR_TELEMETRY_ENABLED=false
"""

import configparser
import functools
import importlib.metadata
import json
import logging
import os
import platform
import threading
import uuid
from typing import TYPE_CHECKING, Callable, List, TypeVar
from urllib import request

if TYPE_CHECKING:
    from burr.lifecycle import internal

VERSION = importlib.metadata.version("burr")

logger = logging.getLogger(__name__)

STR_VERSION = ".".join([str(i) for i in VERSION])
HOST = "https://app.posthog.com"
TRACK_URL = f"{HOST}/capture/"  # https://posthog.com/docs/api/post-only-endpoints
API_KEY = "phc_qMa4hWDdTruKaDb4Oa0tK0i1SKf69xf81OCFzjX6z4U"
APPLICATION_FUNCTION = "os_burr_application_function_call"
CLI_COMMAND = "os_burr_cli_command"
TIMEOUT = 2
MAX_COUNT_SESSION = 10  # max number of events collected per python process

DEFAULT_CONFIG_LOCATION = os.path.expanduser("~/.burr.conf")


def _load_config(config_location: str) -> configparser.ConfigParser:
    """Pulls config. Gets/sets default anonymous ID.

    Creates the anonymous ID if it does not exist, writes it back if so.
    :param config_location: location of the config file.
    """
    config = configparser.ConfigParser()
    try:
        with open(config_location) as f:
            config.read_file(f)
    except Exception:
        config["DEFAULT"] = {}
    else:
        if "DEFAULT" not in config:
            config["DEFAULT"] = {}

    if "anonymous_id" not in config["DEFAULT"]:
        config["DEFAULT"]["anonymous_id"] = str(uuid.uuid4())
        try:
            with open(config_location, "w") as f:
                config.write(f)
        except Exception:
            pass
    return config


def _check_config_and_environ_for_telemetry_flag(
    telemetry_default: bool, config_obj: configparser.ConfigParser
):
    """Checks the config and environment variables for the telemetry value.

    Note: the environment variable has greater precedence than the config value.
    """
    telemetry_enabled = telemetry_default
    if "telemetry_enabled" in config_obj["DEFAULT"]:
        try:
            telemetry_enabled = config_obj.getboolean("DEFAULT", "telemetry_enabled")
        except ValueError as e:
            logger.debug(
                "Unable to parse value for `telemetry_enabled` from config. " f"Encountered {e}"
            )
    if os.environ.get("BURR_TELEMETRY_ENABLED") is not None:
        env_value = os.environ.get("BURR_TELEMETRY_ENABLED")
        # set the value
        config_obj["DEFAULT"]["telemetry_enabled"] = env_value
        try:
            telemetry_enabled = config_obj.getboolean("DEFAULT", "telemetry_enabled")
        except ValueError as e:
            logger.debug(
                "Unable to parse value for `BURR_TELEMETRY_ENABLED` from environment. "
                f"Encountered {e}"
            )
    return telemetry_enabled


config = _load_config(DEFAULT_CONFIG_LOCATION)
g_telemetry_enabled = _check_config_and_environ_for_telemetry_flag(True, config)
g_anonymous_id = config["DEFAULT"]["anonymous_id"]
call_counter = 0


def disable_telemetry():
    """Disables telemetry tracking."""
    global g_telemetry_enabled
    g_telemetry_enabled = False


def is_telemetry_enabled() -> bool:
    """Returns whether telemetry tracking is enabled or not.

    Increments a counter to stop sending telemetry after 1000 invocations.
    """
    if g_telemetry_enabled:
        global call_counter
        if call_counter == 0:
            # Log only the first time someone calls this function; don't want to spam them.
            logger.info(
                "Note: Burr collects completely anonymous data about usage. "
                "This will help us improve Burr over time. "
                "See https://github.com/dagworks-inc/burr#usage-analytics--data-privacy for details."
            )
        call_counter += 1
        if call_counter > MAX_COUNT_SESSION:
            # we have hit our limit -- disable telemetry.
            return False
        return True
    else:
        return False


# base properties to instantiate on module load.
BASE_PROPERTIES = {
    "os_type": os.name,
    "os_version": platform.platform(),
    "python_version": f"{platform.python_version()}/{platform.python_implementation()}",
    "distinct_id": g_anonymous_id,
    "burr_version": VERSION,
    "telemetry_version": "0.0.1",
}


def create_application_function_run_event(function_name: str) -> dict:
    """Function to create payload for tracking function name invocation.

    :param function_name: the name of the driver function
    :return: dict representing the JSON to send.
    """
    event = {
        "api_key": API_KEY,
        "event": APPLICATION_FUNCTION,
        "properties": {},
    }
    event["properties"].update(BASE_PROPERTIES)
    payload = {
        "function_name": function_name,  # what was the name of the driver function?
    }
    event["properties"].update(payload)
    return event


def _send_event_json(event_json: dict):
    """Internal function to send the event JSON to posthog.

    :param event_json: the dictionary of data to JSON serialize and send
    """
    headers = {
        "Content-Type": "application/json",
        "Authorization": "TODO",
        "User-Agent": f"burr/{STR_VERSION}",
    }
    try:
        data = json.dumps(event_json).encode()
        req = request.Request(TRACK_URL, data=data, headers=headers)
        with request.urlopen(req, timeout=TIMEOUT) as f:
            res = f.read()
            if f.code != 200:
                raise RuntimeError(res)
    except Exception as e:
        if logger.isEnabledFor(logging.DEBUG):
            logging.debug(f"Failed to send telemetry data: {e}")
    else:
        if logger.isEnabledFor(logging.DEBUG):
            logging.debug(f"Succeed in sending telemetry consisting of [{data}].")


def send_event_json(event_json: dict):
    """Sends the event json in its own thread.

    :param event_json: the data to send
    """
    if not g_telemetry_enabled:
        raise RuntimeError("Won't send; tracking is disabled!")
    try:
        th = threading.Thread(target=_send_event_json, args=(event_json,))
        th.start()
    except Exception as e:
        # capture any exception!
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"Encountered error while sending event JSON via it's own thread:\n{e}")


def get_all_adapters_names(adapter: "internal.LifecycleAdapterSet") -> List[str]:
    """Gives a list of all adapter names in the LifecycleAdapterSet.
    Simply a loop over the adapters it contains.

    :param adapter: LifecycleAdapterSet object.
    :return: list of adapter names.
    """
    adapters = adapter.adapters
    out = []
    for adapter in adapters:
        out.append(get_adapter_name(adapter))
    return out


def get_adapter_name(adapter: "internal.LifecycleAdapter") -> str:
    """Get the class name of the ``burr`` adapter used.

    If we detect it's not a Burr one, we do not track it.

    :param adapter: lifecycle.internal.LifecycleAdapter object.
    :return: string module + class name of the adapter.
    """
    # Check whether it's a burr based adapter
    if adapter.__module__.startswith("burr."):
        adapter_name = f"{adapter.__module__}.{adapter.__class__.__name__}"
    else:
        adapter_name = "custom_adapter"
    return adapter_name


def create_and_send_cli_event(command: str):
    """Function that creates JSON and sends to track CLI usage.

    :param command: the CLI command run.
    """
    event = {
        "api_key": API_KEY,
        "event": CLI_COMMAND,
        "properties": {},
    }
    event["properties"].update(BASE_PROPERTIES)

    payload = {
        "command": command,
    }
    event["properties"].update(payload)
    send_event_json(event)


CallableT = TypeVar("CallableT", bound=Callable)


def capture_function_usage(call_fn: CallableT) -> CallableT:
    """Decorator to wrap some application functions for telemetry capture.

    We want to use this for non-execute functions.
    We don't capture information about the arguments at this stage,
    just the function name.

    :param call_fn: the Driver function to capture.
    :return: wrapped function.
    """

    @functools.wraps(call_fn)
    def wrapped_fn(*args, **kwargs):
        try:
            return call_fn(*args, **kwargs)
        finally:
            if is_telemetry_enabled():
                try:
                    function_name = call_fn.__name__
                    event_json = create_application_function_run_event(function_name)
                    send_event_json(event_json)
                except Exception as e:
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.error(
                            f"Failed to send telemetry for function usage. Encountered: {e}\n"
                        )

    return wrapped_fn



---
File: /burr/burr/version.py
---

import importlib.metadata

__version__ = importlib.metadata.version("burr")



---
File: /burr/docs/_static/custom.css
---

/* Enable line wrapping for code blocks */
.highlight pre {
  white-space: pre-wrap;
  word-wrap: break-word;
}



---
File: /burr/docs/_static/testimonials.css
---

.testimonial-container {
    display: flex;
    flex-wrap: wrap;
    gap: 1rem;
    justify-content: center;
    margin: 2rem 0;
}

.testimonial-card {
    background: #fff;
    border: 1px solid #ddd;
    border-radius: 8px;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    max-width: 300px;
    width: 100%;
    padding: 1rem;
    text-align: center;
    transition: transform 0.2s ease-in-out;
}

.testimonial-card:hover {
    transform: scale(1.05);
}

.testimonial-photo img {
    border-radius: 50%;
    height: 80px;
    width: 80px;
    object-fit: cover;
    margin-bottom: 1rem;
}

.testimonial-content p {
    font-style: italic;
    color: #555;
}

.testimonial-content h4 {
    margin: 0.5rem 0 0;
    font-size: 1.1rem;
    font-weight: bold;
    color: #555;
}

.testimonial-content span {
    color: #999;
    font-size: 0.9rem;
}



---
File: /burr/docs/conf.py
---

# Configuration file for the Sphinx documentation builder.
#
# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
import os

project = "Burr"
copyright = "2024, Elijah ben Izzy, Stefan Krawczyk"
author = "Elijah ben Izzy, Stefan Krawczyk"

# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration

extensions = [
    "sphinx.ext.autodoc",
    "sphinx.ext.autosummary",
    "myst_nb",
    "sphinx_sitemap",
    "sphinx_toolbox.collapse",
]

if os.getenv("GITHUB_ACTIONS"):  # only add googleanalytics if building on GitHub Actions
    extensions.append("sphinxcontrib.googleanalytics")
    googleanalytics_id = "G-20Z3J1CR22"

templates_path = ["_templates"]
exclude_patterns = ["_build", "Thumbs.db", ".DS_Store"]

html_theme = "furo"
html_static_path = ["_static"]

html_css_files = [
    "custom.css",
    "testimonials.css",
]

html_title = "Burr"
html_theme_options = {
    "source_repository": "https://github.com/dagworks-inc/burr",
    "source_branch": "main",
    "source_directory": "docs/",
    "light_css_variables": {
        "color-announcement-background": "#ffba00",
        "color-announcement-text": "#091E42",
    },
    "dark_css_variables": {
        "color-announcement-background": "#ffba00",
        "color-announcement-text": "#091E42",
    },
}

nb_execution_mode = "off"

exclude_patterns = ["README-internal.md"]

autodoc_typehints_format = "short"
python_maximum_signature_line_length = 100
python_use_unqualified_type_names = True

# -- for sitemap extension
html_baseurl = "https://burr.dagworks.io/"  # TODO -- update this
html_extra_path = ["robots.txt"]
sitemap_locales = [None]
sitemap_url_scheme = "{link}"



---
File: /burr/docs/make_testimonials.py
---

card_template = """
        <div class="testimonial-card">
            <div class="testimonial-content">
                <p>"{user_quote}"</p>
                <h4>{user_name}</h4>
                <span>{user_title}, {user_company}</span>
            </div>
        </div>"""

testimonials = [
    {
        "user_name": "Ashish Ghosh",
        "user_title": "CTO",
        "user_company": "Peanut Robotics",
        "user_quote": "After evaluating several other obfuscating LLM frame-works, their elegant yet comprehensive state management "
        "solution proved to be the powerful answer to rolling out robots driven by AI decision making.",
        "image_link": "",
    },
    {
        "user_name": "Reddit User",
        "user_title": "LocalLlama",
        "user_company": "Subreddit",
        "user_quote": "Of course, you can use it [LangChain], but whether it's really production-ready and improves the time from 'code-to-prod' [...], "
        "we've been doing LLM apps for two years, and the answer is no [...] All these 'all-in-one' libs suffer from this [...].  "
        "Honestly, take a look at Burr. Thank me later.",
        "image_link": "",
    },
    {
        "user_name": "Ishita",
        "user_title": "Founder",
        "user_company": "Watto.ai",
        "user_quote": "Using Burr is a no-brainer if you want to build a modular AI application. It is so easy to build "
        "with and I especially love their UI which makes debugging, a piece of cake. And the always ready "
        "to help team, is the cherry on top.",
        "image_link": "",
    },
    {
        "user_name": "Matthew Rideout",
        "user_title": "Staff Software Engineer",
        "user_company": "Paxton AI",
        "user_quote": "I just came across Burr and I'm like WOW, this seems like you guys predicted this exact need when"
        " building this. No weird esoteric concepts just because it's AI.",
        "image_link": "",
    },
    {
        "user_name": "Rinat Gareev",
        "user_title": "Senior Solutions Architect",
        "user_company": "Provectus",
        "user_quote": "Burr's state management part is really helpful for creating state snapshots and build debugging, "
        "replaying and even building evaluation cases around that",
        "image_link": "",
    },
    {
        "user_name": "Hadi Nayebi",
        "user_title": "Co-founder",
        "user_company": "CognitiveGraphs",
        "user_quote": "I have been using Burr over the past few months, and compared to many agentic LLM platforms out "
        "there (e.g. LangChain, CrewAi, AutoGen, Agency Swarm, etc), Burr provides a more robust framework"
        " for designing complex behaviors.",
        "image_link": "",
    },
    {
        "user_name": "Aditya K.",
        "user_title": "DS Architect",
        "user_company": "TaskHuman",
        "user_quote": "Moving from LangChain to Burr was a game-changer! "
        "<br/>Time-Saving: It took me just a few hours to get started with Burr, compared to the days and weeks I spent trying to navigate LangChain. "
        "<br/>Cleaner Implementation: With Burr, I could finally have a cleaner, more sophisticated, and stable implementation. No more wrestling with complex codebases. "
        "<br/>Team Adoption: I pitched Burr to my teammates, and we pivoted our entire codebase to it. It's been a smooth ride ever since.",
        "image_link": "",
    },
]

# code to generate testimonials
for testimonial in testimonials:
    print(card_template.format(**testimonial))



---
File: /burr/examples/adaptive-crag/__init__.py
---




---
File: /burr/examples/adaptive-crag/application.py
---

from pathlib import Path
from typing import Iterable, Literal

import google.generativeai as genai
import instructor
import lancedb
from dotenv import load_dotenv
from exa_py.api import Exa
from instructor.exceptions import InstructorRetryException
from lancedb.db import DBConnection
from lancedb.embeddings import get_registry
from lancedb.pydantic import LanceModel, Vector
from lancedb.table import Table
from pydantic import BaseModel, Field

from burr.core import Application, ApplicationBuilder, State, expr, when
from burr.core.action import action
from burr.tracking import LocalTrackingClient

load_dotenv()  # load your GOOGLE_API_KEY and EXA_API_KEY from a .env file


ask_gemini = instructor.from_gemini(
    client=genai.GenerativeModel(model_name="gemini-1.5-flash-latest")
)
exa = Exa(api_key=None)

BURR_DOCS_DIR = Path("burr_docs")
DOC_THRESH = 512
LANCE_URI = "lance/rag"
EMS_MODEL = "BAAI/bge-small-en-v1.5"  # for the embeddings in LanceDB
DEVICE = "cuda"  # or "cpu"
DOCS_LIMIT = 5  # number of documents to retrieve from the database or search engine
ATTEMPTS = 3  # ask_gemini will try to call `create` this many times before giving up


def chat_message(role: str, content: str) -> dict[str, str]:
    return {"role": role, "content": content}


def system_message(content: str) -> dict[str, str]:
    return chat_message(role="system", content=content)


def user_message(content: str) -> dict[str, str]:
    return chat_message(role="user", content=content)


def assistant_message(content: str) -> dict[str, str]:
    return chat_message(role="assistant", content=content)


def merge_short_docs(
    docs: list[str], file_name: str, doc_thresh: int = DOC_THRESH
) -> list[dict[str, str]]:
    """
    Merge short documents into longer ones for better performance.
    `file_name` is the name of the file the documents came from. It's just for metadata.
    """
    merged_docs = [{"text": docs[0], "file_name": file_name}]
    for doc in docs[1:]:
        last_text = merged_docs[-1]["text"]
        if len(last_text) <= doc_thresh:
            merged_docs[-1]["text"] = last_text.strip() + "\n" + doc
        else:
            merged_docs.append({"text": doc, "file_name": file_name})
    return merged_docs


def load_burr_docs(
    file_names: list[str] | str | None = None,
    burr_docs_dir: Path = BURR_DOCS_DIR,
    doc_thresh: int = DOC_THRESH,
) -> list[dict[str, str]]:
    """
    Load strings from text files in `burr_docs_dir` and merge short documents.
    If `file_names` is None, load all files in the directory.
    """
    if file_names is None:
        files_iter = burr_docs_dir.glob("*.txt")
    elif isinstance(file_names, str):
        files_iter = [file_names]
    else:
        files_iter = file_names
    docs = []
    for file in files_iter:
        docs += merge_short_docs(
            docs=(burr_docs_dir / Path(file).name).with_suffix(".txt").read_text().split("\n\n"),
            file_name=Path(file).stem,
            doc_thresh=doc_thresh,
        )
    return docs


def add_table(
    db: DBConnection,
    table_name: str,
    data: list[dict[str, str]],
    schema: type[LanceModel],
) -> Table:
    """
    Drop the table if it exists, create a new table, add data, and create a Full-Text Search index.
    Semenaitc Search is enabled by default. So creating an FTS index makes the table Hybrid Search ready.
    Learn more about Hybrid Search in LanceDB: https://lancedb.github.io/lancedb/hybrid_search/hybrid_search/
    """
    db.drop_table(name=table_name, ignore_missing=True)  # type: ignore
    table = db.create_table(name=table_name, schema=schema)
    table.add(data=data)
    table.create_fts_index(field_names="text")  # type: ignore
    return table


def route_template(
    table_names: Iterable[str],
    query: str,
    chat_history: list[dict[str, str]] | None = None,
) -> str:
    prompt = """\
<task>
You are a world-class router for user queries.
Given a user query, you may need some extra information for answering the query. So you need to select the best place to find the answer.
You have the following options:
1. You will have some vector database tables with information on specific topics. If the query is directly related to any of these topics, return the relevant table name.
2. If you can answer the query directly with your own knowledge, return "assistant".
3. If you don't have the answer to the query, you can search the internet for the answer. In this case, return "web_search".
</task>

<available_tables>
"""
    prompt += "\n".join(table_names)
    prompt += "\n</available_tables>"
    if chat_history:
        prompt += "\n\n<chat_history>\n"
        prompt += "\n".join(f"{msg['role']}: {msg['content']}" for msg in chat_history)
        prompt += "\n</chat_history>"
    prompt += f"\n\n<query>\n{query}\n</query>"
    return prompt


# used to get a structured response from the assistant when rewriting a query
class LanceDBQuery(BaseModel):
    keywords: list[str] = Field(..., min_length=1, max_length=10)
    query: str = Field(..., min_length=10, max_length=300)

    def __str__(self) -> str:
        return ", ".join(self.keywords) + ", " + self.query


def lancedb_query_template(query: str, chat_history: list[dict[str, str]]) -> str:
    prompt = f"""\
<task>
You are a world-class researcher with access to a vector database that can be one of or similar to ChromaDB, Weaviate, LanceDB, VertexAI, etc.
Given a user query, rewrite the query to find the most relevant information in the vector database.
Remember that the vector database has hybrid search capability, which means it can do both full-text search and vector similarity search.
So make sure to not remove any important keywords from the query. You can even add more keywords if you think they are relevant.
Split the query into a list of keywords and a string query.
</task>

<query>
{query}
</query>
"""

    if chat_history:
        chat_history_str = "\n".join(
            [f"{msg['role']}: {msg['content']}" for msg in chat_history]
        ).strip()
        prompt += f"\n<chat_history>\n{chat_history_str}\n</chat_history>"
    return prompt


# used to get a structured response from the assistant when extracting keywords for a web search
class ExaSearchKeywords(BaseModel):
    keywords: list[str] = Field(..., min_length=2, max_length=5)

    def __str__(self) -> str:
        return ", ".join(self.keywords)


def exa_search_query_template(query: str) -> str:
    return f"""\
<task>
You are a world-class internet researcher.
Given a user query, extract 2-5 keywords as queries for the web search. Including the topic background and main intent.
</task>

<examples>

<query>
What is Henry Feildenâ€™s occupation?
</query>
<keywords>
Henry Feilden, occupation
</keywords>

<query>
In what city was Billy Carlson born?
</query>
<keywords>
city, Billy Carlson, born
</keywords>

<query>
What is the religion of John Gwynn?
</query>
<keywords>
religion, John Gwynn
</keywords>

<query>
What sport does Kiribati menâ€™s national basketball team play?
</query>
<keywords>
sport, Kiribati menâ€™s national basketball team play
</keywords>

</examples>

<query>
{query}
</query>
"""


def evaluator_template(query: str, document: str) -> str:
    return f"""\
<task>
You are a world-class document relevance evaluator.
Given a user query, does the following document have the exact information to answer the question? Answer True or False.
</task>

<query>
{query}
</query>

<document>
{document}
</document>
"""


def get_user_query() -> str:
    query = ""
    while not query:
        query = input("(exit, quit, q to exit) > ").strip()
    return query


@action(reads=["db", "chat_history"], writes=["query", "route"])
def router(state: State, query: str, attempts: int = ATTEMPTS) -> tuple[dict[str, str], State]:
    """
    Route the user `query` to the appropriate place for an answer.
    If the `query` is directly related to a table in the database, return the table name.
    If a web search is needed, return "web_search".
    If the `query` can be directly answered by the assistant, return "assistant".
    If the `query` contains "exit", "quit", or "q", return "terminate".
    If there is a `chat_history`, it will also be used as context for the routing.
    If it fails to route after `attempts`, it will return "assistant".
    We use `Instructor` to ensure that the router only returns valid routes.
    """
    if query.lower() in ["exit", "quit", "q"]:
        route = "terminate"
        return {"route": route}, state.update(query=query, route=route)
    db: DBConnection = state["db"]
    table_names = db.table_names()
    chat_history = state["chat_history"]
    # using this as a `response_model` to ensure the route is valid
    routes = Literal[*table_names, "web_search", "assistant"]  # type: ignore
    try:
        route = ask_gemini.create(
            messages=[
                user_message(
                    route_template(table_names=table_names, query=query, chat_history=chat_history)
                )  # type: ignore
            ],
            response_model=routes,  # type: ignore
            max_retries=attempts,
        )
    except Exception as e:
        print(f"Error in router: {e}")
        route = "assistant"
    return {"route": route}, state.update(query=query, route=route)  # type: ignore


@action(reads=["query", "chat_history"], writes=["lancedb_query"])
def rewrite_query_for_lancedb(
    state: State, rewrite_attempts: int = ATTEMPTS
) -> tuple[dict[str, str], State]:
    """
    Rewrite the user `query` to find the most relevant information in the LanceDB vector database.
    """
    query = state["query"]
    chat_history = state["chat_history"]
    try:
        query = str(
            ask_gemini.create(
                messages=[
                    user_message(
                        lancedb_query_template(query=query, chat_history=chat_history)  # type: ignore
                    )
                ],
                response_model=LanceDBQuery,
                max_retries=rewrite_attempts,
            )
        )
    except Exception as e:
        print(f"Error in rewrite_query_for_lancedb: {e}")
    return {"lancedb_query": query}, state.update(lancedb_query=query)


@action(reads=["db", "route", "lancedb_query", "docs_limit"], writes=["lancedb_results"])
def search_lancedb(state: State) -> tuple[dict[str, list[str]], State]:
    db: DBConnection = state["db"]
    try:
        lancedb_results = (
            db.open_table(name=state["route"])
            .search(query=state["lancedb_query"], query_type="hybrid")
            .limit(limit=state["docs_limit"])
            .to_pandas()["text"]
            .tolist()
        )
    except Exception as e:
        print(f"Error in search_lancedb: {e}")
        lancedb_results = []
    return {"lancedb_results": lancedb_results}, state.update(lancedb_results=lancedb_results)


@action(reads=["query", "lancedb_results"], writes=["lancedb_results"])
def remove_irrelevant_lancedb_results(
    state: State,
) -> tuple[dict[str, list[str]], State]:
    """
    Ask the assistant to evaluate the relevance of each retrieved document with the user `query`.
    If the assistant thinks the document is relevant, keep it in the `lancedb_results`.
    """
    lancedb_results = state["lancedb_results"]
    if not lancedb_results:
        return {"lancedb_results": lancedb_results}, state
    filtered_results = []
    for lancedb_result in lancedb_results:
        try:
            is_relevant = ask_gemini.create(
                messages=[
                    user_message(
                        evaluator_template(query=state["query"], document=lancedb_result)
                    )  # type: ignore
                ],
                response_model=bool,  # type: ignore
                max_retries=ATTEMPTS,
            )
        except Exception as e:
            print(f"Error in remove_irrelevant_lancedb_results: {e}")
            is_relevant = True
        if is_relevant:
            filtered_results.append(lancedb_result)
    return {"lancedb_results": filtered_results}, state.update(lancedb_results=filtered_results)


@action(reads=["query"], writes=["exa_search_keywords"])
def extract_keywords_for_exa_search(
    state: State, extraction_attempts: int = ATTEMPTS
) -> tuple[dict[str, str], State]:
    """
    Extract 2-5 keywords from the user `query` for a web search.
    These keywords will be used to search the internet for the answer. Instead of the full query.
    """
    query = state["query"]
    try:
        query = str(
            ask_gemini.create(
                messages=[user_message(exa_search_query_template(query=query))],  # type: ignore
                response_model=ExaSearchKeywords,
                max_retries=extraction_attempts,
            )
        )
    except Exception as e:
        print(f"Error in extract_keywords_for_web_search: {e}")
    return {"exa_search_keywords": query}, state.update(exa_search_keywords=query)


@action(reads=["exa_search_keywords", "docs_limit"], writes=["exa_search_results"])
def search_exa(state: State) -> tuple[dict[str, list[str]], State]:
    """
    More details about the Exa API:
        - https://docs.exa.ai/reference/getting-started
        - https://github.com/exa-labs/exa-py
    """
    try:
        exa_search_results = exa.search_and_contents(
            query=state["exa_search_keywords"],
            num_results=state["docs_limit"],
            highlights=True,
        )
        exa_search_results = [res.highlights[0] for res in exa_search_results.results]
    except Exception as e:
        print(f"Error in search_exa: {e}")
        exa_search_results = []
    return {"exa_search_results": exa_search_results}, state.update(
        exa_search_results=exa_search_results
    )


@action(
    reads=["query", "lancedb_results", "exa_search_results", "chat_history"],
    writes=["chat_history", "lancedb_results", "exa_search_results"],
)
def ask_assistant(state: State, attempts: int = ATTEMPTS) -> tuple[dict[str, str], State]:
    """
    Combine the `chat_history`, `query`, `lancedb_results`, and `exa_search_results` to ask the assistant for an answer.
    `chat_history`, `lancedb_results`, and `exa_search_results` are used as context for the assistant and can be empty lists.
    """
    query = state["query"]
    lancedb_results = state.get("lancedb_results", [])
    exa_search_results = state.get("exa_search_results", [])
    messages = state["chat_history"]
    context = lancedb_results + exa_search_results
    if context:
        context = "\n".join(context)
        query = query.strip() + "\n<additional_context>\n" + context + "\n</additional_context>"
    messages.append({"role": "user", "content": query.strip()})
    try:
        response = ask_gemini.create(messages=messages, response_model=str, max_retries=attempts)
    except InstructorRetryException as e:
        print(f"Error in ask_assistant: {e}")
        response = "Sorry, please try again."
    # lancedb_results and exa_search_results have very specific additional context that's relevant only for the latest query
    # that specific context is no longer needed after the assistant responds, so we clear it
    # the assistant will still have the chat_history as overall context
    # chat_history has the user messages and the assistant responses, so it's more than enough
    return (
        {"assistant_response": response},  # type: ignore
        state.append(chat_history=user_message(query))
        .append(chat_history=assistant_message(response))  # type: ignore
        .update(lancedb_results=[], exa_search_results=[]),
    )


@action(reads=["chat_history"], writes=[])
def terminate(state: State) -> tuple[dict[str, list[dict[str, str]]], State]:
    return {"chat_history": state["chat_history"]}, state


def application(
    db: DBConnection,
    app_id: str | None = None,
    username: str | None = None,
    project: str = "AdaptiveCRAG",
) -> Application:
    tracker = LocalTrackingClient(project=project)
    builder = (
        ApplicationBuilder()
        .with_actions(
            router.bind(attempts=ATTEMPTS),
            rewrite_query_for_lancedb.bind(rewrite_attempts=ATTEMPTS),
            search_lancedb,
            remove_irrelevant_lancedb_results,
            extract_keywords_for_exa_search.bind(extraction_attempts=ATTEMPTS),
            search_exa,
            ask_assistant.bind(attempts=ATTEMPTS),
            terminate,
        )
        .with_transitions(
            # if the user wants to exit the conversation
            ("router", "terminate", when(route="terminate")),  # type: ignore
            # if the `query` can be directly answered by the assistant
            ("router", "ask_assistant", when(route="assistant")),  # type: ignore
            # if a web search is needed
            ("router", "extract_keywords_for_exa_search", when(route="web_search")),  # type: ignore
            # the database can have multiple tables for different topics
            # so it makes sense to have `when` conditions for all other routes and consider this as the default route
            # otherwise we would have to add a `when` condition for each table name
            ("router", "rewrite_query_for_lancedb"),
            ("rewrite_query_for_lancedb", "search_lancedb"),
            ("search_lancedb", "remove_irrelevant_lancedb_results"),
            # if there aren't enough relevant documents left after filtering
            # use web search to find more information
            (
                "remove_irrelevant_lancedb_results",
                "extract_keywords_for_exa_search",
                expr("len(lancedb_results) < docs_limit"),  # type: ignore
            ),
            # if there are enough relevant documents even after filtering, ask the assistant for an answer
            ("remove_irrelevant_lancedb_results", "ask_assistant"),
            ("extract_keywords_for_exa_search", "search_exa"),
            ("search_exa", "ask_assistant"),
            # go back to the router every time the assistant is done answering
            ("ask_assistant", "router"),
        )
        .with_tracker("local", project=project)
        .with_identifiers(app_id=app_id, partition_key=username)  # type: ignore
        .initialize_from(
            tracker,
            resume_at_next_action=True,
            default_entrypoint="router",
            default_state=dict(db=db, chat_history=[], docs_limit=DOCS_LIMIT),
        )
    )
    return builder.build()


if __name__ == "__main__":
    lance_model = get_registry().get("sentence-transformers").create(name=EMS_MODEL, device=DEVICE)

    # define the schema of your data using Pydantic
    class LanceDoc(LanceModel):
        text: str = lance_model.SourceField()
        vector: Vector(dim=lance_model.ndims()) = lance_model.VectorField()  # type: ignore
        file_name: str

    lance_db = lancedb.connect(LANCE_URI)

    burr_docs = load_burr_docs(burr_docs_dir=BURR_DOCS_DIR)
    burr_table = add_table(db=lance_db, table_name="burr_docs", data=burr_docs, schema=LanceDoc)

    app = application(db=lance_db)
    app.visualize(
        output_file_path="statemachine",
        include_conditions=True,
        include_state=False,
        format="png",
    )



---
File: /burr/examples/conversational-rag/graph_db_example/application.py
---

import json
import uuid
from typing import Tuple

import falkordb
import openai
from falkordb import FalkorDB
from graph_schema import graph_schema

from burr.core import Application, ApplicationBuilder, State, default, expr
from burr.core.action import action
from burr.tracking import LocalTrackingClient


# --- helper functions
def schema_to_prompt(schema):
    prompt = "The Knowledge graph contains nodes of the following types:\n"

    for node in schema["nodes"]:
        lbl = node
        node = schema["nodes"][node]
        if len(node["attributes"]) > 0:
            prompt += f"The {lbl} node type has the following set of attributes:\n"
            for attr in node["attributes"]:
                t = node["attributes"][attr]["type"]
                prompt += f"The {attr} attribute is of type {t}\n"
        else:
            prompt += f"The {node} node type has no attributes:\n"

    prompt += "In addition the Knowledge graph contains edge of the following types:\n"

    for edge in schema["edges"]:
        rel = edge
        edge = schema["edges"][edge]
        if len(edge["attributes"]) > 0:
            prompt += f"The {rel} edge type has the following set of attributes:\n"
            for attr in edge["attributes"]:
                t = edge["attributes"][attr]["type"]
                prompt += f"The {attr} attribute is of type {t}\n"
        else:
            prompt += f"The {rel} edge type has no attributes:\n"

        prompt += f"The {rel} edge connects the following entities:\n"
        for conn in edge["connects"]:
            src = conn[0]
            dest = conn[1]
            prompt += f"{src} is connected via {rel} to {dest}, (:{src})-[:{rel}]->(:{dest})\n"

    return prompt


def set_inital_chat_history(schema_prompt: str) -> list[dict]:
    SYSTEM_MESSAGE = "You are a Cypher expert with access to a directed knowledge graph\n"
    SYSTEM_MESSAGE += schema_prompt
    SYSTEM_MESSAGE += (
        "Query the knowledge graph to extract relevant information to help you anwser the users "
        "questions, base your answer only on the context retrieved from the knowledge graph, "
        "do not use preexisting knowledge."
    )
    SYSTEM_MESSAGE += (
        "For example to find out if two fighters had fought each other e.g. did Conor McGregor "
        "every compete against Jose Aldo issue the following query: "
        "MATCH (a:Fighter)-[]->(f:Fight)<-[]-(b:Fighter) WHERE a.Name = 'Conor McGregor' AND "
        "b.Name = 'Jose Aldo' RETURN a, b\n"
    )

    messages = [{"role": "system", "content": SYSTEM_MESSAGE}]
    return messages


# ---  tools


def run_cypher_query(graph, query):
    try:
        results = graph.ro_query(query).result_set
    except Exception:
        results = {"error": "Query failed please try a different variation of this query"}

    if len(results) == 0:
        results = {
            "error": "The query did not return any data, please make sure you're using the right edge "
            "directions and you're following the correct graph schema"
        }

    return str(results)


run_cypher_query_tool_description = {
    "type": "function",
    "function": {
        "name": "run_cypher_query",
        "description": "Runs a Cypher query against the knowledge graph",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Query to execute",
                },
            },
            "required": ["query"],
        },
    },
}


# --- actions


@action(
    reads=[],
    writes=["question", "chat_history"],
)
def human_converse(state: State, user_question: str) -> Tuple[dict, State]:
    """Human converse step -- make sure we get input, and store it as state."""
    new_state = state.update(question=user_question)
    new_state = new_state.append(chat_history={"role": "user", "content": user_question})
    return {"question": user_question}, new_state


@action(
    reads=["question", "chat_history"],
    writes=["chat_history", "tool_calls"],
)
def AI_create_cypher_query(state: State, client: openai.Client) -> tuple[dict, State]:
    """AI step to create the cypher query."""
    messages = state["chat_history"]
    # Call the function
    response = client.chat.completions.create(
        model="gpt-4-turbo-preview",
        messages=messages,
        tools=[run_cypher_query_tool_description],
        tool_choice="auto",
    )
    response_message = response.choices[0].message
    new_state = state.append(chat_history=response_message.to_dict())
    tool_calls = response_message.tool_calls
    if tool_calls:
        new_state = new_state.update(tool_calls=tool_calls)
    return {"ai_response": response_message.content, "usage": response.usage.to_dict()}, new_state


@action(
    reads=["tool_calls", "chat_history"],
    writes=["tool_calls", "chat_history"],
)
def tool_call(state: State, graph: falkordb.Graph) -> Tuple[dict, State]:
    """Tool call step -- execute the tool call."""
    tool_calls = state.get("tool_calls", [])
    new_state = state
    result = {"tool_calls": []}
    for tool_call in tool_calls:
        function_name = tool_call.function.name
        assert function_name == "run_cypher_query"
        function_args = json.loads(tool_call.function.arguments)
        function_response = run_cypher_query(graph, function_args.get("query"))
        new_state = new_state.append(
            chat_history={
                "tool_call_id": tool_call.id,
                "role": "tool",
                "name": function_name,
                "content": function_response,
            }
        )
        result["tool_calls"].append({"tool_call_id": tool_call.id, "response": function_response})
    new_state = new_state.update(tool_calls=[])
    return result, new_state


@action(
    reads=["chat_history"],
    writes=["chat_history"],
)
def AI_generate_response(state: State, client: openai.Client) -> tuple[dict, State]:
    """AI step to generate the response."""
    messages = state["chat_history"]
    response = client.chat.completions.create(
        model="gpt-4-turbo-preview",
        messages=messages,
    )  # get a new response from the model where it can see the function response
    response_message = response.choices[0].message
    new_state = state.append(chat_history=response_message.to_dict())
    return {"ai_response": response_message.content, "usage": response.usage.to_dict()}, new_state


def build_application(
    db_client: FalkorDB, graph_name: str, application_run_id: str, openai_client: openai.OpenAI
) -> Application:
    """Builds the application."""
    # get the graph
    graph = db_client.select_graph(graph_name)
    # get schema
    schema = graph_schema(graph)
    # create a prompt from it
    schema_prompt = schema_to_prompt(schema)
    # set the initial chat history
    base_messages = set_inital_chat_history(schema_prompt)

    tracker = LocalTrackingClient("ufc-falkor")
    # create graph
    burr_application = (
        ApplicationBuilder()
        .with_actions(  # define the actions
            AI_create_cypher_query.bind(client=openai_client),
            tool_call.bind(graph=graph),
            AI_generate_response.bind(client=openai_client),
            human_converse,
        )
        .with_transitions(  # define the edges between the actions based on state conditions
            ("human_converse", "AI_create_cypher_query", default),
            ("AI_create_cypher_query", "tool_call", expr("len(tool_calls)>0")),
            ("AI_create_cypher_query", "human_converse", default),
            ("tool_call", "AI_generate_response", default),
            ("AI_generate_response", "human_converse", default),
        )
        .with_identifiers(app_id=application_run_id)
        .with_state(  # initial state
            **{"chat_history": base_messages, "tool_calls": []},
        )
        .with_entrypoint("human_converse")
        .with_tracker(tracker)
        .build()
    )
    return burr_application


if __name__ == "__main__":
    print(
        """Run
    > burr
    in another terminal to see the UI at http://localhost:7241
    """
    )
    _client = openai.OpenAI()
    _db_client = FalkorDB(host="localhost", port=6379)
    _graph_name = "UFC"
    _app_run_id = str(uuid.uuid4())  # this is a unique identifier for the application run
    # build the app
    _app = build_application(_db_client, _graph_name, _app_run_id, _client)

    # visualize the app
    _app.visualize(output_file_path="ufc-burr", include_conditions=True, view=True, format="png")

    # run it
    while True:
        question = input("What can I help you with?\n")
        if question == "exit":
            break
        action, _, state = _app.run(
            halt_before=["human_converse"],
            inputs={"user_question": question},
        )
        print(f"AI: {state['chat_history'][-1]['content']}\n")



---
File: /burr/examples/conversational-rag/graph_db_example/graph_schema.py
---

"""
Code courtesy of the FalkorDB.
"""


# collect graph's schema
def graph_schema(g):
    schema = {}

    # ---------------------------------------------------------------------------
    # process nodes
    # ---------------------------------------------------------------------------

    nodes = {}

    q = "CALL db.labels()"
    labels = [x[0] for x in g.query(q).result_set]

    for label in labels:
        nodes[label] = {}
        nodes[label]["attributes"] = {}

        q = f"MATCH (n:{label}) RETURN n LIMIT 50"
        result = g.query(q).result_set

        for row in result:
            node = row[0]
            for attr in node.properties:
                val = node.properties[attr]
                if attr not in nodes[label]["attributes"]:
                    nodes[label]["attributes"][attr] = {"type": type(val).__name__}

    schema["nodes"] = nodes

    # ---------------------------------------------------------------------------
    # process relations
    # ---------------------------------------------------------------------------

    edges = {}

    q = "CALL db.relationshiptypes()"
    rels = [x[0] for x in g.query(q).result_set]

    for r in rels:
        edges[r] = {}
        edges[r]["attributes"] = {}

        q = f"MATCH ()-[e:{r}]->() RETURN e LIMIT 50"
        result = g.query(q).result_set

        for row in result:
            edge = row[0]
            for attr in edge.properties:
                val = edge.properties[attr]
                if attr not in edges[r]["attributes"]:
                    edges[r]["attributes"][attr] = {"type": type(val).__name__}

        edges[r]["connects"] = []

        # collect edge endpoints
        for src in labels:
            for dest in labels:
                q = f"MATCH (:{src})-[:{r}]->(:{dest}) RETURN 1 LIMIT 1"
                res = g.query(q).result_set
                if len(res) == 1:
                    edges[r]["connects"].append((src, dest))

    schema["edges"] = edges

    return schema



---
File: /burr/examples/conversational-rag/graph_db_example/hamilton_ingest.py
---

import ingest_fighters
import ingest_fights
from falkordb import FalkorDB
from hamilton import driver
from hamilton.execution import executors


def main():
    # Connect to FalkorDB
    db = FalkorDB(host="localhost", port=6379)
    g = db.select_graph("UFC")

    # Clear previous graph
    if "UFC" in db.list_graphs():
        g.delete()

    # ---- load fighters ----

    # Note if you want to track the progress of the load you can use the Hamilton UI:
    # from hamilton_sdk import adapters
    # tracker = adapters.HamiltonTracker(
    #    project_id=44,  # modify this as needed
    #    username="elijah@dagworks.io",
    #    dag_name="load_fighters",
    #    tags={"environment": "DEV", "team": "MY_TEAM", "version": "X"}
    # )
    # build the hamilton Driver
    fighter_loader = (
        driver.Builder()
        .with_modules(ingest_fighters)
        .enable_dynamic_execution(allow_experimental_mode=True)
        .with_remote_executor(executors.MultiThreadingExecutor(5))
        # .with_adapters(tracker)  # <-- uncomment this line if you want to track the progress
        .build()
    )
    # display the functions in the module
    fighter_loader.display_all_functions("ingest_fighters.png")
    fighter_results = fighter_loader.execute(["write_to_graph"], inputs={"graph": g})

    # ---- load fights ----

    # if you have the Hamilton UI you can see progress:
    # from hamilton_sdk import adapters
    # tracker = adapters.HamiltonTracker(
    #    project_id=44,  # modify this as needed
    #    username="elijah@dagworks.io",
    #    dag_name="load_fights",
    #    tags={"environment": "DEV", "team": "MY_TEAM", "version": "X"}
    # )
    fights_loader = (
        driver.Builder()
        .with_modules(ingest_fights)
        .enable_dynamic_execution(allow_experimental_mode=True)
        .with_remote_executor(
            executors.MultiThreadingExecutor(5)
        )  # this will do 5 concurrent inserts
        # .with_adapters(tracker)  # <-- uncomment this line if you want to track the progress
        .build()
    )
    # display the functions in the module
    fights_loader.display_all_functions("ingest_fights.png")
    fight_results = fights_loader.execute(["collect_writes"], inputs={"graph": g})

    print(
        f"All done - loaded {fighter_results['write_to_graph']} fighters and {fight_results['collect_writes']} fights."
    )


if __name__ == "__main__":
    main()



---
File: /burr/examples/conversational-rag/graph_db_example/ingest_fighters.py
---

"""
Hamilton pipeline to load fighter data into FalkorDB.
"""
import falkordb
import pandas as pd
import utils
from hamilton.htypes import Collect, Parallelizable


def raw_fighter_details() -> pd.DataFrame:
    """Load fighter data"""
    _df = pd.read_csv("../data/raw_fighter_details.csv")
    return _df


def fighter(raw_fighter_details: pd.DataFrame) -> Parallelizable[pd.Series]:
    """We then want to do something for each record. That's what this code sets up"""
    for idx, row in raw_fighter_details.iterrows():
        yield row


def record(fighter: pd.Series) -> dict:
    """Given a single fighter record, process it into a dictionary."""
    attrs = {}
    attrs["Name"] = fighter.fighter_name

    if isinstance(fighter.Height, str) and fighter.Height != "":
        attrs["Height"] = utils.height_to_cm(fighter.Height)

    if isinstance(fighter.Weight, str) and fighter.Weight != "":
        Weight = int(fighter.Weight.replace(" lbs.", ""))
        attrs["Weight"] = Weight

    if isinstance(fighter.Reach, str) and fighter.Reach != "":
        attrs["Reach"] = utils.reach_to_cm(fighter.Reach)

    if isinstance(fighter.Stance, str) and fighter.Stance != "":
        attrs["Stance"] = fighter.Stance

    if isinstance(fighter.DOB, str) and fighter.DOB != "":
        attrs["DOB"] = utils.date_to_timestamp(fighter.DOB)

    # Significant Strikes Landed per Minute
    attrs["SLpM"] = float(fighter.SLpM)

    # Strike accuracy
    attrs["Str_Acc"] = utils.percentage_to_float(fighter.Str_Acc)

    # Significant Strikes Absorbed per Minute.
    attrs["SApM"] = float(fighter.SApM)

    # strikes defended
    attrs["Str_Def"] = utils.percentage_to_float(fighter.Str_Def)

    # Takedown average
    attrs["TD_Avg"] = float(fighter.TD_Avg)

    # Takedown accuracy
    attrs["TD_Acc"] = utils.percentage_to_float(fighter.TD_Acc)

    # Takedown defense
    attrs["TD_Def"] = utils.percentage_to_float(fighter.TD_Def)

    # Submission average
    attrs["Sub_Avg"] = float(fighter.Sub_Avg)
    return attrs


def write_to_graph(record: Collect[dict], graph: falkordb.Graph) -> int:
    """Take all records and then push to the DB"""
    records = list(record)
    # Load all fighters in one go.
    q = "UNWIND $fighters as fighter CREATE (f:Fighter) SET f = fighter"
    graph.query(q, {"fighters": records})
    return len(records)



---
File: /burr/examples/conversational-rag/graph_db_example/ingest_fights.py
---

"""
Hamilton module to ingest fight data into FalkorDB.
"""
import falkordb
import pandas as pd
import utils
from hamilton.htypes import Collect, Parallelizable


def raw_total_fight_data() -> pd.DataFrame:
    """Loads the raw fight data"""
    _df = pd.read_csv("../data/raw_total_fight_data.csv", delimiter=";")
    return _df


def transformed_data(raw_total_fight_data: pd.DataFrame) -> pd.DataFrame:
    """Does some light transformation on the data and adds some columns"""
    _df = raw_total_fight_data
    _df.last_round = _df.last_round.astype(int)
    _df.last_round_time = _df.last_round_time.apply(lambda x: utils.time_to_seconds(x))
    _df.date = _df.date.apply(lambda x: utils.date_to_timestamp(x))
    _df["Loser"] = _df.apply(
        lambda x: x["B_fighter"] if x["Winner"] == x["R_fighter"] else x["R_fighter"], axis=1
    )
    return _df


def columns_of_interest() -> list[str]:
    """Returns the columns that we're interested in processing"""
    return [
        "R_fighter",
        "B_fighter",
        #    R_KD, B_KD, R_SIG_STR.,B_SIG_STR.,
        # R_SIG_STR_pct, B_SIG_STR_pct, R_TOTAL_STR., B_TOTAL_STR.,
        # R_TD, B_TD, R_TD_pct, B_TD_pct, R_SUB_ATT, B_SUB_ATT,
        # R_REV, B_REV, R_CTRL, B_CTRL, R_HEAD, B_HEAD, R_BODY,
        # B_BODY, R_LEG, B_LEG, R_DISTANCE, B_DISTANCE, R_CLINCH,
        # B_CLINCH, R_GROUND, B_GROUND, win_by,
        "last_round",
        "last_round_time",
        "Format",
        "Referee",
        "date",
        "location",
        "Fight_type",
        "Winner",
        "Loser",
    ]


def fight(
    transformed_data: pd.DataFrame,
    columns_of_interest: list[str],
) -> Parallelizable[pd.Series]:
    """Enables us to process each fight. We pass in a client along with each row"""
    for _idx, _row in transformed_data[columns_of_interest].iterrows():
        yield _row


def write_to_graph(fight: pd.Series, graph: falkordb.Graph) -> str:
    _row, _graph = fight, graph
    # create referee
    q = "MERGE (:Referee {Name: $name})"
    _graph.query(q, {"name": _row.Referee if isinstance(_row.Referee, str) else ""})

    # create card
    q = "MERGE (c:Card {Date: $date, Location: $location})"
    _graph.query(q, {"date": _row.date, "location": _row.location})

    # create fight
    q = """MATCH (c:Card {Date: $date, Location: $location})
           MATCH (ref:Referee {Name: $referee})
           MATCH (r:Fighter {Name:$R_fighter})
           MATCH (b:Fighter {Name:$B_fighter})
           CREATE (f:Fight)-[:PART_OF]->(c)
           SET f = $fight
           CREATE (f)-[:RED]->(r)
           CREATE (f)-[:BLUE]->(b)
           CREATE (ref)-[:REFEREED]->(f)
           RETURN ID(f)
        """
    f_id = _graph.query(
        q,
        {
            "date": _row.date,
            "location": _row.location,
            "referee": _row.Referee if isinstance(_row.Referee, str) else "",
            "R_fighter": _row.R_fighter,
            "B_fighter": _row.B_fighter,
            "fight": {
                "Last_round": _row.last_round,
                "Last_round_time": _row.last_round_time,
                "Format": _row.Format,
                "Fight_type": _row.Fight_type,
            },
        },
    ).result_set[0][0]

    # mark winner & loser
    q = """MATCH (f:Fight) WHERE ID(f) = $fight_id
           MATCH (l:Fighter {Name:$loser})
           MATCH (w:Fighter {Name:$winner})
           CREATE (w)-[:WON]->(f), (l)-[:LOST]->(f)
        """
    _graph.query(
        q,
        {
            "fight_id": f_id,
            "loser": _row.Loser,
            "winner": _row.Winner if isinstance(_row.Winner, str) else "",
        },
    )
    return "success"


def collect_writes(write_to_graph: Collect[str]) -> int:
    return len(list(write_to_graph))



---
File: /burr/examples/conversational-rag/graph_db_example/utils.py
---

"""
Code courtesy of the FalkorDB.
"""
from datetime import datetime


# convert hight in feet and inches to centimeters
def height_to_cm(height):
    # Split the height string into feet and inches
    feet, inches = height.split("'")
    feet = int(feet)
    inches = int(inches.replace('"', ""))

    # Convert feet and inches to centimeters
    total_inches = feet * 12 + inches
    cm = total_inches * 2.54

    return cm


# Convert reach from inches to centimeters
def reach_to_cm(reach):
    # Convert inches to centimeters
    inches = int(reach.replace('"', ""))
    return inches * 2.54


# Convert datetime to UNIX timestamp
def date_to_timestamp(date_str):
    # Parse the date string into a datetime object
    try:
        date_obj = datetime.strptime(date_str, "%b %d, %Y")
    except ValueError:
        try:
            date_obj = datetime.strptime(date_str, "%B %d, %Y")
        except ValueError:
            return None

    # Convert the datetime object to a Unix timestamp
    timestamp = date_obj.timestamp()

    return int(timestamp)


# Convert time in min:seconds format to number of seconds
def time_to_seconds(time):
    # 1:58
    minutes, seconds = map(int, time.split(":"))
    return minutes * 60 + seconds


# Convert percentage to float
def percentage_to_float(precentage):
    p = int(precentage.replace("%", ""))
    return float(p / 100.0)


# Convert ratio in the format x of y to float
def percentage_from_ratio(ratio):
    # 41 of 103
    count, total = map(int, ratio.split(" of "))
    if total == 0:
        return 0.0

    return float(count / total)



---
File: /burr/examples/conversational-rag/simple_example/__init__.py
---




---
File: /burr/examples/conversational-rag/simple_example/application.py
---

import pprint
from typing import List, Optional, Tuple

from hamilton import dataflows, driver

import burr.core
from burr.core import Action, Application, ApplicationBuilder, State, default, expr
from burr.core.action import action
from burr.core.graph import GraphBuilder
from burr.lifecycle import LifecycleAdapter, PostRunStepHook, PreRunStepHook

# create the pipeline
conversational_rag = dataflows.import_module("conversational_rag")
conversational_rag_driver = (
    driver.Builder()
    .with_config({})  # replace with configuration as appropriate
    .with_modules(conversational_rag)
    .build()
)


def bootstrap_vector_db(rag_driver: driver.Driver, input_texts: List[str]) -> object:
    """Bootstrap the vector database with some input texts."""
    return rag_driver.execute(["vector_store"], inputs={"input_texts": input_texts})["vector_store"]


class PrintStepHook(PostRunStepHook, PreRunStepHook):
    """Custom hook to print the action/result after each step."""

    def pre_run_step(self, action: Action, **future_kwargs):
        if action.name == "ai_converse":
            print("ðŸ¤” AI is thinking...")
        if action.name == "human_converse":
            print("â³Processing input from user...")

    def post_run_step(self, *, state: "State", action: Action, result: dict, **future_kwargs):
        if action.name == "human_converse":
            print("ðŸŽ™ðŸ’¬", result["question"], "\n")
        if action.name == "ai_converse":
            print("ðŸ¤–ðŸ’¬", result["conversational_rag_response"], "\n")


@action(
    reads=["question", "chat_history"],
    writes=["chat_history"],
)
def ai_converse(state: State, vector_store: object) -> Tuple[dict, State]:
    """AI conversing step. Uses Hamilton to execute the conversational pipeline."""
    result = conversational_rag_driver.execute(
        ["conversational_rag_response"],
        inputs={
            "question": state["question"],
            "chat_history": state["chat_history"],
        },
        # we use overrides here because we want to pass in the vector store
        overrides={
            "vector_store": vector_store,
        },
    )
    new_history = f"AI: {result['conversational_rag_response']}"
    return result, state.append(chat_history=new_history)


@action(
    reads=[],
    writes=["question", "chat_history"],
)
def human_converse(state: State, user_question: str) -> Tuple[dict, State]:
    """Human converse step -- make sure we get input, and store it as state."""
    state = state.update(question=user_question).append(chat_history=f"Human: {user_question}")
    return {"question": user_question}, state


def graph():
    input_text = [
        "harrison worked at kensho",
        "stefan worked at Stitch Fix",
        "stefan likes tacos",
        "elijah worked at TwoSigma",
        "elijah likes mango",
        "stefan used to work at IBM",
        "elijah likes to go biking",
        "stefan likes to bake sourdough",
    ]
    vector_store = bootstrap_vector_db(conversational_rag_driver, input_text)
    return (
        GraphBuilder()
        .with_actions(
            # bind the vector store to the AI conversational step
            ai_converse=ai_converse.bind(vector_store=vector_store),
            human_converse=human_converse,
            terminal=burr.core.Result("chat_history"),
        )
        .with_transitions(
            ("ai_converse", "human_converse", default),
            ("human_converse", "terminal", expr("'exit' in question")),
            ("human_converse", "ai_converse", default),
        )
        .build()
    )


def application(
    app_id: Optional[str] = None,
    storage_dir: Optional[str] = "~/.burr",
    hooks: Optional[List[LifecycleAdapter]] = None,
) -> Application:
    # our initial knowledge base

    app = (
        ApplicationBuilder()
        .with_state(
            **{
                "question": "",
                "chat_history": [],
            }
        )
        .with_graph(graph())
        .with_entrypoint("human_converse")
        .with_tracker(project="demo_conversational-rag", params={"storage_dir": storage_dir})
        .with_identifiers(app_id=app_id, partition_key="sample_user")
        .with_hooks(*hooks if hooks else [])
        .build()
    )
    return app


def main():
    """This is one way -- you provide input via the control flow"""
    app = application(hooks=[PrintStepHook()])
    # Comment back in to visualize
    # app.visualize(
    #     output_file_path="conversational_rag", include_conditions=True, view=True, format="png"
    # )
    print(f"Running RAG with initial state:\n {pprint.pformat(app.state.get_all())}")
    while True:
        user_question = input("Ask something (or type exit to quit): ")
        previous_action, result, state = app.run(
            halt_before=["human_converse"],
            halt_after=["terminal"],
            inputs={"user_question": user_question},
        )
        if previous_action.name == "terminal":
            # reached the end
            pprint.pprint(result)
            return


if __name__ == "__main__":
    main()



---
File: /burr/examples/conversational-rag/__init__.py
---




---
File: /burr/examples/custom-serde/__init__.py
---




---
File: /burr/examples/custom-serde/application.py
---

import pydantic
from langchain_core import documents

from burr import core
from burr.core import State, action, expr, serde, state
from burr.tracking import client as tracking_client


# say we have have a custom class that we want to serialize
# and deserialize using custom serde
class CustomClass(object):
    """Custom class we'll use to test custom serde"""

    def __init__(self, value):
        self.value = value

    def __eq__(self, other):
        # required for asserts etc to work
        return self.value == other.value


# create custom serializer for the custom class & register it
@serde.serialize.register(CustomClass)
def serialize_customclass(value: CustomClass, **kwargs) -> dict:
    """Serializes the custom class however we want

    :param value: the value to serialize.
    :param kwargs:
    :return: dictionary of serde.KEY and value
    """
    return {
        serde.KEY: "CustomClass",  # this has to map to the value regisered for the deserializer below
        "value": f"[value=={value.value}]",  # serialize the value however we want
    }


# create custom deserializer for the custom class & register it
@serde.deserializer.register("CustomClass")
def deserialize_customclass(value: dict, **kwargs) -> CustomClass:
    """Deserializes the value using whatever meands we want

    :param value: the value to deserialize from.
    :param kwargs:
    :return: CustomClass
    """
    # deserialize the value however we want
    return CustomClass(value=value["value"].split("==")[1][0:-1])


# register custom serde for the custom field
def my_field_serializer(value: documents.Document, **kwargs) -> dict:
    serde_value = f"serialized::{value.page_content}"
    return {"value": serde_value}


def my_field_deserializer(value: dict, **kwargs) -> documents.Document:
    serde_value = value["value"]
    return documents.Document(page_content=serde_value.replace("serialized::", ""))


state.register_field_serde("custom_field", my_field_serializer, my_field_deserializer)

# --- define the actions


@action(reads=[], writes=["dict"])
def basic_action(state: State, user_input: str) -> tuple[dict, State]:
    v = {
        "foo": 1,
        "bar": CustomClass("example value"),
        "bool": True,
        "None": None,
        "input": user_input,
    }
    return {"dict": v}, state.update(dict=v)


class PydanticField(pydantic.BaseModel):
    """burr handles serializing custom pydantic fields"""

    f1: int = 0
    f2: bool = False


@action(reads=["dict"], writes=["pydantic_field"])
def pydantic_action(state: State) -> tuple[dict, State]:
    v = PydanticField(f1=state["dict"]["foo"], f2=state["dict"]["bool"])
    return {"pydantic_field": v}, state.update(pydantic_field=v)


@action(reads=["pydantic_field"], writes=["lc_doc"])
def langchain_action(state: State) -> tuple[dict, State]:
    v = documents.Document(
        page_content=f"foo: {state['pydantic_field'].f1}, bar: {state['pydantic_field'].f2}"
    )
    return {"lc_doc": v}, state.update(lc_doc=v)


@action(reads=["lc_doc"], writes=[])
def terminal_action(state: State) -> tuple[dict, State]:
    return {"output": state["lc_doc"].page_content}, state


# build the application
def build_application(partition_key, app_id):
    """Builds the application"""
    tracker = tracking_client.LocalTrackingClient("serde-example")
    app = (
        core.ApplicationBuilder()
        .with_actions(basic_action, pydantic_action, langchain_action, terminal_action)
        .with_transitions(
            ("basic_action", "terminal_action", expr("dict['foo'] == 0")),
            ("basic_action", "pydantic_action"),
            ("pydantic_action", "langchain_action"),
            ("langchain_action", "terminal_action"),
        )
        .with_identifiers(partition_key=partition_key, app_id=app_id)
        .initialize_from(
            tracker,
            resume_at_next_action=True,
            default_state={
                "custom_field": documents.Document(
                    page_content="this is a custom field to serialize"
                )
            },
            default_entrypoint="basic_action",
        )
        .with_tracker(tracker)
        .build()
    )
    return app


if __name__ == "__main__":
    """
    Note: if you try to use the test case creation commandline from the result
    of running this application using the code below it will
    fail because the above SERDE classes were registered/run as the `__main__`
    module. You can either move the SERDE classes to a separate module so
    that you could import them directly, or run the code below, but in another file (e.g. run.py).
    """
    import pprint
    import uuid

    # build
    app = build_application("client-123", str(uuid.uuid4()))
    app.visualize(
        output_file_path="statemachine", include_conditions=True, include_state=True, format="png"
    )
    # run
    action, result, state = app.run(
        halt_after=["terminal_action"], inputs={"user_input": "hello world"}
    )
    # serialize
    serialized_state = state.serialize()
    pprint.pprint(serialized_state)
    # deserialize
    deserialized_state = State.deserialize(serialized_state)
    # assert that the state is the same after serialization and deserialization
    assert state.get_all() == deserialized_state.get_all()



---
File: /burr/examples/custom-serde/run.py
---

"""
Example of running the application
from another module to make sure the
SERDE classes are registered in a non __main__
module namespace.

e.g. python run.py
and then
burr-test-case create --project-name serde-example --app-id APP_ID --sequence-id 3 --serde-module application.py
"""
import pprint
import uuid

import application  # noqa
from application import build_application

from burr.core import State

# build
app = build_application("client-123", str(uuid.uuid4()))
app.visualize(
    output_file_path="statemachine", include_conditions=True, include_state=True, format="png"
)
# run
action, result, state = app.run(
    halt_after=["terminal_action"], inputs={"user_input": "hello world"}
)
# serialize
serialized_state = state.serialize()
pprint.pprint(serialized_state)
# deserialize
deserialized_state = State.deserialize(serialized_state)
# assert that the state is the same after serialization and deserialization
assert state.get_all() == deserialized_state.get_all()



---
File: /burr/examples/deep-researcher/__init__.py
---




---
File: /burr/examples/deep-researcher/application.py
---

import functools
import importlib
import json
from typing import Optional

import openai

from burr.core import Application, ApplicationBuilder, State, action, expr, when
from burr.core.graph import GraphBuilder
from burr.tracking import LocalTrackingClient

try:
    prompts = importlib.import_module("burr.examples.deep-researcher.prompts")
except ModuleNotFoundError:
    import prompts

try:
    utils = importlib.import_module("burr.examples.deep-researcher.utils")
except ModuleNotFoundError:
    import utils


@functools.lru_cache
def _get_openai_client():
    """
    Creates and caches an OpenAI client instance.

    Returns:
        openai.Client: A cached OpenAI client instance.
    """
    openai_client = openai.Client()
    return openai_client


def query_openai(system_instructions, human_message_content, stream=False):
    """
    Sends a query to the OpenAI API and retrieves the response.

    Args:
        system_instructions (str): Instructions for the system message.
        human_message_content (str): Content of the human message.
        stream (bool, optional): Whether to stream the response. Defaults to False.

    Returns:
        str: The content of the response from the OpenAI API.
    """
    client = _get_openai_client()
    system_message = {"role": "system", "content": system_instructions}
    human_message = {"role": "user", "content": human_message_content}
    messages = []
    messages.append(system_message)
    messages.append(human_message)

    response = client.chat.completions.create(model="gpt-4o", messages=messages, stream=stream)
    content = response.choices[0].message.content
    return content


@action(
    reads=[],
    writes=[
        "search_query",
        "research_topic",
        "sources_gathered",
        "web_research_results",
        "research_loop_count",
        "running_summary",
    ],
)
def generate_query(state: State, research_topic: str) -> State:
    """
    Generates a search query based on the research topic.

    Args:
        state (State): The current application state.
        research_topic (str): The topic to research.

    Returns:
        State: The updated state with the research topic and generated search query.
    """
    system_prompt_formatted = prompts.query_writer_instructions.format(
        research_topic=research_topic
    )
    human_prompt = "Generate a query for web search:"
    my_query = query_openai(system_prompt_formatted, human_prompt)
    as_dict = json.loads(my_query)
    return state.update(
        search_query=as_dict["query"],
        research_topic=research_topic,
        sources_gathered=[],
        web_research_results=[],
        research_loop_count=0,
        running_summary=None,
    )


@action(
    reads=[
        "search_query",
        "research_loop_count",
        "sources_gathered",
        "web_research_results",
    ],
    writes=["sources_gathered", "research_loop_count", "web_research_results"],
)
def web_research(state: State) -> State:
    """
    Performs web research based on the search query and updates the state.

    Args:
        state (State): The current application state.

    Returns:
        State: The updated state with gathered sources, research loop count, and web research results.
    """
    search_results = utils.tavily_search(
        state["search_query"], include_raw_content=True, max_results=1
    )
    search_str = utils.deduplicate_and_format_sources(
        search_results, max_tokens_per_source=1000, include_raw_content=True
    )
    sources_gathered = [utils.format_sources(search_results)]
    sources_gathered = state["sources_gathered"] + sources_gathered
    web_research_results = state["web_research_results"] + [search_str]
    research_loop_count = state["research_loop_count"] + 1

    return state.update(
        sources_gathered=sources_gathered,
        research_loop_count=research_loop_count,
        web_research_results=web_research_results,
    )


@action(
    reads=["running_summary", "web_research_results", "research_topic"],
    writes=["running_summary"],
)
def summarize_sources(state: State):
    """
    Summarizes the gathered sources and updates the running summary.

    Args:
        state (State): The current application state.

    Returns:
        State: The updated state with the new running summary.
    """
    existing_summary = state["running_summary"]
    most_recent_web_research = state["web_research_results"][-1]
    research_topic = state["research_topic"]

    if existing_summary:
        human_message_content = (
            f"<User Input> \n {research_topic} \n <User Input>\n\n"
            f"<Existing Summary> \n {existing_summary} \n <Existing Summary>\n\n"
            f"<New Search Results> \n {most_recent_web_research} \n <New Search Results>"
        )
    else:
        human_message_content = (
            f"<User Input> \n {research_topic} \n <User Input>\n\n"
            f"<Search Results> \n {most_recent_web_research} \n <Search Results>"
        )
    running_summary = query_openai(prompts.summarizer_instructions, human_message_content)

    while "<think>" in running_summary and "</think>" in running_summary:
        start = running_summary.find("<think>")
        end = running_summary.find("</think>") + len("</think>")
        running_summary = running_summary[:start] + running_summary[end:]
    return state.update(running_summary=running_summary)


@action(reads=["running_summary", "research_topic"], writes=["search_query"])
def reflect_on_summary(state: State):
    """
    Reflects on the running summary to identify knowledge gaps and generate a follow-up query.

    Args:
        state (State): The current application state.

    Returns:
        State: The updated state with the follow-up search query.
    """
    system_instructions = prompts.reflection_instructions.format(
        research_topic=state.get("research_topic")
    )
    human_message_content = f"Identify a knowledge gap and generate a follow-up web search query based on our existing knowledge: {state.get('running_summary')}"
    content = query_openai(system_instructions, human_message_content)
    follow_up_query = json.loads(content)

    query = follow_up_query.get("follow_up_query")
    if not query:
        fallback_query = f"Tell me more about {state.get('research_topic')}"
        state.update(search_query=fallback_query)
    return state.update(search_query=query or fallback_query)


@action(
    reads=["running_summary", "sources_gathered"],
    writes=["running_summary", "research_loop_count"],
)
def finalize_summary(state: State):
    """
    Finalizes the summary by combining the running summary and all gathered sources.

    Args:
        state (State): The current application state.

    Returns:
        State: The updated state with the finalized summary.
    """
    all_sources = "\n".join(source for source in state.get("sources_gathered"))
    running_summary = (
        f"## Summary\n\n{state.get('running_summary')}\n\n ### Sources:\n{all_sources}"
    )
    # reset research loop count
    return state.update(running_summary=running_summary, research_loop_count=0)


graph = (
    GraphBuilder()
    .with_actions(
        generate_query,
        web_research,
        summarize_sources,
        reflect_on_summary,
        finalize_summary,
    )
    .with_transitions(
        ("generate_query", "web_research"),
        ("web_research", "summarize_sources"),
        ("summarize_sources", "reflect_on_summary"),
        ("reflect_on_summary", "finalize_summary", when(research_loop_count=2)),
        (
            "reflect_on_summary",
            "web_research",
            expr("research_loop_count<2"),
        ),
        ("finalize_summary", "generate_query"),
    )
).build()


def application(
    app_id: Optional[str] = None,
    project: str = "demo_deep_researcher",
    username: str = None,
) -> Application:
    """
    Creates and configures an application instance for conducting research.

    Args:
        app_id (Optional[str]): A unique identifier for the application instance. Defaults to None.
        project (str): The project name for tracking. Defaults to "demo_deep_researcher".
        username (Optional[str]): The username associated with the application instance.

    Returns:
        Application: A configured application instance ready to run.
    """
    tracker = LocalTrackingClient(project=project)
    builder = (
        ApplicationBuilder()
        .with_graph(graph)
        .with_tracker("local", project=project)
        .with_identifiers(app_id=app_id, partition_key=username)
        .initialize_from(
            tracker,
            resume_at_next_action=True,
            default_state={"research_loop_count": 0, "running_summary": None},
            default_entrypoint="generate_query",
        )
    )
    return builder.build()


if __name__ == "__main__":
    """
    Entry point for the application. Initializes and runs the research application.
    """
    research_topic = "getting a job in datascience"
    app_id = "1"

    app = application(app_id=app_id)
    app.visualize(
        output_file_path="statemachine",
        include_conditions=True,
        view=False,
        format="png",
    )
    action, state, result = app.run(
        halt_after=["finalize_summary"], inputs={"research_topic": research_topic}
    )
    print(app.state["running_summary"])



---
File: /burr/examples/deep-researcher/prompts.py
---

"""
Based on code from https://github.com/langchain-ai/local-deep-researcher/tree/005db90331e116eb3edb4e9b43822136b211444e/src/ollama_deep_researcher
Copied under the MIT License.
"""

query_writer_instructions = """Your goal is to generate a targeted web search query.
The query will gather information related to a specific topic.

<TOPIC>
{research_topic}
</TOPIC>

<FORMAT>
Format your response as a json object with ALL three of these exact keys:
   - "query": The actual search query string
   - "aspect": The specific aspect of the topic being researched
   - "rationale": Brief explanation of why this query is relevant
Do not use markdown tags to indicate a json code block in the output.
</FORMAT>

<EXAMPLE>
Example output:
{{
    "query": "machine learning transformer architecture explained",
    "aspect": "technical architecture",
    "rationale": "Understanding the fundamental structure of transformer models"
}}
</EXAMPLE>

Provide your response in json format:"""

summarizer_instructions = """
<GOAL>
Generate a high-quality summary of the web search results and keep it concise / related to the user topic.
</GOAL>

<REQUIREMENTS>
When creating a NEW summary:
1. Highlight the most relevant information related to the user topic from the search results
2. Ensure a coherent flow of information

When EXTENDING an existing summary:
1. Read the existing summary and new search results carefully.
2. Compare the new information with the existing summary.
3. For each piece of new information:
    a. If it's related to existing points, integrate it into the relevant paragraph.
    b. If it's entirely new but relevant, add a new paragraph with a smooth transition.
    c. If it's not relevant to the user topic, skip it.
4. Ensure all additions are relevant to the user's topic.
5. Verify that your final output differs from the input summary.
< /REQUIREMENTS >

< FORMATTING >
- Start directly with the updated summary, without preamble or titles. Do not use XML tags in the output.
< /FORMATTING >"""

reflection_instructions = """You are an expert research assistant analyzing a summary about {research_topic}.

<GOAL>
1. Identify knowledge gaps or areas that need deeper exploration
2. Generate a follow-up question that would help expand your understanding
3. Focus on technical details, implementation specifics, or emerging trends that weren't fully covered
</GOAL>

<REQUIREMENTS>
Ensure the follow-up question is self-contained and includes necessary context for web search.
</REQUIREMENTS>

<FORMAT>
Format your response as a json object with these exact keys:
- knowledge_gap: Describe what information is missing or needs clarification
- follow_up_query: Write a specific question to address this gap

Do not use markdown tags to indicate a json code block in the output.
</FORMAT>

<EXAMPLE>
Example output:
{{
    "knowledge_gap": "The summary lacks information about performance metrics and benchmarks",
    "follow_up_query": "What are typical performance benchmarks and metrics used to evaluate [specific technology]?"
}}
</EXAMPLE>

Provide your analysis in json format:"""



---
File: /burr/examples/deep-researcher/server.py
---

import functools
import importlib

# from fastapi import FastAPI
import os
from typing import Optional

from fastapi import APIRouter

from burr.core import Application, ApplicationBuilder
from burr.tracking import LocalTrackingClient

# uncomment for local testing
# import application as deep_researcher_application

# We're doing dynamic import cause this lives within examples/ (and that module has dashes)
# navigate to the examples directory to read more about this!
deep_researcher_application = importlib.import_module(
    "burr.examples.deep-researcher.application"
)  # noqa: F401

import pydantic

# uncomment for local testing
# app = FastAPI()
router = APIRouter()

try:
    from opentelemetry.instrumentation.openai import OpenAIInstrumentor

    OpenAIInstrumentor().instrument()
    opentelemetry_available = True
except ImportError:
    opentelemetry_available = False

# pydantic types


class ResearchSummary(pydantic.BaseModel):
    running_summary: str


@functools.lru_cache(maxsize=128)
def _get_application(project_id: str, app_id: str) -> Application:
    """Utility to get the application. Depending on
    your use-case you might want to reload from state every time (or consider cache invalidation)"""
    graph = deep_researcher_application.graph
    tracker = LocalTrackingClient(project=project_id)
    builder = (
        ApplicationBuilder()
        .with_graph(graph)
        .with_tracker(
            tracker := LocalTrackingClient(project=project_id),
            use_otel_tracing=opentelemetry_available,
        )
        .with_identifiers(app_id=app_id)
        .initialize_from(
            tracker,
            resume_at_next_action=True,
            default_state={"research_loop_count": 0, "running_summary": None},
            default_entrypoint="generate_query",
        )
    )
    return builder.build()


@router.post("/response/{project_id}/{app_id}")
def research_response(project_id: str, app_id: str, topic: str) -> ResearchSummary:
    burr_app = _get_application(project_id, app_id)
    research_topic = topic
    action, state, result = burr_app.run(
        halt_after=["finalize_summary"], inputs={"research_topic": research_topic}
    )
    summary = burr_app.state.get("running_summary")
    # reset state machine?
    return {"running_summary": summary}


@router.post("/create/{project_id}/{app_id}")
def create_new_application(project_id: str, app_id: str) -> str:
    app = _get_application(project_id, app_id)
    return app.uid


@router.get("/validate")
def validate_environment() -> Optional[str]:
    """Validate the environment"""
    has_openai_api_key = "OPENAI_API_KEY" in os.environ
    has_tavily_api_key = "TAVILY_API_KEY" in os.environ
    if has_openai_api_key and has_tavily_api_key:
        return
    message = ""
    openai_api_message = """
        You have not set an API key for [OpenAI](https://www.openai.com). Do this
        by setting the environment variable `OPENAI_API_KEY` to your key.
        You can get a key at https://platform.openai.com.
    """
    tavily_api_message = """
        You have not set an API key for [Tavily Search](https://tavily.com). Do this
        by setting the environment variable `TAVILY_API_KEY` to your key.
        You can get a key at https://app.tavily.com/home.
    """
    if not has_openai_api_key:
        message = message + openai_api_message
    if not has_tavily_api_key:
        message = message + tavily_api_message
    return message


# uncomment for local testing
# @app.get("/")
# def root():
#    return {"message": "Deep Researcher API"}


# app.include_router(router, prefix="/deep_researcher", tags=["deep-researcher-api"])


if __name__ == "__main__":
    pass
    # uncomment for local testing
    # import uvicorn
    # uvicorn.run(app, host="localhost", port=7242)



---
File: /burr/examples/deep-researcher/utils.py
---

"""
Based on code from https://github.com/langchain-ai/local-deep-researcher/tree/005db90331e116eb3edb4e9b43822136b211444e/src/ollama_deep_researcher
Copied under the MIT License.
"""
import logging

logger = logging.getLogger(__name__)

try:
    from tavily import TavilyClient
except ImportError:
    logger.warning(
        "Please install tavily with `pip install tavily-python` if you want the deep research example to function."
    )
    TavilyClient = None


def deduplicate_and_format_sources(
    search_response, max_tokens_per_source, include_raw_content=False
):
    """
    Takes either a single search response or list of responses from search APIs and formats them.
    Limits the raw_content to approximately max_tokens_per_source.
    include_raw_content specifies whether to include the raw_content from Tavily in the formatted string.

    Args:
        search_response: Either:
            - A dict with a 'results' key containing a list of search results
            - A list of dicts, each containing search results

    Returns:
        str: Formatted string with deduplicated sources
    """
    # Convert input to list of results
    if isinstance(search_response, dict):
        sources_list = search_response["results"]
    elif isinstance(search_response, list):
        sources_list = []
        for response in search_response:
            if isinstance(response, dict) and "results" in response:
                sources_list.extend(response["results"])
            else:
                sources_list.extend(response)
    else:
        raise ValueError("Input must be either a dict with 'results' or a list of search results")

    # Deduplicate by URL
    unique_sources = {}
    for source in sources_list:
        if source["url"] not in unique_sources:
            unique_sources[source["url"]] = source

    # Format output
    formatted_text = "Sources:\n\n"
    for i, source in enumerate(unique_sources.values(), 1):
        formatted_text += f"Source {source['title']}:\n===\n"
        formatted_text += f"URL: {source['url']}\n===\n"
        formatted_text += f"Most relevant content from source: {source['content']}\n===\n"
        if include_raw_content:
            # Using rough estimate of 4 characters per token
            char_limit = max_tokens_per_source * 4
            # Handle None raw_content
            raw_content = source.get("raw_content", "")
            if raw_content is None:
                raw_content = ""
                print(f"Warning: No raw_content found for source {source['url']}")
            if len(raw_content) > char_limit:
                raw_content = raw_content[:char_limit] + "... [truncated]"
            formatted_text += (
                f"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\n\n"
            )

    return formatted_text.strip()


def format_sources(search_results):
    """Format search results into a bullet-point list of sources.

    Args:
        search_results (dict): Tavily search response containing results

    Returns:
        str: Formatted string with sources and their URLs
    """
    return "\n".join(
        f"* {source['title']} : {source['url']}" for source in search_results["results"]
    )


def tavily_search(query, include_raw_content=True, max_results=3):
    """Search the web using the Tavily API.

    Args:
        query (str): The search query to execute
        include_raw_content (bool): Whether to include the raw_content from Tavily in the formatted string
        max_results (int): Maximum number of results to return

    Returns:
        dict: Search response containing:
            - results (list): List of search result dictionaries, each containing:
                - title (str): Title of the search result
                - url (str): URL of the search result
                - content (str): Snippet/summary of the content
                - raw_content (str): Full content of the page if available"""

    tavily_client = TavilyClient()
    return tavily_client.search(
        query, max_results=max_results, include_raw_content=include_raw_content
    )



---
File: /burr/examples/deployment/aws/lambda/app/__init__.py
---




---
File: /burr/examples/deployment/aws/lambda/app/counter_app.py
---

"""
This is a very simple counting application.

It's here to help you get the mechanics of deploying a Burr application to AWS Lambda.
"""

import time

import burr.core
from burr.core import Application, Result, State, default, expr
from burr.core.action import action
from burr.core.graph import GraphBuilder


@action(reads=["counter"], writes=["counter"])
def counter(state: State) -> State:
    result = {"counter": state["counter"] + 1}
    time.sleep(0.5)  # sleep to simulate a longer running function
    return state.update(**result)


# our graph.
graph = (
    GraphBuilder()
    .with_actions(counter=counter, result=Result("counter"))
    .with_transitions(
        ("counter", "counter", expr("counter < counter_limit")),
        ("counter", "result", default),
    )
    .build()
)


def application(count_up_to: int = 10) -> Application:
    """function to return a burr application"""
    return (
        burr.core.ApplicationBuilder()
        .with_graph(graph)
        .with_state(**{"counter": 0, "counter_limit": count_up_to})
        .with_entrypoint("counter")
        .build()
    )



---
File: /burr/examples/deployment/aws/lambda/app/lambda_handler.py
---

from . import counter_app


def lambda_handler(event, context):
    count_up_to = int(event["body"]["number"])

    app = counter_app.application(count_up_to)
    action, result, state = app.run(halt_after=["result"])

    return {"statusCode": 200, "body": state.serialize()}


if __name__ == "__main__":
    print(lambda_handler({"body": {"number": 10}}, None))



---
File: /burr/examples/email-assistant/__init__.py
---




---
File: /burr/examples/email-assistant/application.py
---

import functools
import os
from typing import Tuple

import openai

from burr.core import Application, ApplicationBuilder, State, action, expr
from burr.core.graph import GraphBuilder
from burr.tracking import LocalTrackingClient


@functools.lru_cache
def _get_openai_client():
    openai_client = openai.Client()
    return openai_client


@action(reads=[], writes=["incoming_email", "response_instructions"])
def process_input(state: State, email_to_respond: str, response_instructions: str) -> State:
    """Processes input from user and updates state with the input."""
    result = {"incoming_email": email_to_respond, "response_instructions": response_instructions}
    return state.update(**result)


@action(reads=[], writes=["has_openai_key"])
def check_openai_key(state: State) -> State:
    result = {"has_openai_key": "OPENAI_API_KEY" in os.environ}
    return state.update(**result)


@action(reads=["response_instructions", "incoming_email"], writes=["clarification_questions"])
def determine_clarifications(state: State) -> State:
    """Determines if the response instructions require clarification."""
    # TODO -- query an LLM to determine if the response instructions are clear, or if it needs more information
    incoming_email = state["incoming_email"]
    response_instructions = state["response_instructions"]
    client = _get_openai_client()
    # TODO -- use instructor to get a pydantic model
    result = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {
                "role": "system",
                "content": "You are a chatbot that has the task of generating responses to an email on behalf of a user. ",
            },
            {
                "role": "user",
                "content": (
                    f"The email you are to respond to is: {incoming_email}."
                    f"Your instructions are: {response_instructions}."
                    "Your first task is to ask any clarifying questions for the user"
                    " who is asking you to respond to this email. These clarifying questions are for the user, "
                    "*not* for the original sender of the email. Please "
                    "generate a list of at most 3 questions (and you really can do less -- 2, 1, or even none are OK! joined by newlines, included only if you feel that you could leverage "
                    "clarification (my time is valuable)."
                    "The questions, joined by newlines, must be the only text you return. If you do not need clarification, return an empty string."
                ),
            },
        ],
    )
    content = result.choices[0].message.content
    all_questions = content.split("\n") if content else []
    return state.update(clarification_questions=all_questions)


@action(reads=["clarification_questions"], writes=["clarification_answers"])
def clarify_instructions(state: State, clarification_inputs: list[str]) -> State:
    """Clarifies the response instructions if needed."""
    clarification_answers = list(clarification_inputs)
    return state.update(clarification_answers=clarification_answers)


@action(
    reads=[
        "incoming_email",
        "response_instructions",
        "clarification_answers",
        "clarification_questions",
        "draft_history",
        "feedback",
    ],
    writes=["current_draft", "draft_history"],
)
def formulate_draft(state: State) -> Tuple[dict, State]:
    """Formulates the draft response based on the incoming email, response instructions, and any clarifications."""
    # TODO -- query an LLM to generate the draft response
    incoming_email = state["incoming_email"]
    response_instructions = state["response_instructions"]
    client = _get_openai_client()
    # TODO -- use instructor to get a pydantic model
    clarification_answers_formatted_q_a = "\n".join(
        [
            f"Q: {q}\nA: {a}"
            for q, a in zip(
                state["clarification_questions"], state.get("clarification_answers", [])
            )
        ]
    )
    instructions = [
        f"The email you are to respond to is: {incoming_email}.",
        f"Your instructions are: {response_instructions}.",
        "You have already asked the following questions and received the following answers: ",
        clarification_answers_formatted_q_a,
    ]
    if state["draft_history"]:
        instructions.append("Your previous draft was: ")
        instructions.append(state["draft_history"][-1])
        instructions.append(
            "you received the following feedback, please incorporate this into your response: "
        )
        instructions.append(state["feedback"])
    instructions.append("Please generate a draft response using all this information!")
    prompt = " ".join(instructions)

    result = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {
                "role": "system",
                "content": "You are a chatbot that has the task of generating responses to an email. ",
            },
            {"role": "user", "content": prompt},
        ],
    )
    content = result.choices[0].message.content
    # returning some intermediate results for debugging as well
    return {"prompt": prompt, "current_draft": content}, state.update(current_draft=content).append(
        draft_history=content
    )


@action(reads=[], writes=["feedback", "feedback_history"])
def process_feedback(state: State, feedback: str) -> Tuple[dict, State]:
    """Processes feedback from user and updates state with the feedback."""
    result = {"feedback": feedback}
    return result, state.update(feedback=feedback).append(feedback_history=feedback)


@action(reads=["current_draft", "feedback"], writes=["final_draft"])
def final_result(state: State) -> Tuple[dict, State]:
    """Returns the final result of the process."""
    result = {"final_draft": state["current_draft"]}
    return result, state.update(final_draft=result["final_draft"])


graph = (
    GraphBuilder()
    .with_actions(
        process_input,
        determine_clarifications,
        clarify_instructions,
        formulate_draft,
        process_feedback,
        final_result,
    )
    .with_transitions(
        ("process_input", "determine_clarifications"),
        (
            "determine_clarifications",
            "clarify_instructions",
            expr("len(clarification_questions) > 0"),
        ),
        ("determine_clarifications", "formulate_draft"),
        ("clarify_instructions", "formulate_draft"),
        ("formulate_draft", "process_feedback"),
        ("process_feedback", "formulate_draft", expr("len(feedback) > 0")),
        ("process_feedback", "final_result"),
    )
).build()


def application(
    app_id: str = None, project: str = "demo_email_assistant", username: str = None
) -> Application:
    tracker = LocalTrackingClient(project=project)
    builder = (
        ApplicationBuilder()
        .with_graph(graph)
        .with_tracker("local", project=project)
        .with_identifiers(app_id=app_id, partition_key=username)
        .initialize_from(
            tracker,
            resume_at_next_action=True,
            default_state={"draft_history": []},
            default_entrypoint="process_input",
        )
    )
    return builder.build()


if __name__ == "__main__":
    app = application()
    app.visualize(
        output_file_path="statemachine", include_conditions=True, include_state=True, format="png"
    )



---
File: /burr/examples/email-assistant/server.py
---

import functools

from burr.tracking import LocalTrackingClient

"""
This module contains the FastAPI code that interacts with the Burr application.
"""

import importlib
import os
from typing import Any, Dict, List, Literal, Optional

import pydantic
from fastapi import APIRouter, FastAPI

from burr.core import Application, ApplicationBuilder

email_assistant_application = importlib.import_module(
    "burr.examples.email-assistant.application"
)  # noqa: F401

app = FastAPI()
router = APIRouter()


try:
    from opentelemetry.instrumentation.openai import OpenAIInstrumentor

    OpenAIInstrumentor().instrument()
    opentelemetry_available = True
except ImportError:
    opentelemetry_available = False


# we want to render this after every response
class EmailAssistantState(pydantic.BaseModel):
    app_id: str
    email_to_respond: Optional[str]
    response_instructions: Optional[str]
    questions: Optional[List[str]]
    answers: Optional[List[str]]
    drafts: List[str]
    feedback_history: List[str]
    final_draft: Optional[str]
    # This stores the next step, which tells the frontend which ones to call
    next_step: Literal["process_input", "clarify_instructions", "process_feedback", None]

    @staticmethod
    def from_app(app: Application):
        state = app.state
        next_action = app.get_next_action()
        # TODO -- consolidate with the above

        if next_action is not None and next_action.name not in (
            "clarify_instructions",
            "process_feedback",
            "process_input",
        ):
            # quick hack -- this just means we're done if its an error
            # TODO -- add recovery
            next_action = None
        return EmailAssistantState(
            app_id=app.uid,
            email_to_respond=state.get("incoming_email"),
            response_instructions=state.get("response_instructions"),
            questions=state.get("clarification_questions"),
            answers=state.get("clarification_answers"),
            drafts=state.get("draft_history", []),
            feedback_history=state.get("feedback_history", []),
            final_draft=state.get("final_draft"),
            next_step=next_action.name if next_action is not None else None,
        )


@functools.lru_cache(maxsize=128)
def _get_application(project_id: str, app_id: str) -> Application:
    """Utility to get the application. Depending on
    your use-case you might want to reload from state every time (or consider cache invalidation)"""
    # graph = email_assistant_application.application(app_id=app_id, project=project_id)
    graph = email_assistant_application.graph
    tracker = LocalTrackingClient(project=project_id)
    builder = (
        ApplicationBuilder()
        .with_graph(graph)
        .with_tracker(
            tracker := LocalTrackingClient(project=project_id),
            use_otel_tracing=opentelemetry_available,
        )
        .with_identifiers(app_id=app_id)
        .initialize_from(
            tracker,
            resume_at_next_action=True,
            default_state={"draft_history": []},
            default_entrypoint="process_input",
        )
    )
    return builder.build()


def _run_through(project_id: str, app_id: str, inputs: Dict[str, Any]) -> EmailAssistantState:
    """This advances the state machine, moving through to the next 'halting' point"""
    if app_id == "create_new":  # quick hack to allow for null
        app_id = None
    email_assistant_app = _get_application(project_id, app_id)
    email_assistant_app.run(  # Using this as a side-effect, we'll just get the state aft
        halt_before=["clarify_instructions", "process_feedback"],
        halt_after=["final_result"],
        inputs=inputs,
    )
    return EmailAssistantState.from_app(email_assistant_app)


class DraftInit(pydantic.BaseModel):
    email_to_respond: str
    response_instructions: str


@router.post("/create_new/{project_id}/{app_id}")
def create_new_application(project_id: str, app_id: str) -> str:
    app = _get_application(project_id, app_id)
    return app.uid


@router.post("/create/{project_id}/{app_id}")
def initialize_draft(project_id: str, app_id: str, draft_data: DraftInit) -> EmailAssistantState:
    """Endpoint to initialize the draft with the email and instructions

    :param project_id: ID of the project (used by telemetry tracking/storage)
    :param app_id: ID of the application (used to reference the app)
    :param draft_data: Data to initialize the draft
    :return: The state of the application after initialization
    """
    return _run_through(
        project_id,
        app_id,
        dict(
            email_to_respond=draft_data.email_to_respond,
            response_instructions=draft_data.response_instructions,
        ),
    )


class QuestionAnswers(pydantic.BaseModel):
    answers: List[str]


@router.post("/answer_questions/{project_id}/{app_id}")
def answer_questions(
    project_id: str, app_id: str, question_answers: QuestionAnswers
) -> EmailAssistantState:
    """Endpoint to answer questions the LLM provides

    :param project_id: ID of the project (used by telemetry tracking/storage)
    :param app_id: ID of the application (used to reference the app)
    :param question_answers: Answers to the questions
    :return: The state of the application after answering the questions
    """
    return _run_through(project_id, app_id, dict(clarification_inputs=question_answers.answers))


class Feedback(pydantic.BaseModel):
    feedback: str


@router.post("/provide_feedback/{project_id}/{app_id}")
def provide_feedback(project_id: str, app_id: str, feedback: Feedback) -> EmailAssistantState:
    """Endpoint to provide feedback to the LLM

    :param project_id: ID of the project (used by telemetry tracking/storage)
    :param app_id: ID of the application (used to reference the app)
    :param feedback: Feedback to provide to the LLM
    :return: The state of the application after providing feedback
    """
    return _run_through(project_id, app_id, dict(feedback=feedback.feedback))


@router.get("/state/{project_id}/{app_id}")
def get_state(project_id: str, app_id: str) -> EmailAssistantState:
    """Get the current state of the application

    :param project_id: ID of the project (used by telemetry tracking/storage)
    :param app_id:  ID of the application (used to reference the app)
    :return: The state of the application
    """
    email_assistant_app = _get_application(project_id, app_id)
    return EmailAssistantState.from_app(email_assistant_app)


@router.get("/validate/{project_id}/{app_id}")
def validate_environment() -> Optional[str]:
    """Validate the environment"""
    if "OPENAI_API_KEY" in os.environ:
        return
    return (
        "You have not set an API key for [OpenAI](https://www.openai.com). Do this "
        "by setting the environment variable `OPENAI_API_KEY` to your key. "
        "You can get a key at https://platform.openai.com. "
        "You can still look at chat history/examples."
    )


app.include_router(router, prefix="/email_assistant", tags=["email-assistant-api"])


@app.get("/")
def root():
    return {"message": "Email Assistant API"}


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=7242)



---
File: /burr/examples/hamilton-integration/actions/__init__.py
---




---
File: /burr/examples/hamilton-integration/actions/ask_question.py
---

import lancedb
import openai


def relevant_chunks(user_query: str) -> list[dict]:
    chunks_table = lancedb.connect("./blogs").open_table("chunks")
    search_results = (
        chunks_table.search(user_query).select(["text", "url", "position"]).limit(3).to_list()
    )
    return search_results


def system_prompt(relevant_chunks: list[dict]) -> str:
    relevant_content = "\n".join([c["text"] for c in relevant_chunks])
    return (
        "Answer the user's questions based on the provided blog post content. "
        "Answer in a concise and helpful manner, and tell the user "
        "if you don't know the answer or you're unsure.\n\n"
        f"BLOG CONTENT:\n{relevant_content}"
    )


def llm_answer(system_prompt: str, user_query: str) -> str:
    client = openai.OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query},
        ],
    )
    return response.choices[0].message.content



---
File: /burr/examples/hamilton-integration/actions/ingest_blog.py
---

import re

import lancedb
import requests
from bs4 import BeautifulSoup
from lancedb.embeddings import get_registry
from lancedb.pydantic import LanceModel, Vector

embedding_model = get_registry().get("openai").create()


class TextDocument(LanceModel):
    """Simple data structure to hold a piece of text associated with a url."""

    url: str
    position: int
    text: str = embedding_model.SourceField()
    vector: Vector(dim=embedding_model.ndims()) = embedding_model.VectorField()


def html_content(blog_post_url: str) -> str:
    return requests.get(blog_post_url).text


def parsed_text(html_content: str) -> str:
    soup = BeautifulSoup(html_content, "html.parser")
    return soup.get_text(separator=" ", strip=True)


def sentences(parsed_text: str) -> list[str]:
    return [sentence.strip() for sentence in re.split(r"[.!?]+", parsed_text) if sentence.strip()]


def overlapping_chunks(
    sentences: list[str], window: int = 5, stride: int = 3, min_window_size: int = 2
) -> list[str]:
    overlapping_chunks = []
    n_chunks = len(sentences)
    for start_i in range(0, n_chunks, stride):
        if (start_i + window <= n_chunks) or (n_chunks - start_i >= min_window_size):
            overlapping_chunks.append(
                " ".join(sentences[start_i : min(start_i + window, n_chunks)])
            )
    return overlapping_chunks


def embed_chunks(overlapping_chunks: list[str], blog_post_url: str) -> dict:
    # embed and store the chunks using LanceDB
    con = lancedb.connect("./blogs")
    table = con.create_table("chunks", exist_ok=True, schema=TextDocument)
    table.add(
        [{"text": c, "url": blog_post_url, "position": i} for i, c in enumerate(overlapping_chunks)]
    )
    return {"n_chunks_embedded": len(overlapping_chunks)}



---
File: /burr/examples/hamilton-integration/__init__.py
---




---
File: /burr/examples/hamilton-integration/application.py
---

from hamilton.driver import Builder, Driver

from burr.core import ApplicationBuilder, State, action


@action(reads=[], writes=[])
def ingest_blog(state: State, blog_post_url: str, dr: Driver) -> State:
    """Download a blog post and parse it"""
    dr.execute(["embed_chunks"], inputs={"blog_post_url": blog_post_url})
    return state


@action(reads=[], writes=["llm_answer"])
def ask_question(state: State, user_query: str, dr: Driver) -> State:
    """Reply to the user's query using the blog's content."""
    results = dr.execute(["llm_answer"], inputs={"user_query": user_query})
    return state.update(llm_answer=results["llm_answer"])


if __name__ == "__main__":
    # renames to avoid name conflicts with the @action functions
    from actions import ask_question as ask_module
    from actions import ingest_blog as ingest_module
    from hamilton.plugins.h_opentelemetry import OpenTelemetryTracer
    from opentelemetry.instrumentation.lancedb import LanceInstrumentor
    from opentelemetry.instrumentation.openai import OpenAIInstrumentor

    OpenAIInstrumentor().instrument()
    LanceInstrumentor().instrument()

    dr = (
        Builder()
        .with_modules(ingest_module, ask_module)
        .with_adapters(OpenTelemetryTracer())
        .build()
    )

    app = (
        ApplicationBuilder()
        .with_actions(ingest_blog.bind(dr=dr), ask_question.bind(dr=dr))
        .with_transitions(("ingest_blog", "ask_question"))
        .with_entrypoint("ingest_blog")
        .with_tracker(project="modular-rag", use_otel_tracing=True)
        .build()
    )

    action_name, results, state = app.run(
        halt_after=["ask_question"],
        inputs={
            "blog_post_url": "https://blog.dagworks.io/p/from-blog-to-bot-build-a-rag-app",
            "user_query": "What do you need to monitor in a RAG app?",
        },
    )
    print(state["llm_answer"])



---
File: /burr/examples/haystack-integration/__init__.py
---




---
File: /burr/examples/haystack-integration/application.py
---

from haystack.components.builders import PromptBuilder
from haystack.components.embedders import OpenAITextEmbedder
from haystack.components.generators import OpenAIGenerator
from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
from haystack.document_stores.in_memory import InMemoryDocumentStore

from burr.core import ApplicationBuilder, State, action
from burr.integrations.haystack import HaystackAction


@action(reads=["answer"], writes=[])
def display_answer(state: State) -> State:
    print(state["answer"])
    return state


def build_application():
    embed_text = HaystackAction(
        component=OpenAITextEmbedder(model="text-embedding-3-small"),
        name="embed_text",
        reads=[],
        writes={"query_embedding": "embedding"},
    )

    retrieve_documents = HaystackAction(
        component=InMemoryEmbeddingRetriever(InMemoryDocumentStore()),
        name="retrieve_documents",
        reads=["query_embedding"],
        writes=["documents"],
    )

    build_prompt = HaystackAction(
        component=PromptBuilder(template="Document: {{documents}} Question: {{question}}"),
        name="build_prompt",
        reads=["documents"],
        writes={"question_prompt": "prompt"},
    )

    generate_answer = HaystackAction(
        component=OpenAIGenerator(model="gpt-4o-mini"),
        name="generate_answer",
        reads={"prompt": "question_prompt"},
        writes={"answer": "replies"},
    )

    return (
        ApplicationBuilder()
        .with_actions(
            embed_text,
            retrieve_documents,
            build_prompt,
            generate_answer,
            display_answer,
        )
        .with_transitions(
            ("embed_text", "retrieve_documents"),
            ("retrieve_documents", "build_prompt"),
            ("build_prompt", "generate_answer"),
            ("generate_answer", "display_answer"),
        )
        .with_entrypoint("embed_text")
        .build()
    )


if __name__ == "__main__":
    import os

    os.environ["OPENAI_API_KEY"] = "sk-..."

    app = build_application()

    _, _, state = app.run(
        halt_after=["display_answer"],
        inputs={
            "text": "What is the capital of France?",
            "question": "What is the capital of France?",
        },
    )
    app.visualize(include_state=True)



---
File: /burr/examples/hello-world-counter/__init__.py
---




---
File: /burr/examples/hello-world-counter/application_classbased.py
---

"""
Class based action example.
"""
import logging
from typing import List, Optional

import burr.core
from burr.core import Action, Application, ApplicationContext, Result, State, default, expr
from burr.core.persistence import SQLLitePersister
from burr.lifecycle import LifecycleAdapter

logger = logging.getLogger(__name__)


class Counter(Action):
    @property
    def reads(self) -> list[str]:
        return ["counter"]

    def run(self, state: State, increment_by: int, __context: ApplicationContext) -> dict:
        # can access the app_id from the __context
        print(__context.app_id)
        return {"counter": state["counter"] + increment_by}

    # alternate way via **kwargs
    # def run(self,
    #         state: State,
    #         increment_by: int,
    #         **kwargs) -> dict:
    #     # can access the app_id from the __context via kwargs
    #     print(kwargs["__context"].app_id)
    #     return {"counter": state["counter"] + increment_by}

    @property
    def writes(self) -> list[str]:
        return ["counter"]

    def update(self, result: dict, state: State) -> State:
        return state.update(**result)

    @property
    def inputs(self) -> list[str]:
        # to get __context injected you must declare it here.
        return ["increment_by", "__context"]


def application(
    count_up_to: int = 10,
    partition_key: str = "demo-user",
    app_id: Optional[str] = None,
    storage_dir: Optional[str] = "~/.burr",
    hooks: Optional[List[LifecycleAdapter]] = None,
) -> Application:
    persister = SQLLitePersister("demos.db", "counter", connect_kwargs={"check_same_thread": False})
    persister.initialize()
    logger.info(
        f"{partition_key} has these prior invocations: {persister.list_app_ids(partition_key)}"
    )
    return (
        burr.core.ApplicationBuilder()
        .with_actions(counter=Counter(), result=Result("counter"))
        .with_transitions(
            ("counter", "counter", expr(f"counter < {count_up_to}")),
            ("counter", "result", default),
        )
        .with_identifiers(partition_key=partition_key, app_id=app_id)
        .initialize_from(
            persister,
            resume_at_next_action=True,
            default_state={"counter": 0},
            default_entrypoint="counter",
        )
        .with_state_persister(persister)
        .with_tracker(project="demo_counter", params={"storage_dir": storage_dir})
        .with_hooks(*hooks if hooks else [])
        .build()
    )


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    app = application()  # app_id="a7c8e525-58f9-4e84-b4b3-f5b80b5b0d0e")
    action, result, state = app.run(halt_after=["result"], inputs={"increment_by": 1})
    app.visualize(
        output_file_path="statemachine_classbased.png",
        include_conditions=True,
        view=False,
        format="png",
    )
    print(state["counter"])



---
File: /burr/examples/hello-world-counter/application.py
---

import logging
from typing import List, Optional

import burr.core
from burr.core import Application, Result, State, default, expr
from burr.core.action import action
from burr.core.persistence import SQLLitePersister
from burr.lifecycle import LifecycleAdapter

logger = logging.getLogger(__name__)


@action(reads=["counter"], writes=["counter"])
def counter(state: State) -> State:
    result = {"counter": state["counter"] + 1}
    print(f"counted to {result['counter']}")
    return state.update(**result)


def application(
    count_up_to: int = 10,
    partition_key: str = "demo-user",
    app_id: Optional[str] = None,
    storage_dir: Optional[str] = "~/.burr",
    hooks: Optional[List[LifecycleAdapter]] = None,
) -> Application:
    persister = SQLLitePersister("demos.db", "counter", connect_kwargs={"check_same_thread": False})
    persister.initialize()
    logger.info(
        f"{partition_key} has these prior invocations: {persister.list_app_ids(partition_key)}"
    )
    return (
        burr.core.ApplicationBuilder()
        .with_actions(counter=counter, result=Result("counter"))
        .with_transitions(
            ("counter", "counter", expr(f"counter < {count_up_to}")),
            ("counter", "result", default),
        )
        .with_identifiers(partition_key=partition_key, app_id=app_id)
        .initialize_from(
            persister,
            resume_at_next_action=True,
            default_state={"counter": 0},
            default_entrypoint="counter",
        )
        .with_state_persister(persister)
        .with_tracker(project="demo_counter", params={"storage_dir": storage_dir})
        .with_hooks(*hooks if hooks else [])
        .build()
    )


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    app = application(app_id="a7c8e525-58f9-4e84-b4b3-f5b80b5b0d0e")
    action, result, state = app.run(halt_after=["result"])
    app.visualize(
        output_file_path="statemachine.png", include_conditions=True, view=False, format="png"
    )
    print(state["counter"])



---
File: /burr/examples/hello-world-counter/streamlit_app.py
---

import application as counter
import streamlit as st

from burr.integrations.streamlit import (
    AppState,
    Record,
    get_state,
    render_explorer,
    set_slider_to_current,
    update_state,
)


def counter_view(app_state: AppState):
    application = app_state.app
    button = st.button("Forward", use_container_width=True)
    if button:
        step_output = application.step()
        if step_output is not None:
            action, result, state = step_output
            app_state.history.append(Record(state.get_all(), action.name, result))
            set_slider_to_current()
        else:
            application.update_state(application.state.update(counter=0))
            application.reset_to_entrypoint()
            action, result, state = application.step()
            app_state.history.append(Record(state.get_all(), action.name, result))


def retrieve_state():
    if "burr_state" not in st.session_state:
        state = AppState.from_empty(app=counter.application())
    else:
        state = get_state()
    return state


def main():
    st.set_page_config(layout="wide")
    sidebar = st.sidebar
    with sidebar:
        st.markdown(
            """
            <style>
                section[data-testid="stSidebar"] {
                    width: 400px !important; # Set the width to your desired value
                }
            </style>
            """,
            unsafe_allow_html=True,
        )
        st.title("Counting numbers with Burr")
        st.write(
            "This is a simple counter app. It counts to 10, then loops back to 0. You can reset it at any time. "
            "While we know that this is easy to do with a simple loop + streamlit, it highlights the state that Burr manages."
            "Use the slider to rewind/see what happened in the past, and the visualizations to understand how we navigate "
            "through the state machine!"
        )
    app_state = retrieve_state()  # retrieve first so we can use for the ret of the step
    columns = st.columns(2)
    with columns[0]:
        counter_view(app_state)
        with st.container(height=800):
            md_lines = []
            for item in app_state.history:
                if item.action == "counter":
                    md_lines.append(f"Counted to {item.state['counter']}!")
                else:
                    md_lines.append("Looping back! ")
            st.code("\n".join(md_lines), language=None)
    with columns[1]:
        render_explorer(app_state)
    update_state(app_state)  # update so the next iteration knows what to do


if __name__ == "__main__":
    main()



---
File: /burr/examples/image-telephone/__init__.py
---




---
File: /burr/examples/image-telephone/application.py
---

"""
This module demonstrates a telephone application
using Burr that:
 - captions an image
 - creates caption embeddings (for analysis)
 - creates a new image based on the created caption

We use pre-defined Hamilton DAGs to perform the
image captioning and image generation tasks. Unlike other frameworks
Hamilton doesn't hide the contents of the defined DAG from the user.
You can easily introspect, and modify the code as needed.

The Hamilton DAGs used in this example can be found here:

  - https://hub.dagworks.io/docs/Users/elijahbenizzy/caption_images/
  - https://hub.dagworks.io/docs/Users/elijahbenizzy/generate_images/
"""
import os
import uuid

import requests
from hamilton import dataflows, driver

from burr.core import Action, ApplicationBuilder, State, default, expr
from burr.core.action import action
from burr.lifecycle import PostRunStepHook

# import hamilton modules that define the DAGs for image captioning and image generation
caption_images = dataflows.import_module("caption_images", "elijahbenizzy")
generate_images = dataflows.import_module("generate_images", "elijahbenizzy")


class ImageSaverHook(PostRunStepHook):
    """Class to save images to a directory.
    This is an example of a custom way to interact indirectly.

    This is one way you could save to S3 by writing something like this.
    """

    def __init__(self, save_dir: str = "saved_images"):
        self.save_dir = save_dir
        self.run_id = str(uuid.uuid4())[0:8]
        self.path = os.path.join(self.save_dir, self.run_id)
        if not os.path.exists(self.path):
            os.makedirs(self.path)

    def post_run_step(self, *, state: "State", action: "Action", **future_kwargs):
        """Pulls the image URL from the state and saves it to the save directory."""
        if action.name == "generate":
            image_url = state["current_image_location"]
            image_name = "image_" + str(state["__SEQUENCE_ID"]) + ".png"
            with open(os.path.join(self.path, image_name), "wb") as f:
                f.write(requests.get(image_url).content)
                print(f"Saved image to {self.path}/{image_name}")


@action(
    reads=["current_image_location"],
    writes=["current_image_caption", "image_location_history"],
)
def image_caption(state: State, caption_image_driver: driver.Driver) -> tuple[dict, State]:
    """Action to caption an image.
    This delegates to the Hamilton DAG for image captioning.
    For more details go here: https://hub.dagworks.io/docs/Users/elijahbenizzy/caption_images/.
    """
    current_image = state["current_image_location"]
    result = caption_image_driver.execute(
        ["generated_caption"], inputs={"image_url": current_image}
    )
    updates = {
        "current_image_caption": result["generated_caption"],
    }
    # You could save to S3 here
    return result, state.update(**updates).append(image_location_history=current_image)


@action(
    reads=["current_image_caption"],
    writes=["caption_analysis"],
)
def caption_embeddings(state: State, caption_image_driver: driver.Driver) -> tuple[dict, State]:
    """Action to analyze the caption and create embeddings for analysis.

    This delegates to the Hamilton DAG for getting embeddings for the caption.
    For more details go here: https://hub.dagworks.io/docs/Users/elijahbenizzy/caption_images/.

    This uses the overrides functionality to use the result of the prior Hamilton DAG run
    to avoid re-computation.
    """
    result = caption_image_driver.execute(
        ["metadata"],
        inputs={"image_url": state["current_image_location"]},
        overrides={"generated_caption": state["current_image_caption"]},
    )
    # You could save to S3 here
    return result, state.append(caption_analysis=result["metadata"])


@action(
    reads=["current_image_caption"],
    writes=["current_image_location", "image_caption_history"],
)
def image_generation(state: State, generate_image_driver: driver.Driver) -> tuple[dict, State]:
    """Action to create an image.

    This delegates to the Hamilton DAG for image generation.
    For more details go here: https://hub.dagworks.io/docs/Users/elijahbenizzy/generate_images/.
    """
    current_caption = state["current_image_caption"]
    result = generate_image_driver.execute(
        ["generated_image"], inputs={"image_generation_prompt": current_caption}
    )
    updates = {
        "current_image_location": result["generated_image"],
    }
    # You could save to S3 here
    return result, state.update(**updates).append(image_caption_history=current_caption)


@action(reads=["image_location_history", "image_caption_history", "caption_analysis"], writes=[])
def terminal_step(state: State) -> tuple[dict, State]:
    """This is a terminal step. We can do any final processing here."""
    result = {
        "image_location_history": state["image_location_history"],
        "image_caption_history": state["image_caption_history"],
        "caption_analysis": state["caption_analysis"],
    }
    # Could save everything to S3 here.
    return result, state


def build_application(
    starting_image: str = "statemachine.png", number_of_images_to_caption: int = 4
):
    """This builds the Burr application and returns it.

    :param starting_image: the starting image to use
    :param number_of_images_to_caption: the number of iterations to go through
    :return: the built application
    """
    # instantiate hamilton drivers and then bind them to the actions.
    caption_image_driver = (
        driver.Builder()
        .with_config({"include_embeddings": True})
        .with_modules(caption_images)
        .build()
    )
    generate_image_driver = driver.Builder().with_config({}).with_modules(generate_images).build()
    app = (
        ApplicationBuilder()
        .with_state(
            current_image_location=starting_image,
            current_image_caption="",
            image_location_history=[],
            image_caption_history=[],
            caption_analysis=[],
        )
        .with_actions(
            caption=image_caption.bind(caption_image_driver=caption_image_driver),
            analyze=caption_embeddings.bind(caption_image_driver=caption_image_driver),
            generate=image_generation.bind(generate_image_driver=generate_image_driver),
            terminal=terminal_step,
        )
        .with_transitions(
            ("caption", "analyze", default),
            (
                "analyze",
                "terminal",
                expr(f"len(image_caption_history) == {number_of_images_to_caption}"),
            ),
            ("analyze", "generate", default),
            ("generate", "caption", default),
        )
        .with_entrypoint("caption")
        .with_hooks(ImageSaverHook())
        .with_tracker(project="image-telephone")
        .build()
    )
    return app


if __name__ == "__main__":
    import random

    coin_flip = random.choice([True, False])
    # app = build_application("path/to/my/image.png")
    app = build_application()
    app.visualize(output_file_path="statemachine", include_conditions=True, view=True, format="png")
    if coin_flip:
        _last_action, _result, _state = app.run(halt_after=["terminal"])
        # save to S3 / download images etc.
    else:
        # alternate way to run:
        while True:
            _action, _result, _state = app.step()
            print("action=====\n", _action)
            print("result=====\n", _result)
            # you could save to S3 / download images etc. here.
            if _action.name == "terminal":
                break
    print(_state)



---
File: /burr/examples/instructor-gemini-flash/__init__.py
---




---
File: /burr/examples/instructor-gemini-flash/application.py
---

import time
from copy import deepcopy
from typing import Annotated

import google.generativeai as genai
import instructor
from dotenv import load_dotenv
from pydantic import AfterValidator, BaseModel, Field

from burr.core import Application, ApplicationBuilder, State, expr
from burr.core.action import action
from burr.tracking import LocalTrackingClient

load_dotenv()  # load the GOOGLE_API_KEY from your .env file

ask_gemini = instructor.from_gemini(
    client=genai.GenerativeModel(model_name="gemini-1.5-flash-latest")
)

MIN_WORDS = 3
MAX_WORDS = 5
NUM_REQUIRED_TOPICS = 3
MIN_SUBTOPICS = 2
MAX_SUBTOPICS = 4
MIN_CONCEPTS = 2
MAX_CONCEPTS = 4
ATTEMPTS = 3
TEMPERATURE = 0.3


class Subtopic(BaseModel):
    name: Annotated[str, AfterValidator(str.title)]
    # pydantic will ensure that we have at least MIN_CONCEPTS and at most MAX_CONCEPTS concepts
    # if we don't, the LLM will be asked to generate the concepts again
    concepts: Annotated[
        list[str], AfterValidator(lambda concepts: [x.title() for x in concepts])
    ] = Field(
        description=f"{MIN_CONCEPTS}-{MAX_CONCEPTS} concepts covered in the subtopic.",
        min_length=MIN_CONCEPTS,
        max_length=MAX_CONCEPTS,
    )


class Topic(BaseModel):
    name: Annotated[str, AfterValidator(str.title)]
    # pydantic will ensure that we have at least MIN_SUBTOPICS and at most MAX_SUBTOPICS subtopics
    # if we don't, the LLM will be asked to generate the subtopics again
    subtopics: list[Subtopic] = Field(
        description=f"{MIN_SUBTOPICS}-{MAX_SUBTOPICS} ordered subtopics with concepts.",
        min_length=MIN_SUBTOPICS,
        max_length=MAX_SUBTOPICS,
    )

    def __str__(self) -> str:
        topic_str = f"TOPIC: {self.name}\n"
        for i, subtopic in enumerate(self.subtopics):
            topic_str += f"SUBTOPIC {i + 1}: {subtopic.name}\n"
            for j, concept in enumerate(subtopic.concepts):
                topic_str += f"CONCEPT {j + 1}: {concept}\n"
        return topic_str


def topics_system_template(
    num_required_topics: int = NUM_REQUIRED_TOPICS,
    min_words: int = MIN_WORDS,
    max_words: int = MAX_WORDS,
    min_subtopics: int = MIN_SUBTOPICS,
    max_subtopics: int = MAX_SUBTOPICS,
    min_concepts: int = MIN_CONCEPTS,
    max_concepts: int = MAX_CONCEPTS,
) -> str:
    return f"""\
"You are a world class course instructor."
You'll be given a course outline and you have to generate {num_required_topics} topics.
For each topic:
    1. Generate a {min_words}-{max_words} word topic name that encapsulates the description.
    2. Generate {min_subtopics}-{max_subtopics} subtopics for the topic. Also {min_words}-{max_words} words each.
    For each subtopic:
        Generate {min_concepts}-{max_concepts} concepts. Also {min_words}-{max_words} words each. The concepts should be related to the subtopic.
        Think of concepts as the smallest unit of knowledge that can be taught from the subtopic. And add a verb to the concept to make it actionable.
        For example:
            "Calculate Derivatives" instead of "Derivatives".
            "Identify Finite Sets" instead of "Finite Sets".
            "Find the y-intercept" instead of "y-intercept".
    The subtopics and concepts should be in the correct order.\
"""


def creation_template(
    outline: str,
    topics_so_far: list[Topic],
    num_required_topics: int = NUM_REQUIRED_TOPICS,
) -> str:
    prompt_strs = [
        f"<outline>\n{outline}\n</outline>",
        f"<num_required_topics>\n{num_required_topics}\n</num_required_topics>",
    ]
    topics_so_far_str = "<topics_so_far>\n"
    if topics_so_far:
        for i, topic in enumerate(topics_so_far):
            topics_so_far_str += f"{i + 1}/{num_required_topics}\n{topic}\n"
    topics_so_far_str = topics_so_far_str.strip() + "\n</topics_so_far>"
    prompt_strs.append(topics_so_far_str)
    prompt_strs.append("Generate the next topic.")
    return "\n\n".join(prompt_strs)


@action(reads=[], writes=["outline", "num_required_topics", "chat_history"])
def setup(state: State, outline: str, num_required_topics: int) -> State:
    """
    Initializes the state of the application with the course outline and the number of topics to be generated.
    """
    return state.update(
        outline=outline,
        num_required_topics=num_required_topics,
        chat_history=[
            {
                "role": "system",
                "content": topics_system_template(num_required_topics=num_required_topics),
            }
        ],
    )


@action(
    reads=[
        "chat_history",
        "outline",
        "num_required_topics",
        "topics_so_far",
        "topic_feedback",
    ],
    writes=["generated_topic", "chat_history"],
)
def creator(
    state: State,
    attempts: int = ATTEMPTS,
) -> tuple[dict, State]:
    """
    Generates a topic based on the outline and topics generated so far using the create function from the Instructor library.
    https://python.useinstructor.com/#using-gemini

    Parameters:
    - state (State): The current state of the application.
    - attempts (int, optional): The number of times the model should try to generate a response. 3 by default. If the model fails to generate a response after the specified number of attempts, the function will return with 'generated_topic' set to None.

    Returns:
    - tuple[dict, State]: A tuple containing the generated topic and the updated state.

    Instructor Parameters Used:
    - response_model: The Pydantic model or the type that we want the response to be. In our case, it's the `Topic` model.
    - messages: The chat history that we want to pass to the model. This could have the prompt for generating the next topic, or the previously generated topic with user feedback.
    - max_retries: The number of times we want to retry if the model fails to generate a response. 1 means just the original attempt, 2 means the original attempt and one retry.
    - temperature: The temperature of the model. A float between 0 and 1. Lower values give more deterministic results. 0.3 by default.
    """

    num_required_topics = state.get("num_required_topics", NUM_REQUIRED_TOPICS)
    topics_so_far = state.get("topics_so_far", [])
    topic_feedback = state.get("topic_feedback", "")
    messages = deepcopy(state["chat_history"])

    if topic_feedback:
        # this means that we had some user feedback on the previously generated topic
        # so instead of prompting for a new topic, we will add the feedback as a user message
        # the previously generated topic is already an assistant message at the end of the chat history
        user_message = f"User feedback: {topic_feedback.strip()}"
    else:
        # either no previous topic or no feedback on the previous topic
        # so we will prompt for a new topic based on the outline and topics generated so far
        user_message = creation_template(
            outline=state["outline"],
            topics_so_far=topics_so_far,
            num_required_topics=num_required_topics,
        )
    messages.append({"role": "user", "content": user_message})
    # these are the parameters that we will pass to the create function from the Instructor library
    create_kwargs = dict(
        response_model=Topic,
        messages=messages,
        max_retries=attempts,
        temperature=TEMPERATURE,
    )
    try:
        res = ask_gemini.create(**create_kwargs)  # type: ignore
    except Exception as e:
        print(f"ERROR in creator: {e}")
        # if the model fails to generate a response even after multiple attempts, we will return None
        # creator will be called again until a valid response is generated
        return {"generated_topic": None}, state.update(generated_topic=None)
    # add the user message and the generated topic to the chat history
    return {"generated_topic": res}, state.update(generated_topic=res).append(
        chat_history={"role": "user", "content": user_message}
    ).append(chat_history={"role": "assistant", "content": str(res)})


@action(
    reads=["generated_topic", "num_required_topics", "topics_so_far"],
    writes=["topic_feedback"],
)
def get_topic_feedback(state: State) -> tuple[dict, State]:
    """
    Asks the user for feedback on the generated topic.
    If the user is happy with the generated topic, they can leave the feedback empty.
    This feedback will be added as a user message to the chat history.
    """
    time.sleep(2)
    num_required_topics = state.get("num_required_topics", NUM_REQUIRED_TOPICS)
    num_topics_so_far = len(state.get("topics_so_far", []))
    generated_topic = state["generated_topic"]
    generated_topic_str = f"{num_topics_so_far + 1}/{num_required_topics}\n{generated_topic}"
    feedback = (
        input(
            f"Give your feedback on this generated topic. Leave empty if it's perfect.\n\n{generated_topic_str}"
        )
        or None
    )

    return {"topic_feedback": feedback}, state.update(topic_feedback=feedback)


@action(reads=["generated_topic"], writes=["topics_so_far"])
def update_topics_so_far(state: State) -> State:
    """
    This is called if there is no feedback on the generated topic.
    We assume that the user is happy with the generated topic and we add it to the list of topics generated so far.
    This list will be incorporated in the prompt for the next topic.
    """
    return state.append(topics_so_far=state["generated_topic"])


@action(reads=["topics_so_far"], writes=[])
def terminal(state: State) -> tuple[dict, State]:
    """
    This is the terminal state of the application.
    We will reach this state when the required number of topics have been generated.
    """
    return {"topics_so_far": state["topics_so_far"]}, state


def application(
    app_id: str | None = None,
    username: str | None = None,
    project: str = "topics_creator",
) -> Application:
    tracker = LocalTrackingClient(project=project)
    builder = (
        ApplicationBuilder()
        .with_actions(setup, creator, get_topic_feedback, update_topics_so_far, terminal)
        .with_transitions(
            ("setup", "creator"),
            # go to creator again if the generated topic is None
            ("creator", "creator", expr("generated_topic is None")),  # type: ignore
            # get feedback if the generated topic is not None
            ("creator", "get_topic_feedback"),
            # if the feedback is empty, add the generated topic to the list of topics generated so far
            (
                "get_topic_feedback",
                "update_topics_so_far",
                expr("topic_feedback is None"),  # type: ignore
            ),
            # if the feedback is not empty, go back to creator and add the feedback as a user message
            ("get_topic_feedback", "creator"),
            # if the number of topics generated so far is equal to the required number of topics, go to terminal
            (
                "update_topics_so_far",
                "terminal",
                expr("len(topics_so_far) == num_required_topics"),  # type: ignore
            ),
            # otherwise, go back to creator after updating the topics generated so far
            ("update_topics_so_far", "creator"),
        )
        .with_tracker("local", project=project)
        .with_identifiers(app_id=app_id, partition_key=username)  # type: ignore
        .initialize_from(
            tracker,
            resume_at_next_action=True,
            default_entrypoint="setup",
            default_state={},
        )
    )
    return builder.build()


if __name__ == "__main__":
    app = application()
    app.visualize(
        output_file_path="statemachine",
        include_conditions=True,
        include_state=False,
        format="png",
    )



---
File: /burr/examples/integrations/hamilton/image-telephone/application.py
---

"""
This module demonstrates the Hamilton plugin that Burr has.
It is equivalent to the telephone example, but the way actions
are specified are different. Specifically it uses Burr's Hamilton
plugin to provide some syntactic sugar for defining actions that run
Hamilton DAGs.
"""
import os
import uuid

import requests
from hamilton import dataflows, driver

from burr.core import Action, ApplicationBuilder, State, default, expr
from burr.core.action import action
from burr.integrations import hamilton
from burr.lifecycle import PostRunStepHook

caption_images = dataflows.import_module("caption_images", "elijahbenizzy")
generate_images = dataflows.import_module("generate_images", "elijahbenizzy")


class ImageSaverHook(PostRunStepHook):
    """Class to save images to a directory."""

    def __init__(self, save_dir: str = "saved_images"):
        self.save_dir = save_dir
        self.run_id = str(uuid.uuid4())[0:8]
        self.path = os.path.join(self.save_dir, self.run_id)
        if not os.path.exists(self.path):
            os.makedirs(self.path)

    def post_run_step(self, *, state: "State", action: "Action", **future_kwargs):
        """Pulls the image URL from the state and saves it to the save directory."""
        if action.name == "generate":
            image_url = state["current_image_location"]
            image_name = "image_" + str(state["__SEQUENCE_ID"]) + ".png"
            with open(os.path.join(self.path, image_name), "wb") as f:
                f.write(requests.get(image_url).content)
                print(f"Saved image to {self.path}/{image_name}")


# instantiate hamilton drivers and then bind them to the actions.
caption_image_driver = (
    driver.Builder().with_config({"include_embeddings": True}).with_modules(caption_images).build()
)
generate_image_driver = driver.Builder().with_config({}).with_modules(generate_images).build()

caption = hamilton.Hamilton(
    inputs={"image_url": hamilton.from_state("current_image_location")},
    outputs={
        "generated_caption": hamilton.update_state("current_image_caption"),
        "image_url": hamilton.append_state("image_location_history"),
    },
    driver=caption_image_driver,
)
analysis = hamilton.Hamilton(
    inputs={
        "image_url": hamilton.from_state("current_image_location"),
        "generated_caption": hamilton.from_state("current_image_caption"),
    },
    outputs={
        "metadata": hamilton.append_state("caption_analysis"),
    },
    driver=caption_image_driver,
)
generate = hamilton.Hamilton(
    inputs={"image_generation_prompt": hamilton.from_state("current_image_caption")},
    outputs={
        "generated_image": hamilton.update_state("current_image_location"),
        "image_generation_prompt": hamilton.append_state("image_caption_history"),
    },
    driver=generate_image_driver,
)


@action(reads=["image_location_history", "image_caption_history", "caption_analysis"], writes=[])
def terminal_step(state: State) -> tuple[dict, State]:
    result = {
        "image_location_history": state["image_location_history"],
        "image_caption_history": state["image_caption_history"],
        "caption_analysis": state["caption_analysis"],
    }
    # Could save everything to S3 here.
    return result, state


def hamilton_action_main(
    max_iterations: int = 5, starting_image_location: str = "statemachine.png"
):
    """This shows how to use the Hamilton syntactic wrapper for the nodes."""
    app = (
        ApplicationBuilder()
        .with_state(
            current_image_location=starting_image_location,
            current_image_caption="",
            image_location_history=[],
            image_caption_history=[],
            caption_analysis=[],
        )
        .with_actions(
            caption=caption,
            analysis=analysis,
            generate=generate,
            terminal=terminal_step,
        )
        .with_transitions(
            ("caption", "terminal", expr(f"len(image_caption_history) == {max_iterations}")),
            ("caption", "analysis", default),
            ("analysis", "generate", default),
            ("generate", "caption", default),
        )
        .with_entrypoint("caption")
        .with_hooks(ImageSaverHook())
        .with_tracker(project="image-telephone")
        .build()
    )
    return app


if __name__ == "__main__":
    import random

    coin_flip = random.choice([True, False])
    app = hamilton_action_main()
    app.visualize(output_file_path="statemachine", include_conditions=True, view=True, format="png")
    if coin_flip:
        _last_action, _result, _state = app.run(halt_after=["terminal"])
    else:
        # alternate way to run:
        while True:
            _action, _result, _state = app.step()
            print("action=====\n", _action)
            print("result=====\n", _result)
            print("state======\n", _state)
            if _action.name == "terminal":
                break



---
File: /burr/examples/llm-adventure-game/__init__.py
---




---
File: /burr/examples/llm-adventure-game/application.py
---

import json
from typing import Optional, Tuple

from openai import Client

import burr.core
from burr.core import Application, State, default, when
from burr.core.action import action

RESTRICTIONS = """You're a small corgi with short legs. You can't jump high,
 you can't run fast, you can't perform feats of athleticism in general
 to achieve any of your goals. You can't open doors, you can't use tools,
 you can't communicate with humans, you can't use your paws to manipulate
 objects, you can't use your mouth to manipulate objects, you can't use
 your mouth to communicate with humans"""


challenges = [
    "There is a dish of dog food on the floor. You want to eat it",
    "There is a dish of dog food on a table. You want to eat it",
    "There is a dish of dog food in a locked car. You want to eat it",
]


@action(reads=[], writes=["current_challenge"])
def start(state: State) -> Tuple[dict, State]:
    result = {"current_challenge": challenges[0]}
    return result, state.update(**result)


@action(reads=["current_challenge"], writes=["attempts"])
def prompt_for_challenge(state: State) -> Tuple[dict, State]:
    response = input(f'{state["current_challenge"]}. What do you do?\n $ ')
    result = {"attempt": response}
    return result, state.append(attempts=result["attempt"])


@action(
    reads=["attempts", "current_challenge"],
    writes=["challenge_solved", "what_happened"],
)
def evaluate_attempt(state: State) -> Tuple[dict, State]:
    result = Client().chat.completions.create(
        model="gpt-4",
        messages=[
            {
                "role": "system",
                "content": f"""You are evaluating responses for
             whether they constitute solutions to the provided challenge in a text
             based game, whose protagonist is a dog subject to the following limitations:
             {RESTRICTIONS}. You respond ONLY with a json object containing two fields: "solved", which is a
             boolean indicating whether the challenge was solved by the attempt, and "what_happened",
             which is a string containing a brief narrative, written in the second person and addressed
             to the player, of what happened during the protagonist's attempt""",
            },
            {
                "role": "user",
                "content": f"The current challenge is: {state['current_challenge']} "
                f"and the player's attempt is: {state['attempts'][-1]}",
            },
        ],
    )
    content = result.choices[0].message.content
    try:
        json_result = json.loads(content)
    except json.JSONDecodeError:
        print("bad json: ", content)
        json_result = {
            "solved": False,
            "what_happened": "Not sure, really. I'm a dog. I can't read json. I can't read at all.",
        }

    result = {"challenge_solved": json_result["solved"], "txt_result": content}

    return result, state.update(
        challenge_solved=result["challenge_solved"],
        what_happened=json_result["what_happened"],
    )


@action(
    reads=["challenge_solved", "current_challenge", "what_happened"],
    writes=["current_challenge", "did_win"],
)
def maybe_progress(state: State) -> Tuple[dict, State]:
    print("What happened:", state["what_happened"])
    if state["challenge_solved"]:
        if state["current_challenge"] == challenges[-1]:
            result = {"did_win": True}
        else:
            result = {
                "current_challenge": challenges[challenges.index(state["current_challenge"]) + 1]
            }
    else:
        result = {"current_challenge": state["current_challenge"]}
    return result, state.update(**result)


@action(reads=["challenges"], writes=[])
def win(state: State) -> Tuple[dict, State]:
    # get summary of actions taken from openai
    print("you won")
    return {}, state


def application(
    app_id: Optional[str] = None,
    storage_dir: Optional[str] = "~/.burr",
) -> Application:
    return (
        burr.core.ApplicationBuilder()
        .with_state(did_win=False)
        .with_actions(
            start=start,
            prompt_for_challenge=prompt_for_challenge,
            evaluate_attempt=evaluate_attempt,
            maybe_progress=maybe_progress,
            win=win,
        )
        .with_transitions(
            ("start", "prompt_for_challenge", default),
            ("prompt_for_challenge", "evaluate_attempt", default),
            ("evaluate_attempt", "maybe_progress", default),
            ("maybe_progress", "win", when(did_win=True)),
            ("maybe_progress", "prompt_for_challenge", default),
        )
        .with_entrypoint("start")
        .with_tracker(project="demo_corgi_adventure", params={"storage_dir": storage_dir})
        .with_identifiers(app_id=app_id)
        .build()
    )


if __name__ == "__main__":
    app = application()
    app.visualize(output_file_path="digraph", include_conditions=True, view=False, format="png")
    action, state, result = app.run(halt_after=["win"])



---
File: /burr/examples/ml-training/application.py
---

"""
This is a sketch of an example of a machine learning training pipeline using Burr where you
want to do something dynamic like train a model for a variable number of iterations and then
pick the best model from each of the iterations based on some validation metric. E.g. hyperparameter search.

You could adjust this example to continue training a model until you reach a certain validation metric,
or hit a max number of iterations (or epochs), etc.

Note: this example uses the class based API to define the actions. You could also use the function+decorator API.
"""
import burr.core.application
from burr.core import Action, Condition, State, default


class ProcessDataAction(Action):
    @property
    def reads(self) -> list[str]:
        return ["data_path"]

    def run(self, state: State, **run_kwargs) -> dict:
        """This is where you would load and process your data.
        You can take in state and reference object fields.

        You need to return a dictionary of the results that `update()` will then use to update the state.
        """
        # load data, and run some feature engineering -- create the data sets
        # TODO: fill in
        return {"training_data": "SOME_OBJECT/PATH", "evaluation_data": "SOME_OBJECT/PATH"}

    @property
    def writes(self) -> list[str]:
        return ["training_data", "evaluation_data"]

    def update(self, result: dict, state: State) -> State:
        return state.update(
            training_data=result["training_data"], evaluation_data=result["evaluation_data"]
        )


class TrainModel(Action):
    @property
    def reads(self) -> list[str]:
        return ["training_data", "iterations"]

    def run(self, state: State, **run_kwargs) -> dict:
        """This is where you'd build the model and on each iteration do something different.
        E.g. hyperparameter search.
        """
        # train the model, i.e. run one "iteration" whatever that means for you.
        # TODO: fill in
        return {
            "model": "SOME_MODEL",
            "iterations": state["iterations"] + 1,
            "metrics": "SOME_METRICS",
        }

    @property
    def writes(self) -> list[str]:
        return ["models", "training_metrics", "iterations"]

    def update(self, result: dict, state: State) -> State:
        return state.update(
            iterations=result["iterations"],  # set current iteration value
        ).append(
            models=result["model"],  # append -- note this can get big if your model is big
            # so you'll want to overwrite but store the conditions, or log somewhere
            metrics=result["metrics"],  # append the metrics
        )


class ValidateModel(Action):
    @property
    def reads(self) -> list[str]:
        return ["models", "evaluation_data"]

    def run(self, state: State, **run_kwargs) -> dict:
        """Compute validation metrics using the model and evaluation data."""
        # TODO: fill in
        return {"validation_metrics": "SOME_METRICS"}

    @property
    def writes(self) -> list[str]:
        return ["validation_metrics"]

    def update(self, result: dict, state: State) -> State:
        return state.append(validation_metrics=result["validation_metrics"])


class BestModel(Action):
    @property
    def reads(self) -> list[str]:
        return ["validation_metrics", "models"]

    def run(self, state: State, **run_kwargs) -> dict:
        """Select the best model based on the validation metrics."""
        # TODO: fill in
        return {"best_model": "SOME_MODEL"}

    @property
    def writes(self) -> list[str]:
        return ["best_model"]

    def update(self, result: dict, state: State) -> State:
        return state.update(best_model=result["best_model"])


def application(iterations: int) -> burr.core.application.Application:
    return (
        burr.core.ApplicationBuilder()
        .with_state(
            data_path="data.csv",
            iterations=10,
            training_data=None,
            evaluation_data=None,
            models=[],
            training_metrics=[],
            validation_metrics=[],
            best_model=None,
        )
        .with_actions(
            process_data=ProcessDataAction(),
            train_model=TrainModel(),
            validate_model=ValidateModel(),
            best_model=BestModel(),
        )
        .with_transitions(
            ("process_data", "train_model", default),
            ("train_model", "validate_model", default),
            ("validate_model", "best_model", Condition.expr(f"iterations>{iterations}")),
            ("validate_model", "train_model", default),
        )
        .with_entrypoint("process_data")
        .build()
    )


if __name__ == "__main__":
    app = application(100)  # doing good data science is up to you...
    app.visualize(output_file_path="statemachine", include_conditions=True, view=True, format="png")
    # you could run things like this:
    # last_action, result, state = app.run(halt_after=["best_model"])



---
File: /burr/examples/multi-agent-collaboration/hamilton/__init__.py
---




---
File: /burr/examples/multi-agent-collaboration/hamilton/alternative_implementation.py
---

from typing import Optional

import func_agent
from hamilton import driver
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_experimental.utilities import PythonREPL

from burr import core
from burr.core import Action, ApplicationBuilder, State
from burr.core.action import action
from burr.lifecycle import PostRunStepHook
from burr.tracking import client as burr_tclient

# Initialize some things needed for tools.
# see func_agent.py for the code for this pipeline
tool_dag = driver.Builder().with_modules(func_agent).build()
# Will run code provided by the LLM.
repl = PythonREPL()


def python_repl(code: str) -> dict:
    """Use this to execute python code. If you want to see the output of a value,
    you should print it out with `print(...)`. This is visible to the user.

    :param code: string. The python code to execute.
    :return: the output
    """
    try:
        result = repl.run(code)
    except BaseException as e:
        return {"error": repr(e), "status": "error"}
    return {"status": "success", "code": f"```python\n{code}\n```", "Stdout": result}


@action(reads=["query", "messages"], writes=["messages", "next_hop"])
def chart_generator(state: State) -> tuple[dict, State]:
    """
    Generates a chart based on the provided query and updates the state.

    :param state: The current state of the application.
    :return: A tuple containing the result of the tool execution and the updated state.
    """
    query = state["query"]
    # see func_agent.py for the code for this pipeline
    result = tool_dag.execute(
        ["executed_tool_calls", "parsed_tool_calls", "llm_function_message"],
        inputs={
            "tools": [python_repl],
            "system_message": "Any charts you display will be visible by the user.",
            "user_query": query,
            "messages": state["messages"],
        },
    )
    # _code = result["parsed_tool_calls"][0]["function_args"]["code"]
    new_messages = [result["llm_function_message"]]
    for tool_name, tool_result in result["executed_tool_calls"]:
        new_messages.append(tool_result)
    if len(new_messages) == 1:
        if new_messages[0]["content"] and "FINAL ANSWER" in new_messages[0]["content"]:
            next_hop = "complete"
        else:
            next_hop = "continue"
    else:
        # assess tool results
        next_hop = "self"
    state = state.update(next_hop=next_hop)
    for message in new_messages:
        state = state.append(messages=message)
    return result, state


tavily_tool = TavilySearchResults(max_results=5)


@action(reads=["query", "messages"], writes=["messages", "next_hop"])
def researcher(state: State) -> tuple[dict, State]:
    """
    Performs research based on the provided query and updates the state.

    :param state: The current state of the application.
    :return: A tuple containing the result of the tool execution and the updated state.
    """
    query = state["query"]
    # see func_agent.py for the code for this pipeline
    result = tool_dag.execute(
        ["executed_tool_calls", "parsed_tool_calls", "llm_function_message"],
        inputs={
            "tools": [tavily_tool],
            "system_message": "You should provide accurate data for the chart generator to use.",
            "user_query": query,
            "messages": state["messages"],
        },
    )
    new_messages = [result["llm_function_message"]]
    for tool_name, tool_result in result["executed_tool_calls"]:
        new_messages.append(tool_result)
    if len(new_messages) == 1:
        if "FINAL ANSWER" in new_messages[0]["content"]:
            next_hop = "complete"
        else:
            next_hop = "continue"
    else:
        # assess tool results
        next_hop = "self"
    state = state.update(next_hop=next_hop)
    for message in new_messages:
        state = state.append(messages=message)
    return result, state


@action(reads=[], writes=[])
def terminal_step(state: State) -> tuple[dict, State]:
    """One can express a terminal action like this."""
    return {}, state


class PrintStepHook(PostRunStepHook):
    """Example of a post run step hook that prints the state and action."""

    def post_run_step(self, *, state: "State", action: "Action", **future_kwargs):
        print("action=====\n", action)
        print("state======\n", state)


def default_state_and_entry_point() -> tuple[dict, str]:
    """Default state and entry point for the application."""
    return {
        "messages": [],
        "query": "Fetch the UK's GDP over the past 5 years,"
        " then draw a line graph of it."
        " Once you code it up, finish.",
        "next_hop": "",
    }, "researcher"


def application(app_instance_id: Optional[str] = None):
    project_name = "demo_hamilton-multi-agent"
    # TODO -- use the new persistence API
    if app_instance_id:
        state, entry_point = burr_tclient.LocalTrackingClient.load_state(
            project_name,
            app_instance_id,
        )
    else:
        state, entry_point = default_state_and_entry_point()
    app = (
        ApplicationBuilder()
        .with_state(**state)
        .with_actions(
            researcher=researcher,
            chart_generator=chart_generator,
            terminal=terminal_step,
        )
        .with_transitions(
            ("researcher", "researcher", core.expr("next_hop == 'self'")),
            ("researcher", "chart_generator", core.expr("next_hop == 'continue'")),
            ("chart_generator", "chart_generator", core.expr("next_hop == 'self'")),
            ("chart_generator", "researcher", core.expr("next_hop == 'continue'")),
            (
                "researcher",
                "terminal",
                core.expr("next_hop == 'complete'"),
            ),
            (
                "chart_generator",
                "terminal",
                core.expr("next_hop == 'complete'"),
            ),
        )
        .with_entrypoint(entry_point)
        .with_hooks(PrintStepHook())
        .with_tracker(project=project_name)
        .build()
    )
    return app


if __name__ == "__main__":
    """
    The entry point of the application.

    If an app_id is provided, the application will restart from the last
    sequence in that state.
    E.g. fine the ID in the UI and then put it in here "app_4d1618d2-79d1-4d89-8e3f-70c216c71e63"
    """
    app = application(app_instance_id=None)
    app.visualize(output_file_path="statemachine", include_conditions=True, view=True, format="png")
    app.run(halt_after=["terminal"])

    # some test code
    # tavily_tool = TavilySearchResults(max_results=5)
    # result = tool_dag.execute(
    #     ["executed_tool_calls"],
    #     inputs={
    #         "tools": [tavily_tool],
    #         "system_message": "You should provide accurate data for the chart generator to use.",
    #         "user_query": "Fetch the UK's GDP over the past 5 years,"
    #         " then draw a line graph of it."
    #         " Once you have written code for the graph, finish.",
    #     },
    # )
    # import pprint
    #
    # pprint.pprint(result)
    #
    # result = tool_dag.execute(
    #     ["executed_tool_calls"],
    #     inputs={
    #         "tools": [python_repl],
    #         "system_message": "Any charts you display will be visible by the user.",
    #         "user_query": "Draw a simple line graph of y = x",
    #     },
    # )
    # import pprint
    #
    # pprint.pprint(result)



---
File: /burr/examples/multi-agent-collaboration/hamilton/application.py
---

"""
Hamilton version of the multi-agent collaboration example.

This also adds a tracer to the Hamilton DAG to trace the execution of the nodes
within the Action so that they also show up in the Burr UI.
"""
import json
from typing import Any, Dict, Optional

import func_agent
from hamilton import driver
from hamilton import lifecycle as h_lifecycle
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_experimental.utilities import PythonREPL

from burr import core
from burr.core import Action, ApplicationBuilder, State, action, default
from burr.lifecycle import PostRunStepHook
from burr.tracking import client as burr_tclient
from burr.visibility import ActionSpanTracer, TracerFactory

# --- some set up for the tools ---

repl = PythonREPL()
tavily_tool = TavilySearchResults(max_results=5)


def python_repl(code: str) -> dict:
    """Use this to execute python code. If you want to see the output of a value,
    you should print it out with `print(...)`. This is visible to the user.

    :param code: string. The python code to execute.
    :return: the output
    """
    try:
        result = repl.run(code)
    except BaseException as e:
        return {"error": repr(e), "status": "error", "code": f"```python\n{code}\n```"}
    return {"status": "success", "code": f"```python\n{code}\n```", "Stdout": result}


# These are our tools that we will use in the application.
tools = [tavily_tool, python_repl]


class SimpleTracer(h_lifecycle.NodeExecutionHook):
    """Simple Hamilton Tracer that plugs into Burr's tracing capture.
    This will tell Burr about what Hamilton is doing internally.
    """

    def __init__(self, tracer: TracerFactory):
        self._tracer: TracerFactory = tracer
        self.active_spans = {}

    def run_before_node_execution(
        self,
        *,
        node_name: str,
        node_tags: Dict[str, Any],
        node_kwargs: Dict[str, Any],
        node_return_type: type,
        task_id: Optional[str],
        run_id: str,
        node_input_types: Dict[str, Any],
        **future_kwargs: Any,
    ):
        context_manager: ActionSpanTracer = self._tracer(node_name)
        context_manager.__enter__()
        self.active_spans[node_name] = context_manager

    def run_after_node_execution(
        self,
        *,
        node_name: str,
        node_tags: Dict[str, Any],
        node_kwargs: Dict[str, Any],
        node_return_type: type,
        result: Any,
        error: Optional[Exception],
        success: bool,
        task_id: Optional[str],
        run_id: str,
        **future_kwargs: Any,
    ):
        context_manager = self.active_spans.pop(node_name)
        context_manager.__exit__(None, None, None)


def initialize_agent_dag(agent_name: str, tracer: TracerFactory) -> driver.Driver:
    """Initialize the agent DAG with the tracer.

    Right now there is no difference between the agents, but this is here for future use.
    """
    tracer = SimpleTracer(tracer)
    # Initialize some things needed for tools.
    agent_dag = driver.Builder().with_modules(func_agent).with_adapters(tracer).build()
    return agent_dag


# --- End Tool Setup

# --- Start defining Action


@action(reads=["query", "messages"], writes=["messages"])
def chart_generator(state: State, __tracer: TracerFactory) -> tuple[dict, State]:
    """The chart generator action.

    :param state: state of the application
    :param __tracer: burr tracer that gets injected.
    :return:
    """
    query = state["query"]
    agent_dag = initialize_agent_dag("chart_generator", __tracer)
    result = agent_dag.execute(
        ["parsed_tool_calls", "llm_function_message"],
        inputs={
            "tools": [python_repl],
            "system_message": "Any charts you display will be visible by the user. When done say 'FINAL ANSWER'.",
            "user_query": query,
            "messages": state["messages"],
        },
    )
    # _code = result["parsed_tool_calls"][0]["function_args"]["code"]
    new_message = result["llm_function_message"]
    parsed_tool_calls = result["parsed_tool_calls"]
    state = state.update(parsed_tool_calls=parsed_tool_calls)
    state = state.append(messages=new_message)
    state = state.update(sender="chart_generator")
    return result, state


@action(reads=["query", "messages"], writes=["messages"])
def researcher(state: State, __tracer: TracerFactory) -> tuple[dict, State]:
    """The researcher action.

    :param state: state of the application
    :param __tracer: the burr tracer that gets injected.
    :return:
    """
    query = state["query"]
    agent_dag = initialize_agent_dag("researcher", __tracer)
    result = agent_dag.execute(
        ["parsed_tool_calls", "llm_function_message"],
        inputs={
            "tools": [tavily_tool],
            "system_message": "You should provide accurate data for the chart generator to use. When done say 'FINAL ANSWER'.",
            "user_query": query,
            "messages": state["messages"],
        },
    )
    new_message = result["llm_function_message"]
    parsed_tool_calls = result["parsed_tool_calls"]
    state = state.update(parsed_tool_calls=parsed_tool_calls)
    state = state.append(messages=new_message)
    state = state.update(sender="researcher")
    return result, state


@action(reads=["messages", "parsed_tool_calls"], writes=["messages", "parsed_tool_calls"])
def tool_node(state: State) -> tuple[dict, State]:
    """Given a tool call, execute it and return the result."""
    new_messages = []
    parsed_tool_calls = state["parsed_tool_calls"]

    for tool_call in parsed_tool_calls:
        tool_name = tool_call["function_name"]
        tool_args = tool_call["function_args"]
        tool_found = False
        for tool in tools:
            name = getattr(tool, "name", None)
            if name is None:
                name = tool.__name__
            if name == tool_name:
                tool_found = True
                kwargs = json.loads(tool_args)
                # Execute the tool!
                if hasattr(tool, "_run"):
                    result = tool._run(**kwargs)
                else:
                    result = tool(**kwargs)
                new_messages.append(
                    {
                        "tool_call_id": tool_call["id"],
                        "role": "tool",
                        "name": tool_name,
                        "content": result,
                    }
                )
        if not tool_found:
            raise ValueError(f"Tool {tool_name} not found.")

    for tool_result in new_messages:
        state = state.append(messages=tool_result)
    state = state.update(parsed_tool_calls=[])
    # We return a list, because this will get added to the existing list
    return {"messages": new_messages}, state


@action(reads=[], writes=[])
def terminal_step(state: State) -> tuple[dict, State]:
    """Terminal step we have here that does nothing, but it could"""
    return {}, state


class PrintStepHook(PostRunStepHook):
    """Prints the action and state after each step."""

    def post_run_step(self, *, state: "State", action: "Action", **future_kwargs):
        print("action=====\n", action)
        print("state======\n", state)


def default_state_and_entry_point(query: str = None) -> tuple[dict, str]:
    """Returns the default state and entry point for the application."""
    if query is None:
        query = (
            "Fetch the UK's GDP over the past 5 years,"
            " then draw a line graph of it."
            " Once the python code has been written and the graph drawn, the task is complete."
        )
    return {
        "messages": [],
        "query": query,
        "sender": "",
        "parsed_tool_calls": [],
    }, "researcher"


def main(query: str = None, app_instance_id: str = None, sequence_number: int = None):
    """Main function to run the application.

    :param query: the query for the agents to run over.
    :param app_instance_id: a prior app instance id to restart from.
    :param sequence_number: a prior sequence number to restart from.
    :return:
    """
    project_name = "demo_hamilton-multi-agent-v1"
    if app_instance_id:
        tracker = burr_tclient.LocalTrackingClient(project_name)
        persisted_state = tracker.load("demo", app_id=app_instance_id, sequence_no=sequence_number)
        if not persisted_state:
            print(f"Warning: No persisted state found for app_id {app_instance_id}.")
            state, entry_point = default_state_and_entry_point(query)
        else:
            state = persisted_state["state"]
            entry_point = persisted_state["position"]
    else:
        state, entry_point = default_state_and_entry_point(query)
    # look up app_id for particular user
    # if None -- then proceed with defaults
    # else load from state, and set entry point
    app = (
        ApplicationBuilder()
        .with_state(**state)
        .with_actions(
            researcher=researcher,
            chart_generator=chart_generator,
            tool_node=tool_node,
            terminal=terminal_step,
        )
        .with_transitions(
            ("researcher", "tool_node", core.expr("len(parsed_tool_calls) > 0")),
            (
                "researcher",
                "terminal",
                core.expr("'FINAL ANSWER' in messages[-1]['content']"),
            ),
            ("researcher", "chart_generator", default),
            ("chart_generator", "tool_node", core.expr("len(parsed_tool_calls) > 0")),
            (
                "chart_generator",
                "terminal",
                core.expr("'FINAL ANSWER' in messages[-1]['content']"),
            ),
            ("chart_generator", "researcher", default),
            ("tool_node", "researcher", core.expr("sender == 'researcher'")),
            ("tool_node", "chart_generator", core.expr("sender == 'chart_generator'")),
        )
        .with_identifiers(partition_key="demo")
        .with_entrypoint(entry_point)
        .with_hooks(PrintStepHook())
        .with_tracker(project=project_name)
        .build()
    )
    app.visualize(
        output_file_path="hamilton-multi-agent-v2", include_conditions=True, view=True, format="png"
    )
    app.run(halt_after=["terminal"])


if __name__ == "__main__":
    # Add an app_id to restart from last sequence in that state
    # e.g. fine the ID in the UI and then put it in here "app_f0e4a918-b49c-4ee1-9d2b-30c15104c51c"
    _app_id = None  # "app_4ed5b3b3-0f38-4b37-aed7-559d506174c7"
    _app_id = "8458dc58-7b6c-430b-9ab3-23450774f883"
    # _sequence_no = None  # 23
    _sequence_no = 5
    # main(None, None)
    main(_app_id, _sequence_no)

    # some test code
    # tavily_tool = TavilySearchResults(max_results=5)
    # result = tool_dag.execute(
    #     ["executed_tool_calls"],
    #     inputs={
    #         "tools": [tavily_tool],
    #         "system_message": "You should provide accurate data for the chart generator to use.",
    #         "user_query": "Fetch the UK's GDP over the past 5 years,"
    #         " then draw a line graph of it."
    #         " Once you have written code for the graph, finish.",
    #     },
    # )
    # import pprint
    #
    # pprint.pprint(result)
    #
    # result = tool_dag.execute(
    #     ["executed_tool_calls"],
    #     inputs={
    #         "tools": [python_repl],
    #         "system_message": "Any charts you display will be visible by the user.",
    #         "user_query": "Draw a simple line graph of y = x",
    #     },
    # )
    # import pprint
    #
    # pprint.pprint(result)



---
File: /burr/examples/multi-agent-collaboration/hamilton/func_agent.py
---

"""
Hamilton module that defines the pipeline for
hitting an LLM model and asking it what to do.
"""

import inspect
import json
from typing import Callable

import openai
from langchain_core.utils.function_calling import convert_to_openai_function


def llm_client() -> openai.OpenAI:
    return openai.OpenAI()


def tool_names(tool_function_specs: list[dict]) -> list[str]:
    """Get the names of the tools from the tool function specs."""
    return [tool["function"]["name"] for tool in tool_function_specs]


def _langchain_tool_spec(tool: Callable) -> dict:
    """Converts a tool to a langchain tool spec."""
    t = convert_to_openai_function(tool)
    # print(t)
    return t


def _tool_function_spec(tool: Callable) -> dict:
    """Converts a python function into a specification for function calling.

    This is a little hacky. But it works.

    It takes a function, introspects it, and returns a spec.

    :param tool:
    :return:
    """
    # TODO: maybe just get people to wrap any external tool in a function
    # to make it clear what is going on.
    if hasattr(tool, "name") and hasattr(tool, "description") and hasattr(tool, "args_schema"):
        return {"type": "function", "function": _langchain_tool_spec(tool)}
    func_sig = inspect.signature(tool)
    name = tool.__name__
    docstring = inspect.getdoc(tool)
    doc_lines = docstring.split("\n") if docstring else []
    description = ""
    for line in doc_lines:
        stripped = line.strip()
        if stripped.startswith(":"):
            # we have reached the end of the description
            break
        description += stripped + "\n"
    description = description.strip()
    param_descriptions = {}
    for line in doc_lines:
        stripped = line.strip()
        if stripped.startswith(":param"):
            parts = stripped.split(" ", 2)
            param_name = parts[1].strip(":")
            param_description = parts[2]
            param_descriptions[param_name] = param_description
    parameters = func_sig.parameters
    func_parameters = {}
    required = []
    for param_name, param in parameters.items():
        param_type = param.annotation
        param_description = param_descriptions.get(param_name, "")
        if param_type == str:
            param_type = "string"
        elif param_type == int:
            param_type = "integer"
        elif param_type == float:
            param_type = "float"
        else:
            raise ValueError(f"Unsupported parameter type: {param_type}")
        func_parameters[param_name] = {
            "type": param_type,
            "description": param_description,
        }
        if param.default == inspect.Parameter.empty:
            required.append(param_name)
        else:
            func_parameters[param_name]["description"] += f" Defaults to {param.default}."

    return {
        "type": "function",
        "function": {
            "name": name,
            "description": description,
            "parameters": {
                "type": "object",
                "properties": func_parameters,
                "required": required,
            },
        },
    }


def tool_function_specs(tools: list[Callable]) -> list[dict]:
    """Converts a list of tools into a list of tool function specs."""
    return [_tool_function_spec(tool) for tool in tools]


def base_system_prompt(tool_names: list[str], system_message: str) -> str:
    """Creates the base system prompt for the pipeline."""
    return (
        "You are a helpful AI assistant, collaborating with other assistants."
        " Use the provided tools to progress towards answering the question."
        " If you are unable to fully answer, that's OK, another assistant with different tools "
        " will help where you left off. Execute what you can to make progress.\n\n"
        "If you or any of the other assistants have the final answer or deliverable,"
        " prefix your response with 'FINAL ANSWER' so the team knows to stop.\n\n"
        f"You have access to the following tools: {tool_names}.\n{system_message}\n\n"
        "Remember to prefix your response with 'FINAL ANSWER' if you or another assistant "
        "thinks the task is complete; assume the user can visualize the result."
    )


def message_history(base_system_prompt: str, user_query: str, messages: list[dict]) -> list[dict]:
    """Creates the message history for the LLM model.

    :param base_system_prompt:
    :param user_query:
    :param messages:
    :return:
    """
    base = [
        {"role": "system", "content": base_system_prompt},
        {"role": "user", "content": user_query},
    ]
    sanitized_messages = []
    for message in messages:
        message_copy = message.copy()
        if not isinstance(message_copy["content"], str) and message_copy["content"] is not None:
            message_copy["content"] = (
                json.dumps(message_copy["content"]) if message_copy["content"] else None
            )
        sanitized_messages.append(message_copy)
    return base + sanitized_messages


def llm_function_response(
    message_history: list[dict],
    tool_function_specs: list[dict],
    llm_client: openai.OpenAI,
) -> openai.types.chat.chat_completion.ChatCompletion:
    """Creates the function response from the LLM model for the given prompt & functions.

    :param message_history:
    :param tool_function_specs:
    :param llm_client:
    :return:
    """
    response = llm_client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=message_history,
        tools=tool_function_specs,
        tool_choice="auto",
    )
    return response


def llm_function_message(
    llm_function_response: openai.types.chat.chat_completion.ChatCompletion,
) -> dict:
    """Parses the LLM response message. Does extra parsing for tool invocations."""
    response_message = llm_function_response.choices[0].message
    if response_message.tool_calls:
        return {
            "role": response_message.role,
            "content": None,
            "tool_calls": [
                {
                    "id": t.id,
                    "type": "function",
                    "function": {"name": t.function.name, "arguments": t.function.arguments},
                }
                for t in response_message.tool_calls
            ],
        }
    return {
        "role": "assistant",
        "content": response_message.content,
    }


def parsed_tool_calls(
    llm_function_response: openai.types.chat.chat_completion.ChatCompletion,
) -> list[dict]:
    """Parses the tool calls from the LLM response message.

    :param llm_function_response: The response from the LLM model.
    :return: The parsed tool calls.
    """
    response_message = llm_function_response.choices[0].message
    tool_calls = response_message.tool_calls
    parsed_calls = []
    if tool_calls:
        for tool_call in tool_calls:
            func_call = {
                "id": tool_call.id,
                "function_name": tool_call.function.name,
                "function_args": tool_call.function.arguments,
            }
            parsed_calls.append(func_call)
    return parsed_calls


def executed_tool_calls(
    parsed_tool_calls: list[dict], tools: list[Callable]
) -> list[tuple[str, dict]]:
    """Executes the parsed tool calls.

    :param parsed_tool_calls: The parsed tool calls.
    :param tools: The tools to execute.
    :return: The results of the tool calls.
    """
    results = []
    for tool_call in parsed_tool_calls:
        tool_name = tool_call["function_name"]
        tool_args = tool_call["function_args"]
        tool_found = False
        for tool in tools:
            name = getattr(tool, "name", None)
            if name is None:
                name = tool.__name__
            if name == tool_name:
                tool_found = True
                kwargs = json.loads(tool_args)
                if hasattr(tool, "_run"):
                    result = tool._run(**kwargs)
                else:
                    result = tool(**kwargs)
                results.append(
                    (
                        tool_name,
                        {
                            "tool_call_id": tool_call["id"],
                            "role": "tool",
                            "name": tool_name,
                            "content": result,  # note: might not be a string.
                        },
                    )
                )
        if not tool_found:
            raise ValueError(f"Tool {tool_name} not found.")
    # TODO: do we add a sentinel if no tool call was required.
    return results


def _get_current_weather(location: str, unit: str = "fahrenheit") -> str:
    """Get the current weather in a given location

    Dummy function to simulate a weather API call.

    :param location: the location to get the weather for.
    :param unit: the unit of temperature to return. Celsius or Fahrenheit.
    :return: JSON string with the location and temperature, and unit.
    """
    if "tokyo" in location.lower():
        return json.dumps({"location": "Tokyo", "temperature": "10", "unit": unit})
    elif "san francisco" in location.lower():
        return json.dumps({"location": "San Francisco", "temperature": "72", "unit": unit})
    elif "paris" in location.lower():
        return json.dumps({"location": "Paris", "temperature": "22", "unit": unit})
    else:
        return json.dumps({"location": location, "temperature": "unknown"})


if __name__ == "__main__":
    # some code to test a few things.
    jspec = _tool_function_spec(_get_current_weather)
    import pprint

    pprint.pprint(jspec)

    import __main__
    from hamilton import driver

    dr = driver.Builder().with_modules(__main__).build()
    result = dr.execute(
        ["executed_tool_calls"],
        inputs={
            "tools": [_get_current_weather],
            "system_message": "You are an accurate weather forecaster.",
            "user_query": "What is the weather in Tokyo, Japan? Use celsius.",
            "messages": [],
        },
    )
    print(result)



---
File: /burr/examples/multi-agent-collaboration/lcel/__init__.py
---




---
File: /burr/examples/multi-agent-collaboration/lcel/application.py
---

"""
Langchain version of the multi-agent collaboration example.

This also adds a tracer to the Langchain calls to trace the execution of the nodes
within the Action so that they also show up in the Burr UI. This is a
very simple tracer, it could easily be extended to include more information.
"""
import json
import uuid
from typing import Annotated, Any, Optional
from uuid import UUID

from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import FunctionMessage, HumanMessage
from langchain_core.outputs import LLMResult
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool
from langchain_core.utils.function_calling import convert_to_openai_function
from langchain_experimental.utilities import PythonREPL
from langchain_openai import ChatOpenAI
from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation

from burr import core
from burr.core import Action, State, action, default, expr
from burr.lifecycle import PostRunStepHook
from burr.tracking import client as burr_tclient
from burr.visibility import ActionSpanTracer, TracerFactory

# ---- Define the tools -----
tavily_tool = TavilySearchResults(max_results=5)
repl = PythonREPL()


@tool
def python_repl(code: Annotated[str, "The python code to execute to generate your chart."]):
    """Use this to execute python code. If you want to see the output of a value,
    you should print it out with `print(...)`. This is visible to the user."""
    try:
        # Warning: This executes code locally, which can be unsafe when not sandboxed
        result = repl.run(code)
    except BaseException as e:
        return f"Failed to execute. Error: {repr(e)}"
    return f"Succesfully executed:\n```python\n{code}\n```\nStdout: {result}"


tools = [tavily_tool, python_repl]
tool_executor = ToolExecutor(tools)


# Define the tracer
class LangChainTracer(BaseCallbackHandler):
    """Example tracer to plug into Burr's tracing capture."""

    def __init__(self, tracer: TracerFactory):
        self._tracer: TracerFactory = tracer
        self.active_spans = {}

    def on_llm_start(self, serialized: dict[str, Any], prompts: list[str], **kwargs: Any) -> Any:
        """Run when LLM starts running."""
        model_name = kwargs["invocation_params"]["model_name"]
        run_id = kwargs["run_id"]
        name = (model_name + "_" + str(run_id))[:30]
        context_manager: ActionSpanTracer = self._tracer(name)
        context_manager.__enter__()
        self.active_spans[name] = context_manager

    def on_llm_end(
        self,
        response: LLMResult,
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        **kwargs: Any,
    ) -> Any:
        """Run when LLM ends running."""
        model_name = response.llm_output["model_name"]
        name = (model_name + "_" + str(run_id))[:30]
        context_manager = self.active_spans.pop(name)
        context_manager.__exit__(None, None, None)


# Agents / actions
def create_agent(llm, tools, system_message: str):
    """Helper function to create an agent with a system message and tools."""
    functions = [convert_to_openai_function(t) for t in tools]

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a helpful AI assistant, collaborating with other assistants."
                " Use the provided tools to progress towards answering the question."
                " If you are unable to fully answer, that's OK, another assistant with different tools "
                " will help where you left off. Execute what you can to make progress."
                " If you or any of the other assistants have the final answer or deliverable,"
                " prefix your response with FINAL ANSWER so the team knows to stop."
                " You have access to the following tools: {tool_names}.\n{system_message}",
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )
    prompt = prompt.partial(system_message=system_message)
    prompt = prompt.partial(tool_names=", ".join([tool.name for tool in tools]))
    return prompt | llm.bind_functions(functions)


def _exercise_agent(messages: list, sender: str, agent, name: str, tracer: TracerFactory) -> dict:
    """Helper function to exercise the agent code."""
    tracer = LangChainTracer(tracer)
    result = agent.invoke({"messages": messages, "sender": sender}, config={"callbacks": [tracer]})
    # We convert the agent output into a format that is suitable to append to the global state
    if isinstance(result, FunctionMessage):
        pass
    else:
        result = HumanMessage(**result.dict(exclude={"type", "name"}), name=name)
    return {
        "messages": result,
        # Since we have a strict workflow, we can
        # track the sender so we know who to pass to next.
        "sender": name,
    }


# Define the actual agents via langchain
llm = ChatOpenAI(model="gpt-4-1106-preview")
research_agent = create_agent(
    llm,
    [tavily_tool],
    system_message="You should provide accurate data for the chart generator to use.",
)
chart_agent = create_agent(
    llm,
    [python_repl],
    system_message="Any charts you display will be visible by the user.",
)


@action(reads=["messages", "sender"], writes=["messages", "sender"])
def research_node(state: State, __tracer: TracerFactory) -> tuple[dict, State]:
    # Research agent and node
    result = _exercise_agent(
        state["messages"], state["sender"], research_agent, "Researcher", __tracer
    )
    return result, state.append(messages=result["messages"]).update(sender="Researcher")


@action(reads=["messages", "sender"], writes=["messages", "sender"])
def chart_node(state: State, __tracer: TracerFactory) -> tuple[dict, State]:
    # Chart agent and node
    result = _exercise_agent(
        state["messages"], state["sender"], chart_agent, "Chart Generator", __tracer
    )
    return result, state.append(messages=result["messages"]).update(sender="Chart Generator")


@action(reads=["messages"], writes=["messages"])
def tool_node(state: State) -> tuple[dict, State]:
    """This runs tools in the graph

    It takes in an agent action and calls that tool and returns the result."""
    messages = state["messages"]
    # Based on the continue condition
    # we know the last message involves a function call
    last_message = messages[-1]
    # We construct an ToolInvocation from the function_call
    tool_input = json.loads(last_message.additional_kwargs["function_call"]["arguments"])
    # We can pass single-arg inputs by value
    if len(tool_input) == 1 and "__arg1" in tool_input:
        tool_input = next(iter(tool_input.values()))
    tool_name = last_message.additional_kwargs["function_call"]["name"]
    action = ToolInvocation(
        tool=tool_name,
        tool_input=tool_input,
    )
    # We call the tool_executor and get back a response
    response = tool_executor.invoke(action)
    # We use the response to create a FunctionMessage
    function_message = FunctionMessage(
        content=f"{tool_name} response: {str(response)}", name=action.tool
    )
    # We return a list, because this will get added to the existing list
    return {"messages": [function_message]}, state.append(messages=function_message)


@action(reads=[], writes=[])
def terminal_step(state: State) -> tuple[dict, State]:
    """Terminal step we have here that does nothing, but it could"""
    return {}, state


class PrintStepHook(PostRunStepHook):
    def post_run_step(self, *, state: "State", action: "Action", **future_kwargs):
        print("action=====\n", action)
        print("state======\n", state)


def default_state_and_entry_point(query: str = None) -> tuple[dict, str]:
    """Sets the default state & entry point
    :param query: the query for the agents to work on.
    :return:
    """
    if query is None:
        query = (
            "Fetch the UK's GDP over the past 5 years,"
            " then draw a line graph of it."
            " Once you code it up, finish."
        )
    return (
        dict(
            messages=[HumanMessage(content=query)],
            sender=None,
        ),
        "researcher",
    )


def main(query: str = None, app_instance_id: str = None, sequence_id: int = None):
    """Main function to run the multi-agent collaboration example.

    Pass in a query to start from a specific query.
    Pass in an app_instance_id to restart from a previous run.
    Pass in an sequence_id to restart from a previous run and a specific position in it.
    """
    if app_instance_id is None:
        app_instance_id = str(uuid.uuid4())
    project_name = "demo_lcel-multi-agent"
    tracker_persister = burr_tclient.LocalTrackingClient(project_name)
    default_state, default_entrypoint = default_state_and_entry_point(query)
    app = (
        core.ApplicationBuilder()
        .with_actions(
            researcher=research_node,
            charter=chart_node,
            call_tool=tool_node,
            terminal=terminal_step,
        )
        .with_transitions(
            ("researcher", "call_tool", expr("'function_call' in messages[-1].additional_kwargs")),
            ("researcher", "terminal", expr("'FINAL ANSWER' in messages[-1].content")),
            ("researcher", "charter", default),
            ("charter", "call_tool", expr("'function_call' in messages[-1].additional_kwargs")),
            ("charter", "terminal", expr("'FINAL ANSWER' in messages[-1].content")),
            ("charter", "researcher", default),
            ("call_tool", "researcher", expr("sender == 'Researcher'")),
            ("call_tool", "charter", expr("sender == 'Chart Generator'")),
        )
        .with_identifiers(
            app_id=app_instance_id, partition_key="sample_user", sequence_id=sequence_id
        )
        .initialize_from(
            tracker_persister,
            resume_at_next_action=True,
            default_state=default_state,
            default_entrypoint=default_entrypoint,
        )
        .with_hooks(PrintStepHook())
        .with_tracker(tracker_persister)
        .build()
    )
    app.visualize(
        output_file_path="lcel-multi-agent", include_conditions=True, view=True, format="png"
    )
    app.run(halt_after=["terminal"])


if __name__ == "__main__":
    main(app_instance_id="e80f405b-2c79-4bc9-88d2-23413ceb5881", sequence_id=8)
    # main("Fetch the UK's GDP over the past 5 years,"
    #                  " then draw a line graph of it."
    #                  " Once you code it up, finish.")
    # main(app_instance_id=SOME_APP_ID)  # use this to restart from a previous state



---
File: /burr/examples/multi-agent-collaboration/__init__.py
---




---
File: /burr/examples/multi-modal-chatbot/__init__.py
---




---
File: /burr/examples/multi-modal-chatbot/application.py
---

import copy
import os
from typing import List, Optional

import openai

from burr.core import Application, ApplicationBuilder, State, default, graph, when
from burr.core.action import action
from burr.lifecycle import LifecycleAdapter
from burr.tracking import LocalTrackingClient

MODES = {
    "answer_question": "text",
    "generate_image": "image",
    "generate_code": "code",
    "unknown": "text",
}


@action(reads=[], writes=["chat_history", "prompt"])
def process_prompt(state: State, prompt: str) -> State:
    result = {"chat_item": {"role": "user", "content": prompt, "type": "text"}}
    return (
        state.wipe(keep=["prompt", "chat_history"])
        .append(chat_history=result["chat_item"])
        .update(prompt=prompt)
    )


@action(reads=[], writes=["has_openai_key"])
def check_openai_key(state: State) -> State:
    result = {"has_openai_key": "OPENAI_API_KEY" in os.environ}
    return state.update(**result)


@action(reads=["prompt"], writes=["safe"])
def check_safety(state: State) -> State:
    result = {"safe": "unsafe" not in state["prompt"]}  # quick hack to demonstrate
    return state.update(safe=result["safe"])


def _get_openai_client():
    return openai.Client()


@action(reads=["prompt"], writes=["mode"])
def choose_mode(state: State) -> State:
    prompt = (
        f"You are a chatbot. You've been prompted this: {state['prompt']}. "
        f"You have the capability of responding in the following modes: {', '.join(MODES)}. "
        "Please respond with *only* a single word representing the mode that most accurately "
        "corresponds to the prompt. Fr instance, if the prompt is 'draw a picture of a cat', "
        "the mode would be 'generate_image'. If the prompt is 'what is the capital of France', the mode would be 'answer_question'."
        "If none of these modes apply, please respond with 'unknown'."
    )

    result = _get_openai_client().chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant"},
            {"role": "user", "content": prompt},
        ],
    )
    content = result.choices[0].message.content
    mode = content.lower()
    if mode not in MODES:
        mode = "unknown"
    result = {"mode": mode}
    return state.update(**result)


@action(reads=["prompt", "chat_history"], writes=["response"])
def prompt_for_more(state: State) -> State:
    result = {
        "response": {
            "content": "None of the response modes I support apply to your question. Please clarify?",
            "type": "text",
            "role": "assistant",
        }
    }
    return state.update(**result)


@action(reads=["prompt", "chat_history", "mode"], writes=["response"])
def chat_response(
    state: State, prepend_prompt: str, display_type: str = "text", model: str = "gpt-3.5-turbo"
) -> State:
    chat_history = copy.deepcopy(state["chat_history"])
    chat_history[-1]["content"] = f"{prepend_prompt}: {chat_history[-1]['content']}"
    chat_history_api_format = [
        {
            "role": chat["role"],
            "content": chat["content"],
        }
        for chat in chat_history
    ]
    client = _get_openai_client()
    result = client.chat.completions.create(
        model=model,
        messages=chat_history_api_format,
    )
    response = result.choices[0].message.content
    result = {"response": {"content": response, "type": MODES[state["mode"]], "role": "assistant"}}
    return state.update(**result)


@action(reads=["prompt", "chat_history", "mode"], writes=["response"])
def image_response(state: State, model: str = "dall-e-2") -> State:
    """Generates an image response to the prompt. Optional save function to save the image to a URL."""
    client = _get_openai_client()
    result = client.images.generate(
        model=model, prompt=state["prompt"], size="1024x1024", quality="standard", n=1
    )
    response = result.data[0].url
    result = {"response": {"content": response, "type": MODES[state["mode"]], "role": "assistant"}}
    return state.update(**result)


@action(reads=["response", "mode", "safe", "has_openai_key"], writes=["chat_history"])
def response(state: State) -> State:
    if not state["has_openai_key"]:
        result = {
            "chat_item": {
                "role": "assistant",
                "content": "You have not set an API key for [OpenAI](https://www.openai.com). Do this "
                "by setting the environment variable `OPENAI_API_KEY` to your key. "
                "You can get a key at [OpenAI](https://platform.openai.com). "
                "You can still look at chat history/examples.",
                "type": "error",
            }
        }
    elif not state["safe"]:
        result = {
            "chat_item": {
                "role": "assistant",
                "content": "I'm sorry, I can't respond to that.",
                "type": "error",
            }
        }
    else:
        result = {"chat_item": state["response"]}
    return state.append(chat_history=result["chat_item"])


graph = (
    graph.GraphBuilder()
    .with_actions(
        prompt=process_prompt,
        check_openai_key=check_openai_key,
        check_safety=check_safety,
        decide_mode=choose_mode,
        generate_image=image_response,
        generate_code=chat_response.bind(
            prepend_prompt="Please respond with *only* code and no other text (at all) to the following:",
        ),
        answer_question=chat_response.bind(
            prepend_prompt="Please answer the following question:",
        ),
        prompt_for_more=prompt_for_more,
        response=response,
    )
    .with_transitions(
        ("prompt", "check_openai_key", default),
        ("check_openai_key", "check_safety", when(has_openai_key=True)),
        ("check_openai_key", "response", default),
        ("check_safety", "decide_mode", when(safe=True)),
        ("check_safety", "response", default),
        ("decide_mode", "generate_image", when(mode="generate_image")),
        ("decide_mode", "generate_code", when(mode="generate_code")),
        ("decide_mode", "answer_question", when(mode="answer_question")),
        ("decide_mode", "prompt_for_more", default),
        (
            ["generate_image", "answer_question", "generate_code", "prompt_for_more"],
            "response",
        ),
        ("response", "prompt", default),
    )
    .build()
)


def base_application(
    hooks: List[LifecycleAdapter],
    app_id: str,
    storage_dir: str,
    project_id: str,
):
    if hooks is None:
        hooks = []
    # we're initializing above so we can load from this as well
    # we could also use `with_tracker("local", project=project_id, params={"storage_dir": storage_dir})`
    tracker = LocalTrackingClient(project=project_id, storage_dir=storage_dir)
    return (
        ApplicationBuilder()
        .with_graph(graph)
        # initializes from the tracking log if it does not already exist
        .initialize_from(
            tracker,
            resume_at_next_action=False,  # always resume from entrypoint in the case of failure
            default_state={"chat_history": []},
            default_entrypoint="prompt",
        )
        .with_hooks(*hooks)
        .with_tracker(tracker)
        .with_identifiers(app_id=app_id)
        .build()
    )


def application(
    app_id: Optional[str] = None,
    project_id: str = "demo_chatbot",
    storage_dir: Optional[str] = "~/.burr",
    hooks: Optional[List[LifecycleAdapter]] = None,
) -> Application:
    return base_application(hooks, app_id, storage_dir, project_id=project_id)


if __name__ == "__main__":
    app = application()
    app.visualize(
        output_file_path="statemachine", include_conditions=False, view=True, format="png"
    )
    print(app.run(halt_after=["response"], inputs={"prompt": "Who was Aaron Burr, sir?"}))



---
File: /burr/examples/multi-modal-chatbot/server.py
---

import functools
import importlib
from typing import List, Literal

import pydantic
from fastapi import APIRouter

from burr.core import Application, ApplicationBuilder
from burr.tracking import LocalTrackingClient

"""This file represents a simple chatbot API backed with Burr.
We manage an application, write to it with post endpoints, and read with
get/ endpoints.

This demonstrates how you can build interactive web applications with Burr!
"""
# We're doing dynamic import cause this lives within examples/ (and that module has dashes)
# navigate to the examples directory to read more about this!
chat_application = importlib.import_module(
    "burr.examples.multi-modal-chatbot.application"
)  # noqa: F401

# the app is commented out as we include the router.
# app = FastAPI()

router = APIRouter()

graph = chat_application.graph

try:
    from opentelemetry.instrumentation.openai import OpenAIInstrumentor

    OpenAIInstrumentor().instrument()
    opentelemetry_available = True
except ImportError:
    opentelemetry_available = False


class ChatItem(pydantic.BaseModel):
    """Pydantic model for a chat item. This is used to render the chat history."""

    content: str
    type: Literal["image", "text", "code", "error"]
    role: Literal["user", "assistant"]


@functools.lru_cache(maxsize=128)
def _get_application(project_id: str, app_id: str) -> Application:
    """Quick tool to get the application -- caches it"""
    tracker = LocalTrackingClient(project=project_id, storage_dir="~/.burr")
    return (
        ApplicationBuilder()
        .with_graph(graph)
        # initializes from the tracking log if it does not already exist
        .initialize_from(
            tracker,
            resume_at_next_action=False,  # always resume from entrypoint in the case of failure
            default_state={"chat_history": []},
            default_entrypoint="prompt",
        )
        .with_tracker(tracker, use_otel_tracing=opentelemetry_available)
        .with_identifiers(app_id=app_id)
        .build()
    )


@router.post("/response/{{project_id}}/{{app_id}}", response_model=List[ChatItem])
def chat_response(project_id: str, app_id: str, prompt: str) -> List[ChatItem]:
    """Chat response endpoint. User passes in a prompt and the system returns the
    full chat history, so its easier to render.

    :param project_id: Project ID to run
    :param app_id: Application ID to run
    :param prompt: Prompt to send to the chatbot
    :return:
    """
    burr_app = _get_application(project_id, app_id)
    _, _, state = burr_app.run(halt_after=["response"], inputs=dict(prompt=prompt))
    return state.get("chat_history", [])


@router.get("/response/{project_id}/{app_id}", response_model=List[ChatItem])
def chat_history(project_id: str, app_id: str) -> List[ChatItem]:
    """Endpoint to get chat history. Gets the application and returns the chat history from state.

    :param project_id: Project ID
    :param app_id: App ID.
    :return: The list of chat items in the state
    """
    chat_app = _get_application(project_id, app_id)
    state = chat_app.state
    return state.get("chat_history", [])


@router.post("/create/{project_id}/{app_id}", response_model=str)
async def create_new_application(project_id: str, app_id: str) -> str:
    """Endpoint to create a new application -- used by the FE when
    the user types in a new App ID

    :param project_id: Project ID
    :param app_id: App ID
    :return: The app ID
    """
    # side-effect of this persists it -- see the application function for details
    _get_application(app_id=app_id, project_id=project_id)
    return app_id  # just return it for now


# comment this back in fro a standalone chatbot API
# app.include_router(router, prefix="/api/v0/chatbot")



---
File: /burr/examples/multi-modal-chatbot/simple_streamlit_app.py
---

import uuid

import application as chatbot_application
import streamlit as st

import burr.core


def render_chat_message(chat_item: dict):
    content = chat_item["content"]
    content_type = chat_item["type"]
    role = chat_item["role"]
    with st.chat_message(role):
        if content_type == "image":
            st.image(content)
        elif content_type == "code":
            st.code(content)
        elif content_type == "text":
            st.write(content)


def initialize_app() -> burr.core.Application:
    if "burr_app" not in st.session_state:
        st.session_state.burr_app = chatbot_application.application(
            app_id=f"chat:{str(uuid.uuid4())[0:6]}"
        )
    return st.session_state.burr_app


def main():
    st.title("Chatbot example with Burr")
    app = initialize_app()

    prompt = st.chat_input("Ask me a question!", key="chat_input")
    for chat_message in app.state.get("chat_history", []):
        render_chat_message(chat_message)
    if prompt:
        for action, result, state in app.iterate(
            inputs={"prompt": prompt}, halt_after=["response"]
        ):
            if action.name in ["prompt", "response"]:
                last_chat_item = state.get("chat_history", [])[-1]
                render_chat_message(last_chat_item)


if __name__ == "__main__":
    main()



---
File: /burr/examples/multi-modal-chatbot/streamlit_app.py
---

from typing import Optional

import application as chatbot_application
import streamlit as st

from burr.integrations.streamlit import (
    AppState,
    Record,
    get_state,
    render_explorer,
    set_slider_to_current,
    update_state,
)

st.set_page_config(layout="wide")

st.markdown(
    "This is a prototype streamlit app -- this is a UI for demonstration purposes, but you can"
    " run the real UI by running `burr` -- a window will pop up and you can follow the results live!"
)


def render_chat_message(record: Record):
    if record.action in ["prompt", "response"]:
        recent_chat_message = record.state["chat_history"][-1]
        content = recent_chat_message["content"]
        content_type = recent_chat_message["type"]
        role = recent_chat_message["role"]
        with st.chat_message(role):
            if content_type == "image":
                st.image(content)
            elif content_type == "code":
                st.code(content)
            elif content_type == "text":
                st.write(content)
            elif content_type == "error":
                st.error(content)


def retrieve_state():
    if "burr_state" not in st.session_state:
        # TODO --enable usage of hamilton. Currently its not wiring in inputs
        # But it should be easy enough
        state = AppState.from_empty(
            app=chatbot_application.application(),
        )
    else:
        state = get_state()
    return state


def chatbot_step(app_state: AppState, prompt: Optional[str]) -> bool:
    """Pushes state forward for the chatbot. Returns whether or not to rerun the app.

    :param app_state: State of the app
    :param prompt: Prompt to set the chatbot to. If this is None it means it should continue and not be reset.
    :return:
    """
    inputs = None
    if prompt is not None:
        # We need to update
        inputs = {"prompt": prompt}
        # app_state.app.update_state(app_state.app.state.update(prompt=prompt))
        st.session_state.running = True  # set to running
    # if its not running this is a no-op
    if not st.session_state.get("running", False):
        return False
    application = app_state.app
    step_output = application.step(inputs=inputs)
    if step_output is None:
        st.session_state.running = False
        return False
    action, result, state = step_output
    app_state.history.append(Record(state.get_all(), action.name, result))
    set_slider_to_current()
    if action.name == "response":
        # we've gotten to the end
        st.session_state.running = False
        return True  # run one last time
    return True


def main():
    st.title("Chatbot example with Burr")
    app_state = retrieve_state()  # retrieve first so we can use for the ret of the step
    columns = st.columns(2)
    with columns[0]:
        prompt = st.chat_input(
            "...", disabled=st.session_state.get("running", False), key="chat_input"
        )
        should_rerun = chatbot_step(app_state, prompt)
        with st.container(height=850):
            for item in app_state.history:
                render_chat_message(item)
    with columns[1]:
        render_explorer(app_state)
    update_state(app_state)  # update so the next iteration knows what to do
    if should_rerun:
        st.rerun()


if __name__ == "__main__":
    main()



---
File: /burr/examples/openai-compatible-agent/__init__.py
---




---
File: /burr/examples/openai-compatible-agent/application.py
---

import datetime

import burr.core
from burr.core import Application, State
from burr.core.action import action


@action(reads=[], writes=[])
def dummy_bot(state: State, user_input: str):
    if "time" in user_input:
        current_time = datetime.datetime.now()
        reply = f"It is currently {current_time}"
    else:
        reply = "ðŸ¤– Ask me about the time"

    results = dict(content=reply)
    return results, state.update(**results)


def build_application() -> Application:
    return (
        burr.core.ApplicationBuilder()
        .with_actions(dummy_bot)
        .with_transitions(("dummy_bot", "dummy_bot"))
        .with_identifiers(app_id="burr-openai")
        .with_entrypoint("dummy_bot")
        .build()
    )


if __name__ == "__main__":
    app = build_application()
    app.visualize(
        output_file_path="statemachine",
        include_conditions=False,
        view=True,
        format="png",
    )



---
File: /burr/examples/openai-compatible-agent/server.py
---

import logging
import time
import uuid
from contextlib import asynccontextmanager
from typing import Optional

import application as my_agent
import fastapi
import uvicorn
from openai.types.chat import ChatCompletion, ChatCompletionMessage
from openai.types.chat.chat_completion import Choice

from burr.core import Application

logger = logging.getLogger(__name__)


burr_app: Optional[Application] = None


def get_burr_app() -> Application:
    """Retrieve the global Burr app."""
    if burr_app is None:
        raise RuntimeError("Burr app wasn't instantiated.")
    return burr_app


@asynccontextmanager
async def lifespan(app: fastapi.FastAPI):
    """Instantiate the Burr application on FastAPI startup."""
    global burr_app
    burr_app = my_agent.build_application()
    yield


app = fastapi.FastAPI(lifespan=lifespan)


@app.post("/v1/chat/completions")
async def create_chat_completion(
    request: fastapi.Request, burr_app: Application = fastapi.Depends(get_burr_app)
):
    """Creates a completion for the chat message"""
    request_json = await request.json()
    latest_message = request_json["messages"][-1]["content"]

    _, result, _ = burr_app.run(halt_after=["dummy_bot"], inputs={"user_input": latest_message})

    return ChatCompletion(
        id=f"{uuid.uuid4()}",
        created=int(time.time()),
        model="burr-app",
        object="chat.completion",
        choices=[
            Choice(
                index=0,
                message=ChatCompletionMessage(
                    role="assistant",
                    content=result["content"],
                ),
                finish_reason="stop",
            )
        ],
    )


if __name__ == "__main__":
    uvicorn.run("server:app", host="127.0.0.1", port=7443, reload=True)



---
File: /burr/examples/opentelemetry/__init__.py
---




---
File: /burr/examples/opentelemetry/application.py
---

import os
from typing import Optional, Tuple

import openai
from opentelemetry.instrumentation.openai import OpenAIInstrumentor
from traceloop.sdk import Traceloop

from burr.core import Application, ApplicationBuilder, State, default, when
from burr.core.action import action
from burr.core.graph import GraphBuilder
from burr.integrations.opentelemetry import OpenTelemetryBridge
from burr.visibility import TracerFactory, trace

MODES = {
    "answer_question": "text",
    "generate_image": "image",
    "generate_code": "code",
    "unknown": "text",
}


@action(reads=[], writes=["chat_history", "prompt"])
def process_prompt(state: State, prompt: str, __tracer: TracerFactory) -> Tuple[dict, State]:
    result = {"chat_item": {"role": "user", "content": prompt, "type": "text"}}
    return result, state.wipe(keep=["prompt", "chat_history"]).append(
        chat_history=result["chat_item"]
    ).update(prompt=prompt)


@action(reads=["prompt"], writes=["safe"])
def check_safety(state: State, __tracer: TracerFactory) -> Tuple[dict, State]:
    result = {"safe": "unsafe" not in state["prompt"]}  # quick hack to demonstrate
    return result, state.update(safe=result["safe"])


@trace()
def _get_openai_client():
    return openai.Client()


@trace()
def _query_openai(prompt: str, model: str = "gpt-4", chat_history: Optional[list] = None):
    chat_history = chat_history or []
    client = _get_openai_client()
    result = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are a helpful assistant"},
            *chat_history,
            {"role": "user", "content": prompt},
        ],
    )
    return result.choices[0].message.content


@action(reads=["prompt"], writes=["mode"])
def decide_mode(state: State, __tracer: TracerFactory) -> Tuple[dict, State]:
    prompt = (
        f"You are a chatbot. You've been prompted this: {state['prompt']}. "
        f"You have the capability of responding in the following modes: {', '.join(MODES)}. "
        "Please respond with *only* a single word representing the mode that most accurately "
        "corresponds to the prompt. For instance, if the prompt is 'draw a picture of a cat', "
        "the mode would be 'generate_image'. If the prompt is 'what is the capital of France', the mode would be 'answer_question'."
        "If none of these modes apply, please respond with 'unknown'."
    )
    content = _query_openai(prompt, model="gpt-4o")
    mode = content.lower()
    if mode not in MODES:
        mode = "unknown"
    result = {"mode": mode}
    return result, state.update(**result)


@action(reads=["prompt", "chat_history"], writes=["response"])
def prompt_for_more(state: State) -> Tuple[dict, State]:
    result = {
        "response": {
            "content": "None of the response modes I support apply to your question. Please clarify?",
            "type": "text",
            "role": "assistant",
        }
    }
    return result, state.update(**result)


@action(reads=["prompt", "chat_history", "mode"], writes=["response"])
def chat_response(
    state: State,
    prepend_prompt: str,
    model: str = "gpt-4o",
) -> State:
    chat_history = state["chat_history"].copy()
    chat_history_to_send = chat_history[:-1]
    prompt = f"{prepend_prompt}: {chat_history[-1]['content']}"
    response = _query_openai(prompt=prompt, chat_history=chat_history_to_send, model=model)
    return state.update(response={"content": response, "type": "text", "role": "assistant"})


@action(reads=["prompt", "chat_history", "mode"], writes=["response"])
def image_response(
    state: State, __tracer: TracerFactory, model: str = "dall-e-2"
) -> Tuple[dict, State]:
    client = _get_openai_client()
    with __tracer("query_openai_image", span_dependencies=["create_openai_client"]):
        result = client.images.generate(
            model=model, prompt=state["prompt"], size="1024x1024", quality="standard", n=1
        )
        response = result.data[0].url
    result = {"response": {"content": response, "type": MODES[state["mode"]], "role": "assistant"}}
    return result, state.update(**result)


@action(reads=["response", "safe", "mode"], writes=["chat_history"])
def response(state: State) -> Tuple[dict, State]:
    if not state["safe"]:
        result = {
            "chat_item": {
                "role": "assistant",
                "content": "I'm sorry, I can't respond to that.",
                "type": "text",
            }
        }
    else:
        result = {"chat_item": state["response"]}
    return result, state.append(chat_history=result["chat_item"])


graph = (
    GraphBuilder()
    .with_actions(
        prompt=process_prompt,
        check_safety=check_safety,
        decide_mode=decide_mode,
        generate_image=image_response,
        generate_code=chat_response.bind(
            prepend_prompt="Please respond with *only* code and no other text (at all) to the following:",
        ),
        answer_question=chat_response.bind(
            prepend_prompt="Please answer the following question:",
        ),
        prompt_for_more=prompt_for_more,
        response=response,
    )
    .with_transitions(
        ("prompt", "check_safety", default),
        ("check_safety", "decide_mode", when(safe=True)),
        ("check_safety", "response", default),
        ("decide_mode", "generate_image", when(mode="generate_image")),
        ("decide_mode", "generate_code", when(mode="generate_code")),
        ("decide_mode", "answer_question", when(mode="answer_question")),
        ("decide_mode", "prompt_for_more", default),
        (
            ["generate_image", "answer_question", "generate_code", "prompt_for_more"],
            "response",
        ),
        ("response", "prompt", default),
    )
    .build()
)


def application_burr_as_otel_provider(
    app_id: Optional[str] = None,
    storage_dir: Optional[str] = "~/.burr",
) -> Application:
    """Runs the application with Burr as the opentelemetry provider"""
    return (
        ApplicationBuilder()
        .with_entrypoint("prompt")
        .with_state(chat_history=[])
        .with_graph(graph)
        .with_tracker(
            project="demo_trace_decorator",
            params={"storage_dir": storage_dir},
            use_otel_tracing=True,
        )
        .with_identifiers(app_id=app_id)
        .build()
    )


def application_traceloop_as_otel_provider(
    app_id: Optional[str] = None,
    storage_dir: Optional[str] = "~/.burr",
) -> Application:
    if "TRACELOOP_API_KEY" not in os.environ:
        raise ValueError("Please set the TRACELOOP_API_KEY environment variable")
    Traceloop.init(app_name="burr_demo_traceloop", api_key=os.environ.get("TRACELOOP_API_KEY"))
    return (
        ApplicationBuilder()
        .with_entrypoint("prompt")
        .with_state(chat_history=[])
        .with_graph(graph)
        .with_tracker(project="demo_traceloop", params={"storage_dir": storage_dir})
        .with_hooks(OpenTelemetryBridge())
        .with_identifiers(app_id=app_id)
        .build()
    )


if __name__ == "__main__":
    # Instrument OpenAI API
    OpenAIInstrumentor().instrument()
    # Choose which one to use
    # To use traceloop, uncomment this
    # app = application_traceloop_as_otel_provider()
    # To use Burr, keep this uncommented
    app = application_burr_as_otel_provider(app_id="with_decorator")
    # app = application_burr_as_otel_provider(app_id="vanilla_tracing")
    app.visualize(output_file_path="statemachine", include_conditions=True, view=True, format="png")
    app.run(halt_after=["response"], inputs={"prompt": "Who was Aaron Burr, sir?."})
    import time

    time.sleep(1.5)
    app.run(
        halt_after=["response"],
        inputs={
            "prompt": "Please write a function that prints a quick poem about Aaron Burr and Alexander Hamilton."
        },
    )



---
File: /burr/examples/other-examples/cowsay/application.py
---

import random
import time
from typing import Tuple

import cowsay

from burr.core import Action, Application, ApplicationBuilder, State, default, expr
from burr.core.action import action
from burr.lifecycle import PostRunStepHook


class PrintWhatTheCowSaid(PostRunStepHook):
    def post_run_step(self, *, state: "State", action: "Action", **future_kwargs):
        if action.name != "cow_should_say" and state["cow_said"] is not None:
            print(state["cow_said"])


class CowCantSpeakFast(PostRunStepHook):
    def __init__(self, sleep_time: float):
        super(PostRunStepHook, self).__init__()
        self.sleep_time = sleep_time

    def post_run_step(self, *, state: "State", action: "Action", **future_kwargs):
        if action.name != "cow_should_say":  # no need to print if we're not saying anything
            time.sleep(self.sleep_time)


@action(reads=[], writes=["cow_said"])
def cow_said(state: State, say_what: list[str]) -> Tuple[dict, State]:
    said = random.choice(say_what)
    result = {"cow_said": cowsay.get_output_string("cow", said) if say_what is not None else None}
    return result, state.update(**result)


@action(reads=[], writes=["cow_should_speak"])
def cow_should_speak(state: State) -> Tuple[dict, State]:
    result = {"cow_should_speak": random.randint(0, 3) == 0}
    return result, state.update(**result)


def application(in_terminal: bool = False) -> Application:
    hooks = (
        [
            PrintWhatTheCowSaid(),
            CowCantSpeakFast(sleep_time=2.0),
        ]
        if in_terminal
        else []
    )
    return (
        ApplicationBuilder()
        .with_state(cow_said=None)
        .with_actions(
            say_nothing=cow_said.bind(say_what=None),
            say_hello=cow_said.bind(
                say_what=["Hello world!", "What's up?", "Are you Aaron Burr, sir?"]
            ),
            cow_should_speak=cow_should_speak,
        )
        .with_transitions(
            ("cow_should_speak", "say_hello", expr("cow_should_speak")),
            ("say_hello", "cow_should_speak", default),
            ("cow_should_speak", "say_nothing", expr("not cow_should_speak")),
            ("say_nothing", "cow_should_speak", default),
        )
        .with_entrypoint("cow_should_speak")
        .with_hooks(*hooks)
        .build()
    )


if __name__ == "__main__":
    app = application(in_terminal=True)
    app.visualize(output_file_path="digraph", include_conditions=True, view=True, format="png")
    while True:
        action, result, state = app.step()



---
File: /burr/examples/other-examples/cowsay/streamlit_app.py
---

import application as cowsay_application
import streamlit as st

from burr.integrations.streamlit import (
    AppState,
    Record,
    get_state,
    render_explorer,
    set_slider_to_current,
    update_state,
)


def cow_say_view(app_state: AppState):
    application = app_state.app
    button = st.button("Cow say?", use_container_width=True)
    if button:
        step_output = application.step()
        action, result, state = step_output
        app_state.history.append(Record(state.get_all(), action.name, result))
        set_slider_to_current()


def render_cow_said(record: Record):
    cow_said = record.state.get("cow_said")
    cow_should_speak = record.state.get("cow_should_speak")
    with st.chat_message("cow", avatar="ðŸ®" if cow_should_speak and cow_said else "ðŸ’­"):
        grey = "#A9A9A9"
        if record.action == "cow_should_say":
            if cow_should_speak:
                st.markdown(
                    f"<p style='color: {grey};'>Cow will not speak.</p>", unsafe_allow_html=True
                )
            else:
                st.markdown(
                    f"<p style='color: {grey};'>Cow will not speak.</p>", unsafe_allow_html=True
                )
        else:
            if cow_said is None:
                st.markdown(f"<p style='color: {grey};'>...</p>", unsafe_allow_html=True)
            if cow_said is not None:
                st.code(cow_said, language="plaintext")


def retrieve_state():
    if "burr_state" not in st.session_state:
        state = AppState.from_empty(app=cowsay_application.application())
    else:
        state = get_state()
    return state


def main():
    st.set_page_config(layout="wide")
    st.title("Talking cows with Burr")
    app_state = retrieve_state()  # retrieve first so we can use for the ret of the step
    columns = st.columns(2)
    with columns[0]:
        cow_say_view(app_state)
        with st.container(height=800):
            for item in app_state.history:
                render_cow_said(item)
    with columns[1]:
        render_explorer(app_state)
    update_state(app_state)  # update so the next iteration knows what to do


if __name__ == "__main__":
    main()



---
File: /burr/examples/other-examples/hamilton-multi-modal/__init__.py
---




---
File: /burr/examples/other-examples/hamilton-multi-modal/application.py
---

from typing import List

import dag
from hamilton import driver

from burr.core import ApplicationBuilder, default, when
from burr.integrations.hamilton import Hamilton, append_state, from_state, update_state
from burr.lifecycle import LifecycleAdapter


def application(hooks: List[LifecycleAdapter], app_id: str, storage_dir: str, project_id: str):
    dr = driver.Driver({"provider": "openai"}, dag)  # TODO -- add modules
    Hamilton.set_driver(dr)
    application = (
        ApplicationBuilder()
        .with_state(chat_history=[], prompt="Draw an image of a turtle saying 'hello, world'")
        .with_entrypoint("prompt")
        .with_state(chat_history=[])
        .with_actions(
            prompt=Hamilton(
                inputs={"prompt": from_state("prompt")},
                outputs={"processed_prompt": append_state("chat_history")},
            ),
            check_safety=Hamilton(
                inputs={"prompt": from_state("prompt")},
                outputs={"safe": update_state("safe")},
            ),
            decide_mode=Hamilton(
                inputs={"prompt": from_state("prompt")},
                outputs={"mode": update_state("mode")},
            ),
            generate_image=Hamilton(
                inputs={"prompt": from_state("prompt")},
                outputs={"generated_image": update_state("response")},
            ),
            generate_code=Hamilton(  # TODO -- implement
                inputs={"chat_history": from_state("chat_history")},
                outputs={"generated_code": update_state("response")},
            ),
            answer_question=Hamilton(  # TODO -- implement
                inputs={"chat_history": from_state("chat_history")},
                outputs={"answered_question": update_state("response")},
            ),
            prompt_for_more=Hamilton(
                inputs={},
                outputs={"prompt_for_more": update_state("response")},
            ),
            response=Hamilton(
                inputs={
                    "response": from_state("response"),
                    "safe": from_state("safe"),
                    "mode": from_state("mode"),
                },
                outputs={"processed_response": append_state("chat_history")},
            ),
        )
        .with_transitions(
            ("prompt", "check_safety", default),
            ("check_safety", "decide_mode", when(safe=True)),
            ("check_safety", "response", default),
            ("decide_mode", "generate_image", when(mode="generate_image")),
            ("decide_mode", "generate_code", when(mode="generate_code")),
            ("decide_mode", "answer_question", when(mode="answer_question")),
            ("decide_mode", "prompt_for_more", default),
            (
                ["generate_image", "answer_question", "generate_code", "prompt_for_more"],
                "response",
            ),
            ("response", "prompt", default),
        )
        .with_hooks(*hooks)
        .with_identifiers(app_id=app_id)
        .with_tracker("local", project=project_id, params={"storage_dir": storage_dir})
        .build()
    )
    return application



---
File: /burr/examples/other-examples/hamilton-multi-modal/dag.py
---

import copy
from typing import Dict, Optional, TypedDict

import openai
from hamilton.function_modifiers import config

ChatContents = TypedDict("ChatContents", {"role": str, "content": str, "type": str})


def processed_prompt(prompt: str) -> dict:
    return {"role": "user", "content": prompt, "type": "text"}


@config.when(provider="openai")
def client() -> openai.Client:
    return openai.Client()


def text_model() -> str:
    return "gpt-3.5-turbo"


def image_model() -> str:
    return "dall-e-2"


def safe(prompt: str) -> bool:
    if "unsafe" in prompt:
        return False
    return True


def modes() -> Dict[str, str]:
    return {
        "answer_question": "text",
        "generate_image": "image",
        "generate_code": "code",
        "unknown": "text",
    }


def find_mode_prompt(prompt: str, modes: Dict[str, str]) -> str:
    return (
        f"You are a chatbot. You've been prompted this: {prompt}. "
        f"You have the capability of responding in the following modes: {', '.join(modes)}. "
        "Please respond with *only* a single word representing the mode that most accurately"
        " corresponds to the prompt. Fr instance, if the prompt is 'draw a picture of a cat', "
        "the mode would be 'generate_image'. If the prompt is 'what is the capital of France', the mode would be 'answer_question'."
        "If none of these modes apply, please respond with 'unknown'."
    )


def suggested_mode(find_mode_prompt: str, client: openai.Client, text_model: str) -> str:
    result = client.chat.completions.create(
        model=text_model,
        messages=[
            {"role": "system", "content": "You are a helpful assistant"},
            {"role": "user", "content": find_mode_prompt},
        ],
    )
    content = result.choices[0].message.content
    return content


def mode(suggested_mode: str, modes: Dict[str, str]) -> str:
    # TODO -- use instructor!
    print(f"Mode: {suggested_mode}")
    lowercase = suggested_mode.lower()
    if lowercase not in modes:
        return "unknown"  # default to unknown
    return lowercase


def generated_text(chat_history: list[dict], text_model: str, client: openai.Client) -> str:
    chat_history_api_format = [
        {
            "role": chat["role"],
            "content": chat["content"],
        }
        for i, chat in enumerate(chat_history)
    ]
    result = client.chat.completions.create(
        model=text_model,
        messages=chat_history_api_format,
    )
    return result.choices[0].message.content


# def generated_text_error(generated_text) -> dict:
#     ...


def generated_image(prompt: str, image_model: str, client: openai.Client) -> str:
    result = client.images.generate(
        model=image_model, prompt=prompt, size="1024x1024", quality="standard", n=1
    )
    return result.data[0].url


def answered_question(chat_history: list[dict], text_model: str, client: openai.Client) -> str:
    chat_history = copy.deepcopy(chat_history)
    chat_history[-1][
        "content"
    ] = f"Please answer the following question: {chat_history[-1]['content']}"

    chat_history_api_format = [
        {
            "role": chat["role"],
            "content": chat["content"],
        }
        for i, chat in enumerate(chat_history)
    ]
    response = client.chat.completions.create(
        model=text_model,
        messages=chat_history_api_format,
    )
    return response.choices[0].message.content


def generated_code(chat_history: list[dict], text_model: str, client: openai.Client) -> str:
    chat_history = copy.deepcopy(chat_history)
    chat_history[-1][
        "content"
    ] = f"Please respond to the following with *only* code: {chat_history[-1]['content']}"

    chat_history_api_format = [
        {
            "role": chat["role"],
            "content": chat["content"],
        }
        for i, chat in enumerate(chat_history)
    ]
    response = client.chat.completions.create(
        model=text_model,
        messages=chat_history_api_format,
    )
    return response.choices[0].message.content


def prompt_for_more(modes: Dict[str, str]) -> str:
    return (
        f"I can't find a mode that applies to your input. Can you"
        f" please clarify? I support: {', '.join(modes)}."
    )


def processed_response(
    response: Optional[str], mode: str, modes: Dict[str, str], safe: bool
) -> ChatContents:
    if not safe:
        return {"role": "assistant", "content": "I'm sorry, I can't do that.", "type": "text"}
    return {"role": "assistant", "type": modes[mode], "content": response}


def processed_error(error: str) -> dict:
    return {"role": "assistant", "error": error, "type": "text"}



---
File: /burr/examples/pytest/conftest.py
---

import subprocess

import pytest


class ResultCollector:
    """Example of a custom fixture that collects results from tests."""

    def __init__(self):
        self.results = []

    def append(self, result):
        self.results.append(result)

    def values(self):
        return self.results

    def __str__(self):
        return "\n".join(str(result) for result in self.results)


@pytest.fixture(scope="session")
def result_collector():
    """Fixture that collects results from tests. This is a toy example."""
    collector = ResultCollector()
    yield collector
    print("\nCollected Results:\n", collector)


@pytest.fixture
def git_info():
    """Fixture that returns the git commit, branch, latest_tag.

    Note if there are uncommitted changes, the commit will have '-dirty' appended.
    """
    try:
        commit = subprocess.check_output(["git", "rev-parse", "HEAD"]).strip().decode("utf-8")
        dirty = subprocess.check_output(["git", "status", "--porcelain"]).strip() != b""
        commit = f"{commit}{'-dirty' if dirty else ''}"
    except subprocess.CalledProcessError:
        commit = None
    try:
        latest_tag = (
            subprocess.check_output(["git", "describe", "--tags", "--abbrev=0"])
            .strip()
            .decode("utf-8")
        )
    except subprocess.CalledProcessError:
        latest_tag = None
    try:
        branch = (
            subprocess.check_output(["git", "rev-parse", "--abbrev-ref", "HEAD"])
            .strip()
            .decode("utf-8")
        )
    except subprocess.CalledProcessError:
        branch = None
    return {"commit": commit, "latest_tag": latest_tag, "branch": branch}


def pytest_configure(config):
    """Code to stop custom mark warnings"""
    config.addinivalue_line(
        "markers", "file_name: mark test to run using Burr file-based parameterization."
    )



---
File: /burr/examples/pytest/some_actions.py
---

"""
This is an example module that defines a Burr application.

It hypothetically transcribes audio and then runs a hypothesis on the transcription to determine a medical diagnosis.
"""
from typing import Any, Callable, Dict, Generator, List, Tuple

import openai

from burr import core
from burr.core import Action, ApplicationContext, GraphBuilder, State, action
from burr.core.parallelism import MapStates, RunnableGraph
from burr.tracking import LocalTrackingClient


@action(reads=["audio"], writes=["transcription"])
def transcribe_audio(state: State) -> State:
    """Action to transcribe audio."""
    # here we fake transcription. For this example the audio is text already...
    return state.update(transcription=state["audio"])


@action(reads=["hypothesis", "transcription"], writes=["diagnosis"])
def run_hypothesis(state: State) -> State:
    """Action to run a hypothesis on a transcription."""
    client = openai.Client()  # here for simplicity because clients and SERDE don't mix well.
    hypothesis = state["hypothesis"]
    transcription = state["transcription"]
    prompt = (
        "Given the following diagnosis hypothesis and medical transcription:\n\n"
        f"Diagnosis hypothesis:{hypothesis}\n\n"
        f"Transcription:\n```{transcription}```\n\n"
        "Answer 'yes' if you believe the hypothesis has a strong reason to hold given the content of the transcription. "
        "Answer 'no' if you do not believe the hypothesis holds given the content of the transcription. "
        "Answer 'unsure' if you are unsure and need more details. "
    )
    response = client.chat.completions.create(
        messages=[
            {
                "role": "system",
                "content": "You are a doctor diagnosing illnesses that is methodical"
                " in determining whether a diagnosis hypothesis is supported by a transcription.",
            },
            {"role": "user", "content": prompt},
        ],
        model="gpt-4o-mini",
    )
    return state.update(diagnosis=response.choices[0].message.content)


class TestMultipleHypotheses(MapStates):
    """Parallel action to test multiple hypotheses."""

    def action(self, state: State, inputs: Dict[str, Any]) -> Action | Callable | RunnableGraph:
        """which action to run for each state."""
        return run_hypothesis

    def states(
        self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
    ) -> Generator[State, None, None]:
        """Generate the states to run the action on.
        You could easily have a list_hypotheses upstream action that writes to "hypothesis" in state
        And loop through those
        This hardcodes for simplicity
        """
        for hypothesis in [
            "Common cold",
            "Sprained ankle",
            "Broken arm",
        ]:
            yield state.update(hypothesis=hypothesis)

    def reduce(self, state: State, states: Generator[State, None, None]) -> State:
        """Combine the outputs of the parallel action."""
        all_diagnoses_outputs = []
        for _sub_state in states:
            all_diagnoses_outputs.append(
                {"hypothesis": _sub_state["hypothesis"], "diagnosis": _sub_state["diagnosis"]}
            )
        return state.update(diagnosis_outputs=all_diagnoses_outputs)

    @property
    def reads(self) -> List[str]:
        return ["transcription"]

    @property
    def writes(self) -> List[str]:
        return ["diagnosis_outputs"]


@action(reads=["diagnosis_outputs"], writes=["final_diagnosis"])
def determine_diagnosis(state: State) -> State:
    """Action to determine the final diagnosis."""
    # could also get an LLM to decide here, or have a human decide, etc.
    possible_hypotheses = [d for d in state["diagnosis_outputs"] if d["diagnosis"].lower() == "yes"]
    if len(possible_hypotheses) == 1:
        return state.update(final_diagnosis=possible_hypotheses[0]["hypothesis"])
    elif len(possible_hypotheses) > 1:
        return state.update(
            final_diagnosis="Need further clarification. Multiple diagnoses possible."
        )
    else:
        return state.update(final_diagnosis="Healthy individual")


def build_graph() -> core.Graph:
    """Builds the graph for the application"""
    graph = (
        GraphBuilder()
        .with_actions(
            transcribe_audio=transcribe_audio,
            hypotheses=TestMultipleHypotheses(),
            determine_diagnosis=determine_diagnosis,
        )
        .with_transitions(
            ("transcribe_audio", "hypotheses"),
            ("hypotheses", "determine_diagnosis"),
        )
        .build()
    )
    return graph


def build_application(
    app_id,
    graph,
    initial_state,
    initial_entrypoint,
    partition_key,
    tracker,
    use_otel_tracing: bool = False,
) -> core.Application:
    """Builds an application with the given parameters.

    :param app_id:
    :param graph:
    :param initial_state:
    :param initial_entrypoint:
    :param partition_key:
    :param tracker:
    :param use_otel_tracing:
    :return:
    """
    app_builder = (
        core.ApplicationBuilder()
        .with_graph(graph)
        .with_state(**initial_state)
        .with_entrypoint(initial_entrypoint)
        .with_identifiers(partition_key=partition_key, app_id=app_id)
    )
    if tracker:
        app_builder = app_builder.with_tracker(tracker, use_otel_tracing=use_otel_tracing)
    app = app_builder.build()
    return app


def run_my_agent(
    input_audio: str, partition_key: str = None, app_id: str = None, tracking_project: str = None
) -> Tuple[str, str]:
    """Runs the agent with the given input audio (in this case a string transcription...).
    :param input_audio: we fake it here, and ask for a string...
    :param partition_key:
    :param app_id:
    :param tracking_project:
    :return:
    """
    graph = build_graph()
    tracker = None
    if tracking_project:
        tracker = LocalTrackingClient(project=tracking_project)
    # we fake the input audio to be a string here rather than a waveform.
    app = build_application(
        app_id, graph, {"audio": input_audio}, "transcribe_audio", partition_key, tracker=tracker
    )
    # app.visualize("diagnosis.png", include_conditions=True, view=False, format="png")
    last_action, _, agent_state = app.run(
        halt_after=["determine_diagnosis"],
        inputs={"audio": input_audio},
    )
    return input_audio, agent_state["final_diagnosis"]


if __name__ == "__main__":
    print(run_my_agent("Patient exhibits mucus dripping from nostrils and coughing."))
    print(run_my_agent("Patient has a limp and is unable to flex right ankle. Ankle is swollen."))
    print(
        run_my_agent(
            "Patient fell off and landed on their right arm. Their right wrist is swollen, they can still move their "
            "fingers, and there is only minor pain or discomfort when the wrist is moved or touched."
        )
    )



---
File: /burr/examples/pytest/test_some_actions.py
---

"""This module shows example tests for testing actions and agents."""
import pytest
import some_actions

from burr.core import state
from burr.tracking import LocalTrackingClient


def test_example1(result_collector):
    """Example test that uses a custom fixture."""
    result_collector.append("Test result 1")
    result_collector.append("Test result 2")
    assert True


def test_example2(results_bag):
    """Example that uses pytest-harvest results_bag fixture."""
    # the following become columns in the final results
    results_bag.input = "..."
    results_bag.actual = "foo"
    results_bag.expected = "foo bar baz"
    results_bag.cosine = 0.3
    results_bag.jaccard = 0.2
    assert True


def test_example3(module_results_df):
    """Example that shows how to access the module_results_df fixture."""
    # note pytest runs these tests in order - so in practice this
    # would be placed at the end of the test file
    print(module_results_df.columns)


def test_run_hypothesis(results_bag):
    """Tests the run_hypothesis action for a single case"""
    input = "Patient has a limp and is unable to flex right ankle. Ankle is swollen."
    hypothesis = "Common cold"
    expected = "no"
    results_bag.input = input
    results_bag.expected = expected
    results_bag.test_function = "test_run_hypothesis"
    input_state = state.State({"hypothesis": hypothesis, "transcription": input})
    end_state = some_actions.run_hypothesis(input_state)
    results_bag.actual = end_state["diagnosis"]
    results_bag.exact_match = end_state["diagnosis"].lower() == expected
    # results_bag.jaccard = ... # other measures here
    # e.g. LLM as judge if applicable
    # place asserts at end
    assert end_state["diagnosis"] is not None
    assert end_state["diagnosis"] != ""


@pytest.mark.parametrize(
    "input,hypothesis,expected",
    [
        ("Patient exhibits mucus dripping from nostrils and coughing.", "Common cold", "yes"),
        (
            "Patient has a limp and is unable to flex right ankle. Ankle is swollen.",
            "Sprained ankle",
            "yes",
        ),
        (
            "Patient fell off and landed on their right arm. Their right wrist is swollen, "
            "they can still move their fingers, and there is only minor pain or discomfort when the wrist is moved or "
            "touched.",
            "Broken arm",
            "no",
        ),
    ],
    ids=["common_cold", "sprained_ankle", "broken_arm"],
)
def test_run_hypothesis_parameterized(input, hypothesis, expected, results_bag):
    """Example showing how to parameterize this."""
    results_bag.input = input
    results_bag.hypothesis = hypothesis
    results_bag.expected = expected
    results_bag.test_function = "test_run_hypothesis_parameterized"
    input_state = state.State({"hypothesis": hypothesis, "transcription": input})
    end_state = some_actions.run_hypothesis(input_state)
    results_bag.actual = end_state["diagnosis"]
    results_bag.exact_match = end_state["diagnosis"].lower() == expected
    # results_bag.jaccard = ... # other measures here
    # e.g. LLM as judge if applicable
    # place asserts at end
    assert end_state["diagnosis"] is not None
    assert end_state["diagnosis"] != ""


# the following is required to run file based parameterized tests
from burr.testing import pytest_generate_tests  # noqa: F401


@pytest.mark.file_name(
    "hypotheses_test_cases.json"
)  # our fixture file with the expected inputs and outputs
def test_run_hypothesis_burr_fixture(input_state, expected_state, results_bag):
    """This example shows how to scale parameterized with a file of inputs and expected outputs using Burr's."""
    input_state = state.State.deserialize(input_state)
    expected_state = state.State.deserialize(expected_state)
    results_bag.input = input_state["transcription"]
    results_bag.hypothesis = input_state["hypothesis"]
    results_bag.expected = expected_state["diagnosis"]
    results_bag.test_function = "test_run_hypothesis_parameterized"
    input_state = state.State(
        {"hypothesis": input_state["hypothesis"], "transcription": input_state["transcription"]}
    )
    end_state = some_actions.run_hypothesis(input_state)
    results_bag.actual = end_state["diagnosis"]
    results_bag.exact_match = end_state["diagnosis"].lower() == expected_state["diagnosis"]
    print(results_bag)
    # results_bag.jaccard = ... # other measures here
    # e.g. LLM as judge if applicable
    # place asserts at end
    assert end_state["diagnosis"] is not None
    assert end_state["diagnosis"] != ""


@pytest.mark.file_name(
    "e2e_test_cases.json"
)  # our fixture file with the expected inputs and outputs
def test_run_hypothesis_burr_fixture_e2e_with_tracker(input_state, expected_state, results_bag):
    """This example shows a parameterized example of running the agent end-to-end with a tracker."""
    from opentelemetry.instrumentation.openai import OpenAIInstrumentor

    # this will auto instrument the openAI client. No swapping of imports required!
    OpenAIInstrumentor().instrument()
    tracker = LocalTrackingClient(project="pytest-example")
    graph = some_actions.build_graph()
    input_state = state.State.deserialize(input_state)
    app = some_actions.build_application(
        app_id=None,
        graph=graph,
        initial_state=input_state,
        initial_entrypoint="transcribe_audio",
        partition_key="pytest_example",
        tracker=tracker,
        use_otel_tracing=True,
    )
    expected_state = state.State.deserialize(expected_state)
    results_bag.input = input_state["audio"]
    results_bag.expected = expected_state["final_diagnosis"]
    results_bag.test_function = "test_run_hypothesis_burr_fixture_e2e_with_tracker"
    last_action, _, agent_state = app.run(
        halt_after=["determine_diagnosis"],
    )

    results_bag.actual = agent_state["final_diagnosis"]
    results_bag.exact_match = (
        agent_state["final_diagnosis"].lower() == expected_state["final_diagnosis"].lower()
    )
    print(results_bag)
    # results_bag.jaccard = ... # other measures here
    # e.g. LLM as judge if applicable
    # place asserts at end
    assert agent_state["final_diagnosis"] is not None
    assert agent_state["final_diagnosis"] != ""


def test_print_results(module_results_df, git_info):
    """This is an example using pytest-harvest and our custom git fixture to return results to a central location.
    You could use other plugins, or create your own fixtures (e.g. see conftest.py for a simpler custom fixture).
    """
    module_results_df["git_commit"] = git_info["commit"]
    module_results_df["git_branch"] = git_info["branch"]
    print(module_results_df.columns)
    print(module_results_df.head())
    # compute statistics
    # this is where you could use pandas to compute statistics like accuracy, etc.
    tests_of_interest = module_results_df[
        module_results_df["test_function"].fillna("").str.startswith("test_run_hypothesis")
    ]
    accuracy = sum(tests_of_interest["exact_match"]) / len(tests_of_interest)
    # save to CSV
    tests_of_interest[
        [
            "test_function",
            "git_branch",
            "git_commit",
            "duration_ms",
            "status",
            "input",
            "hypothesis",
            "expected",
            "actual",
            "exact_match",
        ]
    ].to_csv("results.csv", index=True, quoting=1)
    # upload to google sheets or other storage, etc.

    assert accuracy > 0.9  # and then assert on the computed statistics



---
File: /burr/examples/rag-lancedb-ingestion/__init__.py
---




---
File: /burr/examples/rag-lancedb-ingestion/application.py
---

import os
import textwrap

import lancedb
import openai

from burr.core import Application, ApplicationBuilder, State, action
from burr.lifecycle import PostRunStepHook


@action(reads=[], writes=["relevant_chunks", "chat_history"])
def relevant_chunk_retrieval(
    state: State,
    user_query: str,
    lancedb_con: lancedb.DBConnection,
) -> State:
    """Search LanceDB with the user query and return the top 4 results"""
    text_chunks_table = lancedb_con.open_table("dagworks___contexts")
    search_results = (
        text_chunks_table.search(user_query).select(["text", "id__"]).limit(4).to_list()
    )

    return state.update(relevant_chunks=search_results).append(chat_history=user_query)


@action(reads=["chat_history", "relevant_chunks"], writes=["chat_history"])
def bot_turn(state: State, llm_client: openai.OpenAI) -> State:
    """Collect relevant chunks and produce a response to the user query"""
    user_query = state["chat_history"][-1]
    relevant_chunks = state["relevant_chunks"]

    system_prompt = textwrap.dedent(
        """You are a conversational agent designed to discuss and provide \
        insights about various blog posts. Your task is to engage users in \
        meaningful conversations based on the content of the blog articles they mention.
        """
    )
    joined_chunks = " ".join([c["text"] for c in relevant_chunks])
    user_prompt = "BLOGS CONTENT\n" + joined_chunks + "\nUSER QUERY\n" + user_query

    response = llm_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )
    bot_answer = response.choices[0].message.content

    return state.append(chat_history=bot_answer)


class PrintBotAnswer(PostRunStepHook):
    """Hook to print the bot's answer"""

    def post_run_step(self, *, state, action, **future_kwargs):
        if action.name == "bot_turn":
            print("\nðŸ¤–: ", state["chat_history"][-1])


def build_application() -> Application:
    """Create the Burr `Application`. This is responsible for instantiating the
    OpenAI client and the LanceDB connection
    """
    llm_client = openai.OpenAI()
    lancedb_con = lancedb.connect(os.environ["DESTINATION__LANCEDB__CREDENTIALS__URI"])

    return (
        ApplicationBuilder()
        .with_actions(
            relevant_chunk_retrieval.bind(lancedb_con=lancedb_con),
            bot_turn.bind(llm_client=llm_client),
        )
        .with_transitions(
            ("relevant_chunk_retrieval", "bot_turn"),
            ("bot_turn", "relevant_chunk_retrieval"),
        )
        .with_entrypoint("relevant_chunk_retrieval")
        .with_tracker("local", project="substack-rag", use_otel_tracing=True)
        .with_hooks(PrintBotAnswer())
        .build()
    )


if __name__ == "__main__":
    import utils

    from burr.integrations.opentelemetry import init_instruments

    utils.set_environment_variables()  # set environment variables for LanceDB
    init_instruments("openai", "lancedb")  # register the OpenTelemetry instrumentation

    # build the Burr `Application`
    app = build_application()
    app.visualize("statemachine.png")

    # Launch the Burr application in a `while` loop
    print("\n## Lauching RAG application ##")
    while True:
        user_query = input("\nAsk something or type `quit/q` to exit: ")
        if user_query.lower() in ["quit", "q"]:
            break

        _, _, _ = app.run(
            halt_after=["bot_turn"],
            inputs={"user_query": user_query},
        )



---
File: /burr/examples/rag-lancedb-ingestion/ingestion.py
---

import re
from typing import Generator

import dlt
import feedparser
import requests
import utils
from bs4 import BeautifulSoup


def split_text(text):
    """Split text on punction (., !, ?)."""
    sentence_endings = r"[.!?]+"
    for sentence in re.split(sentence_endings, text):
        sentence = sentence.strip()
        if sentence:
            yield sentence


def contextualize(chunks: list[str], window=5, stride=3, min_window_size=2):
    """Rolling window operation to join consecutive sentences into larger chunks."""
    n_chunks = len(chunks)
    for start_i in range(0, n_chunks, stride):
        if (start_i + window <= n_chunks) or (n_chunks - start_i >= min_window_size):
            yield " ".join(chunks[start_i : min(start_i + window, n_chunks)])


@dlt.resource(name="substack", write_disposition="merge", primary_key="id")
def rss_entries(substack_url: str) -> Generator:
    """Substack blog entries retrieved from a RSS feed"""
    FIELDS_TO_EXCLUDE = [
        "published_parsed",
        "title_detail",
        "summary_detail",
        "author_detail",
        "guidislink",
        "authors",
        "links",
    ]

    r = requests.get(f"{substack_url}/feed")
    rss_feed = feedparser.parse(r.content)
    for entry in rss_feed["entries"]:
        for field in FIELDS_TO_EXCLUDE:
            entry.pop(field)

        yield entry


@dlt.transformer(primary_key="id")
def parsed_html(rss_entry: dict):
    """Parse the HTML from the RSS entry"""
    soup = BeautifulSoup(rss_entry["content"][0]["value"], "html.parser")
    parsed_text = soup.get_text(separator=" ", strip=True)
    yield {"id": rss_entry["id"], "text": parsed_text}


@dlt.transformer(primary_key="chunk_id")
def chunks(parsed_html: dict) -> list[dict]:
    """Chunk text"""
    return [
        dict(
            document_id=parsed_html["id"],
            chunk_id=idx,
            text=text,
        )
        for idx, text in enumerate(split_text(parsed_html["text"]))
    ]


# order is important for reduce / rolling step
# default to order of the batch or specifying sorting key
@dlt.transformer(primary_key="context_id")
def contexts(chunks: list[dict]) -> Generator:
    """Assemble consecutive chunks into larger context windows"""
    # first handle the m-to-n relationship
    # set of foreign keys (i.e., "chunk_id")
    chunk_id_set = set(chunk["chunk_id"] for chunk in chunks)
    context_id = utils.hash_set(chunk_id_set)

    # create a table only containing the keys
    for chunk_id in chunk_id_set:
        yield dlt.mark.with_table_name(
            {"chunk_id": chunk_id, "context_id": context_id},
            "chunks_to_contexts_keys",
        )

    # main transformation logic
    for contextualized in contextualize([chunk["text"] for chunk in chunks]):
        yield dlt.mark.with_table_name(
            {"context_id": context_id, "text": contextualized}, "contexts"
        )


if __name__ == "__main__":
    import dlt
    from dlt.destinations.adapters import lancedb_adapter

    utils.set_environment_variables()

    pipeline = dlt.pipeline(
        pipeline_name="substack-blog", destination="lancedb", dataset_name="dagworks"
    )

    blog_url = "https://blog.dagworks.io/"

    full_entries = lancedb_adapter(rss_entries(blog_url), embed="summary")
    chunked_entries = rss_entries(blog_url) | parsed_html | chunks
    contextualized_chunks = lancedb_adapter(chunked_entries | contexts, embed="text")

    load_info = pipeline.run([full_entries, chunked_entries, contextualized_chunks])
    print(load_info)



---
File: /burr/examples/rag-lancedb-ingestion/utils.py
---

import base64
import getpass
import hashlib
import os


def set_environment_variables():
    import dlt.destinations.impl.lancedb.models  # noqa

    if os.environ.get("OPENAI_API_KEY") is None:
        os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter OPENAI_API_KEY: ")

    os.environ["DESTINATION__LANCEDB__EMBEDDING_MODEL_PROVIDER"] = "openai"
    os.environ["DESTINATION__LANCEDB__EMBEDDING_MODEL"] = "text-embedding-3-small"

    os.environ["DESTINATION__LANCEDB__CREDENTIALS__URI"] = ".lancedb"
    os.environ["DESTINATION__LANCEDB__CREDENTIALS__EMBEDDING_MODEL_PROVIDER_API_KEY"] = os.environ[
        "OPENAI_API_KEY"
    ]


def _compact_hash(digest: bytes) -> str:
    """Compact the hash to a string that's safe to pass around."""
    return base64.urlsafe_b64encode(digest).decode()


def hash_primitive(obj, *args, **kwargs) -> str:
    """Convert the primitive to a string and hash it"""
    hash_object = hashlib.md5(str(obj).encode())
    return _compact_hash(hash_object.digest())


def hash_set(obj, *args, **kwargs) -> str:
    """Hash each element of the set, then sort hashes, and
    create a hash of hashes.
    For the same objects in the set, the hashes will be the
    same.
    """
    hashes = (hash_primitive(elem) for elem in obj)
    sorted_hashes = sorted(hashes)

    hash_object = hashlib.sha224()
    for hash in sorted_hashes:
        hash_object.update(hash.encode())

    return _compact_hash(hash_object.digest())



---
File: /burr/examples/ray/__init__.py
---




---
File: /burr/examples/ray/application.py
---

from typing import Any, Dict, List, Optional, Tuple

import openai
import ray

from burr.common.async_utils import SyncOrAsyncGenerator
from burr.core import Application, ApplicationBuilder, Condition, GraphBuilder, State, action
from burr.core.application import ApplicationContext
from burr.core.parallelism import MapStates, RunnableGraph, SubgraphType
from burr.core.persistence import SQLitePersister
from burr.integrations.ray import RayExecutor


# full agent
def _query_llm(prompt: str) -> str:
    """Simple wrapper around the OpenAI API."""
    client = openai.Client()
    return (
        client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a helpful assistant"},
                {"role": "user", "content": prompt},
            ],
        )
        .choices[0]
        .message.content
    )


@action(
    reads=["feedback", "current_draft", "poem_type", "poem_subject"],
    writes=["current_draft", "draft_history", "num_drafts"],
)
def write(state: State) -> Tuple[dict, State]:
    """Writes a draft of a poem."""
    poem_subject = state["poem_subject"]
    poem_type = state["poem_type"]
    current_draft = state.get("current_draft")
    feedback = state.get("feedback")

    parts = [
        f'You are an AI poet. Create a {poem_type} poem on the following subject: "{poem_subject}". '
        "It is absolutely imperative that you respond with only the poem and no other text."
    ]

    if current_draft:
        parts.append(f'Here is the current draft of the poem: "{current_draft}".')

    if feedback:
        parts.append(f'Please incorporate the following feedback: "{feedback}".')

    parts.append(
        f"Ensure the poem is creative, adheres to the style of a {poem_type}, and improves upon the previous draft."
    )

    prompt = "\n".join(parts)

    draft = _query_llm(prompt)

    return {"draft": draft}, state.update(
        current_draft=draft,
        draft_history=state.get("draft_history", []) + [draft],
    ).increment(num_drafts=1)


@action(reads=["current_draft", "poem_type", "poem_subject"], writes=["feedback"])
def edit(state: State) -> Tuple[dict, State]:
    """Edits a draft of a poem, providing feedback"""
    poem_subject = state["poem_subject"]
    poem_type = state["poem_type"]
    current_draft = state["current_draft"]

    prompt = f"""
    You are an AI poetry critic. Review the following {poem_type} poem based on the subject: "{poem_subject}".
    Here is the current draft of the poem: "{current_draft}".
    Provide detailed feedback to improve the poem. If the poem is already excellent and needs no changes, simply respond with an empty string.
    """
    feedback = _query_llm(prompt)

    return {"feedback": feedback}, state.update(feedback=feedback)


@action(reads=["current_draft"], writes=["final_draft"])
def final_draft(state: State) -> Tuple[dict, State]:
    return {"final_draft": state["current_draft"]}, state.update(final_draft=state["current_draft"])


# full agent
@action(
    reads=[],
    writes=[
        "max_drafts",
        "poem_types",
        "poem_subject",
    ],
)
def user_input(state: State, max_drafts: int, poem_types: List[str], poem_subject: str) -> State:
    """Collects user input for the poem generation process."""
    return state.update(max_drafts=max_drafts, poem_types=poem_types, poem_subject=poem_subject)


class GenerateAllPoems(MapStates):
    def states(
        self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
    ) -> SyncOrAsyncGenerator[State]:
        for poem_type in state["poem_types"]:
            yield state.update(current_draft=None, poem_type=poem_type, feedback=[], num_drafts=0)

    def action(self, state: State, inputs: Dict[str, Any]) -> SubgraphType:
        graph = (
            GraphBuilder()
            .with_actions(
                edit,
                write,
                final_draft,
            )
            .with_transitions(
                ("write", "edit", Condition.expr(f"num_drafts < {state['max_drafts']}")),
                ("write", "final_draft"),
                ("edit", "final_draft", Condition.expr("len(feedback) == 0")),
                ("edit", "write"),
            )
        ).build()
        return RunnableGraph(graph=graph, entrypoint="write", halt_after=["final_draft"])

    def reduce(self, state: State, results: SyncOrAsyncGenerator[State]) -> State:
        proposals = []
        for output_state in results:
            proposals.append(output_state["final_draft"])
        return state.append(proposals=proposals)

    @property
    def writes(self) -> list[str]:
        return ["proposals"]

    @property
    def reads(self) -> list[str]:
        return ["poem_types", "poem_subject", "max_drafts"]


@action(reads=["proposals", "poem_types"], writes=["final_results"])
def final_results(state: State) -> Tuple[dict, State]:
    # joins them into a string
    proposals = state["proposals"]
    final_results = "\n\n".join(
        [f"{poem_type}:\n{proposal}" for poem_type, proposal in zip(state["poem_types"], proposals)]
    )
    return {"final_results": final_results}, state.update(final_results=final_results)


def application_multithreaded() -> Application:
    app = (
        ApplicationBuilder()
        .with_actions(user_input, final_results, generate_all_poems=GenerateAllPoems())
        .with_transitions(
            ("user_input", "generate_all_poems"),
            ("generate_all_poems", "final_results"),
        )
        .with_tracker(project="demo:parallel_agents")
        .with_entrypoint("user_input")
        .build()
    )
    return app


def application(app_id: Optional[str] = None) -> Application:
    persister = SQLitePersister(db_path="./db")
    persister.initialize()
    app = (
        ApplicationBuilder()
        .with_actions(user_input, final_results, generate_all_poems=GenerateAllPoems())
        .with_transitions(
            ("user_input", "generate_all_poems"),
            ("generate_all_poems", "final_results"),
        )
        .with_tracker(project="demo:parallel_agents_fault_tolerance")
        .with_parallel_executor(RayExecutor)
        .with_state_persister(persister)
        .initialize_from(
            persister, resume_at_next_action=True, default_state={}, default_entrypoint="user_input"
        )
        .with_identifiers(app_id=app_id)
        .build()
    )
    return app


if __name__ == "__main__":
    ray.init()
    app = application()
    app_id = app.uid
    act, _, state = app.run(
        halt_after=["final_results"],
        inputs={
            "max_drafts": 2,
            "poem_types": [
                "sonnet",
                "limerick",
                "haiku",
                "acrostic",
            ],
            "poem_subject": "state machines",
        },
    )
    print(state)



---
File: /burr/examples/recursive/__init__.py
---




---
File: /burr/examples/recursive/application.py
---

from typing import List, Tuple

import joblib
import openai
from joblib import parallel_config

from burr.core import Application, ApplicationBuilder, Condition, State, action
from burr.core.application import ApplicationContext


# full agent
def _query_llm(prompt: str) -> str:
    """Simple wrapper around the OpenAI API."""
    client = openai.Client()
    return (
        client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a helpful assistant"},
                {"role": "user", "content": prompt},
            ],
        )
        .choices[0]
        .message.content
    )


@action(
    reads=["feedback", "current_draft", "poem_type", "prompt"],
    writes=["current_draft", "draft_history", "num_drafts"],
)
def write(state: State) -> Tuple[dict, State]:
    """Writes a draft of a poem."""
    poem_subject = state["prompt"]
    poem_type = state["poem_type"]
    current_draft = state.get("current_draft")
    feedback = state.get("feedback")

    parts = [
        f'You are an AI poet. Create a {poem_type} poem on the following subject: "{poem_subject}". '
        "It is absolutely imperative that you respond with only the poem and no other text."
    ]

    if current_draft:
        parts.append(f'Here is the current draft of the poem: "{current_draft}".')

    if feedback:
        parts.append(f'Please incorporate the following feedback: "{feedback}".')

    parts.append(
        f"Ensure the poem is creative, adheres to the style of a {poem_type}, and improves upon the previous draft."
    )

    prompt = "\n".join(parts)

    draft = _query_llm(prompt)

    return {"draft": draft}, state.update(
        current_draft=draft,
        draft_history=state.get("draft_history", []) + [draft],
    ).increment(num_drafts=1)


@action(reads=["current_draft", "poem_type", "prompt"], writes=["feedback"])
def edit(state: State) -> Tuple[dict, State]:
    """Edits a draft of a poem, providing feedback"""
    poem_subject = state["prompt"]
    poem_type = state["poem_type"]
    current_draft = state["current_draft"]

    prompt = f"""
    You are an AI poetry critic. Review the following {poem_type} poem based on the subject: "{poem_subject}".
    Here is the current draft of the poem: "{current_draft}".
    Provide detailed feedback to improve the poem. If the poem is already excellent and needs no changes, simply respond with an empty string.
    """

    feedback = _query_llm(prompt)

    return {"feedback": feedback}, state.update(feedback=feedback)


@action(reads=["current_draft"], writes=["final_draft"])
def final_draft(state: State) -> Tuple[dict, State]:
    return {"final_draft": state["current_draft"]}, state.update(final_draft=state["current_draft"])


def _create_sub_application(
    max_num_drafts: int,
    spawning_application_context: ApplicationContext,
    poem_type: str,
    prompt: str,
) -> Application:
    """Utility to create sub-application -- note"""
    out = (
        ApplicationBuilder()
        .with_actions(
            edit,
            write,
            final_draft,
        )
        .with_transitions(
            ("write", "edit", Condition.expr(f"num_drafts < {max_num_drafts}")),
            ("write", "final_draft"),
            ("edit", "final_draft", Condition.expr("len(feedback) == 0")),
            ("edit", "write"),
        )
        .with_tracker(spawning_application_context.tracker.copy())  # remember to do `copy()` here!
        .with_spawning_parent(
            spawning_application_context.app_id,
            spawning_application_context.sequence_id,
            spawning_application_context.partition_key,
        )
        .with_entrypoint("write")
        .with_state(
            current_draft=None,
            poem_type=poem_type,
            prompt=prompt,
            feedback=None,
        )
        .build()
    )
    return out


# full agent
@action(
    reads=[],
    writes=[
        "max_drafts",
        "poem_types",
        "poem_subject",
    ],
)
def user_input(
    state: State, max_drafts: int, poem_types: List[str], poem_subject: str
) -> Tuple[dict, State]:
    """Collects user input for the poem generation process."""
    return {
        "max_drafts": max_drafts,
        "poem_types": poem_types,
        "poem_subject": poem_subject,
    }, state.update(max_drafts=max_drafts, poem_types=poem_types, poem_subject=poem_subject)


@action(reads=["max_drafts", "poem_types", "poem_subject"], writes=["proposals"])
def generate_all_poems(state: State, __context: ApplicationContext) -> Tuple[dict, State]:
    # create one each
    apps = [
        _create_sub_application(state["max_drafts"], __context, poem_type, state["poem_subject"])
        for poem_type in state["poem_types"]
    ]
    # run them all in parallel
    with parallel_config(backend="threading", n_jobs=3):
        all_results = joblib.Parallel()(
            joblib.delayed(app.run)(halt_after=["final_draft"])
            for app, poem_type in zip(apps, state["poem_types"])
        )
    proposals = []
    for *_, substate in all_results:
        proposals.append(substate["final_draft"])

    return {"proposals": proposals}, state.update(proposals=proposals)


@action(reads=["proposals", "prompts"], writes=["final_results"])
def final_results(state: State) -> Tuple[dict, State]:
    # joins them into a string
    proposals = state["proposals"]
    final_results = "\n\n".join(
        [f"{poem_type}:\n{proposal}" for poem_type, proposal in zip(state["poem_types"], proposals)]
    )
    return {"final_results": final_results}, state.update(final_results=final_results)


def application() -> Application:
    return (
        ApplicationBuilder()
        .with_actions(
            user_input,
            generate_all_poems,
            final_results,
        )
        .with_transitions(
            ("user_input", "generate_all_poems"),
            ("generate_all_poems", "final_results"),
        )
        .with_tracker(project="demo:parallelism_poem_generation")
        .with_entrypoint("user_input")
        .build()
    )


if __name__ == "__main__":
    app = application()
    app.visualize(output_file_path="statemachine", format="png")
    _create_sub_application(
        2,
        app.context,
        "sonnet",
        "state machines",
    ).visualize(output_file_path="statemachine_sub", format="png")
    app.run(
        halt_after=["final_results"],
        inputs={
            "max_drafts": 2,
            "poem_types": [
                "sonnet",
                "limerick",
                "haiku",
                "acrostic",
            ],
            "poem_subject": "state machines",
        },
    )



---
File: /burr/examples/simple-chatbot-intro/__init__.py
---




---
File: /burr/examples/simple-chatbot-intro/application.py
---

from typing import Tuple

import openai  # replace this with your favorite LLM client library

from burr.core import ApplicationBuilder, State, action, when


@action(reads=[], writes=["prompt", "chat_history"])
def human_input(state: State, prompt: str) -> Tuple[dict, State]:
    """Pulls human input from the outside world and massages it into a standard chat format.
    Note we're adding it into the chat history (with an `append` operation). This
    is just for convenience of reference -- we could easily just store the chat history
    and access it.
    """

    chat_item = {"content": prompt, "role": "user"}
    # return the prompt as the result
    # put the prompt in state and update the chat_history
    return {"prompt": prompt}, state.update(prompt=prompt).append(chat_history=chat_item)


@action(reads=["chat_history"], writes=["response", "chat_history"])
def ai_response(state: State) -> Tuple[dict, State]:
    """Queries OpenAI with the chat. You could easily use langchain, etc... to handle this,
    but we wanted to keep it simple to demonstrate"""
    client = openai.Client()  # replace this with your favorite LLM client library
    content = (
        client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=state["chat_history"],
        )
        .choices[0]
        .message.content
    )
    chat_item = {"content": content, "role": "assistant"}
    # return the response as the result
    # put the response in state and update the chat history
    return {"response": content}, state.update(response=content).append(chat_history=chat_item)


@action(reads=["prompt"], writes=["safe"])
def safety_check(state: State) -> Tuple[dict, State]:
    safe = "unsafe" not in state["prompt"]
    return {"safe": safe}, state.update(safe=safe)


@action(reads=[], writes=["response", "chat_history"])
def unsafe_response(state: State) -> Tuple[dict, State]:
    content = "I'm sorry, my overlords have forbidden me to respond."
    new_state = state.update(response=content).append(
        chat_history={"content": content, "role": "assistant"}
    )
    return {"response": content}, new_state


def application():
    return (
        ApplicationBuilder()
        .with_actions(
            human_input=human_input,
            ai_response=ai_response,
            safety_check=safety_check,
            unsafe_response=unsafe_response,
        )
        .with_transitions(
            ("human_input", "safety_check"),
            ("safety_check", "unsafe_response", when(safe=False)),
            ("safety_check", "ai_response", when(safe=True)),
            (["unsafe_response", "ai_response"], "human_input"),
        )
        .with_state(chat_history=[])
        .with_entrypoint("human_input")
        .with_tracker("local", project="demo_getting_started")
        .build()
    )


if __name__ == "__main__":
    app = application()
    app.visualize(include_conditions=True, format="png", view=True, output_file_path="statemachine")
    # for prompt in [
    #     "Who was Aaron Burr, sir?",
    #     "Who was Aaron Burr, sir (unsafe)?",
    #     "If you had ml/ai libraries called 'Hamilton' and 'Burr', what would they do?",
    #     "Who was Aaron Burr, sir?",
    #     "Who was Aaron Burr, sir (unsafe)?",
    #     "If you had ml/ai libraries called 'Hamilton' and 'Burr', what would they do?",
    # ]:
    #     action_we_ran, result, state = app.run(
    #         halt_after=["ai_response", "unsafe_response"],
    #         inputs={"prompt": prompt}
    #     )
    # for item in state['chat_history']:
    #     print(item['role'] + ':' + item['content'] + '\n')



---
File: /burr/examples/simulation/application.py
---

"""
This module contains a stubbed out structure for one way of doing a simulation
 for forecasting purposes.

For more sophisticated forecasts where prediction of one timestep
depends on the previous timestep, you need to forecast on a per-step basis. So
you're effectively simulating the future at each step. This is a simple example of
how you might structure such a simulation.
"""

from typing import Tuple

from burr.core import Application, ApplicationBuilder, State, expr
from burr.core.action import action


@action(reads=["data_path"], writes=["data"])
def prepare_data(state: State) -> Tuple[dict, State]:
    """This would pull the data, prepare it, and return it."""
    # pull data, prepare as necessary
    result = {"data": _process_data(state["data_path"])}
    return result, state.update(**result)


@action(reads=["data", "simulation_start"], writes=["model"])
def build_model(state: State) -> Tuple[dict, State]:
    """This would fit the model on data before the simulation start date."""
    training_data = state["data"]
    model = _fit_model(training_data, upto=state["simulation_start"])
    result = {"model": model}
    return result, state.update(**result)


@action(
    reads=["simulation_start", "simulation_end", "model", "data", "current_timestep"],
    writes=["data"],
)
def forecast(state: State) -> Tuple[dict, State]:
    """This action forecasts the next timestep in the simulation and appends it to data."""
    model = state["model"]
    data = state["data"]
    start_time = state["simulation_start"]
    current_time_step = state.get("current_timestep", start_time)
    next_timestep = _forecast_timestep(model, data, current_time_step)
    data = _update_data(data, next_timestep)
    result = {"data": data, "current_timestep": next_timestep}
    return result, state.update(**result)


@action(reads=["data"], writes=[])
def terminate(state: State) -> Tuple[dict, State]:
    """This is a terminal step that would save the forecast or do something else."""
    return {}, state


def application() -> Application:
    app = (
        ApplicationBuilder()
        .with_actions(prepare_data, build_model, forecast, terminate)
        .with_transitions(
            ("prepare_data", "build_model"),
            ("build_model", "forecast"),
            ("forecast", "forecast", expr("current_timestep<simulation_end")),
            ("forecast", "terminate", expr("current_timestep>=simulation_end")),
        )
        .with_state(
            data_path="SOME_PATH",
            simulation_start="some-value",
            simulation_end="some-end-value",
        )
        .with_entrypoint("prepare_data")
        .build()
    )
    return app


# --- stubbed out functions to fill out for the simulation
def _process_data(data_path: str) -> object:
    """This function would read and process data from a file."""
    return {}


def _fit_model(data: object, upto: str) -> object:
    """This function would fit a model on data upto a certain date."""
    return {}


def _forecast_timestep(model: object, data: object, current_timestep: str) -> object:
    """This function would forecast the next timestep in the simulation given the model and data."""
    return {}


def _update_data(data: object, next_timestep: object) -> object:
    """This function would update the data with the forecasted timestep."""
    return {}


if __name__ == "__main__":
    _app = application()
    _app.visualize(
        output_file_path="statemachine", include_conditions=True, view=True, format="png"
    )
    # you could run things like this:
    # last_action, result, state = _app.run(halt_after=["terminate"])



---
File: /burr/examples/streaming-fastapi/__init__.py
---




---
File: /burr/examples/streaming-fastapi/application.py
---

import asyncio
import copy
from typing import AsyncGenerator, Optional, Tuple

import openai

from burr.core import ApplicationBuilder, State, default, when
from burr.core.action import action, streaming_action
from burr.core.graph import GraphBuilder

MODES = [
    "answer_question",
    "generate_poem",
    "generate_code",
    "unknown",
]


@action(reads=[], writes=["chat_history", "prompt"])
def process_prompt(state: State, prompt: str) -> Tuple[dict, State]:
    result = {"chat_item": {"role": "user", "content": prompt, "type": "text"}}
    return result, state.wipe(keep=["prompt", "chat_history"]).append(
        chat_history=result["chat_item"]
    ).update(prompt=prompt)


@action(reads=["prompt"], writes=["safe"])
def check_safety(state: State) -> Tuple[dict, State]:
    result = {"safe": "unsafe" not in state["prompt"]}  # quick hack to demonstrate
    return result, state.update(safe=result["safe"])


def _get_openai_client():
    return openai.AsyncOpenAI()


@action(reads=["prompt"], writes=["mode"])
async def choose_mode(state: State) -> Tuple[dict, State]:
    prompt = (
        f"You are a chatbot. You've been prompted this: {state['prompt']}. "
        f"You have the capability of responding in the following modes: {', '.join(MODES)}. "
        "Please respond with *only* a single word representing the mode that most accurately "
        "corresponds to the prompt. Fr instance, if the prompt is 'write a poem about Alexander Hamilton and Aaron Burr', "
        "the mode would be 'generate_poem'. If the prompt is 'what is the capital of France', the mode would be 'answer_question'."
        "And so on, for every mode. If none of these modes apply, please respond with 'unknown'."
    )

    result = await _get_openai_client().chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant"},
            {"role": "user", "content": prompt},
        ],
    )
    content = result.choices[0].message.content
    mode = content.lower()
    if mode not in MODES:
        mode = "unknown"
    result = {"mode": mode}
    return result, state.update(**result)


@streaming_action(reads=["prompt", "chat_history"], writes=["response"])
async def prompt_for_more(state: State) -> AsyncGenerator[Tuple[dict, Optional[State]], None]:
    """Not streaming, as we have the result immediately."""
    result = {
        "response": {
            "content": "None of the response modes I support apply to your question. Please clarify?",
            "type": "text",
            "role": "assistant",
        }
    }
    for word in result["response"]["content"].split():
        await asyncio.sleep(0.1)
        yield {"delta": word + " "}, None
    yield result, state.update(**result).append(chat_history=result["response"])


@streaming_action(reads=["prompt", "chat_history", "mode"], writes=["response"])
async def chat_response(
    state: State, prepend_prompt: str, model: str = "gpt-3.5-turbo"
) -> AsyncGenerator[Tuple[dict, Optional[State]], None]:
    """Streaming action, as we don't have the result immediately. This makes it more interactive"""
    chat_history = copy.deepcopy(state["chat_history"])
    chat_history[-1]["content"] = f"{prepend_prompt}: {chat_history[-1]['content']}"
    chat_history_api_format = [
        {
            "role": chat["role"],
            "content": chat["content"],
        }
        for chat in chat_history
    ]
    client = _get_openai_client()
    result = await client.chat.completions.create(
        model=model, messages=chat_history_api_format, stream=True
    )
    buffer = []
    async for chunk in result:
        chunk_str = chunk.choices[0].delta.content
        if chunk_str is None:
            continue
        buffer.append(chunk_str)
        yield {
            "delta": chunk_str,
        }, None

    result = {
        "response": {"content": "".join(buffer), "type": "text", "role": "assistant"},
        "modified_chat_history": chat_history,
    }
    yield result, state.update(**result).append(chat_history=result["response"])


@streaming_action(reads=["prompt", "chat_history"], writes=["response"])
async def unsafe_response(state: State) -> Tuple[dict, State]:
    result = {
        "response": {
            "content": "I am afraid I can't respond to that...",
            "type": "text",
            "role": "assistant",
        }
    }
    for word in result["response"]["content"].split():
        await asyncio.sleep(0.1)
        yield {"delta": word + " "}, None
    yield result, state.update(**result).append(chat_history=result["response"])


graph = (
    GraphBuilder()
    .with_actions(
        prompt=process_prompt,
        check_safety=check_safety,
        unsafe_response=unsafe_response,
        decide_mode=choose_mode,
        generate_code=chat_response.bind(
            prepend_prompt="Please respond with *only* code and no other text (at all) to the following",
        ),
        answer_question=chat_response.bind(
            prepend_prompt="Please answer the following question",
        ),
        generate_poem=chat_response.bind(
            prepend_prompt="Please generate a poem based on the following prompt",
        ),
        prompt_for_more=prompt_for_more,
    )
    .with_transitions(
        ("prompt", "check_safety", default),
        ("check_safety", "decide_mode", when(safe=True)),
        ("check_safety", "unsafe_response", default),
        ("decide_mode", "generate_code", when(mode="generate_code")),
        ("decide_mode", "answer_question", when(mode="answer_question")),
        ("decide_mode", "generate_poem", when(mode="generate_poem")),
        ("decide_mode", "prompt_for_more", default),
        (
            [
                "answer_question",
                "generate_poem",
                "generate_code",
                "prompt_for_more",
                "unsafe_response",
            ],
            "prompt",
        ),
    )
    .build()
)


def application(app_id: Optional[str] = None):
    return (
        ApplicationBuilder()
        .with_entrypoint("prompt")
        .with_state(chat_history=[])
        .with_graph(graph)
        .with_tracker(project="demo_chatbot_streaming")
        .with_identifiers(app_id=app_id)
        .build()
    )


# TODO -- replace these with action tags when we have the availability
TERMINAL_ACTIONS = [
    "answer_question",
    "generate_code",
    "prompt_for_more",
    "unsafe_response",
    "generate_poem",
]
if __name__ == "__main__":
    app = application()
    app.visualize(output_file_path="statemachine", include_conditions=True, view=True, format="png")



---
File: /burr/examples/streaming-fastapi/server.py
---

import functools
import importlib
import json
from typing import List, Literal

import pydantic
from fastapi import APIRouter
from starlette.responses import StreamingResponse

from burr.core import Application, ApplicationBuilder
from burr.tracking import LocalTrackingClient

"""This file represents a simple chatbot API backed with Burr.
We manage an application, write to it with post endpoints, and read with
get/ endpoints.

This demonstrates how you can build interactive web applications with Burr!
"""
# We're doing dynamic import cause this lives within examples/ (and that module has dashes)
# navigate to the examples directory to read more about this!
chat_application = importlib.import_module(
    "burr.examples.streaming-fastapi.application"
)  # noqa: F401

# the app is commented out as we include the router.
# app = FastAPI()

router = APIRouter()

graph = chat_application.graph


try:
    from opentelemetry.instrumentation.openai import OpenAIInstrumentor

    OpenAIInstrumentor().instrument()
    opentelemetry_available = True
except ImportError:
    opentelemetry_available = False


class ChatItem(pydantic.BaseModel):
    """Pydantic model for a chat item. This is used to render the chat history."""

    content: str
    type: Literal["image", "text", "code", "error"]
    role: Literal["user", "assistant"]


@functools.lru_cache(maxsize=128)
def _get_application(project_id: str, app_id: str) -> Application:
    """Quick tool to get the application -- caches it"""
    tracker = LocalTrackingClient(project=project_id, storage_dir="~/.burr")
    return (
        ApplicationBuilder()
        .with_graph(graph)
        # initializes from the tracking log if it does not already exist
        .initialize_from(
            tracker,
            resume_at_next_action=False,  # always resume from entrypoint in the case of failure
            default_state={"chat_history": []},
            default_entrypoint="prompt",
        )
        .with_tracker(tracker, use_otel_tracing=opentelemetry_available)
        .with_identifiers(app_id=app_id)
        .build()
    )


class PromptInput(pydantic.BaseModel):
    prompt: str


@router.post("/response/{project_id}/{app_id}", response_class=StreamingResponse)
async def chat_response(project_id: str, app_id: str, prompt: PromptInput) -> StreamingResponse:
    """Chat response endpoint. User passes in a prompt and the system returns the
    full chat history, so its easier to render.

    :param project_id: Project ID to run
    :param app_id: Application ID to run
    :param prompt: Prompt to send to the chatbot
    :return:
    """
    burr_app = _get_application(project_id, app_id)
    chat_history = burr_app.state.get("chat_history", [])
    action, streaming_container = await burr_app.astream_result(
        halt_after=chat_application.TERMINAL_ACTIONS, inputs=dict(prompt=prompt.prompt)
    )

    async def sse_generator():
        """This is a generator that yields Server-Sent Events (SSE) to the client
        It is necessary to yield them in a special format to ensure the client can
        access them streaming. We type them (using our own simple typing system) then
        parse on the client side. Unfortunately, typing these in FastAPI is not feasible."""
        yield f"data: {json.dumps({'type': 'chat_history', 'value': chat_history})}\n\n"

        async for item in streaming_container:
            yield f"data: {json.dumps({'type': 'delta', 'value': item['delta']})} \n\n"

    return StreamingResponse(sse_generator())


@router.get("/history/{project_id}/{app_id}", response_model=List[ChatItem])
def chat_history(project_id: str, app_id: str) -> List[ChatItem]:
    """Endpoint to get chat history. Gets the application and returns the chat history from state.

    :param project_id: Project ID
    :param app_id: App ID.
    :return: The list of chat items in the state
    """
    chat_app = _get_application(project_id, app_id)
    state = chat_app.state
    return state.get("chat_history", [])


@router.post("/create/{project_id}/{app_id}", response_model=str)
async def create_new_application(project_id: str, app_id: str) -> str:
    """Endpoint to create a new application -- used by the FE when
    the user types in a new App ID

    :param project_id: Project ID
    :param app_id: App ID
    :return: The app ID
    """
    # side-effect of this persists it -- see the application function for details
    _get_application(app_id=app_id, project_id=project_id)
    return app_id  # just return it for now


# # comment this back in for a standalone chatbot API
# import fastapi
#
# app = fastapi.FastAPI()
# app.include_router(router, prefix="/api/v0/chatbot")



---
File: /burr/examples/streaming-fastapi/streamlit_app.py
---

import asyncio
import logging
import uuid

import application as chatbot_application
import streamlit as st
from hamilton.log_setup import setup_logging

import burr.core
from burr.core.action import AsyncStreamingResultContainer

setup_logging(logging.INFO)


def render_chat_message(chat_item: dict):
    content = chat_item["content"]
    role = chat_item["role"]
    with st.chat_message(role):
        st.write(content)


async def render_streaming_chat_message(stream: AsyncStreamingResultContainer):
    buffer = ""
    with st.chat_message("assistant"):
        # This is very ugly as streamlit does not support async generators
        # Thus we have to ignore the benefit of writing the delta and instead write *everything*
        with st.empty():
            async for item in stream:
                buffer += item["delta"]
                st.write(buffer)


def initialize_app() -> burr.core.Application:
    if "burr_app" not in st.session_state:
        st.session_state.burr_app = chatbot_application.application(
            app_id=f"chat_streaming:{str(uuid.uuid4())[0:6]}"
        )
    return st.session_state.burr_app


async def main():
    st.title("Streaming chatbot with Burr")
    app = initialize_app()

    prompt = st.chat_input("Ask me a question!", key="chat_input")
    for chat_message in app.state.get("chat_history", []):
        render_chat_message(chat_message)

    if prompt:
        render_chat_message({"role": "user", "content": prompt, "type": "text"})
        with st.spinner(text="Waiting for response..."):
            action, streaming_container = await app.astream_result(
                halt_after=chatbot_application.TERMINAL_ACTIONS, inputs={"prompt": prompt}
            )
        await render_streaming_chat_message(streaming_container)


if __name__ == "__main__":
    asyncio.run(main())



---
File: /burr/examples/streaming-overview/__init__.py
---




---
File: /burr/examples/streaming-overview/application.py
---

from typing import Generator, Optional, Tuple

import openai

from burr.core import ApplicationBuilder, State, default, when
from burr.core.action import action, streaming_action

MODES = {
    "answer_question": "text",
    "generate_image": "image",
    "generate_code": "code",
    "unknown": "text",
}


@action(reads=[], writes=["chat_history", "prompt"])
def process_prompt(state: State, prompt: str) -> Tuple[dict, State]:
    result = {"chat_item": {"role": "user", "content": prompt, "type": "text"}}
    return result, state.wipe(keep=["prompt", "chat_history"]).append(
        chat_history=result["chat_item"]
    ).update(prompt=prompt)


@action(reads=["prompt"], writes=["safe"])
def check_safety(state: State) -> Tuple[dict, State]:
    result = {"safe": "unsafe" not in state["prompt"]}  # quick hack to demonstrate
    return result, state.update(safe=result["safe"])


def _get_openai_client():
    return openai.Client()


@action(reads=["prompt"], writes=["mode"])
def choose_mode(state: State) -> Tuple[dict, State]:
    prompt = (
        f"You are a chatbot. You've been prompted this: {state['prompt']}. "
        f"You have the capability of responding in the following modes: {', '.join(MODES)}. "
        "Please respond with *only* a single word representing the mode that most accurately "
        "corresponds to the prompt. Fr instance, if the prompt is 'draw a picture of a cat', "
        "the mode would be 'generate_image'. If the prompt is 'what is the capital of France', the mode would be 'answer_question'."
        "If none of these modes apply, please respond with 'unknown'."
    )

    result = _get_openai_client().chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant"},
            {"role": "user", "content": prompt},
        ],
    )
    content = result.choices[0].message.content
    mode = content.lower()
    if mode not in MODES:
        mode = "unknown"
    result = {"mode": mode}
    return result, state.update(**result)


@action(reads=["prompt", "chat_history"], writes=["response"])
def prompt_for_more(state: State) -> Tuple[dict, State]:
    """Not streaming, as we have the result immediately."""
    result = {
        "response": {
            "content": "None of the response modes I support apply to your question. Please clarify?",
            "type": "text",
            "role": "assistant",
        }
    }
    return result, state.update(**result).append(chat_history=result["response"])


@streaming_action(reads=["prompt", "chat_history", "mode"], writes=["response"])
def chat_response(
    state: State, prepend_prompt: str, model: str = "gpt-3.5-turbo"
) -> Generator[Tuple[dict, Optional[State]], None, None]:
    """Streaming action, as we don't have the result immediately. This makes it more interactive"""
    chat_history = state["chat_history"].copy()
    chat_history[-1]["content"] = f"{prepend_prompt}: {chat_history[-1]['content']}"
    chat_history_api_format = [
        {
            "role": chat["role"],
            "content": chat["content"],
        }
        for chat in chat_history
    ]
    client = _get_openai_client()
    result = client.chat.completions.create(
        model=model, messages=chat_history_api_format, stream=True
    )
    buffer = []
    for chunk in result:
        chunk_str = chunk.choices[0].delta.content
        if chunk_str is None:
            continue  # consider breaking...
        buffer.append(chunk_str)
        yield {
            "response": {"content": chunk_str, "type": MODES[state["mode"]], "role": "assistant"}
        }, None

    result = {
        "response": {"content": "".join(buffer), "type": MODES[state["mode"]], "role": "assistant"}
    }
    yield result, state.update(**result).append(chat_history=result["response"])


@action(reads=["prompt", "chat_history"], writes=["response"])
def unsafe_response(state: State) -> Tuple[dict, State]:
    result = {
        "response": {
            "content": "I am afraid I can't respond to that...",
            "type": "text",
            "role": "assistant",
        }
    }
    return result, state.update(**result).append(chat_history=result["response"])


@action(reads=["prompt", "chat_history", "mode"], writes=["response"])
def image_response(state: State, model: str = "dall-e-2") -> Tuple[dict, State]:
    client = _get_openai_client()
    result = client.images.generate(
        model=model, prompt=state["prompt"], size="1024x1024", quality="standard", n=1
    )
    response = result.data[0].url
    result = {"response": {"content": response, "type": MODES[state["mode"]], "role": "assistant"}}
    return result, state.update(**result).append(chat_history=result["response"])


def application(app_id: Optional[str] = None):
    return (
        ApplicationBuilder()
        .with_actions(
            prompt=process_prompt,
            check_safety=check_safety,
            unsafe_response=unsafe_response,
            decide_mode=choose_mode,
            generate_image=image_response,
            generate_code=chat_response.bind(
                prepend_prompt="Please respond with *only* code and no other text (at all) to the following:",
            ),
            answer_question=chat_response.bind(
                prepend_prompt="Please answer the following question:",
            ),
            prompt_for_more=prompt_for_more,
        )
        .with_entrypoint("prompt")
        .with_state(chat_history=[])
        .with_transitions(
            ("prompt", "check_safety", default),
            ("check_safety", "decide_mode", when(safe=True)),
            ("check_safety", "unsafe_response", default),
            ("decide_mode", "generate_image", when(mode="generate_image")),
            ("decide_mode", "generate_code", when(mode="generate_code")),
            ("decide_mode", "answer_question", when(mode="answer_question")),
            ("decide_mode", "prompt_for_more", default),
            (
                [
                    "generate_image",
                    "answer_question",
                    "generate_code",
                    "prompt_for_more",
                    "unsafe_response",
                ],
                "prompt",
            ),
        )
        .with_tracker(project="demo_chatbot_streaming")
        .with_identifiers(app_id=app_id)
        .build()
    )


# TODO -- replace these with action tags when we have the availability
TERMINAL_ACTIONS = [
    "generate_image",
    "answer_question",
    "generate_code",
    "prompt_for_more",
    "unsafe_response",
]
if __name__ == "__main__":
    app = application()
    app.visualize(
        output_file_path="statemachine", include_conditions=False, view=True, format="png"
    )



---
File: /burr/examples/streaming-overview/async_application.py
---

import asyncio
from typing import Optional, Tuple

import openai

from burr.core import Application, ApplicationBuilder, State, default, when
from burr.core.action import action, streaming_action

MODES = {
    "answer_question": "text",
    "generate_image": "image",
    "generate_code": "code",
    "unknown": "text",
}


@action(reads=[], writes=["chat_history", "prompt"])
def process_prompt(state: State, prompt: str) -> Tuple[dict, State]:
    result = {"chat_item": {"role": "user", "content": prompt, "type": "text"}}
    return result, state.wipe(keep=["prompt", "chat_history"]).append(
        chat_history=result["chat_item"]
    ).update(prompt=prompt)


@action(reads=["prompt"], writes=["safe"])
def check_safety(state: State) -> Tuple[dict, State]:
    result = {"safe": "unsafe" not in state["prompt"]}  # quick hack to demonstrate
    return result, state.update(safe=result["safe"])


def _get_openai_client():
    return openai.AsyncClient()


@action(reads=["prompt"], writes=["mode"])
async def choose_mode(state: State) -> Tuple[dict, State]:
    prompt = (
        f"You are a chatbot. You've been prompted this: {state['prompt']}. "
        f"You have the capability of responding in the following modes: {', '.join(MODES)}. "
        "Please respond with *only* a single word representing the mode that most accurately "
        "corresponds to the prompt. Fr instance, if the prompt is 'draw a picture of a cat', "
        "the mode would be 'generate_image'. If the prompt is 'what is the capital of France', the mode would be 'answer_question'."
        "If none of these modes apply, please respond with 'unknown'."
    )

    result = await _get_openai_client().chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant"},
            {"role": "user", "content": prompt},
        ],
    )
    content = result.choices[0].message.content
    mode = content.lower()
    if mode not in MODES:
        mode = "unknown"
    result = {"mode": mode}
    return result, state.update(**result)


@action(reads=["prompt", "chat_history"], writes=["response"])
def prompt_for_more(state: State) -> Tuple[dict, State]:
    """Not streaming, as we have the result immediately."""
    result = {
        "response": {
            "content": "None of the response modes I support apply to your question. Please clarify?",
            "type": "text",
            "role": "assistant",
        }
    }
    return result, state.update(**result).append(chat_history=result["response"])


@streaming_action(reads=["prompt", "chat_history", "mode"], writes=["response"])
async def chat_response(
    state: State, prepend_prompt: str, model: str = "gpt-3.5-turbo"
) -> Tuple[dict, State]:
    """Streaming action, as we don't have the result immediately. This makes it more interactive"""
    chat_history = state["chat_history"].copy()
    chat_history[-1]["content"] = f"{prepend_prompt}: {chat_history[-1]['content']}"
    chat_history_api_format = [
        {
            "role": chat["role"],
            "content": chat["content"],
        }
        for chat in chat_history
    ]
    client = _get_openai_client()
    result = await client.chat.completions.create(
        model=model, messages=chat_history_api_format, stream=True
    )
    buffer = []
    async for chunk in result:
        chunk_str = chunk.choices[0].delta.content
        if chunk_str is None:
            continue  # consider breaking...
        buffer.append(chunk_str)
        yield {
            "response": {"content": chunk_str, "type": MODES[state["mode"]], "role": "assistant"}
        }, None

    result = {
        "response": {"content": "".join(buffer), "type": MODES[state["mode"]], "role": "assistant"}
    }
    yield result, state.update(**result).append(chat_history=result["response"])


@action(reads=["prompt", "chat_history"], writes=["response"])
def unsafe_response(state: State) -> Tuple[dict, State]:
    result = {
        "response": {
            "content": "I am afraid I can't respond to that...",
            "type": "text",
            "role": "assistant",
        }
    }
    return result, state.update(**result).append(chat_history=result["response"])


@action(reads=["prompt", "chat_history", "mode"], writes=["response"])
def image_response(state: State, model: str = "dall-e-2") -> Tuple[dict, State]:
    client = _get_openai_client()
    result = client.images.generate(
        model=model, prompt=state["prompt"], size="1024x1024", quality="standard", n=1
    )
    response = result.data[0].url
    result = {"response": {"content": response, "type": MODES[state["mode"]], "role": "assistant"}}
    return result, state.update(**result).append(chat_history=result["response"])


def application(app_id: Optional[str] = None) -> Application:
    return (
        ApplicationBuilder()
        .with_actions(
            prompt=process_prompt,
            check_safety=check_safety,
            unsafe_response=unsafe_response,
            decide_mode=choose_mode,
            generate_image=image_response,
            generate_code=chat_response.bind(
                prepend_prompt="Please respond with *only* code and no other text (at all) to the following:",
            ),
            answer_question=chat_response.bind(
                prepend_prompt="Please answer the following question:",
            ),
            prompt_for_more=prompt_for_more,
        )
        .with_entrypoint("prompt")
        .with_state(chat_history=[])
        .with_transitions(
            ("prompt", "check_safety", default),
            ("check_safety", "decide_mode", when(safe=True)),
            ("check_safety", "unsafe_response", default),
            ("decide_mode", "generate_image", when(mode="generate_image")),
            ("decide_mode", "generate_code", when(mode="generate_code")),
            ("decide_mode", "answer_question", when(mode="answer_question")),
            ("decide_mode", "prompt_for_more", default),
            (
                [
                    "generate_image",
                    "answer_question",
                    "generate_code",
                    "prompt_for_more",
                    "unsafe_response",
                ],
                "prompt",
            ),
        )
        .with_tracker(project="demo_chatbot_streaming_async")
        .with_identifiers(app_id=app_id)
        .build()
    )


# TODO -- replace these with action tags when we have the availability
TERMINAL_ACTIONS = [
    "generate_image",
    "answer_question",
    "generate_code",
    "prompt_for_more",
    "unsafe_response",
]


async def run(app):
    """Runs the application. Queries the input for prompt"""
    action, result = await app.astream_result(
        halt_after=TERMINAL_ACTIONS, inputs={"prompt": input("Please enter a prompt: ")}
    )
    async for item in result:
        print(item["response"]["content"], end="\n")
    result, state = await result.get()
    print(result["response"]["content"])


async def main():
    app = application()
    app.visualize(
        output_file_path="statemachine", include_conditions=False, view=True, format="png"
    )
    await run(app)


if __name__ == "__main__":
    asyncio.run(main())



---
File: /burr/examples/streaming-overview/streamlit_app.py
---

import uuid

import application as chatbot_application
import streamlit as st

import burr.core
from burr.core.action import StreamingResultContainer


def render_chat_message(chat_item: dict):
    content = chat_item["content"]
    content_type = chat_item["type"]
    role = chat_item["role"]
    with st.chat_message(role):
        if content_type == "image":
            st.image(content)
        elif content_type == "code":
            st.code(content)
        elif content_type == "text":
            st.write(content)


def render_streaming_chat_message(stream: StreamingResultContainer):
    with st.chat_message("assistant"):
        st.write_stream(item["response"]["content"] for item in stream)


def initialize_app() -> burr.core.Application:
    if "burr_app" not in st.session_state:
        st.session_state.burr_app = chatbot_application.application(
            app_id=f"chat_streaming:{str(uuid.uuid4())[0:6]}"
        )
    return st.session_state.burr_app


def main():
    st.title("Streaming chatbot with Burr")
    app = initialize_app()

    prompt = st.chat_input("Ask me a question!", key="chat_input")
    for chat_message in app.state.get("chat_history", []):
        render_chat_message(chat_message)

    if prompt:
        render_chat_message({"role": "user", "content": prompt, "type": "text"})
        with st.spinner(text="Waiting for response..."):
            action, streaming_container = app.stream_result(
                halt_after=chatbot_application.TERMINAL_ACTIONS, inputs={"prompt": prompt}
            )
        if action.streaming:
            render_streaming_chat_message(streaming_container)
        else:
            render_chat_message(streaming_container.get()[0]["response"])


if __name__ == "__main__":
    main()



---
File: /burr/examples/templates/agent_supervisor.py
---

"""
Template for agent supervisor with multiple agents.

This example is similar to the multi_agent_collaboration.py example, but that
instead a supervisor agent is used to manage it all and whether to stop or continue.
"""
from burr import core
from burr.core import ApplicationBuilder, State, action, default
from burr.tracking import client as burr_tclient

# --- Define some tools

tools = []  # these could be functions, etc.

# --- Start defining Action


@action(reads=["query", "messages"], writes=["next_step"])
def supervisor_agent(state: State) -> tuple[dict, State]:
    """This agent decides what to do next given the current contxt.

    i.e. call which agent, or terminate.

    :param state: state of the application
    :return:
    """
    _query = state["query"]  # noqa:F841
    _message = state["messages"]  # noqa:F841
    _agents = ["some_agent_1", "some_agent_2"]  # noqa:F841
    # Do LLM call, passing in current information and options
    # determine what to do next
    _result = {"next_step": "agent_or_terminate"}
    return _result, state.update(**_result)


@action(reads=["query", "messages"], writes=["parsed_tool_calls"])
def some_agent_1(state: State) -> tuple[dict, State]:
    """An agent specializing in some task.

    :param state: state of the application
    :return:
    """
    _query = state["query"]  # noqa:F841
    _message = state["messages"]  # noqa:F841
    # Do LLM call, passing in tool information
    # determine which tool to call
    _result = {"tool_call": "some_tool", "tool_args": {"arg1": "value1"}}
    return _result, state.update(parsed_tool_calls=[_result], sender="some_agent_1")


@action(reads=["query", "messages"], writes=["parsed_tool_calls"])
def some_agent_2(state: State) -> tuple[dict, State]:
    """An agent specializing in some other task.

    :param state: state of the application
    :return:
    """
    _query = state["query"]  # noqa:F841
    _message = state["messages"]  # noqa:F841
    # Do LLM call, passing in tool information
    # determine which tool to call
    _result = {"tool_call": "some_tool2", "tool_args": {"arg1": "value1"}}
    return _result, state.update(parsed_tool_calls=[_result], sender="some_agent_2")


@action(reads=["messages", "parsed_tool_calls"], writes=["messages", "parsed_tool_calls"])
def tool_node(state: State) -> tuple[dict, State]:
    """Given a tool call, execute it and return the result."""
    _messages = state["messages"]  # noqa:F841
    _parsed_tool_calls = state["parsed_tool_calls"]  # noqa:F841
    # execute the tool call
    # get result and create new message
    _tool_results = [{"some": "message", "from": "tool_node"}]
    new_state = state.append(messages=_tool_results)
    new_state = new_state.update(parsed_tool_calls=[])
    # We return a list, because this will get added to the existing list
    return {"tool_results": _tool_results}, new_state


@action(reads=["messages"], writes=[])
def terminal_step(state: State) -> tuple[dict, State]:
    """Terminal step we have here that does nothing, but it could"""
    return {}, state


def default_state_and_entry_point(query: str = None) -> tuple[dict, str]:
    """Returns the default state and entry point for the application."""
    return {
        "messages": [],
        "query": query,
        "sender": "",
        "parsed_tool_calls": [],
    }, "supervisor_agent"


def main(query: str = None, app_instance_id: str = None, sequence_number: int = None):
    """Main function to run the application.

    :param query: the query for the agents to run over.
    :param app_instance_id: a prior app instance id to restart from.
    :param sequence_number: a prior sequence number to restart from.
    :return:
    """
    project_name = "template:multi-agent-collaboration"
    if app_instance_id:
        tracker = burr_tclient.LocalTrackingClient(project_name)
        persisted_state = tracker.load("", app_id=app_instance_id, sequence_no=sequence_number)
        if not persisted_state:
            print(f"Warning: No persisted state found for app_id {app_instance_id}.")
            state, entry_point = default_state_and_entry_point(query)
        else:
            state = persisted_state["state"]
            entry_point = persisted_state["position"]
    else:
        state, entry_point = default_state_and_entry_point(query)
    # look up app_id for particular user
    # if None -- then proceed with defaults
    # else load from state, and set entry point
    app = (
        ApplicationBuilder()
        .with_state(**state)
        .with_actions(
            supervisor_agent=supervisor_agent,
            some_agent_1=some_agent_1,
            some_agent_2=some_agent_2,
            tool_node=tool_node,
            terminal=terminal_step,
        )
        .with_transitions(
            ("supervisor_agent", "some_agent_1", core.when(next_step="some_agent_1")),
            ("supervisor_agent", "some_agent_2", core.when(next_step="some_agent_2")),
            ("supervisor_agent", "terminal", core.when(next_step="terminate")),
            ("some_agent_1", "tool_node", core.expr("len(parsed_tool_calls) > 0")),
            ("some_agent_1", "supervisor_agent", default),
            ("some_agent_2", "tool_node", core.expr("len(parsed_tool_calls) > 0")),
            ("some_agent_2", "supervisor_agent", default),
            ("tool_node", "some_agent_1", core.when(sender="some_agent_1")),
            ("tool_node", "some_agent_2", core.when(sender="some_agent_2")),
        )
        .with_identifiers(partition_key="demo")
        .with_entrypoint(entry_point)
        .with_tracker(project=project_name)
        .build()
    )
    return app


if __name__ == "__main__":
    # Add an app_id to restart from last sequence in that state
    _app_id = "8458dc58-7b6c-430b-9ab3-23450774f883"
    app = main("some user query", _app_id)
    app.visualize(
        output_file_path="agent_supervisor", include_conditions=True, view=True, format="png"
    )
    # app.run(halt_after=["terminal"])



---
File: /burr/examples/templates/hierarchical_agent_teams.py
---

"""
This shows how to do a nested example of a hierarchical agent that uses
teams. What that means is that we will use Burr within Burr!

We're still deciding on a nicer way to do this, but for now, this is one
way to do it.

Note: you could unroll this into a single application.
"""
import uuid

import agent_supervisor

from burr import core
from burr.core import ApplicationBuilder, State, action, default
from burr.tracking import client as burr_tclient

# --- Define some tools

tools = []  # these could be functions, etc.

# --- Start defining Action


@action(reads=["query", "messages"], writes=["next_step"])
def supervisor_agent(state: State) -> tuple[dict, State]:
    """This agent decides what to do next given the current context.

    i.e. call which team, or terminate.

    :param state: state of the application
    :return:
    """
    _query = state["query"]  # noqa:F841
    _message = state["messages"]  # noqa:F841
    _agents = ["team_1", "team_2"]  # noqa:F841
    # Do LLM call, passing in current information and options
    # determine what to do next
    _result = {"next_step": "team_or_terminate"}
    return _result, state.update(**_result)


@action(reads=["query", "messages"], writes=["result"])
def team_1(state: State) -> tuple[dict, State]:
    """A team specializing in some task.

    :param state: state of the application
    :return:
    """
    _query = state["query"]  # noqa:F841
    _message = state["messages"]  # noqa:F841
    _id = str(uuid.uuid4())
    _app = agent_supervisor.main("some user query", _id)
    _last_action, _result, _state = _app.run(halt_after=["terminal"])  # noqa:F841
    # pull results and updates accordingly
    _result = {"result": _result}
    return _result, state.update(**_result)


@action(reads=["query", "messages"], writes=["result"])
def team_2(state: State) -> tuple[dict, State]:
    """A team specializing in some other task.

    :param state: state of the application
    :return:
    """
    _query = state["query"]  # noqa:F841
    _message = state["messages"]  # noqa:F841
    _id = str(uuid.uuid4())
    _app = agent_supervisor.main("some user query", _id)
    _last_action, _result, _state = _app.run(halt_after=["terminal"])  # noqa:F841
    # pull results and updates accordingly
    _result = {"result": _result}
    return _result, state.update(**_result)


@action(reads=["messages"], writes=[])
def terminal_step(state: State) -> tuple[dict, State]:
    """Terminal step we have here that does nothing, but it could"""
    return {}, state


def default_state_and_entry_point(query: str = None) -> tuple[dict, str]:
    """Returns the default state and entry point for the application."""
    return {
        "messages": [],
        "query": query,
        "sender": "",
        "parsed_tool_calls": [],
    }, "supervisor_agent"


def main(query: str = None, app_instance_id: str = None, sequence_number: int = None):
    """Main function to run the application.

    :param query: the query for the agents to run over.
    :param app_instance_id: a prior app instance id to restart from.
    :param sequence_number: a prior sequence number to restart from.
    :return:
    """
    project_name = "template:hierarchical-agent-teams"
    if app_instance_id:
        tracker = burr_tclient.LocalTrackingClient(project_name)
        persisted_state = tracker.load("", app_id=app_instance_id, sequence_no=sequence_number)
        if not persisted_state:
            print(f"Warning: No persisted state found for app_id {app_instance_id}.")
            state, entry_point = default_state_and_entry_point(query)
        else:
            state = persisted_state["state"]
            entry_point = persisted_state["position"]
    else:
        state, entry_point = default_state_and_entry_point(query)
    # look up app_id for particular user
    # if None -- then proceed with defaults
    # else load from state, and set entry point
    app = (
        ApplicationBuilder()
        .with_state(**state)
        .with_actions(
            supervisor_agent=supervisor_agent,
            team_1=team_1,
            team_2=team_2,
            terminal=terminal_step,
        )
        .with_transitions(
            ("supervisor_agent", "team_1", core.when(next_step="team_1")),
            ("supervisor_agent", "team_2", core.when(next_step="team_2")),
            ("supervisor_agent", "terminal", core.when(next_step="terminate")),
            ("team_1", "supervisor_agent", default),
            ("team_2", "supervisor_agent", default),
        )
        .with_identifiers(partition_key="demo")
        .with_entrypoint(entry_point)
        .with_tracker(project=project_name)
        .build()
    )
    return app


if __name__ == "__main__":
    # Add an app_id to restart from last sequence in that state
    _app_id = "8458dc58-7b6c-430b-9ab3-23450774f883"
    app = main("some user query", _app_id)
    app.visualize(
        output_file_path="hierarchical_agent_teams",
        include_conditions=True,
        view=True,
        format="png",
    )
    # app.run(halt_after=["terminal"])



---
File: /burr/examples/templates/multi_agent_collaboration.py
---

"""
Template for multi-agent collaboration.

This contains a simple example of how to set up a multi-agent collaboration.
The functions are to be filled in with the actual code to run the agents.
"""
from burr import core
from burr.core import ApplicationBuilder, State, action, default
from burr.tracking import client as burr_tclient

# --- Define some tools

tools = []  # these could be functions, etc.

# --- Start defining Action


@action(reads=["query", "messages"], writes=["parsed_tool_calls"])
def some_agent_1(state: State) -> tuple[dict, State]:
    """An agent specializing in some task.

    :param state: state of the application
    :return:
    """
    _query = state["query"]  # noqa:F841
    _message = state["messages"]  # noqa:F841
    # Do LLM call, passing in tool information
    # determine which tool to call
    _result = {"tool_call": "some_tool", "tool_args": {"arg1": "value1"}}
    return _result, state.update(parsed_tool_calls=[_result], sender="some_agent_1")


@action(reads=["query", "messages"], writes=["parsed_tool_calls"])
def some_agent_2(state: State) -> tuple[dict, State]:
    """An agent specializing in some other task.

    :param state: state of the application
    :return:
    """
    _query = state["query"]  # noqa:F841
    _message = state["messages"]  # noqa:F841
    # Do LLM call, passing in tool information
    # determine which tool to call
    _result = {"tool_call": "some_tool2", "tool_args": {"arg1": "value1"}}
    return _result, state.update(parsed_tool_calls=[_result], sender="some_agent_2")


@action(reads=["messages", "parsed_tool_calls"], writes=["messages", "parsed_tool_calls"])
def tool_node(state: State) -> tuple[dict, State]:
    """Given a tool call, execute it and return the result."""
    _messages = state["messages"]  # noqa:F841
    _parsed_tool_calls = state["parsed_tool_calls"]  # noqa:F841
    # execute the tool call
    # get result and create new message
    _tool_results = [{"some": "message", "from": "tool_node"}]
    new_state = state.append(messages=_tool_results)
    new_state = new_state.update(parsed_tool_calls=[])
    # We return a list, because this will get added to the existing list
    return {"tool_results": _tool_results}, new_state


@action(reads=["messages"], writes=[])
def terminal_step(state: State) -> tuple[dict, State]:
    """Terminal step we have here that does nothing, but it could"""
    return {}, state


def default_state_and_entry_point(query: str = None) -> tuple[dict, str]:
    """Returns the default state and entry point for the application."""
    return {
        "messages": [],
        "query": query,
        "sender": "",
        "parsed_tool_calls": [],
    }, "some_agent_1"


def main(query: str = None, app_instance_id: str = None, sequence_number: int = None):
    """Main function to run the application.

    :param query: the query for the agents to run over.
    :param app_instance_id: a prior app instance id to restart from.
    :param sequence_number: a prior sequence number to restart from.
    :return:
    """
    project_name = "template:multi-agent-collaboration"
    if app_instance_id:
        tracker = burr_tclient.LocalTrackingClient(project_name)
        persisted_state = tracker.load("", app_id=app_instance_id, sequence_no=sequence_number)
        if not persisted_state:
            print(f"Warning: No persisted state found for app_id {app_instance_id}.")
            state, entry_point = default_state_and_entry_point(query)
        else:
            state = persisted_state["state"]
            entry_point = persisted_state["position"]
    else:
        state, entry_point = default_state_and_entry_point(query)
    # look up app_id for particular user
    # if None -- then proceed with defaults
    # else load from state, and set entry point
    app = (
        ApplicationBuilder()
        .with_state(**state)
        .with_actions(
            some_agent_1=some_agent_1,
            some_agent_2=some_agent_2,
            tool_node=tool_node,
            terminal=terminal_step,
        )
        .with_transitions(
            ("some_agent_1", "tool_node", core.expr("len(parsed_tool_calls) > 0")),
            (
                "some_agent_1",
                "terminal",
                core.expr("'SOMETHING_IN_MESSAGES' in messages[-1]['content']"),
            ),
            ("some_agent_1", "some_agent_2", default),
            ("some_agent_2", "tool_node", core.expr("len(parsed_tool_calls) > 0")),
            (
                "some_agent_2",
                "terminal",
                core.expr("'SOMETHING_IN_MESSAGES' in messages[-1]['content']"),
            ),
            ("some_agent_2", "some_agent_1", default),
            ("tool_node", "some_agent_1", core.expr("sender == 'some_agent_1'")),
            ("tool_node", "some_agent_2", core.expr("sender == 'some_agent_2'")),
        )
        .with_identifiers(partition_key="demo")
        .with_entrypoint(entry_point)
        .with_tracker(project=project_name)
        .build()
    )
    return app


if __name__ == "__main__":
    # Add an app_id to restart from last sequence in that state
    _app_id = "8458dc58-7b6c-430b-9ab3-23450774f883"
    app = main("some user query", _app_id)
    app.visualize(
        output_file_path="multi_agent_collaboration",
        include_conditions=True,
        view=True,
        format="png",
    )
    # app.run(halt_after=["terminal"])



---
File: /burr/examples/templates/multi_modal_agent.py
---

"""
Template for a multi-modal agent.

The idea is that this whole application functions as an agent that can do
multi-modal things.

Fill in the functions, and adjust/create new actions as needed.
"""
from typing import Tuple

from burr import tracking
from burr.core import Application, ApplicationBuilder, State, default, when
from burr.core.action import action


@action(reads=[], writes=["chat_history", "query"])
def apply_input_query(state: State, user_query: str) -> Tuple[dict, State]:
    # this is a simple example of how you might process a user query -- stick it directly into the chat history
    result = {"chat_item": {"role": "user", "content": user_query, "type": "text"}}
    new_state = state.append(chat_history=result["chat_item"]).update(query=user_query)
    return result, new_state


@action(reads=["prompt"], writes=["mode"])
def choose_mode(state: State) -> Tuple[dict, State]:
    """Code to choose a mode"""
    result = {"mode": "example"}
    return result, state.update(**result)


@action(reads=[], writes=["response"])
def unknown_action(state: State) -> Tuple[dict, State]:
    result = {
        "response": {
            "content": "None of the response modes I support apply to your question. Please clarify?",
            "type": "text",
            "role": "assistant",
        }
    }
    return result, state.update(**result)


@action(reads=["query", "chat_history", "mode"], writes=["response"])
def some_action(state: State) -> Tuple[dict, State]:
    """An example action -- create more of these to handle different modes"""
    _query = state["query"]  # noqa:F841
    _mode = state["mode"]  # noqa:F841
    _chat_history = state["chat_history"]  # noqa:F841
    result = {"response": {"some": "response"}}
    return result, state.update(**result)


@action(reads=["query", "mode", "response"], writes=["chat_history"])
def response(state: State) -> Tuple[dict, State]:
    """Function to create a response based on the prior action."""
    _result = {
        "content": state["response"],
        "type": state["mode"],
        "role": "assistant",
    }
    return _result, state.append(chat_history=_result)


def base_application(app_id: str, storage_dir: str, project_id: str) -> Application:
    """Creates the Burr application"""
    tracker = tracking.LocalTrackingClient(project=project_id, storage_dir=storage_dir)
    return (
        ApplicationBuilder()
        .with_actions(
            apply_input_query=apply_input_query,
            decide_mode=choose_mode,
            unknown_action=unknown_action,
            some_action=some_action,
            response=response,
        )
        .with_transitions(
            ("apply_input_query", "decide_mode", default),
            ("decide_mode", "unknown_action", when(mode="unknown")),
            ("decide_mode", "some_action", when(mode="some_action")),
            (["unknown_action", "some_action"], "response", default),
            ("response", "apply_input_query", default),
        )
        # initializes from the tracking log if it does not already exist
        .initialize_from(
            tracker,
            resume_at_next_action=False,  # always resume from entrypoint in the case of failure
            default_state={"chat_history": []},
            default_entrypoint="apply_input_query",
        )
        .with_tracker(tracker)
        .with_identifiers(app_id=app_id)
        .build()
    )


if __name__ == "__main__":
    import uuid

    app_id: str = str(uuid.uuid4())
    storage_dir: str = "~/.burr"
    project_id: str = "template:multi-modal-agent"
    app = base_application(app_id, storage_dir, project_id)
    app.visualize(
        output_file_path="multi_modal_agent", include_conditions=False, view=True, format="png"
    )
    # this is how you could run one cycle of the agent:
    # app.run(halt_after=["response"], inputs={"user_query": "Hello, world!"})



---
File: /burr/examples/test-case-creation/__init__.py
---




---
File: /burr/examples/test-case-creation/application.py
---

"""This file is truncated to just the relevant parts for the example."""
from typing import Tuple

import openai

from burr.core import State
from burr.core.action import action

# Reminder: This file is truncated to just the relevant parts for the example.

MODES = {
    "answer_question": "text",
    "generate_image": "image",
    "generate_code": "code",
    "unknown": "text",
}


def _get_openai_client():
    return openai.Client()


@action(reads=["prompt"], writes=["mode"])
def choose_mode(state: State) -> Tuple[dict, State]:
    prompt = (
        f"You are a chatbot. You've been prompted this: {state['prompt']}. "
        f"You have the capability of responding in the following modes: {', '.join(MODES)}. "
        "Please respond with *only* a single word representing the mode that most accurately "
        "corresponds to the prompt. For instance, if the prompt is 'draw a picture of a cat', "
        "the mode would be 'generate_image'. If the prompt is 'what is the capital of France', the mode would be 'answer_question'."
        "If none of these modes apply, please respond with 'unknown'."
    )

    result = _get_openai_client().chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant"},
            {"role": "user", "content": prompt},
        ],
    )
    content = result.choices[0].message.content
    mode = content.lower()
    if mode not in MODES:
        mode = "unknown"
    result = {"mode": mode}
    return result, state.update(**result)


@action(reads=["prompt", "chat_history"], writes=["response"])
def prompt_for_more(state: State) -> Tuple[dict, State]:
    result = {
        "response": {
            "content": "None of the response modes I support apply to your question. Please clarify?",
            "type": "text",
            "role": "assistant",
        }
    }
    return result, state.update(**result)


# Reminder: this file is truncated to just the relevant parts for the example.



---
File: /burr/examples/test-case-creation/test_application.py
---

"""
This is an example test module showing how you can test an action in your application.

1. There is a very straightforward unit testing style approach.
2. There is a more sophisticated approach that uses pytest_generate_tests to dynamically generate test cases.
To use this approach you need to do `from burr.testing import pytest_generate_tests  # noqa: F401`.

"""
import pytest
from application import prompt_for_more

from burr.core import state

# the following is required to run file based tests
from burr.testing import pytest_generate_tests  # noqa: F401


def test_prompt_for_more():
    """A basic test showing how to 'unit' test an action."""
    # set up state to pass in
    input_state = state.State(
        {
            "prompt": "",
            "chat_history": [],
        }
    )
    # Set up expected result and output state
    expected_state = state.State(
        {
            "prompt": "",
            "chat_history": [],
            "response": {
                "content": "None of the response modes I support apply to your question. Please clarify?",
                "type": "text",
                "role": "assistant",
            },
        }
    )
    result, output_state = prompt_for_more(input_state)
    # evaluate the output
    # TODO: choose appropriate way to evaluate the output based on your needs.
    # e.g. exact match, fuzzy match, LLM grade, etc.
    assert output_state == expected_state
    assert result == {
        "response": {
            "content": "None of the response modes I support apply to your question. Please clarify?",
            "type": "text",
            "role": "assistant",
        }
    }


@pytest.mark.file_name("prompt_for_more.json")
def test_prompt_for_more_from_file(input_state, expected_state, results_bag):
    """Function for testing the action"""
    input_state = state.State.deserialize(input_state)
    expected_state = state.State.deserialize(expected_state)
    _, output_state = prompt_for_more(input_state)  # exercising the action
    # TODO: choose appropriate way to evaluate the output
    # e.g. exact match, fuzzy match, LLM grade, etc.
    # this is exact match here on all values in state
    assert output_state == expected_state
    # for output that varies, you can do something like this
    # assert 'some value' in output_state["response"]["content"]
    # or, have an LLM Grade things -- you need to create the llm_evaluator function:
    # assert llm_evaluator("are these two equivalent responses. Respond with Y for yes, N for no",
    # output_state["response"]["content"], expected_state["response"]["content"]) == "Y"
    results_bag.input_state = input_state
    results_bag.expected_state = expected_state
    results_bag.output_state = output_state
    results_bag.foo = "bar"


def test_print_results(module_results_df):
    print(module_results_df.columns)
    print(module_results_df.head())
    # save to CSV
    # upload to google sheets
    # compute statistics



---
File: /burr/examples/tool-calling/__init__.py
---




---
File: /burr/examples/tool-calling/application.py
---

import inspect
import json
import os
from typing import Callable, Optional

import openai
import requests

from burr.core import State, action, when
from burr.core.application import ApplicationBuilder


@action(reads=[], writes=["query"])
def process_input(state: State, query: str) -> State:
    """Simple action to process input for the assistant."""
    return state.update(query=query)


# All the tools are functions below
# They have parameters (given by their annotations), and return a dictionary
# This dictionary is free-form -- it will be interpreted by the LLM later
# In your implementation you may want to change the return type to be more specific and use it programmatically
# But for the case of a generic assistant, this is a nice way to exrpess it
def _weather_tool(latitude: float, longitude: float) -> dict:
    """Queries the weather for a given latitude and longitude."""
    api_key = os.environ.get("TOMORROW_API_KEY")
    url = f"https://api.tomorrow.io/v4/weather/forecast?location={latitude},{longitude}&apikey={api_key}"

    response = requests.get(url)
    if response.status_code == 200:
        return response.json()
    else:
        return {"error": f"Failed to get weather data. Status code: {response.status_code}"}


def _text_wife_tool(message: str) -> dict:
    """Texts your wife with a message."""
    # Dummy implementation for the text wife tool
    # Replace this with actual SMS API logic
    return {"action": f"Texted wife: {message}"}


def _order_coffee_tool(
    size: str, coffee_preparation: str, any_modifications: Optional[str] = None
) -> dict:
    """Orders a coffee with the given size, preparation, and any modifications."""
    # Dummy implementation for the order coffee tool
    # Replace this with actual coffee shop API logic
    return {
        "action": (
            f"Ordered a {size} {coffee_preparation}" + f"with {any_modifications}"
            if any_modifications
            else ""
        )
    }


def _fallback(response: str) -> dict:
    """Tells the user that the assistant can't do that -- this should be a fallback"""
    return {"response": response}


# You'll want to add more types here as needed

TYPE_MAP = {
    str: "string",
    int: "integer",
    float: "number",
    bool: "boolean",
}

# You can also consider using a library like pydantic to further integrate with OpenAI
OPENAI_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": fn_name,
            "description": fn.__doc__ or fn_name,
            "parameters": {
                "type": "object",
                "properties": {
                    param.name: {
                        "type": TYPE_MAP.get(param.annotation)
                        or "string",  # TODO -- add error cases
                        "description": param.name,
                    }
                    for param in inspect.signature(fn).parameters.values()
                },
                "required": [param.name for param in inspect.signature(fn).parameters.values()],
            },
        },
    }
    for fn_name, fn in {
        "query_weather": _weather_tool,
        "order_coffee": _order_coffee_tool,
        "text_wife": _text_wife_tool,
        "fallback": _fallback,
    }.items()
]


@action(reads=["query"], writes=["tool_parameters", "tool"])
def select_tool(state: State) -> State:
    """Selects the tool + assigns the parameters. Uses the tool-calling API."""
    messages = [
        {
            "role": "system",
            "content": (
                "You are a helpful assistant. Use the supplied tools to assist the user, if they apply in any way. Remember to use the tools! They can do stuff you can't."
                "If you can't use only the tools provided to answer the question but know the answer, please provide the answer"
                "If you cannot use the tools provided to answer the question, use the fallback tool and provide a reason. "
                "Again, if you can't use one tool provided to answer the question, use the fallback tool and provide a reason. "
                "You must select exactly one tool no matter what, filling in every parameters with your best guess. Do not skip out on parameters!"
            ),
        },
        {"role": "user", "content": state["query"]},
    ]
    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        tools=OPENAI_TOOLS,
    )

    # Extract the tool name and parameters from OpenAI's response
    if response.choices[0].message.tool_calls is None:
        return state.update(
            tool="fallback",
            tool_parameters={
                "response": "No tool was selected, instead response was: {response.choices[0].message}."
            },
        )
    fn = response.choices[0].message.tool_calls[0].function

    return state.update(tool=fn.name, tool_parameters=json.loads(fn.arguments))


@action(reads=["tool_parameters"], writes=["raw_response"])
def call_tool(state: State, tool_function: Callable) -> State:
    """Action to call the tool. This will be bound to the tool function."""
    response = tool_function(**state["tool_parameters"])
    return state.update(raw_response=response)


@action(reads=["query", "raw_response"], writes=["final_output"])
def format_results(state: State) -> State:
    """Action to format the results in a usable way. Note we're not cascading in context for the chat history.
    This is largely due to keeping it simple, but you'll likely want to pass IDs around or maintain the chat history yourself
    """
    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": (
                    "You are a helpful assistant. Your goal is to take the"
                    "data presented and use it to answer the original question:"
                ),
            },
            {
                "role": "user",
                "content": (
                    f"The original question was: {state['query']}."
                    f"The data is: {state['raw_response']}. Please format"
                    "the data and provide a response that responds to the original query."
                    "As always, be concise (tokens aren't free!)."
                ),
            },
        ],
    )

    return state.update(final_output=response.choices[0].message.content)


def application():
    """Builds an application"""
    return (
        ApplicationBuilder()
        .with_actions(
            process_input,
            select_tool,
            format_results,
            query_weather=call_tool.bind(tool_function=_weather_tool),
            text_wife=call_tool.bind(tool_function=_text_wife_tool),
            order_coffee=call_tool.bind(tool_function=_order_coffee_tool),
            fallback=call_tool.bind(tool_function=_fallback),
        )
        .with_transitions(
            ("process_input", "select_tool"),
            ("select_tool", "query_weather", when(tool="query_weather")),
            ("select_tool", "text_wife", when(tool="text_wife")),
            ("select_tool", "order_coffee", when(tool="order_coffee")),
            ("select_tool", "fallback", when(tool="fallback")),
            (["query_weather", "text_wife", "order_coffee", "fallback"], "format_results"),
            ("format_results", "process_input"),
        )
        .with_entrypoint("process_input")
        .with_tracker(project="demo_tool_calling", use_otel_tracing=True)
        .build()
    )


if __name__ == "__main__":
    from opentelemetry.instrumentation.openai import OpenAIInstrumentor

    OpenAIInstrumentor().instrument()
    app = application()
    app.visualize(output_file_path="./statemachine.png")
    action, result, state = app.run(
        halt_after=["format_results"],
        inputs={"query": "What's the weather like in San Francisco?"},
    )
    print(state["final_output"])



---
File: /burr/examples/tracing-and-spans/__init__.py
---




---
File: /burr/examples/tracing-and-spans/application.py
---

from typing import Optional, Tuple

import openai

from burr.core import Application, ApplicationBuilder, State, default, when
from burr.core.action import action
from burr.core.graph import GraphBuilder
from burr.visibility import TracerFactory

MODES = {
    "answer_question": "text",
    "generate_image": "image",
    "generate_code": "code",
    "unknown": "text",
}


@action(reads=[], writes=["chat_history", "prompt"])
def process_prompt(state: State, prompt: str, __tracer: TracerFactory) -> Tuple[dict, State]:
    with __tracer("process_prompt") as tracer:
        result = {"chat_item": {"role": "user", "content": prompt, "type": "text"}}
        tracer.log_attributes(prompt=prompt)
    return result, state.wipe(keep=["prompt", "chat_history"]).append(
        chat_history=result["chat_item"]
    ).update(prompt=prompt)


@action(reads=["prompt"], writes=["safe"])
def check_safety(state: State, __tracer: TracerFactory) -> Tuple[dict, State]:
    with __tracer("check_safety"):
        result = {"safe": "unsafe" not in state["prompt"]}  # quick hack to demonstrate
    return result, state.update(safe=result["safe"])


def _get_openai_client():
    return openai.Client()


@action(reads=["prompt"], writes=["mode"])
def choose_mode(state: State, __tracer: TracerFactory) -> Tuple[dict, State]:
    with __tracer("generate_prompt"):
        prompt = (
            f"You are a chatbot. You've been prompted this: {state['prompt']}. "
            f"You have the capability of responding in the following modes: {', '.join(MODES)}. "
            "Please respond with *only* a single word representing the mode that most accurately "
            "corresponds to the prompt. Fr instance, if the prompt is 'draw a picture of a cat', "
            "the mode would be 'generate_image'. If the prompt is 'what is the capital of France', the mode would be 'answer_question'."
            "If none of these modes apply, please respond with 'unknown'."
        )
    with __tracer("query_openai", span_dependencies=["generate_prompt"]):
        with __tracer("create_openai_client"):
            client = _get_openai_client()
        with __tracer("query_openai") as tracer:
            result = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant"},
                    {"role": "user", "content": prompt},
                ],
            )
            tracer.log_attributes(
                response=result.choices[0].message.content,
                prompt_tokens=result.usage.prompt_tokens,
                total_tokens=result.usage.total_tokens,
                completion_tokens=result.usage.completion_tokens,
            )
    with __tracer("process_openai_response", span_dependencies=["query_openai"]):
        content = result.choices[0].message.content
        mode = content.lower()
        if mode not in MODES:
            mode = "unknown"
        result = {"mode": mode}
    return result, state.update(**result)


@action(reads=["prompt", "chat_history"], writes=["response"])
def prompt_for_more(state: State) -> Tuple[dict, State]:
    result = {
        "response": {
            "content": "None of the response modes I support apply to your question. Please clarify?",
            "type": "text",
            "role": "assistant",
        }
    }
    return result, state.update(**result)


@action(reads=["prompt", "chat_history", "mode"], writes=["response"])
def chat_response(
    state: State,
    prepend_prompt: str,
    __tracer: TracerFactory,
    model: str = "gpt-3.5-turbo",
) -> Tuple[dict, State]:
    __tracer.log_attributes(model=model, prepend_prompt=prepend_prompt)
    with __tracer("process_chat_history"):
        chat_history = state["chat_history"].copy()
        chat_history[-1]["content"] = f"{prepend_prompt}: {chat_history[-1]['content']}"
        chat_history_api_format = [
            {
                "role": chat["role"],
                "content": chat["content"],
            }
            for chat in chat_history
        ]
    with __tracer("query_openai", span_dependencies=["change_chat_history"]):
        with __tracer("create_openai_client"):
            client = _get_openai_client()
        with __tracer("query_openai", span_dependencies=["create_openai_client"]) as tracer:
            result = client.chat.completions.create(
                model=model,
                messages=chat_history_api_format,
            )
            tracer.log_attributes(
                response=result.choices[0].message.content,
                prompt_tokens=result.usage.prompt_tokens,
                total_tokens=result.usage.total_tokens,
                completion_tokens=result.usage.completion_tokens,
            )
    with __tracer("process_openai_response", span_dependencies=["query_openai"]):
        response = result.choices[0].message.content
        result = {
            "response": {"content": response, "type": MODES[state["mode"]], "role": "assistant"}
        }
    return result, state.update(**result)


@action(reads=["prompt", "chat_history", "mode"], writes=["response"])
def image_response(
    state: State, __tracer: TracerFactory, model: str = "dall-e-2"
) -> Tuple[dict, State]:
    __tracer.log_attributes(model=model)
    with __tracer("create_openai_client"):
        client = _get_openai_client()
    with __tracer("query_openai_image", span_dependencies=["create_openai_client"]):
        result = client.images.generate(
            model=model, prompt=state["prompt"], size="1024x1024", quality="standard", n=1
        )
        response = result.data[0].url
    with __tracer("process_openai_response", span_dependencies=["query_openai_image"]):
        result = {
            "response": {"content": response, "type": MODES[state["mode"]], "role": "assistant"}
        }
        __tracer.log_attributes(response=response)
    return result, state.update(**result)


@action(reads=["response", "safe", "mode"], writes=["chat_history"])
def response(state: State, __tracer: TracerFactory) -> Tuple[dict, State]:
    with __tracer("process_response"):
        if not state["safe"]:
            with __tracer("unsafe"):
                result = {
                    "chat_item": {
                        "role": "assistant",
                        "content": "I'm sorry, I can't respond to that.",
                        "type": "text",
                    }
                }
        else:
            with __tracer("safe"):
                result = {"chat_item": state["response"]}
    return result, state.append(chat_history=result["chat_item"])


graph = (
    GraphBuilder()
    .with_actions(
        prompt=process_prompt,
        check_safety=check_safety,
        decide_mode=choose_mode,
        generate_image=image_response,
        generate_code=chat_response.bind(
            prepend_prompt="Please respond with *only* code and no other text (at all) to the following:",
        ),
        answer_question=chat_response.bind(
            prepend_prompt="Please answer the following question:",
        ),
        prompt_for_more=prompt_for_more,
        response=response,
    )
    .with_transitions(
        ("prompt", "check_safety", default),
        ("check_safety", "decide_mode", when(safe=True)),
        ("check_safety", "response", default),
        ("decide_mode", "generate_image", when(mode="generate_image")),
        ("decide_mode", "generate_code", when(mode="generate_code")),
        ("decide_mode", "answer_question", when(mode="answer_question")),
        ("decide_mode", "prompt_for_more", default),
        (
            ["generate_image", "answer_question", "generate_code", "prompt_for_more"],
            "response",
        ),
        ("response", "prompt", default),
    )
    .build()
)


def application(
    app_id: Optional[str] = None,
    storage_dir: Optional[str] = "~/.burr",
) -> Application:
    return (
        ApplicationBuilder()
        .with_entrypoint("prompt")
        .with_state(chat_history=[])
        .with_graph(graph)
        .with_tracker(project="demo_tracing", params={"storage_dir": storage_dir})
        .with_identifiers(app_id=app_id)
        .build()
    )


if __name__ == "__main__":
    app = application()
    app.visualize(output_file_path="statemachine", include_conditions=True, view=True, format="png")
    app.run(halt_after=["response"], inputs={"prompt": "What is the capital of France?"})



---
File: /burr/examples/typed-state/__init__.py
---




---
File: /burr/examples/typed-state/application.py
---

from typing import AsyncGenerator, Generator, Optional, Tuple, Union

import instructor
import openai
from pydantic import BaseModel, Field
from pydantic.json_schema import SkipJsonSchema
from rich.console import Console
from youtube_transcript_api import YouTubeTranscriptApi

from burr.core import Application, ApplicationBuilder, action
from burr.core.action import (
    AsyncStreamingResultContainer,
    StreamingResultContainer,
    streaming_action,
)
from burr.integrations.pydantic import PydanticTypingSystem


class Concept(BaseModel):
    term: str = Field(description="A key term or concept mentioned.")
    definition: str = Field(description="A brief definition or explanation of the term.")
    timestamp: float = Field(description="Timestamp when the concept is explained.")


class SocialMediaPost(BaseModel):
    """A social media post about a YouTube video generated its transcript"""

    topic: str = Field(description="Main topic discussed.")
    hook: str = Field(
        description="Statement to grab the attention of the reader and announce the topic."
    )
    body: str = Field(
        description="The body of the social media post. It should be informative and make the reader curious about viewing the video."
    )
    concepts: list[Concept] = Field(
        description="Important concepts about Hamilton or Burr mentioned in this post -- please have at least 1",
        min_items=0,
        max_items=3,
        validate_default=False,
    )
    key_takeaways: list[str] = Field(
        description="A list of informative key takeways for the reader -- please have at least 1",
        min_items=0,
        max_items=4,
        validate_default=False,
    )
    youtube_url: SkipJsonSchema[Union[str, None]] = None


class ApplicationState(BaseModel):
    # Make these have defaults as they are only set in actions
    transcript: Optional[str] = Field(
        description="The full transcript of the YouTube video.", default=None
    )
    post: Optional[SocialMediaPost] = Field(
        description="The generated social media post.", default=None
    )


@action.pydantic(reads=[], writes=["transcript"])
def get_youtube_transcript(state: ApplicationState, youtube_url: str) -> ApplicationState:
    """Get the official YouTube transcript for a video given its URL"""
    _, _, video_id = youtube_url.partition("?v=")

    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=["en"])
    state.transcript = " ".join([f"ts={entry['start']} - {entry['text']}" for entry in transcript])
    return state


@action.pydantic(reads=["transcript"], writes=["post"])
def generate_post(state: ApplicationState, llm_client) -> ApplicationState:
    """Use the Instructor LLM client to generate `SocialMediaPost` from the YouTube transcript."""

    # read the transcript from state
    transcript = state.transcript

    response = llm_client.chat.completions.create(
        model="gpt-4o-mini",
        response_model=SocialMediaPost,
        messages=[
            {
                "role": "system",
                "content": "Analyze the given YouTube transcript and generate a compelling social media post.",
            },
            {"role": "user", "content": transcript},
        ],
    )
    state.post = response

    # store the chapters in state
    return state


@streaming_action.pydantic(
    reads=["transcript"],
    writes=["post"],
    state_input_type=ApplicationState,
    state_output_type=ApplicationState,
    stream_type=SocialMediaPost,
)
def generate_post_streaming(
    state: ApplicationState, llm_client
) -> Generator[Tuple[SocialMediaPost, Optional[ApplicationState]], None, None]:
    """Streams a post as it's getting created. This allows for interacting data on the UI side of partial
    results, using instructor's streaming capabilities for partial responses:
    https://python.useinstructor.com/concepts/partial/

    :param state: input state -- of the shape `ApplicationState`
    :param llm_client: the LLM client, we will bind this in the application
    :yield: a tuple of the post and the state -- state will be non-null when it's done
    """

    transcript = state.transcript
    response = llm_client.chat.completions.create_partial(
        model="gpt-4o-mini",
        response_model=SocialMediaPost,
        messages=[
            {
                "role": "system",
                "content": "Analyze the given YouTube transcript and generate a compelling social media post.",
            },
            {"role": "user", "content": transcript},
        ],
        stream=True,
    )
    final_post: SocialMediaPost = None  # type: ignore
    for post in response:
        final_post = post
        yield post, None

    yield final_post, state


@streaming_action.pydantic(
    reads=["transcript"],
    writes=["post"],
    state_input_type=ApplicationState,
    state_output_type=ApplicationState,
    stream_type=SocialMediaPost,
)
async def generate_post_streaming_async(
    state: ApplicationState, llm_client
) -> AsyncGenerator[Tuple[SocialMediaPost, Optional[ApplicationState]], None]:
    """Async implementation of the streaming action above"""

    transcript = state.transcript
    response = llm_client.chat.completions.create_partial(
        model="gpt-4o-mini",
        response_model=SocialMediaPost,
        messages=[
            {
                "role": "system",
                "content": "Analyze the given YouTube transcript and generate a compelling social media post.",
            },
            {"role": "user", "content": transcript},
        ],
        stream=True,
    )
    final_post = None
    async for post in response:
        final_post = post
        yield post, None

    yield final_post, state


def build_application() -> Application[ApplicationState]:
    """Builds the standard application (non-streaming)"""
    llm_client = instructor.from_openai(openai.OpenAI())
    app = (
        ApplicationBuilder()
        .with_actions(
            get_youtube_transcript,
            generate_post.bind(llm_client=llm_client),
        )
        .with_transitions(
            ("get_youtube_transcript", "generate_post"),
            ("generate_post", "get_youtube_transcript"),
        )
        .with_entrypoint("get_youtube_transcript")
        .with_typing(PydanticTypingSystem(ApplicationState))
        .with_state(ApplicationState())
        .with_tracker(project="youtube-post")
        .build()
    )
    return app


def build_streaming_application() -> Application[ApplicationState]:
    """Builds the streaming application -- this uses the generate_post_streaming action"""
    llm_client = instructor.from_openai(openai.OpenAI())
    app = (
        ApplicationBuilder()
        .with_actions(
            get_youtube_transcript,
            generate_post=generate_post_streaming.bind(llm_client=llm_client),
        )
        .with_transitions(
            ("get_youtube_transcript", "generate_post"),
            ("generate_post", "get_youtube_transcript"),
        )
        .with_entrypoint("get_youtube_transcript")
        .with_typing(PydanticTypingSystem(ApplicationState))
        .with_state(ApplicationState())
        .with_tracker(project="youtube-post")
        .build()
    )
    return app


def build_streaming_application_async() -> Application[ApplicationState]:
    """Builds the async streaming application -- uses the generate_post_streaming_async action"""
    llm_client = instructor.from_openai(openai.AsyncOpenAI())
    app = (
        ApplicationBuilder()
        .with_actions(
            get_youtube_transcript,
            generate_post=generate_post_streaming_async.bind(llm_client=llm_client),
        )
        .with_transitions(
            ("get_youtube_transcript", "generate_post"),
            ("generate_post", "get_youtube_transcript"),
        )
        .with_entrypoint("get_youtube_transcript")
        .with_typing(PydanticTypingSystem(ApplicationState))
        .with_state(ApplicationState())
        .with_tracker(project="test-youtube-post")
        .build()
    )
    return app


async def run_async():
    """quick function to run async -- this is not called in the mainline, see commented out code"""
    console = Console()
    app = build_streaming_application_async()

    _, streaming_container = await app.astream_result(
        halt_after=["generate_post"],
        inputs={"youtube_url": "https://www.youtube.com/watch?v=hqutVJyd3TI"},
    )  # type: ignore
    streaming_container: AsyncStreamingResultContainer[ApplicationState, SocialMediaPost]

    async for post in streaming_container:
        obj = post.model_dump()
        console.clear()
        console.print(obj)


# mainline -- runs streaming and prints to console
if __name__ == "__main__":
    # asyncio.run(run_async())
    console = Console()
    app = build_streaming_application()
    _, streaming_container = app.stream_result(
        halt_after=["generate_post"],
        inputs={"youtube_url": "https://www.youtube.com/watch?v=hqutVJyd3TI"},
    )  # type: ignore
    streaming_container: StreamingResultContainer[ApplicationState, SocialMediaPost]
    for post in streaming_container:
        obj = post.model_dump()
        console.clear()
        console.print(obj)



---
File: /burr/examples/typed-state/server.py
---

import contextlib
import json
import logging

import fastapi
import uvicorn
from application import (
    ApplicationState,
    SocialMediaPost,
    build_application,
    build_streaming_application,
    build_streaming_application_async,
)
from fastapi.responses import StreamingResponse

from burr.core import Application
from burr.core.action import AsyncStreamingResultContainer, StreamingResultContainer

logger = logging.getLogger(__name__)

# define a global `burr_app` variable
burr_app: Application[ApplicationState] = None
# This does streaming, in sync mode
burr_app_streaming: Application[ApplicationState] = None
# And this does streaming, in async mode
burr_app_streaming_async: Application[ApplicationState] = None

DEFAULT_YOUTUBE_URL = "https://www.youtube.com/watch?v=hqutVJyd3TI"


@contextlib.asynccontextmanager
async def lifespan(app: fastapi.FastAPI):
    """Instantiate the Burr applications on FastAPI startup."""
    global burr_app, burr_app_streaming, burr_app_streaming_async
    burr_app = build_application()
    burr_app_streaming = build_streaming_application()
    burr_app_streaming_async = build_streaming_application_async()
    yield


app = fastapi.FastAPI(lifespan=lifespan)


@app.get("/social_media_post", response_model=SocialMediaPost)
def social_media_post(youtube_url: str = DEFAULT_YOUTUBE_URL) -> SocialMediaPost:
    """Basic, synchronous single-step API.
    This just returns the social media post, no streaming response.


    :param youtube_url: youtube URL for the transcript, defaults to DEFAULT_YOUTUBE_URL
    :return: the social media post
    """
    # Note that state is of type State[ApplicationState]
    # This means that it has a data field of type ApplicationState
    # which means that our IDE will happily auto-complete for us
    # and that we can get the pydantic model
    _, _, state = burr_app.run(halt_after=["generate_post"], inputs={"youtube_url": youtube_url})
    return state.data.post


@app.get("/social_media_post_streaming", response_class=StreamingResponse)
def social_media_post_streaming(youtube_url: str = DEFAULT_YOUTUBE_URL) -> StreamingResponse:
    """Creates a completion for the chat message"""

    def gen():
        _, streaming_container = burr_app_streaming.stream_result(
            halt_after=["generate_post"],
            inputs={"youtube_url": youtube_url},
        )  # type: ignore
        # We annotate this so we can get the right types cascaded through
        streaming_container: StreamingResultContainer[ApplicationState, SocialMediaPost]
        # Every post is of type SocialMediaPost, and the IDE (if you're using PyLance or an equivalent) should know
        for post in streaming_container:
            obj = post.model_dump()
            yield json.dumps(obj)
        # if we called streaming_container.get(), we would get two objects --
        # This state will have a field data of type ApplicationState, which we can use if we want

        # post, state = streaming_container.get()
        # state.data.transcript # valid + auto-completion in the IDE

    # return a final streaming result -- it'll just have strings
    # for certain SSE frameworks you may want to delimit with data:
    return StreamingResponse(gen())


@app.get("/social_media_post_streaming_async", response_class=StreamingResponse)
async def social_media_post_streaming_async(
    youtube_url: str = DEFAULT_YOUTUBE_URL,
) -> StreamingResponse:
    """Creates a completion for the chat message"""

    async def gen():
        _, streaming_container = await burr_app_streaming_async.astream_result(
            halt_after=["generate_post"],
            inputs={"youtube_url": youtube_url},
        )  # type: ignore
        # We annotate this so we can get the right types cascaded through
        streaming_container: AsyncStreamingResultContainer[ApplicationState, SocialMediaPost]
        # Every post is of type SocialMediaPost, and the IDE (if you're using PyLance or an equivalent) should know
        async for post in streaming_container:
            obj = post.model_dump()
            yield json.dumps(obj)

    return StreamingResponse(gen())


if __name__ == "__main__":
    uvicorn.run("server:app", host="127.0.0.1", port=7443, reload=True)



---
File: /burr/examples/youtube-to-social-media-post/__init__.py
---




---
File: /burr/examples/youtube-to-social-media-post/application.py
---

import textwrap
from typing import Union

import instructor
import openai
from pydantic import BaseModel, Field
from pydantic.json_schema import SkipJsonSchema
from youtube_transcript_api import YouTubeTranscriptApi

from burr.core import Application, ApplicationBuilder, State, action
from burr.core.persistence import SQLLitePersister


class Concept(BaseModel):
    term: str = Field(description="A key term or concept mentioned.")
    definition: str = Field(description="A brief definition or explanation of the term.")
    timestamp: float = Field(description="Timestamp when the concept is explained.")

    def display(self):
        minutes, seconds = divmod(self.timestamp, 60)
        return f"{int(minutes)}:{int(seconds)} - {self.term}: {self.definition}"


class SocialMediaPost(BaseModel):
    """A social media post about a YouTube video generated its transcript"""

    topic: str = Field(description="Main topic discussed.")
    hook: str = Field(
        description="Statement to grab the attention of the reader and announce the topic."
    )
    body: str = Field(
        description="The body of the social media post. It should be informative and make the reader curious about viewing the video."
    )
    concepts: list[Concept] = Field(
        description="Important concepts about Hamilton or Burr mentioned in this post.",
        min_items=1,
        max_items=3,
    )
    key_takeaways: list[str] = Field(
        description="A list of informative key takeways for the reader.",
        min_items=1,
        max_items=4,
    )
    youtube_url: SkipJsonSchema[Union[str, None]] = None

    def display(self) -> str:
        formatted_takeways = " ".join([t for t in self.key_takeaways])
        formatted_concepts = "CONCEPTS\n" + "\n".join([c.display() for c in self.concepts])
        link = f"link: {self.youtube_url}\n\n" if self.youtube_url else ""

        return (
            textwrap.dedent(
                f"""\
            TOPIC: {self.topic}

            {self.hook}

            {self.body}

            {formatted_takeways}

            """
            )
            + link
            + formatted_concepts
        )


@action(reads=[], writes=["transcript"])
def get_youtube_transcript(state: State, youtube_url: str) -> State:
    """Get the official YouTube transcript for a video given it's URL"""
    _, _, video_id = youtube_url.partition("?v=")

    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=["en"])
    full_transcript = " ".join([f"ts={entry['start']} - {entry['text']}" for entry in transcript])

    # store the transcript in state
    return state.update(transcript=full_transcript)


@action(reads=["transcript"], writes=["post"])
def generate_post(state: State, llm_client) -> State:
    """Use the Instructor LLM client to generate `SocialMediaPost` from the YouTube transcript."""

    # read the transcript from state
    transcript = state["transcript"]

    response = llm_client.chat.completions.create(
        model="gpt-4o-mini",
        response_model=SocialMediaPost,
        messages=[
            {
                "role": "system",
                "content": "Analyze the given YouTube transcript and generate a compelling social media post.",
            },
            {"role": "user", "content": transcript},
        ],
    )

    # store the chapters in state
    return state.update(post=response)


@action(reads=["post"], writes=["post"])
def rewrite_post(state: State, llm_client, user_prompt: str):
    post = state["post"]

    response = llm_client.chat.completions.create(
        model="gpt-4o-mini",
        response_model=SocialMediaPost,
        messages=[
            {
                "role": "system",
                "content": f"Take the previously generated social media post and modify it according to the following instructions: {user_prompt}",
            },
            {"role": "user", "content": post.model_dump_json()},
        ],
    )

    # pass the youtube_url from the previous post version
    response.youtube_url = post.youtube_url

    return state.update(post=response)


def build_application() -> Application:
    llm_client = instructor.from_openai(openai.OpenAI())
    return (
        ApplicationBuilder()
        .with_actions(
            get_youtube_transcript,
            generate_post.bind(llm_client=llm_client),
            rewrite_post.bind(llm_client=llm_client),
        )
        .with_transitions(
            ("get_youtube_transcript", "generate_post"),
            ("generate_post", "rewrite_post"),
            ("rewrite_post", "rewrite_post"),
        )
        .with_state_persister(SQLLitePersister(db_path=".burr.db", table_name="state"))
        .with_entrypoint("get_youtube_transcript")
        .with_tracker(project="youtube-post")
        .build()
    )


if __name__ == "__main__":
    app = build_application()
    app.visualize(output_file_path="statemachine.png")

    _, _, state = app.run(
        halt_after=["generate_post"],
        inputs={"youtube_url": "https://www.youtube.com/watch?v=hqutVJyd3TI"},
    )



---
File: /burr/examples/youtube-to-social-media-post/server.py
---

import contextlib
import logging
from typing import Optional

import application
import fastapi
import uvicorn

from burr.core import Application

logger = logging.getLogger(__name__)

# define a global `burr_app` variable
burr_app: Optional[Application] = None


def get_burr_app() -> Application:
    """Retrieve the global Burr app."""
    if burr_app is None:
        raise RuntimeError("Burr app wasn't instantiated.")
    return burr_app


@contextlib.asynccontextmanager
async def lifespan(app: fastapi.FastAPI):
    """Instantiate the Burr application on FastAPI startup."""
    # set value for the global `burr_app` variable
    global burr_app
    burr_app = application.build_application()
    yield


app = fastapi.FastAPI(lifespan=lifespan)


@app.get("/social_media_post")
def social_media_post(youtube_url: str, burr_app: Application = fastapi.Depends(get_burr_app)):
    """Creates a completion for the chat message"""
    _, _, state = burr_app.run(halt_after=["generate_post"], inputs={"youtube_url": youtube_url})

    post = state["post"]
    return {"formatted_post": post.display(), "post": post.model_dump()}


if __name__ == "__main__":
    uvicorn.run("server:app", host="127.0.0.1", port=7443, reload=True)



---
File: /burr/examples/__init__.py
---




---
File: /burr/examples/validate_examples.py
---

import os

# hardcoded to work in current directory for now, but we can easily change
EXAMPLES_DIR = "."

# Required files for each example
REQUIRED_FILES = [
    "notebook.ipynb",
    "application.py",
    "README.md",
    "statemachine.png",
    "__init__.py",  # included as we want them all to be importable
]

FILTERLIST = [
    "openai-compatible-agent",
    "other-examples",
    "conversational-rag",
    # todo -- remove the following once we've fleshed them out
    "ml-training",
    "simulation",
    "multi-agent-collaboration",
    "web-server",
    "integrations",
    "templates",
    "deployment",
    "talks",
    "parallelism",  # TODO - remove this shortly
    "pytest",
]


def should_validate(directory: str) -> bool:
    """Return True if the given directory is an example directory."""
    return (
        os.path.isdir(os.path.join(EXAMPLES_DIR, directory))
        and not directory.startswith(".")
        and not directory.startswith("_")
        and directory not in FILTERLIST
    )


def get_directories(base_path: str) -> list[str]:
    """Return a list of directories under the given base_path."""
    return [d for d in os.listdir(base_path) if should_validate(d)]


def check_files_exist(directory, files):
    """Check if each file in 'files' exists in 'directory'."""
    missing_files = []
    for file in files:
        if not os.path.exists(os.path.join(EXAMPLES_DIR, directory, file)):
            missing_files.append(file)
    return missing_files


# Use pytest_generate_tests to dynamically parameterize the fixture
def pytest_generate_tests(metafunc):
    # if "directory" in metafunc.fixturenames:
    directories = get_directories(EXAMPLES_DIR)
    metafunc.parametrize("directory", directories, scope="module")


def test_directory_name(directory):
    if "_" in directory:
        assert (
            False
        ), f"Example Directory '{directory}' should contain dashes, not underscores! It should be: {os.path.basename(directory).replace('_', '-')}."


def test_directory_contents(directory):
    missing_files = check_files_exist(directory, REQUIRED_FILES)
    assert (
        not missing_files
    ), f"Missing files in example dir '{directory}': {', '.join(missing_files)}"



---
File: /burr/telemetry/ui/public/index.html
---

<!doctype html>
<html lang="en" class="h-full bg-white">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta name="description" content="Web site created using create-react-app" />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
    <title>Burr</title>
  </head>
  <body class="h-full">
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
  </body>
</html>



---
File: /burr/telemetry/ui/scripts/token_costs.py
---

url = "https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json"

# Load the data
import requests
import json

response = requests.get(url)
data = response.json()

keys_wanted = ["input_cost_per_token", "output_cost_per_token", "max_tokens", "max_input_tokens", "max_output_tokens"]

del data["sample_spec"]
# Extract the keys we want
for model_entry in data:
    for key in list(data[model_entry].keys()):
        if key not in keys_wanted:
            del data[model_entry][key]

# save the data
with open("model_costs.json", "w") as f:
    json.dump(data, f)



---
File: /burr/telemetry/ui/src/api/core/ApiError.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { ApiRequestOptions } from './ApiRequestOptions';
import type { ApiResult } from './ApiResult';

export class ApiError extends Error {
  public readonly url: string;
  public readonly status: number;
  public readonly statusText: string;
  public readonly body: any;
  public readonly request: ApiRequestOptions;

  constructor(request: ApiRequestOptions, response: ApiResult, message: string) {
    super(message);

    this.name = 'ApiError';
    this.url = response.url;
    this.status = response.status;
    this.statusText = response.statusText;
    this.body = response.body;
    this.request = request;
  }
}



---
File: /burr/telemetry/ui/src/api/core/ApiRequestOptions.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
export type ApiRequestOptions = {
  readonly method: 'GET' | 'PUT' | 'POST' | 'DELETE' | 'OPTIONS' | 'HEAD' | 'PATCH';
  readonly url: string;
  readonly path?: Record<string, any>;
  readonly cookies?: Record<string, any>;
  readonly headers?: Record<string, any>;
  readonly query?: Record<string, any>;
  readonly formData?: Record<string, any>;
  readonly body?: any;
  readonly mediaType?: string;
  readonly responseHeader?: string;
  readonly errors?: Record<number, string>;
};



---
File: /burr/telemetry/ui/src/api/core/ApiResult.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
export type ApiResult = {
  readonly url: string;
  readonly ok: boolean;
  readonly status: number;
  readonly statusText: string;
  readonly body: any;
};



---
File: /burr/telemetry/ui/src/api/core/CancelablePromise.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
export class CancelError extends Error {
  constructor(message: string) {
    super(message);
    this.name = 'CancelError';
  }

  public get isCancelled(): boolean {
    return true;
  }
}

export interface OnCancel {
  readonly isResolved: boolean;
  readonly isRejected: boolean;
  readonly isCancelled: boolean;

  (cancelHandler: () => void): void;
}

export class CancelablePromise<T> implements Promise<T> {
  #isResolved: boolean;
  #isRejected: boolean;
  #isCancelled: boolean;
  readonly #cancelHandlers: (() => void)[];
  readonly #promise: Promise<T>;
  #resolve?: (value: T | PromiseLike<T>) => void;
  #reject?: (reason?: any) => void;

  constructor(
    executor: (
      resolve: (value: T | PromiseLike<T>) => void,
      reject: (reason?: any) => void,
      onCancel: OnCancel
    ) => void
  ) {
    this.#isResolved = false;
    this.#isRejected = false;
    this.#isCancelled = false;
    this.#cancelHandlers = [];
    this.#promise = new Promise<T>((resolve, reject) => {
      this.#resolve = resolve;
      this.#reject = reject;

      const onResolve = (value: T | PromiseLike<T>): void => {
        if (this.#isResolved || this.#isRejected || this.#isCancelled) {
          return;
        }
        this.#isResolved = true;
        if (this.#resolve) this.#resolve(value);
      };

      const onReject = (reason?: any): void => {
        if (this.#isResolved || this.#isRejected || this.#isCancelled) {
          return;
        }
        this.#isRejected = true;
        if (this.#reject) this.#reject(reason);
      };

      const onCancel = (cancelHandler: () => void): void => {
        if (this.#isResolved || this.#isRejected || this.#isCancelled) {
          return;
        }
        this.#cancelHandlers.push(cancelHandler);
      };

      Object.defineProperty(onCancel, 'isResolved', {
        get: (): boolean => this.#isResolved
      });

      Object.defineProperty(onCancel, 'isRejected', {
        get: (): boolean => this.#isRejected
      });

      Object.defineProperty(onCancel, 'isCancelled', {
        get: (): boolean => this.#isCancelled
      });

      return executor(onResolve, onReject, onCancel as OnCancel);
    });
  }

  get [Symbol.toStringTag]() {
    return 'Cancellable Promise';
  }

  public then<TResult1 = T, TResult2 = never>(
    onFulfilled?: ((value: T) => TResult1 | PromiseLike<TResult1>) | null,
    onRejected?: ((reason: any) => TResult2 | PromiseLike<TResult2>) | null
  ): Promise<TResult1 | TResult2> {
    return this.#promise.then(onFulfilled, onRejected);
  }

  public catch<TResult = never>(
    onRejected?: ((reason: any) => TResult | PromiseLike<TResult>) | null
  ): Promise<T | TResult> {
    return this.#promise.catch(onRejected);
  }

  public finally(onFinally?: (() => void) | null): Promise<T> {
    return this.#promise.finally(onFinally);
  }

  public cancel(): void {
    if (this.#isResolved || this.#isRejected || this.#isCancelled) {
      return;
    }
    this.#isCancelled = true;
    if (this.#cancelHandlers.length) {
      try {
        for (const cancelHandler of this.#cancelHandlers) {
          cancelHandler();
        }
      } catch (error) {
        console.warn('Cancellation threw an error', error);
        return;
      }
    }
    this.#cancelHandlers.length = 0;
    if (this.#reject) this.#reject(new CancelError('Request aborted'));
  }

  public get isCancelled(): boolean {
    return this.#isCancelled;
  }
}



---
File: /burr/telemetry/ui/src/api/core/OpenAPI.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { ApiRequestOptions } from './ApiRequestOptions';

type Resolver<T> = (options: ApiRequestOptions) => Promise<T>;
type Headers = Record<string, string>;

export type OpenAPIConfig = {
  BASE: string;
  VERSION: string;
  WITH_CREDENTIALS: boolean;
  CREDENTIALS: 'include' | 'omit' | 'same-origin';
  TOKEN?: string | Resolver<string> | undefined;
  USERNAME?: string | Resolver<string> | undefined;
  PASSWORD?: string | Resolver<string> | undefined;
  HEADERS?: Headers | Resolver<Headers> | undefined;
  ENCODE_PATH?: ((path: string) => string) | undefined;
};

export const OpenAPI: OpenAPIConfig = {
  BASE: '',
  VERSION: '0.1.0',
  WITH_CREDENTIALS: false,
  CREDENTIALS: 'include',
  TOKEN: undefined,
  USERNAME: undefined,
  PASSWORD: undefined,
  HEADERS: undefined,
  ENCODE_PATH: undefined
};



---
File: /burr/telemetry/ui/src/api/core/request.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import { ApiError } from './ApiError';
import type { ApiRequestOptions } from './ApiRequestOptions';
import type { ApiResult } from './ApiResult';
import { CancelablePromise } from './CancelablePromise';
import type { OnCancel } from './CancelablePromise';
import type { OpenAPIConfig } from './OpenAPI';

export const isDefined = <T>(
  value: T | null | undefined
): value is Exclude<T, null | undefined> => {
  return value !== undefined && value !== null;
};

export const isString = (value: any): value is string => {
  return typeof value === 'string';
};

export const isStringWithValue = (value: any): value is string => {
  return isString(value) && value !== '';
};

export const isBlob = (value: any): value is Blob => {
  return (
    typeof value === 'object' &&
    typeof value.type === 'string' &&
    typeof value.stream === 'function' &&
    typeof value.arrayBuffer === 'function' &&
    typeof value.constructor === 'function' &&
    typeof value.constructor.name === 'string' &&
    /^(Blob|File)$/.test(value.constructor.name) &&
    /^(Blob|File)$/.test(value[Symbol.toStringTag])
  );
};

export const isFormData = (value: any): value is FormData => {
  return value instanceof FormData;
};

export const base64 = (str: string): string => {
  try {
    return btoa(str);
  } catch (err) {
    // @ts-ignore
    return Buffer.from(str).toString('base64');
  }
};

export const getQueryString = (params: Record<string, any>): string => {
  const qs: string[] = [];

  const append = (key: string, value: any) => {
    qs.push(`${encodeURIComponent(key)}=${encodeURIComponent(String(value))}`);
  };

  const process = (key: string, value: any) => {
    if (isDefined(value)) {
      if (Array.isArray(value)) {
        value.forEach((v) => {
          process(key, v);
        });
      } else if (typeof value === 'object') {
        Object.entries(value).forEach(([k, v]) => {
          process(`${key}[${k}]`, v);
        });
      } else {
        append(key, value);
      }
    }
  };

  Object.entries(params).forEach(([key, value]) => {
    process(key, value);
  });

  if (qs.length > 0) {
    return `?${qs.join('&')}`;
  }

  return '';
};

const getUrl = (config: OpenAPIConfig, options: ApiRequestOptions): string => {
  const encoder = config.ENCODE_PATH || encodeURI;

  const path = options.url
    .replace('{api-version}', config.VERSION)
    .replace(/{(.*?)}/g, (substring: string, group: string) => {
      if (options.path?.hasOwnProperty(group)) {
        return encoder(String(options.path[group]));
      }
      return substring;
    });

  const url = `${config.BASE}${path}`;
  if (options.query) {
    return `${url}${getQueryString(options.query)}`;
  }
  return url;
};

export const getFormData = (options: ApiRequestOptions): FormData | undefined => {
  if (options.formData) {
    const formData = new FormData();

    const process = (key: string, value: any) => {
      if (isString(value) || isBlob(value)) {
        formData.append(key, value);
      } else {
        formData.append(key, JSON.stringify(value));
      }
    };

    Object.entries(options.formData)
      .filter(([_, value]) => isDefined(value))
      .forEach(([key, value]) => {
        if (Array.isArray(value)) {
          value.forEach((v) => process(key, v));
        } else {
          process(key, value);
        }
      });

    return formData;
  }
  return undefined;
};

type Resolver<T> = (options: ApiRequestOptions) => Promise<T>;

export const resolve = async <T>(
  options: ApiRequestOptions,
  resolver?: T | Resolver<T>
): Promise<T | undefined> => {
  if (typeof resolver === 'function') {
    return (resolver as Resolver<T>)(options);
  }
  return resolver;
};

export const getHeaders = async (
  config: OpenAPIConfig,
  options: ApiRequestOptions
): Promise<Headers> => {
  const [token, username, password, additionalHeaders] = await Promise.all([
    resolve(options, config.TOKEN),
    resolve(options, config.USERNAME),
    resolve(options, config.PASSWORD),
    resolve(options, config.HEADERS)
  ]);

  const headers = Object.entries({
    Accept: 'application/json',
    ...additionalHeaders,
    ...options.headers
  })
    .filter(([_, value]) => isDefined(value))
    .reduce(
      (headers, [key, value]) => ({
        ...headers,
        [key]: String(value)
      }),
      {} as Record<string, string>
    );

  if (isStringWithValue(token)) {
    headers['Authorization'] = `Bearer ${token}`;
  }

  if (isStringWithValue(username) && isStringWithValue(password)) {
    const credentials = base64(`${username}:${password}`);
    headers['Authorization'] = `Basic ${credentials}`;
  }

  if (options.body) {
    if (options.mediaType) {
      headers['Content-Type'] = options.mediaType;
    } else if (isBlob(options.body)) {
      headers['Content-Type'] = options.body.type || 'application/octet-stream';
    } else if (isString(options.body)) {
      headers['Content-Type'] = 'text/plain';
    } else if (!isFormData(options.body)) {
      headers['Content-Type'] = 'application/json';
    }
  }

  return new Headers(headers);
};

export const getRequestBody = (options: ApiRequestOptions): any => {
  if (options.body !== undefined) {
    if (options.mediaType?.includes('/json')) {
      return JSON.stringify(options.body);
    } else if (isString(options.body) || isBlob(options.body) || isFormData(options.body)) {
      return options.body;
    } else {
      return JSON.stringify(options.body);
    }
  }
  return undefined;
};

export const sendRequest = async (
  config: OpenAPIConfig,
  options: ApiRequestOptions,
  url: string,
  body: any,
  formData: FormData | undefined,
  headers: Headers,
  onCancel: OnCancel
): Promise<Response> => {
  const controller = new AbortController();

  const request: RequestInit = {
    headers,
    body: body ?? formData,
    method: options.method,
    signal: controller.signal
  };

  if (config.WITH_CREDENTIALS) {
    request.credentials = config.CREDENTIALS;
  }

  onCancel(() => controller.abort());

  return await fetch(url, request);
};

export const getResponseHeader = (
  response: Response,
  responseHeader?: string
): string | undefined => {
  if (responseHeader) {
    const content = response.headers.get(responseHeader);
    if (isString(content)) {
      return content;
    }
  }
  return undefined;
};

export const getResponseBody = async (response: Response): Promise<any> => {
  if (response.status !== 204) {
    try {
      const contentType = response.headers.get('Content-Type');
      if (contentType) {
        const jsonTypes = ['application/json', 'application/problem+json'];
        const isJSON = jsonTypes.some((type) => contentType.toLowerCase().startsWith(type));
        if (isJSON) {
          return await response.json();
        } else {
          return await response.text();
        }
      }
    } catch (error) {
      console.error(error);
    }
  }
  return undefined;
};

export const catchErrorCodes = (options: ApiRequestOptions, result: ApiResult): void => {
  const errors: Record<number, string> = {
    400: 'Bad Request',
    401: 'Unauthorized',
    403: 'Forbidden',
    404: 'Not Found',
    500: 'Internal Server Error',
    502: 'Bad Gateway',
    503: 'Service Unavailable',
    ...options.errors
  };

  const error = errors[result.status];
  if (error) {
    throw new ApiError(options, result, error);
  }

  if (!result.ok) {
    const errorStatus = result.status ?? 'unknown';
    const errorStatusText = result.statusText ?? 'unknown';
    const errorBody = (() => {
      try {
        return JSON.stringify(result.body, null, 2);
      } catch (e) {
        return undefined;
      }
    })();

    throw new ApiError(
      options,
      result,
      `Generic Error: status: ${errorStatus}; status text: ${errorStatusText}; body: ${errorBody}`
    );
  }
};

/**
 * Request method
 * @param config The OpenAPI configuration object
 * @param options The request options from the service
 * @returns CancelablePromise<T>
 * @throws ApiError
 */
export const request = <T>(
  config: OpenAPIConfig,
  options: ApiRequestOptions
): CancelablePromise<T> => {
  return new CancelablePromise(async (resolve, reject, onCancel) => {
    try {
      const url = getUrl(config, options);
      const formData = getFormData(options);
      const body = getRequestBody(options);
      const headers = await getHeaders(config, options);

      if (!onCancel.isCancelled) {
        const response = await sendRequest(config, options, url, body, formData, headers, onCancel);
        const responseBody = await getResponseBody(response);
        const responseHeader = getResponseHeader(response, options.responseHeader);

        const result: ApiResult = {
          url,
          ok: response.ok,
          status: response.status,
          statusText: response.statusText,
          body: responseHeader ?? responseBody
        };

        catchErrorCodes(options, result);

        resolve(result.body);
      }
    } catch (error) {
      reject(error);
    }
  });
};



---
File: /burr/telemetry/ui/src/api/models/ActionModel.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Pydantic model that represents an action for storing/visualization in the UI
 */
export type ActionModel = {
  type?: string;
  name: string;
  reads: Array<string>;
  writes: Array<string>;
  code: string;
  inputs?: Array<string>;
  optional_inputs?: Array<string>;
};



---
File: /burr/telemetry/ui/src/api/models/AnnotationCreate.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { AnnotationObservation } from './AnnotationObservation';
/**
 * Generic link for indexing job -- can be exposed in 'admin mode' in the UI
 */
export type AnnotationCreate = {
  span_id: string | null;
  step_name: string;
  tags: Array<string>;
  observations: Array<AnnotationObservation>;
};



---
File: /burr/telemetry/ui/src/api/models/AnnotationDataPointer.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
export type AnnotationDataPointer = {
  type: AnnotationDataPointer.type;
  field_name: string;
  span_id: string | null;
};
export namespace AnnotationDataPointer {
  export enum type {
    STATE_FIELD = 'state_field',
    ATTRIBUTE = 'attribute'
  }
}



---
File: /burr/telemetry/ui/src/api/models/AnnotationObservation.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { AnnotationDataPointer } from './AnnotationDataPointer';
export type AnnotationObservation = {
  data_fields: Record<string, any>;
  thumbs_up_thumbs_down: boolean | null;
  data_pointers: Array<AnnotationDataPointer>;
};



---
File: /burr/telemetry/ui/src/api/models/AnnotationOut.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { AnnotationObservation } from './AnnotationObservation';
/**
 * Generic link for indexing job -- can be exposed in 'admin mode' in the UI
 */
export type AnnotationOut = {
  span_id: string | null;
  step_name: string;
  tags: Array<string>;
  observations: Array<AnnotationObservation>;
  id: number;
  project_id: string;
  app_id: string;
  partition_key: string | null;
  step_sequence_id: number;
  created: string;
  updated: string;
};



---
File: /burr/telemetry/ui/src/api/models/AnnotationUpdate.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { AnnotationObservation } from './AnnotationObservation';
/**
 * Generic link for indexing job -- can be exposed in 'admin mode' in the UI
 */
export type AnnotationUpdate = {
  span_id?: string | null;
  step_name: string;
  tags?: Array<string> | null;
  observations: Array<AnnotationObservation>;
};



---
File: /burr/telemetry/ui/src/api/models/ApplicationLogs.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { ApplicationModel } from './ApplicationModel';
import type { ChildApplicationModel } from './ChildApplicationModel';
import type { PointerModel } from './PointerModel';
import type { Step } from './Step';
/**
 * Application logs are purely flat --
 * we will likely be rethinking this but for now this provides for easy parsing.
 */
export type ApplicationLogs = {
  application: ApplicationModel;
  children: Array<ChildApplicationModel>;
  steps: Array<Step>;
  parent_pointer?: PointerModel | null;
  spawning_parent_pointer?: PointerModel | null;
};



---
File: /burr/telemetry/ui/src/api/models/ApplicationModel.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { ActionModel } from './ActionModel';
import type { TransitionModel } from './TransitionModel';
/**
 * Pydantic model that represents an application for storing/visualization in the UI
 */
export type ApplicationModel = {
  type?: string;
  entrypoint: string;
  actions: Array<ActionModel>;
  transitions: Array<TransitionModel>;
};



---
File: /burr/telemetry/ui/src/api/models/ApplicationPage.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { ApplicationSummary } from './ApplicationSummary';
export type ApplicationPage = {
  applications: Array<ApplicationSummary>;
  total: number;
  has_another_page: boolean;
};



---
File: /burr/telemetry/ui/src/api/models/ApplicationSummary.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { PointerModel } from './PointerModel';
export type ApplicationSummary = {
  app_id: string;
  partition_key: string | null;
  first_written: string;
  last_written: string;
  num_steps: number;
  tags: Record<string, string>;
  parent_pointer?: PointerModel | null;
  spawning_parent_pointer?: PointerModel | null;
};



---
File: /burr/telemetry/ui/src/api/models/AttributeModel.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Represents a logged artifact
 */
export type AttributeModel = {
  type?: string;
  key: string;
  action_sequence_id: number;
  span_id: string | null;
  value: Record<string, any> | string | number | boolean | null;
  tags: Record<string, string>;
  time_logged?: string | null;
};



---
File: /burr/telemetry/ui/src/api/models/BackendSpec.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Generic link for indexing job -- can be exposed in 'admin mode' in the UI
 */
export type BackendSpec = {
  indexing: boolean;
  snapshotting: boolean;
  supports_demos: boolean;
  supports_annotations: boolean;
};



---
File: /burr/telemetry/ui/src/api/models/BeginEntryModel.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Pydantic model that represents an entry for the beginning of a step
 */
export type BeginEntryModel = {
  type?: string;
  start_time: string;
  action: string;
  inputs: Record<string, any>;
  sequence_id: number;
};



---
File: /burr/telemetry/ui/src/api/models/BeginSpanModel.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Pydantic model that represents an entry for the beginning of a span
 */
export type BeginSpanModel = {
  type?: string;
  start_time: string;
  action_sequence_id: number;
  span_id: string;
  span_name: string;
  parent_span_id: string | null;
  span_dependencies: Array<string>;
};



---
File: /burr/telemetry/ui/src/api/models/ChatItem.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Pydantic model for a chat item. This is used to render the chat history.
 */
export type ChatItem = {
  content: string;
  type: ChatItem.type;
  role: ChatItem.role;
};
export namespace ChatItem {
  export enum type {
    IMAGE = 'image',
    TEXT = 'text',
    CODE = 'code',
    ERROR = 'error'
  }
  export enum role {
    USER = 'user',
    ASSISTANT = 'assistant'
  }
}



---
File: /burr/telemetry/ui/src/api/models/ChildApplicationModel.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { PointerModel } from './PointerModel';
/**
 * Stores data about a child application (either a fork or a spawned application).
 * This allows us to link from parent -> child in the UI.
 */
export type ChildApplicationModel = {
  type?: string;
  child: PointerModel;
  event_time: string;
  event_type: ChildApplicationModel.event_type;
  sequence_id: number | null;
};
export namespace ChildApplicationModel {
  export enum event_type {
    FORK = 'fork',
    SPAWN_START = 'spawn_start'
  }
}



---
File: /burr/telemetry/ui/src/api/models/DraftInit.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
export type DraftInit = {
  email_to_respond: string;
  response_instructions: string;
};



---
File: /burr/telemetry/ui/src/api/models/EmailAssistantState.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
export type EmailAssistantState = {
  app_id: string;
  email_to_respond: string | null;
  response_instructions: string | null;
  questions: Array<string> | null;
  answers: Array<string> | null;
  drafts: Array<string>;
  feedback_history: Array<string>;
  final_draft: string | null;
  next_step: EmailAssistantState.next_step;
};
export namespace EmailAssistantState {
  export enum next_step {
    PROCESS_INPUT = 'process_input',
    CLARIFY_INSTRUCTIONS = 'clarify_instructions',
    PROCESS_FEEDBACK = 'process_feedback'
  }
}



---
File: /burr/telemetry/ui/src/api/models/EndEntryModel.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Pydantic model that represents an entry for the end of a step
 */
export type EndEntryModel = {
  type?: string;
  end_time: string;
  action: string;
  result: Record<string, any> | null;
  exception: string | null;
  state: Record<string, any>;
  sequence_id: number;
};



---
File: /burr/telemetry/ui/src/api/models/EndSpanModel.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Pydantic model that represents an entry for the end of a span
 */
export type EndSpanModel = {
  type?: string;
  end_time: string;
  action_sequence_id: number;
  span_id: string;
};



---
File: /burr/telemetry/ui/src/api/models/EndStreamModel.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Pydantic model that represents an entry for the first item of a stream
 */
export type EndStreamModel = {
  type?: string;
  action_sequence_id: number;
  span_id: string | null;
  end_time: string;
  items_streamed: number;
};



---
File: /burr/telemetry/ui/src/api/models/Feedback.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
export type Feedback = {
  feedback: string;
};



---
File: /burr/telemetry/ui/src/api/models/FirstItemStreamModel.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Pydantic model that represents an entry for the first item of a stream
 */
export type FirstItemStreamModel = {
  type?: string;
  action_sequence_id: number;
  span_id: string | null;
  first_item_time: string;
};



---
File: /burr/telemetry/ui/src/api/models/HTTPValidationError.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { ValidationError } from './ValidationError';
export type HTTPValidationError = {
  detail?: Array<ValidationError>;
};



---
File: /burr/telemetry/ui/src/api/models/IndexingJob.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Generic link for indexing job -- can be exposed in 'admin mode' in the UI
 */
export type IndexingJob = {
  id: number;
  start_time: string;
  end_time: string | null;
  status: string;
  records_processed: number;
  metadata: Record<string, any>;
};



---
File: /burr/telemetry/ui/src/api/models/InitializeStreamModel.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Pydantic model that represents an entry for the beginning of a stream
 */
export type InitializeStreamModel = {
  type?: string;
  action_sequence_id: number;
  span_id: string | null;
  stream_init_time: string;
};



---
File: /burr/telemetry/ui/src/api/models/PointerModel.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Stores pointers to unique identifiers for an application.
 * This is used by a few different places to, say, store parent references
 * bewteen application instances.
 */
export type PointerModel = {
  type?: string;
  app_id: string;
  sequence_id: number | null;
  partition_key: string | null;
};



---
File: /burr/telemetry/ui/src/api/models/Project.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
export type Project = {
  name: string;
  id: string;
  last_written: string;
  created: string;
  num_apps: number;
  uri: string;
};



---
File: /burr/telemetry/ui/src/api/models/PromptInput.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
export type PromptInput = {
  prompt: string;
};



---
File: /burr/telemetry/ui/src/api/models/QuestionAnswers.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
export type QuestionAnswers = {
  answers: Array<string>;
};



---
File: /burr/telemetry/ui/src/api/models/ResearchSummary.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
export type ResearchSummary = {
  running_summary: string;
};



---
File: /burr/telemetry/ui/src/api/models/Span.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { BeginSpanModel } from './BeginSpanModel';
import type { EndSpanModel } from './EndSpanModel';
/**
 * Represents a span. These have action sequence IDs associated with
 * them to put them in order.
 */
export type Span = {
  begin_entry: BeginSpanModel;
  end_entry: EndSpanModel | null;
};



---
File: /burr/telemetry/ui/src/api/models/Step.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { AttributeModel } from './AttributeModel';
import type { BeginEntryModel } from './BeginEntryModel';
import type { EndEntryModel } from './EndEntryModel';
import type { EndStreamModel } from './EndStreamModel';
import type { FirstItemStreamModel } from './FirstItemStreamModel';
import type { InitializeStreamModel } from './InitializeStreamModel';
import type { Span } from './Span';
/**
 * Log of  astep -- has a start and an end.
 */
export type Step = {
  step_start_log: BeginEntryModel;
  step_end_log: EndEntryModel | null;
  spans: Array<Span>;
  attributes: Array<AttributeModel>;
  streaming_events: Array<InitializeStreamModel | FirstItemStreamModel | EndStreamModel>;
};



---
File: /burr/telemetry/ui/src/api/models/TransitionModel.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
/**
 * Pydantic model that represents a transition for storing/visualization in the UI
 */
export type TransitionModel = {
  type?: string;
  from_: string;
  to: string;
  condition: string;
};



---
File: /burr/telemetry/ui/src/api/models/ValidationError.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
export type ValidationError = {
  loc: Array<string | number>;
  msg: string;
  type: string;
};



---
File: /burr/telemetry/ui/src/api/services/DefaultService.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
import type { AnnotationCreate } from '../models/AnnotationCreate';
import type { AnnotationOut } from '../models/AnnotationOut';
import type { AnnotationUpdate } from '../models/AnnotationUpdate';
import type { ApplicationLogs } from '../models/ApplicationLogs';
import type { ApplicationPage } from '../models/ApplicationPage';
import type { BackendSpec } from '../models/BackendSpec';
import type { ChatItem } from '../models/ChatItem';
import type { DraftInit } from '../models/DraftInit';
import type { EmailAssistantState } from '../models/EmailAssistantState';
import type { Feedback } from '../models/Feedback';
import type { IndexingJob } from '../models/IndexingJob';
import type { Project } from '../models/Project';
import type { PromptInput } from '../models/PromptInput';
import type { QuestionAnswers } from '../models/QuestionAnswers';
import type { ResearchSummary } from '../models/ResearchSummary';
import type { CancelablePromise } from '../core/CancelablePromise';
import { OpenAPI } from '../core/OpenAPI';
import { request as __request } from '../core/request';
export class DefaultService {
  /**
   * Is Ready
   * @returns any Successful Response
   * @throws ApiError
   */
  public static isReadyReadyGet(): CancelablePromise<any> {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/ready'
    });
  }
  /**
   * Get App Spec
   * @returns BackendSpec Successful Response
   * @throws ApiError
   */
  public static getAppSpecApiV0MetadataAppSpecGet(): CancelablePromise<BackendSpec> {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/api/v0/metadata/app_spec'
    });
  }
  /**
   * Get Projects
   * Gets all projects visible by the user.
   *
   * :param request: FastAPI request
   * :return:  a list of projects visible by the user
   * @returns Project Successful Response
   * @throws ApiError
   */
  public static getProjectsApiV0ProjectsGet(): CancelablePromise<Array<Project>> {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/api/v0/projects'
    });
  }
  /**
   * Get Apps
   * Gets all apps visible by the user
   *
   * :param request: FastAPI request
   * :param project_id: project name
   * :return: a list of projects visible by the user
   * @param projectId
   * @param partitionKey
   * @param limit
   * @param offset
   * @returns ApplicationPage Successful Response
   * @throws ApiError
   */
  public static getAppsApiV0ProjectIdPartitionKeyAppsGet(
    projectId: string,
    partitionKey: string,
    limit: number = 100,
    offset?: number
  ): CancelablePromise<ApplicationPage> {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/api/v0/{project_id}/{partition_key}/apps',
      path: {
        project_id: projectId,
        partition_key: partitionKey
      },
      query: {
        limit: limit,
        offset: offset
      },
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Get Application Logs
   * Lists steps for a given App.
   * TODO: add streaming capabilities for bi-directional communication
   * TODO: add pagination for quicker loading
   *
   * :param request: FastAPI
   * :param project_id: ID of the project
   * :param app_id: ID of the assIndociated application
   * :return: A list of steps with all associated step data
   * @param projectId
   * @param appId
   * @param partitionKey
   * @returns ApplicationLogs Successful Response
   * @throws ApiError
   */
  public static getApplicationLogsApiV0ProjectIdAppIdPartitionKeyAppsGet(
    projectId: string,
    appId: string,
    partitionKey: string
  ): CancelablePromise<ApplicationLogs> {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/api/v0/{project_id}/{app_id}/{partition_key}/apps',
      path: {
        project_id: projectId,
        app_id: appId,
        partition_key: partitionKey
      },
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Create Annotation
   * @param projectId
   * @param appId
   * @param partitionKey
   * @param sequenceId
   * @param requestBody
   * @returns AnnotationOut Successful Response
   * @throws ApiError
   */
  public static createAnnotationApiV0ProjectIdAppIdPartitionKeySequenceIdAnnotationsPost(
    projectId: string,
    appId: string,
    partitionKey: string,
    sequenceId: number,
    requestBody: AnnotationCreate
  ): CancelablePromise<AnnotationOut> {
    return __request(OpenAPI, {
      method: 'POST',
      url: '/api/v0/{project_id}/{app_id}/{partition_key}/{sequence_id}/annotations',
      path: {
        project_id: projectId,
        app_id: appId,
        partition_key: partitionKey,
        sequence_id: sequenceId
      },
      body: requestBody,
      mediaType: 'application/json',
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Update Annotation
   * @param projectId
   * @param annotationId
   * @param requestBody
   * @returns AnnotationOut Successful Response
   * @throws ApiError
   */
  public static updateAnnotationApiV0ProjectIdAnnotationIdUpdateAnnotationsPut(
    projectId: string,
    annotationId: number,
    requestBody: AnnotationUpdate
  ): CancelablePromise<AnnotationOut> {
    return __request(OpenAPI, {
      method: 'PUT',
      url: '/api/v0/{project_id}/{annotation_id}/update_annotations',
      path: {
        project_id: projectId,
        annotation_id: annotationId
      },
      body: requestBody,
      mediaType: 'application/json',
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Get Annotations
   * @param projectId
   * @param appId
   * @param partitionKey
   * @param stepSequenceId
   * @returns AnnotationOut Successful Response
   * @throws ApiError
   */
  public static getAnnotationsApiV0ProjectIdAnnotationsGet(
    projectId: string,
    appId?: string | null,
    partitionKey?: string | null,
    stepSequenceId?: number | null
  ): CancelablePromise<Array<AnnotationOut>> {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/api/v0/{project_id}/annotations',
      path: {
        project_id: projectId
      },
      query: {
        app_id: appId,
        partition_key: partitionKey,
        step_sequence_id: stepSequenceId
      },
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Ready
   * @returns boolean Successful Response
   * @throws ApiError
   */
  public static readyApiV0ReadyGet(): CancelablePromise<boolean> {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/api/v0/ready'
    });
  }
  /**
   * Get Indexing Jobs
   * @param offset
   * @param limit
   * @param filterEmpty
   * @returns IndexingJob Successful Response
   * @throws ApiError
   */
  public static getIndexingJobsApiV0IndexingJobsGet(
    offset?: number,
    limit: number = 100,
    filterEmpty: boolean = true
  ): CancelablePromise<Array<IndexingJob>> {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/api/v0/indexing_jobs',
      query: {
        offset: offset,
        limit: limit,
        filter_empty: filterEmpty
      },
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Version
   * Returns the burr version
   * @returns any Successful Response
   * @throws ApiError
   */
  public static versionApiV0VersionGet(): CancelablePromise<Record<string, any>> {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/api/v0/version'
    });
  }
  /**
   * Chat Response
   * Chat response endpoint. User passes in a prompt and the system returns the
   * full chat history, so its easier to render.
   *
   * :param project_id: Project ID to run
   * :param app_id: Application ID to run
   * :param prompt: Prompt to send to the chatbot
   * :return:
   * @param projectId
   * @param appId
   * @param prompt
   * @returns ChatItem Successful Response
   * @throws ApiError
   */
  public static chatResponseApiV0ChatbotResponseProjectIdAppIdPost(
    projectId: string,
    appId: string,
    prompt: string
  ): CancelablePromise<Array<ChatItem>> {
    return __request(OpenAPI, {
      method: 'POST',
      url: '/api/v0/chatbot/response/{{project_id}}/{{app_id}}',
      query: {
        project_id: projectId,
        app_id: appId,
        prompt: prompt
      },
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Chat History
   * Endpoint to get chat history. Gets the application and returns the chat history from state.
   *
   * :param project_id: Project ID
   * :param app_id: App ID.
   * :return: The list of chat items in the state
   * @param projectId
   * @param appId
   * @returns ChatItem Successful Response
   * @throws ApiError
   */
  public static chatHistoryApiV0ChatbotResponseProjectIdAppIdGet(
    projectId: string,
    appId: string
  ): CancelablePromise<Array<ChatItem>> {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/api/v0/chatbot/response/{project_id}/{app_id}',
      path: {
        project_id: projectId,
        app_id: appId
      },
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Create New Application
   * Endpoint to create a new application -- used by the FE when
   * the user types in a new App ID
   *
   * :param project_id: Project ID
   * :param app_id: App ID
   * :return: The app ID
   * @param projectId
   * @param appId
   * @returns string Successful Response
   * @throws ApiError
   */
  public static createNewApplicationApiV0ChatbotCreateProjectIdAppIdPost(
    projectId: string,
    appId: string
  ): CancelablePromise<string> {
    return __request(OpenAPI, {
      method: 'POST',
      url: '/api/v0/chatbot/create/{project_id}/{app_id}',
      path: {
        project_id: projectId,
        app_id: appId
      },
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Create New Application
   * @param projectId
   * @param appId
   * @returns string Successful Response
   * @throws ApiError
   */
  public static createNewApplicationApiV0EmailAssistantCreateNewProjectIdAppIdPost(
    projectId: string,
    appId: string
  ): CancelablePromise<string> {
    return __request(OpenAPI, {
      method: 'POST',
      url: '/api/v0/email_assistant/create_new/{project_id}/{app_id}',
      path: {
        project_id: projectId,
        app_id: appId
      },
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Initialize Draft
   * Endpoint to initialize the draft with the email and instructions
   *
   * :param project_id: ID of the project (used by telemetry tracking/storage)
   * :param app_id: ID of the application (used to reference the app)
   * :param draft_data: Data to initialize the draft
   * :return: The state of the application after initialization
   * @param projectId
   * @param appId
   * @param requestBody
   * @returns EmailAssistantState Successful Response
   * @throws ApiError
   */
  public static initializeDraftApiV0EmailAssistantCreateProjectIdAppIdPost(
    projectId: string,
    appId: string,
    requestBody: DraftInit
  ): CancelablePromise<EmailAssistantState> {
    return __request(OpenAPI, {
      method: 'POST',
      url: '/api/v0/email_assistant/create/{project_id}/{app_id}',
      path: {
        project_id: projectId,
        app_id: appId
      },
      body: requestBody,
      mediaType: 'application/json',
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Answer Questions
   * Endpoint to answer questions the LLM provides
   *
   * :param project_id: ID of the project (used by telemetry tracking/storage)
   * :param app_id: ID of the application (used to reference the app)
   * :param question_answers: Answers to the questions
   * :return: The state of the application after answering the questions
   * @param projectId
   * @param appId
   * @param requestBody
   * @returns EmailAssistantState Successful Response
   * @throws ApiError
   */
  public static answerQuestionsApiV0EmailAssistantAnswerQuestionsProjectIdAppIdPost(
    projectId: string,
    appId: string,
    requestBody: QuestionAnswers
  ): CancelablePromise<EmailAssistantState> {
    return __request(OpenAPI, {
      method: 'POST',
      url: '/api/v0/email_assistant/answer_questions/{project_id}/{app_id}',
      path: {
        project_id: projectId,
        app_id: appId
      },
      body: requestBody,
      mediaType: 'application/json',
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Provide Feedback
   * Endpoint to provide feedback to the LLM
   *
   * :param project_id: ID of the project (used by telemetry tracking/storage)
   * :param app_id: ID of the application (used to reference the app)
   * :param feedback: Feedback to provide to the LLM
   * :return: The state of the application after providing feedback
   * @param projectId
   * @param appId
   * @param requestBody
   * @returns EmailAssistantState Successful Response
   * @throws ApiError
   */
  public static provideFeedbackApiV0EmailAssistantProvideFeedbackProjectIdAppIdPost(
    projectId: string,
    appId: string,
    requestBody: Feedback
  ): CancelablePromise<EmailAssistantState> {
    return __request(OpenAPI, {
      method: 'POST',
      url: '/api/v0/email_assistant/provide_feedback/{project_id}/{app_id}',
      path: {
        project_id: projectId,
        app_id: appId
      },
      body: requestBody,
      mediaType: 'application/json',
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Get State
   * Get the current state of the application
   *
   * :param project_id: ID of the project (used by telemetry tracking/storage)
   * :param app_id:  ID of the application (used to reference the app)
   * :return: The state of the application
   * @param projectId
   * @param appId
   * @returns EmailAssistantState Successful Response
   * @throws ApiError
   */
  public static getStateApiV0EmailAssistantStateProjectIdAppIdGet(
    projectId: string,
    appId: string
  ): CancelablePromise<EmailAssistantState> {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/api/v0/email_assistant/state/{project_id}/{app_id}',
      path: {
        project_id: projectId,
        app_id: appId
      },
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Validate Environment
   * Validate the environment
   * @returns any Successful Response
   * @throws ApiError
   */
  public static validateEnvironmentApiV0EmailAssistantValidateProjectIdAppIdGet(): CancelablePromise<
    string | null
  > {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/api/v0/email_assistant/validate/{project_id}/{app_id}'
    });
  }
  /**
   * Chat Response
   * Chat response endpoint. User passes in a prompt and the system returns the
   * full chat history, so its easier to render.
   *
   * :param project_id: Project ID to run
   * :param app_id: Application ID to run
   * :param prompt: Prompt to send to the chatbot
   * :return:
   * @param projectId
   * @param appId
   * @param requestBody
   * @returns any Successful Response
   * @throws ApiError
   */
  public static chatResponseApiV0StreamingChatbotResponseProjectIdAppIdPost(
    projectId: string,
    appId: string,
    requestBody: PromptInput
  ): CancelablePromise<any> {
    return __request(OpenAPI, {
      method: 'POST',
      url: '/api/v0/streaming_chatbot/response/{project_id}/{app_id}',
      path: {
        project_id: projectId,
        app_id: appId
      },
      body: requestBody,
      mediaType: 'application/json',
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Chat History
   * Endpoint to get chat history. Gets the application and returns the chat history from state.
   *
   * :param project_id: Project ID
   * :param app_id: App ID.
   * :return: The list of chat items in the state
   * @param projectId
   * @param appId
   * @returns ChatItem Successful Response
   * @throws ApiError
   */
  public static chatHistoryApiV0StreamingChatbotHistoryProjectIdAppIdGet(
    projectId: string,
    appId: string
  ): CancelablePromise<Array<ChatItem>> {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/api/v0/streaming_chatbot/history/{project_id}/{app_id}',
      path: {
        project_id: projectId,
        app_id: appId
      },
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Create New Application
   * Endpoint to create a new application -- used by the FE when
   * the user types in a new App ID
   *
   * :param project_id: Project ID
   * :param app_id: App ID
   * :return: The app ID
   * @param projectId
   * @param appId
   * @returns string Successful Response
   * @throws ApiError
   */
  public static createNewApplicationApiV0StreamingChatbotCreateProjectIdAppIdPost(
    projectId: string,
    appId: string
  ): CancelablePromise<string> {
    return __request(OpenAPI, {
      method: 'POST',
      url: '/api/v0/streaming_chatbot/create/{project_id}/{app_id}',
      path: {
        project_id: projectId,
        app_id: appId
      },
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Research Response
   * @param projectId
   * @param appId
   * @param topic
   * @returns ResearchSummary Successful Response
   * @throws ApiError
   */
  public static researchResponseApiV0DeepResearcherResponseProjectIdAppIdPost(
    projectId: string,
    appId: string,
    topic: string
  ): CancelablePromise<ResearchSummary> {
    return __request(OpenAPI, {
      method: 'POST',
      url: '/api/v0/deep_researcher/response/{project_id}/{app_id}',
      path: {
        project_id: projectId,
        app_id: appId
      },
      query: {
        topic: topic
      },
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Create New Application
   * @param projectId
   * @param appId
   * @returns string Successful Response
   * @throws ApiError
   */
  public static createNewApplicationApiV0DeepResearcherCreateProjectIdAppIdPost(
    projectId: string,
    appId: string
  ): CancelablePromise<string> {
    return __request(OpenAPI, {
      method: 'POST',
      url: '/api/v0/deep_researcher/create/{project_id}/{app_id}',
      path: {
        project_id: projectId,
        app_id: appId
      },
      errors: {
        422: `Validation Error`
      }
    });
  }
  /**
   * Validate Environment
   * Validate the environment
   * @returns any Successful Response
   * @throws ApiError
   */
  public static validateEnvironmentApiV0DeepResearcherValidateGet(): CancelablePromise<
    string | null
  > {
    return __request(OpenAPI, {
      method: 'GET',
      url: '/api/v0/deep_researcher/validate'
    });
  }
}



---
File: /burr/telemetry/ui/src/api/index.ts
---

/* generated using openapi-typescript-codegen -- do no edit */
/* istanbul ignore file */
/* tslint:disable */
/* eslint-disable */
export { ApiError } from './core/ApiError';
export { CancelablePromise, CancelError } from './core/CancelablePromise';
export { OpenAPI } from './core/OpenAPI';
export type { OpenAPIConfig } from './core/OpenAPI';

export type { ActionModel } from './models/ActionModel';
export type { AnnotationCreate } from './models/AnnotationCreate';
export { AnnotationDataPointer } from './models/AnnotationDataPointer';
export type { AnnotationObservation } from './models/AnnotationObservation';
export type { AnnotationOut } from './models/AnnotationOut';
export type { AnnotationUpdate } from './models/AnnotationUpdate';
export type { ApplicationLogs } from './models/ApplicationLogs';
export type { ApplicationModel } from './models/ApplicationModel';
export type { ApplicationPage } from './models/ApplicationPage';
export type { ApplicationSummary } from './models/ApplicationSummary';
export type { AttributeModel } from './models/AttributeModel';
export type { BackendSpec } from './models/BackendSpec';
export type { BeginEntryModel } from './models/BeginEntryModel';
export type { BeginSpanModel } from './models/BeginSpanModel';
export { ChatItem } from './models/ChatItem';
export { ChildApplicationModel } from './models/ChildApplicationModel';
export type { DraftInit } from './models/DraftInit';
export { EmailAssistantState } from './models/EmailAssistantState';
export type { EndEntryModel } from './models/EndEntryModel';
export type { EndSpanModel } from './models/EndSpanModel';
export type { EndStreamModel } from './models/EndStreamModel';
export type { Feedback } from './models/Feedback';
export type { FirstItemStreamModel } from './models/FirstItemStreamModel';
export type { HTTPValidationError } from './models/HTTPValidationError';
export type { IndexingJob } from './models/IndexingJob';
export type { InitializeStreamModel } from './models/InitializeStreamModel';
export type { PointerModel } from './models/PointerModel';
export type { Project } from './models/Project';
export type { PromptInput } from './models/PromptInput';
export type { QuestionAnswers } from './models/QuestionAnswers';
export type { ResearchSummary } from './models/ResearchSummary';
export type { Span } from './models/Span';
export type { Step } from './models/Step';
export type { TransitionModel } from './models/TransitionModel';
export type { ValidationError } from './models/ValidationError';

export { DefaultService } from './services/DefaultService';



---
File: /burr/telemetry/ui/src/components/common/button.tsx
---

/**
 * Tailwind catalyst component
 */
import {
  Button as HeadlessButton,
  type ButtonProps as HeadlessButtonProps
} from '@headlessui/react';
import { clsx } from 'clsx';
import React from 'react';
import { Link } from './link';

const styles = {
  base: [
    // Base
    'relative isolate inline-flex items-center justify-center gap-x-2 rounded-lg border text-base/6 font-semibold',

    // Sizing
    'px-[calc(theme(spacing[3.5])-1px)] py-[calc(theme(spacing[2.5])-1px)] sm:px-[calc(theme(spacing.3)-1px)] sm:py-[calc(theme(spacing[1.5])-1px)] sm:text-sm/6',

    // Focus
    'focus:outline-none data-[focus]:outline data-[focus]:outline-2 data-[focus]:outline-offset-2 data-[focus]:outline-blue-500',

    // Disabled
    'data-[disabled]:opacity-50',

    // Icon
    '[&>[data-slot=icon]]:-mx-0.5 [&>[data-slot=icon]]:my-0.5 [&>[data-slot=icon]]:size-5 [&>[data-slot=icon]]:shrink-0 [&>[data-slot=icon]]:text-[--btn-icon] [&>[data-slot=icon]]:sm:my-1 [&>[data-slot=icon]]:sm:size-4 forced-colors:[--btn-icon:ButtonText] forced-colors:data-[hover]:[--btn-icon:ButtonText]'
  ],
  solid: [
    // Optical border, implemented as the button background to avoid corner artifacts
    'border-transparent bg-[--btn-border]',

    // Dark mode: border is rendered on `after` so background is set to button background
    'dark:bg-[--btn-bg]',

    // Button background, implemented as foreground layer to stack on top of pseudo-border layer
    'before:absolute before:inset-0 before:-z-10 before:rounded-[calc(theme(borderRadius.lg)-1px)] before:bg-[--btn-bg]',

    // Drop shadow, applied to the inset `before` layer so it blends with the border
    'before:shadow',

    // Background color is moved to control and shadow is removed in dark mode so hide `before` pseudo
    'dark:before:hidden',

    // Dark mode: Subtle white outline is applied using a border
    'dark:border-white/5',

    // Shim/overlay, inset to match button foreground and used for hover state + highlight shadow
    'after:absolute after:inset-0 after:-z-10 after:rounded-[calc(theme(borderRadius.lg)-1px)]',

    // Inner highlight shadow
    'after:shadow-[shadow:inset_0_1px_theme(colors.white/15%)]',

    // White overlay on hover
    'after:data-[active]:bg-[--btn-hover-overlay] after:data-[hover]:bg-[--btn-hover-overlay]',

    // Dark mode: `after` layer expands to cover entire button
    'dark:after:-inset-px dark:after:rounded-lg',

    // Disabled
    'before:data-[disabled]:shadow-none after:data-[disabled]:shadow-none'
  ],
  outline: [
    // Base
    'border-zinc-950/10 text-zinc-950 data-[active]:bg-zinc-950/[2.5%] data-[hover]:bg-zinc-950/[2.5%]',

    // Dark mode
    'dark:border-white/15 dark:text-white dark:[--btn-bg:transparent] dark:data-[active]:bg-white/5 dark:data-[hover]:bg-white/5',

    // Icon
    '[--btn-icon:theme(colors.zinc.500)] data-[active]:[--btn-icon:theme(colors.zinc.700)] data-[hover]:[--btn-icon:theme(colors.zinc.700)] dark:data-[active]:[--btn-icon:theme(colors.zinc.400)] dark:data-[hover]:[--btn-icon:theme(colors.zinc.400)]'
  ],
  plain: [
    // Base
    'border-transparent text-zinc-950 data-[active]:bg-zinc-950/5 data-[hover]:bg-zinc-950/5',

    // Dark mode
    'dark:text-white dark:data-[active]:bg-white/10 dark:data-[hover]:bg-white/10',

    // Icon
    '[--btn-icon:theme(colors.zinc.500)] data-[active]:[--btn-icon:theme(colors.zinc.700)] data-[hover]:[--btn-icon:theme(colors.zinc.700)] dark:[--btn-icon:theme(colors.zinc.500)] dark:data-[active]:[--btn-icon:theme(colors.zinc.400)] dark:data-[hover]:[--btn-icon:theme(colors.zinc.400)]'
  ],
  colors: {
    'dark/zinc': [
      'text-white [--btn-bg:theme(colors.zinc.900)] [--btn-border:theme(colors.zinc.950/90%)] [--btn-hover-overlay:theme(colors.white/10%)]',
      'dark:text-white dark:[--btn-bg:theme(colors.zinc.600)] dark:[--btn-hover-overlay:theme(colors.white/5%)]',
      '[--btn-icon:theme(colors.zinc.400)] data-[active]:[--btn-icon:theme(colors.zinc.300)] data-[hover]:[--btn-icon:theme(colors.zinc.300)]'
    ],
    light: [
      'text-zinc-950 [--btn-bg:white] [--btn-border:theme(colors.zinc.950/10%)] [--btn-hover-overlay:theme(colors.zinc.950/2.5%)] data-[active]:[--btn-border:theme(colors.zinc.950/15%)] data-[hover]:[--btn-border:theme(colors.zinc.950/15%)]',
      'dark:text-white dark:[--btn-hover-overlay:theme(colors.white/5%)] dark:[--btn-bg:theme(colors.zinc.800)]',
      '[--btn-icon:theme(colors.zinc.500)] data-[active]:[--btn-icon:theme(colors.zinc.700)] data-[hover]:[--btn-icon:theme(colors.zinc.700)] dark:[--btn-icon:theme(colors.zinc.500)] dark:data-[active]:[--btn-icon:theme(colors.zinc.400)] dark:data-[hover]:[--btn-icon:theme(colors.zinc.400)]'
    ],
    'dark/white': [
      'text-white [--btn-bg:theme(colors.zinc.900)] [--btn-border:theme(colors.zinc.950/90%)] [--btn-hover-overlay:theme(colors.white/10%)]',
      'dark:text-zinc-950 dark:[--btn-bg:white] dark:[--btn-hover-overlay:theme(colors.zinc.950/5%)]',
      '[--btn-icon:theme(colors.zinc.400)] data-[active]:[--btn-icon:theme(colors.zinc.300)] data-[hover]:[--btn-icon:theme(colors.zinc.300)] dark:[--btn-icon:theme(colors.zinc.500)] dark:data-[active]:[--btn-icon:theme(colors.zinc.400)] dark:data-[hover]:[--btn-icon:theme(colors.zinc.400)]'
    ],
    dark: [
      'text-white [--btn-bg:theme(colors.zinc.900)] [--btn-border:theme(colors.zinc.950/90%)] [--btn-hover-overlay:theme(colors.white/10%)]',
      'dark:[--btn-hover-overlay:theme(colors.white/5%)] dark:[--btn-bg:theme(colors.zinc.800)]',
      '[--btn-icon:theme(colors.zinc.400)] data-[active]:[--btn-icon:theme(colors.zinc.300)] data-[hover]:[--btn-icon:theme(colors.zinc.300)]'
    ],
    white: [
      'text-zinc-950 [--btn-bg:white] [--btn-border:theme(colors.zinc.950/10%)] [--btn-hover-overlay:theme(colors.zinc.950/2.5%)] data-[active]:[--btn-border:theme(colors.zinc.950/15%)] data-[hover]:[--btn-border:theme(colors.zinc.950/15%)]',
      'dark:[--btn-hover-overlay:theme(colors.zinc.950/5%)]',
      '[--btn-icon:theme(colors.zinc.400)] data-[active]:[--btn-icon:theme(colors.zinc.500)] data-[hover]:[--btn-icon:theme(colors.zinc.500)]'
    ],
    zinc: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.zinc.600)] [--btn-border:theme(colors.zinc.700/90%)]',
      'dark:[--btn-hover-overlay:theme(colors.white/5%)]',
      '[--btn-icon:theme(colors.zinc.400)] data-[active]:[--btn-icon:theme(colors.zinc.300)] data-[hover]:[--btn-icon:theme(colors.zinc.300)]'
    ],
    indigo: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.indigo.500)] [--btn-border:theme(colors.indigo.600/90%)]',
      '[--btn-icon:theme(colors.indigo.300)] data-[active]:[--btn-icon:theme(colors.indigo.200)] data-[hover]:[--btn-icon:theme(colors.indigo.200)]'
    ],
    cyan: [
      'text-cyan-950 [--btn-bg:theme(colors.cyan.300)] [--btn-border:theme(colors.cyan.400/80%)] [--btn-hover-overlay:theme(colors.white/25%)]',
      '[--btn-icon:theme(colors.cyan.500)]'
    ],
    red: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.red.600)] [--btn-border:theme(colors.red.700/90%)]',
      '[--btn-icon:theme(colors.red.300)] data-[active]:[--btn-icon:theme(colors.red.200)] data-[hover]:[--btn-icon:theme(colors.red.200)]'
    ],
    orange: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.orange.500)] [--btn-border:theme(colors.orange.600/90%)]',
      '[--btn-icon:theme(colors.orange.300)] data-[active]:[--btn-icon:theme(colors.orange.200)] data-[hover]:[--btn-icon:theme(colors.orange.200)]'
    ],
    amber: [
      'text-amber-950 [--btn-hover-overlay:theme(colors.white/25%)] [--btn-bg:theme(colors.amber.400)] [--btn-border:theme(colors.amber.500/80%)]',
      '[--btn-icon:theme(colors.amber.600)]'
    ],
    yellow: [
      'text-yellow-950 [--btn-hover-overlay:theme(colors.white/25%)] [--btn-bg:theme(colors.yellow.300)] [--btn-border:theme(colors.yellow.400/80%)]',
      '[--btn-icon:theme(colors.yellow.600)] data-[active]:[--btn-icon:theme(colors.yellow.700)] data-[hover]:[--btn-icon:theme(colors.yellow.700)]'
    ],
    lime: [
      'text-lime-950 [--btn-hover-overlay:theme(colors.white/25%)] [--btn-bg:theme(colors.lime.300)] [--btn-border:theme(colors.lime.400/80%)]',
      '[--btn-icon:theme(colors.lime.600)] data-[active]:[--btn-icon:theme(colors.lime.700)] data-[hover]:[--btn-icon:theme(colors.lime.700)]'
    ],
    green: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.green.600)] [--btn-border:theme(colors.green.700/90%)]',
      '[--btn-icon:theme(colors.white/60%)] data-[active]:[--btn-icon:theme(colors.white/80%)] data-[hover]:[--btn-icon:theme(colors.white/80%)]'
    ],
    emerald: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.emerald.600)] [--btn-border:theme(colors.emerald.700/90%)]',
      '[--btn-icon:theme(colors.white/60%)] data-[active]:[--btn-icon:theme(colors.white/80%)] data-[hover]:[--btn-icon:theme(colors.white/80%)]'
    ],
    teal: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.teal.600)] [--btn-border:theme(colors.teal.700/90%)]',
      '[--btn-icon:theme(colors.white/60%)] data-[active]:[--btn-icon:theme(colors.white/80%)] data-[hover]:[--btn-icon:theme(colors.white/80%)]'
    ],
    sky: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.sky.500)] [--btn-border:theme(colors.sky.600/80%)]',
      '[--btn-icon:theme(colors.white/60%)] data-[active]:[--btn-icon:theme(colors.white/80%)] data-[hover]:[--btn-icon:theme(colors.white/80%)]'
    ],
    blue: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.blue.600)] [--btn-border:theme(colors.blue.700/90%)]',
      '[--btn-icon:theme(colors.blue.400)] data-[active]:[--btn-icon:theme(colors.blue.300)] data-[hover]:[--btn-icon:theme(colors.blue.300)]'
    ],
    violet: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.violet.500)] [--btn-border:theme(colors.violet.600/90%)]',
      '[--btn-icon:theme(colors.violet.300)] data-[active]:[--btn-icon:theme(colors.violet.200)] data-[hover]:[--btn-icon:theme(colors.violet.200)]'
    ],
    purple: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.purple.500)] [--btn-border:theme(colors.purple.600/90%)]',
      '[--btn-icon:theme(colors.purple.300)] data-[active]:[--btn-icon:theme(colors.purple.200)] data-[hover]:[--btn-icon:theme(colors.purple.200)]'
    ],
    fuchsia: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.fuchsia.500)] [--btn-border:theme(colors.fuchsia.600/90%)]',
      '[--btn-icon:theme(colors.fuchsia.300)] data-[active]:[--btn-icon:theme(colors.fuchsia.200)] data-[hover]:[--btn-icon:theme(colors.fuchsia.200)]'
    ],
    pink: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.pink.500)] [--btn-border:theme(colors.pink.600/90%)]',
      '[--btn-icon:theme(colors.pink.300)] data-[active]:[--btn-icon:theme(colors.pink.200)] data-[hover]:[--btn-icon:theme(colors.pink.200)]'
    ],
    rose: [
      'text-white [--btn-hover-overlay:theme(colors.white/10%)] [--btn-bg:theme(colors.rose.500)] [--btn-border:theme(colors.rose.600/90%)]',
      '[--btn-icon:theme(colors.rose.300)] data-[active]:[--btn-icon:theme(colors.rose.200)] data-[hover]:[--btn-icon:theme(colors.rose.200)]'
    ]
  }
};

type ButtonProps = (
  | { color?: keyof typeof styles.colors; outline?: never; plain?: never }
  | { color?: never; outline: true; plain?: never }
  | { color?: never; outline?: never; plain: true }
) & { children: React.ReactNode } & (
    | HeadlessButtonProps
    | React.ComponentPropsWithoutRef<typeof Link>
  );

export const Button = React.forwardRef(function Button(
  { color, outline, plain, className, children, ...props }: ButtonProps,
  ref: React.ForwardedRef<HTMLElement>
) {
  const classes = clsx(
    className,
    styles.base,
    outline
      ? styles.outline
      : plain
        ? styles.plain
        : clsx(styles.solid, styles.colors[color ?? 'dark/zinc'])
  );

  return 'href' in props ? (
    <Link {...props} className={classes} ref={ref as React.ForwardedRef<HTMLAnchorElement>}>
      <TouchTarget>{children}</TouchTarget>
    </Link>
  ) : (
    <HeadlessButton {...props} className={clsx(classes, 'cursor-default')} ref={ref}>
      <TouchTarget>{children}</TouchTarget>
    </HeadlessButton>
  );
});

/* Expand the hit area to at least 44Ã—44px on touch devices */
export function TouchTarget({ children }: { children: React.ReactNode }) {
  return (
    <>
      {children}
      <span
        className="absolute left-1/2 top-1/2 size-[max(100%,2.75rem)] -translate-x-1/2 -translate-y-1/2 [@media(pointer:fine)]:hidden"
        aria-hidden="true"
      />
    </>
  );
}



---
File: /burr/telemetry/ui/src/components/common/chip.tsx
---

import { ReactNode } from 'react';

const chipColorMap = {
  stateRead: 'bg-dwdarkblue',
  stateWrite: 'bg-dwred',
  input: 'bg-yellow-500',
  success: 'bg-green-500',
  failure: 'bg-dwred',
  running: 'bg-dwlightblue',
  demo: 'bg-yellow-400',
  test: 'bg-gray-800',
  fork: 'bg-dwdarkblue/80',
  spawn: 'bg-purple-600',
  span: 'bg-yellow-500/80',
  attribute: 'bg-teal-700',
  state: 'bg-dwlightblue',
  error: 'bg-dwred',
  action: 'bg-dwlightblue/90',
  stream: 'bg-dwlightblue/90',
  first_item_stream: 'bg-pink-400',
  end_stream: 'bg-pink-400',
  llm: 'bg-gray-400/50',
  metric: 'bg-gray-400/50',
  tag: 'bg-gray-700/50',
  annotateDataPointerAttribute: 'bg-teal-700', // same as attribute
  annotateDataPointerState: 'bg-dwlightblue'
};

export type ChipType = keyof typeof chipColorMap;

export const Chip = (props: {
  label: string;
  chipType: ChipType;
  className?: string;
  onClick?: (e: React.MouseEvent) => void;
}) => {
  // Function to generate a hash code from a string
  const stringToHash = (str: string) => {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      hash = str.charCodeAt(i) + ((hash << 5) - hash);
      hash |= 0; // Convert to 32bit integer
    }
    return hash;
  };

  // Function to generate a color from a hash
  const hashToColor = (hash: number) => {
    // Generate RGB values between 64 and 192 for more vibrant colors
    const r = 64 + (Math.abs(hash) % 128);
    const g = 64 + (Math.abs(hash >> 8) % 128);
    const b = 64 + (Math.abs(hash >> 16) % 128);

    return `rgb(${r}, ${g}, ${b})`;
  };

  // Generate color if chipType is 'tag'
  const bgStyle = props.chipType === 'tag' ? hashToColor(stringToHash(props.label)) : undefined;
  const bgColor = props.chipType === 'tag' ? '' : chipColorMap[props.chipType];
  const clickable = props.onClick !== undefined;

  return (
    <div
      className={`relative grid select-none items-center whitespace-nowrap rounded-lg
        p-1 px-3 font-sans text-xs font-semibold text-white ${bgColor} ${clickable ? 'cursor-pointer hover:underline' : ''} ${props.className ? props.className : ''}`}
      style={{ backgroundColor: bgStyle }}
      onClick={props.onClick}
    >
      <span>{props.label}</span>
    </div>
  );
};

/**
 * A group of chips -- displays in a row with a "label" (just a react component)
 */
export const ChipGroup = (props: { chips: string[]; type: ChipType; label?: ReactNode }) => {
  return (
    <div className="flex gap-2">
      {props.label ? props.label : <></>}
      {props.chips.map((chip, i) => (
        <Chip key={i} label={chip} chipType={props.type} />
      ))}
    </div>
  );
};



---
File: /burr/telemetry/ui/src/components/common/dates.tsx
---

/**
 * Displays a date in a human-readable format
 */
export const DateDisplay: React.FC<{ date: string }> = ({ date }) => {
  const displayDate = new Date(date).toLocaleDateString('en-US', {
    day: 'numeric',
    month: 'long',
    year: 'numeric'
  });

  return <span className="whitespace-nowrap text-sm text-gray-500">{displayDate}</span>;
};

/**
 * Displays a datetime in a human-readable format
 */
export const DateTimeDisplay: React.FC<{
  date: string;
  mode: 'short' | 'long';
  displayMillis?: boolean;
}> = (props) => {
  const displayDateTime = new Date(props.date).toLocaleString('en-US', {
    day: 'numeric',
    month: props.mode === 'short' ? 'numeric' : 'long',
    year: 'numeric',
    hour: 'numeric',
    minute: '2-digit',
    second: 'numeric',
    fractionalSecondDigits: props.displayMillis ? 3 : undefined,
    hour12: true // Use AM/PM format. Set to false for 24-hour format.
  });

  return <span className="whitespace-nowrap text-sm text-gray-500">{displayDateTime}</span>;
};

export const TimeDisplay: React.FC<{ date: string }> = ({ date }) => {
  const displayTime = new Date(date).toLocaleTimeString('en-US', {
    hour: 'numeric',
    minute: '2-digit',
    second: 'numeric',
    hour12: true // Use AM/PM format. Set to false for 24-hour format.
  });

  return <span className="whitespace-nowrap text-sm text-gray-500">{displayTime}</span>;
};
const formatDuration = (duration: number) => {
  const msInSecond = 1_000;
  const msInMinute = 60_000;
  const msInHour = 3_600_000;
  const msInDay = 86_400_000;

  if (duration < msInSecond) {
    return `${duration} ms`;
  }

  const days = Math.floor(duration / msInDay);
  duration %= msInDay;

  const hours = Math.floor(duration / msInHour);
  duration %= msInHour;

  const minutes = Math.floor(duration / msInMinute);
  duration %= msInMinute;

  const seconds = duration / msInSecond;

  const ms = duration % msInSecond;

  if (days > 0) {
    return `${days} d ${hours} h`;
  }

  if (hours > 0) {
    return `${hours} h ${minutes} m`;
  }

  if (minutes > 0) {
    return `${minutes} m ${seconds.toFixed(3)} s`;
  }

  if (seconds > 0) {
    return `${seconds} s`;
  }

  return `${ms} s`;
};

/**
 * Displays a duration for use in a table
 */
export const DurationDisplay: React.FC<{
  startDate: string | number;
  endDate: string | number;
  clsNames?: string;
}> = (props) => {
  const duration = new Date(props.endDate).getTime() - new Date(props.startDate).getTime();
  const formattedDuration = formatDuration(duration);

  return <span className={`whitespace-nowrap text-sm ${props.clsNames}`}>{formattedDuration}</span>;
};



---
File: /burr/telemetry/ui/src/components/common/drawer.tsx
---

import {
  Dialog,
  DialogBackdrop,
  DialogPanel,
  DialogTitle,
  TransitionChild
} from '@headlessui/react';
import { XMarkIcon } from '@heroicons/react/24/outline';

export const Drawer = (props: {
  open: boolean;
  close: () => void;
  children: React.ReactNode;
  title: string;
}) => {
  const { open, close } = props;
  return (
    <Dialog open={open} onClose={close} className="relative z-50">
      <DialogBackdrop
        transition
        className="fixed inset-0 bg-gray-500 bg-opacity-75 transition-opacity duration-500 ease-in-out data-[closed]:opacity-0"
      />

      <div className="fixed inset-0 overflow-hidden">
        <div className="absolute inset-0 overflow-hidden">
          <div className="pointer-events-none fixed inset-y-0 right-0 flex max-w-full pl-10">
            <DialogPanel
              transition
              className="pointer-events-auto relative w-screen max-w-5xl transform transition duration-500 ease-in-out data-[closed]:translate-x-full sm:duration-700"
            >
              <TransitionChild>
                <div className="absolute left-0 top-0 -ml-8 flex pr-2 pt-4 duration-500 ease-in-out data-[closed]:opacity-0 sm:-ml-10 sm:pr-4">
                  <button
                    type="button"
                    onClick={() => close()}
                    className="relative rounded-md text-gray-300 hover:text-white focus:outline-none focus:ring-2 focus:ring-white"
                  >
                    <span className="absolute -inset-2.5" />
                    <span className="sr-only">Close panel</span>
                    <XMarkIcon aria-hidden="true" className="h-6 w-6" />
                  </button>
                </div>
              </TransitionChild>
              <div className="flex h-full flex-col overflow-y-scroll bg-white py-6 shadow-xl">
                <div className="px-4 sm:px-10">
                  <DialogTitle className="text-base font-semibold leading-6 text-gray-900">
                    {props.title}
                  </DialogTitle>
                </div>
                <div className="relative mt-6 flex-1 px-4 sm:px-6">{props.children}</div>
              </div>
            </DialogPanel>
          </div>
        </div>
      </div>
    </Dialog>
  );
};



---
File: /burr/telemetry/ui/src/components/common/fieldset.tsx
---

/**
 * Tailwind catalyst component
 */

import {
  Description as HeadlessDescription,
  Field as HeadlessField,
  Fieldset as HeadlessFieldset,
  Label as HeadlessLabel,
  Legend as HeadlessLegend,
  type DescriptionProps as HeadlessDescriptionProps,
  type FieldProps as HeadlessFieldProps,
  type FieldsetProps as HeadlessFieldsetProps,
  type LabelProps as HeadlessLabelProps,
  type LegendProps as HeadlessLegendProps
} from '@headlessui/react';
import clsx from 'clsx';
import type React from 'react';

export function Fieldset({ className, ...props }: { disabled?: boolean } & HeadlessFieldsetProps) {
  return (
    <HeadlessFieldset
      {...props}
      className={clsx(className, '[&>*+[data-slot=control]]:mt-6 [&>[data-slot=text]]:mt-1')}
    />
  );
}

export function Legend({ ...props }: HeadlessLegendProps) {
  return (
    <HeadlessLegend
      {...props}
      data-slot="legend"
      className={clsx(
        props.className,
        'text-base/6 font-semibold text-zinc-950 data-[disabled]:opacity-50 sm:text-sm/6 dark:text-white'
      )}
    />
  );
}

export function FieldGroup({ className, ...props }: React.ComponentPropsWithoutRef<'div'>) {
  return <div {...props} data-slot="control" className={clsx(className, 'space-y-8')} />;
}

export function Field({ className, ...props }: HeadlessFieldProps) {
  return (
    <HeadlessField
      className={clsx(
        className,
        '[&>[data-slot=label]+[data-slot=control]]:mt-3',
        '[&>[data-slot=label]+[data-slot=description]]:mt-1',
        '[&>[data-slot=description]+[data-slot=control]]:mt-3',
        '[&>[data-slot=control]+[data-slot=description]]:mt-3',
        '[&>[data-slot=control]+[data-slot=error]]:mt-3',
        '[&>[data-slot=label]]:font-medium'
      )}
      {...props}
    />
  );
}

export function Label({ className, ...props }: { className?: string } & HeadlessLabelProps) {
  return (
    <HeadlessLabel
      {...props}
      data-slot="label"
      className={clsx(
        className,
        'select-none text-base/6 text-zinc-950 data-[disabled]:opacity-50 sm:text-sm/6 dark:text-white'
      )}
    />
  );
}

export function Description({
  className,
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  disabled,
  ...props
}: { className?: string; disabled?: boolean } & HeadlessDescriptionProps) {
  return (
    <HeadlessDescription
      {...props}
      data-slot="description"
      className={clsx(
        className,
        'text-base/6 text-zinc-500 data-[disabled]:opacity-50 sm:text-sm/6 dark:text-zinc-400'
      )}
    />
  );
}

export function ErrorMessage({
  className,
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  disabled,
  ...props
}: { className?: string; disabled?: boolean } & HeadlessDescriptionProps) {
  return (
    <HeadlessDescription
      {...props}
      data-slot="error"
      className={clsx(
        className,
        'text-base/6 text-red-600 data-[disabled]:opacity-50 sm:text-sm/6 dark:text-red-500'
      )}
    />
  );
}



---
File: /burr/telemetry/ui/src/components/common/href.tsx
---

/**
 * Simple component to display text as a link
 * Meant to be used consistently
 */
export const LinkText = (props: { href: string; text: string }) => {
  return (
    <a
      href={props.href}
      className="text-dwlightblue hover:underline"
      onClick={(e) => {
        // Quick trick to ensure that this takes priority and if this has a parent href, it doesn't trigger
        e.stopPropagation();
      }}
    >
      {props.text}
    </a>
  );
};



---
File: /burr/telemetry/ui/src/components/common/input.tsx
---

import { Input as HeadlessInput, type InputProps as HeadlessInputProps } from '@headlessui/react';
import { clsx } from 'clsx';
import { forwardRef } from 'react';

const dateTypes = ['date', 'datetime-local', 'month', 'time', 'week'];
type DateType = (typeof dateTypes)[number];

export const Input = forwardRef<
  HTMLInputElement,
  {
    type?: 'email' | 'number' | 'password' | 'search' | 'tel' | 'text' | 'url' | DateType;
  } & HeadlessInputProps
>(function Input({ className, ...props }, ref) {
  return (
    <span
      data-slot="control"
      className={clsx([
        className,

        // Basic layout
        'relative block w-full',

        // Background color + shadow applied to inset pseudo element, so shadow blends with border in light mode
        'before:absolute before:inset-px before:rounded-[calc(theme(borderRadius.lg)-1px)] before:bg-white before:shadow',

        // Background color is moved to control and shadow is removed in dark mode so hide `before` pseudo
        'dark:before:hidden',

        // Focus ring
        'after:pointer-events-none after:absolute after:inset-0 after:rounded-lg after:ring-inset after:ring-transparent sm:after:focus-within:ring-2 sm:after:focus-within:ring-blue-500',

        // Disabled state
        'has-[[data-disabled]]:opacity-50 before:has-[[data-disabled]]:bg-zinc-950/5 before:has-[[data-disabled]]:shadow-none',

        // Invalid state
        'before:has-[[data-invalid]]:shadow-red-500/10'
      ])}
    >
      <HeadlessInput
        ref={ref}
        className={clsx([
          // Date classes
          props.type &&
            dateTypes.includes(props.type) && [
              '[&::-webkit-datetime-edit-fields-wrapper]:p-0',
              '[&::-webkit-date-and-time-value]:min-h-[1.5em]',
              '[&::-webkit-datetime-edit]:inline-flex',
              '[&::-webkit-datetime-edit]:p-0',
              '[&::-webkit-datetime-edit-year-field]:p-0',
              '[&::-webkit-datetime-edit-month-field]:p-0',
              '[&::-webkit-datetime-edit-day-field]:p-0',
              '[&::-webkit-datetime-edit-hour-field]:p-0',
              '[&::-webkit-datetime-edit-minute-field]:p-0',
              '[&::-webkit-datetime-edit-second-field]:p-0',
              '[&::-webkit-datetime-edit-millisecond-field]:p-0',
              '[&::-webkit-datetime-edit-meridiem-field]:p-0'
            ],

          // Basic layout
          'relative block w-full appearance-none rounded-lg px-[calc(theme(spacing[3.5])-1px)] py-[calc(theme(spacing[2.5])-1px)] sm:px-[calc(theme(spacing[3])-1px)] sm:py-[calc(theme(spacing[1.5])-1px)]',

          // Typography
          'text-base/6 text-zinc-950 placeholder:text-zinc-500 sm:text-sm/6 dark:text-white',

          // Border
          'border border-zinc-950/10 data-[hover]:border-zinc-950/20 dark:border-white/10 dark:data-[hover]:border-white/20',

          // Background color
          'bg-transparent dark:bg-white/5',

          // Hide default focus styles
          'focus:outline-none',

          // Invalid state
          'data-[invalid]:border-red-500 data-[invalid]:data-[hover]:border-red-500 data-[invalid]:dark:border-red-500 data-[invalid]:data-[hover]:dark:border-red-500',

          // Disabled state
          'data-[disabled]:border-zinc-950/20 dark:data-[hover]:data-[disabled]:border-white/15 data-[disabled]:dark:border-white/15 data-[disabled]:dark:bg-white/[2.5%]'
        ])}
        {...props}
      />
    </span>
  );
});



---
File: /burr/telemetry/ui/src/components/common/layout.tsx
---

import React, { ReactNode, useEffect } from 'react';

type TwoPanelLayoutProps = {
  firstItem: ReactNode;
  secondItem: ReactNode;
  mode: 'half' | 'first-minimal' | 'third' | 'expanding-second';
  animateSecondPanel?: boolean;
};
/**
 * A layout component that takes two children and renders them.
 *
 * This is an ugly monolith as we specifically want this to be the same object
 * across react renders (which can be finnicky), as we want the state of the
 * contents to be preserved. This allows you to toglge full screen.
 *
 * TODO -- manage the state of the contents better so we can split this into
 * multiple separate component types.
 *
 */
export const TwoColumnLayout: React.FC<TwoPanelLayoutProps> = ({
  firstItem: firstColumnContent,
  secondItem: secondColumnContent,
  mode,
  animateSecondPanel = false
}) => {
  const [showSecondPanel, setShowSecondPanel] = React.useState(animateSecondPanel);
  useEffect(() => {
    if (mode === 'expanding-second') {
      setShowSecondPanel(animateSecondPanel);
    }
  }, [animateSecondPanel, mode]);
  if (mode === 'first-minimal') {
    return (
      <div className={`flex h-full w-full ${mode === 'first-minimal' ? 'flex flex-1' : ''}`}>
        <div className="h-full">{firstColumnContent}</div>
        <div className="h-full grow">{secondColumnContent}</div>
      </div>
    );
  }
  if (mode === 'third') {
    return (
      <div className={`flex h-full w-full' : ''}`}>
        <div className="w-1/3 h-full">{firstColumnContent}</div>
        <div className="w-2/3 h-full">{secondColumnContent}</div>
      </div>
    );
  }
  if (mode === 'expanding-second') {
    return (
      <div
        className={`flex h-full w-full transition-all duration-500 ${mode === 'expanding-second' && showSecondPanel ? 'overflow-hidden' : ''}`}
      >
        <div
          className={`h-full ${mode === 'expanding-second' ? 'transition-all duration-500' : ''} ${showSecondPanel ? 'w-1/2' : 'w-full'}`}
        >
          {firstColumnContent}
        </div>
        {mode === 'expanding-second' && (
          <div
            className={`h-full ${showSecondPanel ? 'w-1/2' : 'w-0'} transition-all duration-500 overflow-hidden`}
          >
            {secondColumnContent}
          </div>
        )}
        {mode !== 'expanding-second' && (
          <div className={`w-1/2 h-full ${mode === 'third' ? 'w-2/3' : 'w-1/2'}`}>
            {secondColumnContent}
          </div>
        )}
      </div>
    );
  }
  return (
    <div className="flex h-full w-full">
      <div className="w-1/2 h-full">{firstColumnContent}</div>
      <div className="w-1/2 h-full">{secondColumnContent}</div>
    </div>
  );
};

export const TwoRowLayout: React.FC<TwoPanelLayoutProps> = ({
  firstItem: topRowContent,
  secondItem: bottomRowContent
}) => {
  return (
    <div className="flex flex-col h-full w-full gap-2">
      <div className="h-1/2 overflow-auto">{topRowContent}</div>
      <div className="h-1/2">{bottomRowContent}</div>
    </div>
  );
};



---
File: /burr/telemetry/ui/src/components/common/link.tsx
---

/**
 * Tailwind catalyst component
 *
 * This is an abstraction of a link -- we'll need to
 * ensure this does what we want.
 */

import { Link as RouterLink } from 'react-router-dom';
import React from 'react';

export const Link = React.forwardRef(function Link(
  props: { href: string } & React.ComponentPropsWithoutRef<'a'>,
  ref: React.ForwardedRef<HTMLAnchorElement>
) {
  return (
    <RouterLink {...props} to={props.href} ref={ref} />
    // <HeadlessDataInteractive>
    //   <a {...props} ref={ref} />
    // </HeadlessDataInteractive>
  );
});



---
File: /burr/telemetry/ui/src/components/common/loading.tsx
---

/**
 * Simple loading component
 */
export const Loading = () => {
  return (
    <div className="flex items-center justify-center space-x-2 w-full h-full">
      <div className="w-8 h-8 bg-dwred/50 rounded-full animate-pulse animation-delay-0"></div>
      <div className="w-8 h-8 bg-dwdarkblue/50 rounded-full animate-pulse animation-delay-500"></div>
      <div className="w-8 h-8 bg-dwlightblue/50 rounded-full animate-pulse animation-delay-1000"></div>
    </div>
  );
};



---
File: /burr/telemetry/ui/src/components/common/modelCost.tsx
---

export interface ModelCost {
  [key: string]: {
    max_tokens?: number;
    max_input_tokens?: number;
    max_output_tokens?: number;
    input_cost_per_token?: number;
    output_cost_per_token?: number;
  };
}

export const modelCosts: ModelCost = {
  'sambanova/Meta-Llama-3.1-8B-Instruct': {
    max_tokens: 16000,
    max_input_tokens: 16000,
    max_output_tokens: 16000,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 2e-7
  },
  'sambanova/Meta-Llama-3.1-70B-Instruct': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 6e-7,
    output_cost_per_token: 1.2e-6
  },
  'sambanova/Meta-Llama-3.1-405B-Instruct': {
    max_tokens: 16000,
    max_input_tokens: 16000,
    max_output_tokens: 16000,
    input_cost_per_token: 5e-6,
    output_cost_per_token: 1e-5
  },
  'sambanova/Meta-Llama-3.2-1B-Instruct': {
    max_tokens: 16000,
    max_input_tokens: 16000,
    max_output_tokens: 16000,
    input_cost_per_token: 4e-7,
    output_cost_per_token: 8e-7
  },
  'sambanova/Meta-Llama-3.2-3B-Instruct': {
    max_tokens: 4000,
    max_input_tokens: 4000,
    max_output_tokens: 4000,
    input_cost_per_token: 8e-7,
    output_cost_per_token: 1.6e-6
  },
  'sambanova/Qwen2.5-Coder-32B-Instruct': {
    max_tokens: 8000,
    max_input_tokens: 8000,
    max_output_tokens: 8000,
    input_cost_per_token: 1.5e-6,
    output_cost_per_token: 3e-6
  },
  'sambanova/Qwen2.5-72B-Instruct': {
    max_tokens: 8000,
    max_input_tokens: 8000,
    max_output_tokens: 8000,
    input_cost_per_token: 2e-6,
    output_cost_per_token: 4e-6
  },
  'gpt-4': {
    max_tokens: 4096,
    max_input_tokens: 8192,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-5,
    output_cost_per_token: 6e-5
  },
  'gpt-4o': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 2.5e-6,
    output_cost_per_token: 1e-5
  },
  'gpt-4o-audio-preview': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 2.5e-6,
    output_cost_per_token: 1e-5
  },
  'gpt-4o-audio-preview-2024-12-17': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 2.5e-6,
    output_cost_per_token: 1e-5
  },
  'gpt-4o-audio-preview-2024-10-01': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 2.5e-6,
    output_cost_per_token: 1e-5
  },
  'gpt-4o-mini-audio-preview-2024-12-17': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 6e-7
  },
  'gpt-4o-mini': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 6e-7
  },
  'gpt-4o-mini-2024-07-18': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 6e-7
  },
  o1: {
    max_tokens: 100000,
    max_input_tokens: 200000,
    max_output_tokens: 100000,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 6e-5
  },
  'o1-mini': {
    max_tokens: 65536,
    max_input_tokens: 128000,
    max_output_tokens: 65536,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.2e-5
  },
  'o1-mini-2024-09-12': {
    max_tokens: 65536,
    max_input_tokens: 128000,
    max_output_tokens: 65536,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.2e-5
  },
  'o1-preview': {
    max_tokens: 32768,
    max_input_tokens: 128000,
    max_output_tokens: 32768,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 6e-5
  },
  'o1-preview-2024-09-12': {
    max_tokens: 32768,
    max_input_tokens: 128000,
    max_output_tokens: 32768,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 6e-5
  },
  'o1-2024-12-17': {
    max_tokens: 100000,
    max_input_tokens: 200000,
    max_output_tokens: 100000,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 6e-5
  },
  'chatgpt-4o-latest': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-6,
    output_cost_per_token: 1.5e-5
  },
  'gpt-4o-2024-05-13': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-6,
    output_cost_per_token: 1.5e-5
  },
  'gpt-4o-2024-08-06': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 2.5e-6,
    output_cost_per_token: 1e-5
  },
  'gpt-4o-2024-11-20': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 2.5e-6,
    output_cost_per_token: 1e-5
  },
  'gpt-4o-realtime-preview-2024-10-01': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-6,
    output_cost_per_token: 2e-5
  },
  'gpt-4o-realtime-preview': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-6,
    output_cost_per_token: 2e-5
  },
  'gpt-4o-realtime-preview-2024-12-17': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-6,
    output_cost_per_token: 2e-5
  },
  'gpt-4o-mini-realtime-preview': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 6e-7,
    output_cost_per_token: 2.4e-6
  },
  'gpt-4o-mini-realtime-preview-2024-12-17': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 6e-7,
    output_cost_per_token: 2.4e-6
  },
  'gpt-4-turbo-preview': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 3e-5
  },
  'gpt-4-0314': {
    max_tokens: 4096,
    max_input_tokens: 8192,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-5,
    output_cost_per_token: 6e-5
  },
  'gpt-4-0613': {
    max_tokens: 4096,
    max_input_tokens: 8192,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-5,
    output_cost_per_token: 6e-5
  },
  'gpt-4-32k': {
    max_tokens: 4096,
    max_input_tokens: 32768,
    max_output_tokens: 4096,
    input_cost_per_token: 6e-5,
    output_cost_per_token: 0.00012
  },
  'gpt-4-32k-0314': {
    max_tokens: 4096,
    max_input_tokens: 32768,
    max_output_tokens: 4096,
    input_cost_per_token: 6e-5,
    output_cost_per_token: 0.00012
  },
  'gpt-4-32k-0613': {
    max_tokens: 4096,
    max_input_tokens: 32768,
    max_output_tokens: 4096,
    input_cost_per_token: 6e-5,
    output_cost_per_token: 0.00012
  },
  'gpt-4-turbo': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 3e-5
  },
  'gpt-4-turbo-2024-04-09': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 3e-5
  },
  'gpt-4-1106-preview': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 3e-5
  },
  'gpt-4-0125-preview': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 3e-5
  },
  'gpt-4-vision-preview': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 3e-5
  },
  'gpt-4-1106-vision-preview': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 3e-5
  },
  'gpt-3.5-turbo': {
    max_tokens: 4097,
    max_input_tokens: 16385,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-6,
    output_cost_per_token: 2e-6
  },
  'gpt-3.5-turbo-0301': {
    max_tokens: 4097,
    max_input_tokens: 4097,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-6,
    output_cost_per_token: 2e-6
  },
  'gpt-3.5-turbo-0613': {
    max_tokens: 4097,
    max_input_tokens: 4097,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-6,
    output_cost_per_token: 2e-6
  },
  'gpt-3.5-turbo-1106': {
    max_tokens: 16385,
    max_input_tokens: 16385,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 2e-6
  },
  'gpt-3.5-turbo-0125': {
    max_tokens: 16385,
    max_input_tokens: 16385,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 1.5e-6
  },
  'gpt-3.5-turbo-16k': {
    max_tokens: 16385,
    max_input_tokens: 16385,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 4e-6
  },
  'gpt-3.5-turbo-16k-0613': {
    max_tokens: 16385,
    max_input_tokens: 16385,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 4e-6
  },
  'ft:gpt-3.5-turbo': {
    max_tokens: 4096,
    max_input_tokens: 16385,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 6e-6
  },
  'ft:gpt-3.5-turbo-0125': {
    max_tokens: 4096,
    max_input_tokens: 16385,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 6e-6
  },
  'ft:gpt-3.5-turbo-1106': {
    max_tokens: 4096,
    max_input_tokens: 16385,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 6e-6
  },
  'ft:gpt-3.5-turbo-0613': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 6e-6
  },
  'ft:gpt-4-0613': {
    max_tokens: 4096,
    max_input_tokens: 8192,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-5,
    output_cost_per_token: 6e-5
  },
  'ft:gpt-4o-2024-08-06': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 3.75e-6,
    output_cost_per_token: 1.5e-5
  },
  'ft:gpt-4o-2024-11-20': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 3.75e-6,
    output_cost_per_token: 1.5e-5
  },
  'ft:gpt-4o-mini-2024-07-18': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 3e-7,
    output_cost_per_token: 1.2e-6
  },
  'ft:davinci-002': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 4096,
    input_cost_per_token: 2e-6,
    output_cost_per_token: 2e-6
  },
  'ft:babbage-002': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 4096,
    input_cost_per_token: 4e-7,
    output_cost_per_token: 4e-7
  },
  'text-embedding-3-large': {
    max_tokens: 8191,
    max_input_tokens: 8191,
    input_cost_per_token: 1.3e-7,
    output_cost_per_token: 0.0
  },
  'text-embedding-3-small': {
    max_tokens: 8191,
    max_input_tokens: 8191,
    input_cost_per_token: 2e-8,
    output_cost_per_token: 0.0
  },
  'text-embedding-ada-002': {
    max_tokens: 8191,
    max_input_tokens: 8191,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'text-embedding-ada-002-v2': {
    max_tokens: 8191,
    max_input_tokens: 8191,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'text-moderation-stable': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 0,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'text-moderation-007': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 0,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'text-moderation-latest': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 0,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  '256-x-256/dall-e-2': {},
  '512-x-512/dall-e-2': {},
  '1024-x-1024/dall-e-2': {},
  'hd/1024-x-1792/dall-e-3': {},
  'hd/1792-x-1024/dall-e-3': {},
  'hd/1024-x-1024/dall-e-3': {},
  'standard/1024-x-1792/dall-e-3': {},
  'standard/1792-x-1024/dall-e-3': {},
  'standard/1024-x-1024/dall-e-3': {},
  'whisper-1': {},
  'tts-1': {},
  'tts-1-hd': {},
  'azure/tts-1': {},
  'azure/tts-1-hd': {},
  'azure/whisper-1': {},
  'azure/o1-mini': {
    max_tokens: 65536,
    max_input_tokens: 128000,
    max_output_tokens: 65536,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.2e-5
  },
  'azure/o1-mini-2024-09-12': {
    max_tokens: 65536,
    max_input_tokens: 128000,
    max_output_tokens: 65536,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.2e-5
  },
  'azure/o1-preview': {
    max_tokens: 32768,
    max_input_tokens: 128000,
    max_output_tokens: 32768,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 6e-5
  },
  'azure/o1-preview-2024-09-12': {
    max_tokens: 32768,
    max_input_tokens: 128000,
    max_output_tokens: 32768,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 6e-5
  },
  'azure/gpt-4o': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-6,
    output_cost_per_token: 1.5e-5
  },
  'azure/gpt-4o-2024-08-06': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 2.75e-6,
    output_cost_per_token: 1.1e-5
  },
  'azure/gpt-4o-2024-11-20': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 2.75e-6,
    output_cost_per_token: 1.1e-5
  },
  'azure/gpt-4o-2024-05-13': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-6,
    output_cost_per_token: 1.5e-5
  },
  'azure/global-standard/gpt-4o-2024-08-06': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 2.5e-6,
    output_cost_per_token: 1e-5
  },
  'azure/global-standard/gpt-4o-2024-11-20': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 2.5e-6,
    output_cost_per_token: 1e-5
  },
  'azure/global-standard/gpt-4o-mini': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 6e-7
  },
  'azure/gpt-4o-mini': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 1.65e-7,
    output_cost_per_token: 6.6e-7
  },
  'azure/gpt-4o-mini-2024-07-18': {
    max_tokens: 16384,
    max_input_tokens: 128000,
    max_output_tokens: 16384,
    input_cost_per_token: 1.65e-7,
    output_cost_per_token: 6.6e-7
  },
  'azure/gpt-4-turbo-2024-04-09': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 3e-5
  },
  'azure/gpt-4-0125-preview': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 3e-5
  },
  'azure/gpt-4-1106-preview': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 3e-5
  },
  'azure/gpt-4-0613': {
    max_tokens: 4096,
    max_input_tokens: 8192,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-5,
    output_cost_per_token: 6e-5
  },
  'azure/gpt-4-32k-0613': {
    max_tokens: 4096,
    max_input_tokens: 32768,
    max_output_tokens: 4096,
    input_cost_per_token: 6e-5,
    output_cost_per_token: 0.00012
  },
  'azure/gpt-4-32k': {
    max_tokens: 4096,
    max_input_tokens: 32768,
    max_output_tokens: 4096,
    input_cost_per_token: 6e-5,
    output_cost_per_token: 0.00012
  },
  'azure/gpt-4': {
    max_tokens: 4096,
    max_input_tokens: 8192,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-5,
    output_cost_per_token: 6e-5
  },
  'azure/gpt-4-turbo': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 3e-5
  },
  'azure/gpt-4-turbo-vision-preview': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 3e-5
  },
  'azure/gpt-35-turbo-16k-0613': {
    max_tokens: 4096,
    max_input_tokens: 16385,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 4e-6
  },
  'azure/gpt-35-turbo-1106': {
    max_tokens: 4096,
    max_input_tokens: 16384,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 2e-6
  },
  'azure/gpt-35-turbo-0613': {
    max_tokens: 4097,
    max_input_tokens: 4097,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-6,
    output_cost_per_token: 2e-6
  },
  'azure/gpt-35-turbo-0301': {
    max_tokens: 4097,
    max_input_tokens: 4097,
    max_output_tokens: 4096,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 2e-6
  },
  'azure/gpt-35-turbo-0125': {
    max_tokens: 4096,
    max_input_tokens: 16384,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 1.5e-6
  },
  'azure/gpt-35-turbo-16k': {
    max_tokens: 4096,
    max_input_tokens: 16385,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 4e-6
  },
  'azure/gpt-35-turbo': {
    max_tokens: 4096,
    max_input_tokens: 4097,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 1.5e-6
  },
  'azure/gpt-3.5-turbo-instruct-0914': {
    max_tokens: 4097,
    max_input_tokens: 4097,
    input_cost_per_token: 1.5e-6,
    output_cost_per_token: 2e-6
  },
  'azure/gpt-35-turbo-instruct': {
    max_tokens: 4097,
    max_input_tokens: 4097,
    input_cost_per_token: 1.5e-6,
    output_cost_per_token: 2e-6
  },
  'azure/gpt-35-turbo-instruct-0914': {
    max_tokens: 4097,
    max_input_tokens: 4097,
    input_cost_per_token: 1.5e-6,
    output_cost_per_token: 2e-6
  },
  'azure/mistral-large-latest': {
    max_tokens: 32000,
    max_input_tokens: 32000,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'azure/mistral-large-2402': {
    max_tokens: 32000,
    max_input_tokens: 32000,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'azure/command-r-plus': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'azure/ada': {
    max_tokens: 8191,
    max_input_tokens: 8191,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'azure/text-embedding-ada-002': {
    max_tokens: 8191,
    max_input_tokens: 8191,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'azure/text-embedding-3-large': {
    max_tokens: 8191,
    max_input_tokens: 8191,
    input_cost_per_token: 1.3e-7,
    output_cost_per_token: 0.0
  },
  'azure/text-embedding-3-small': {
    max_tokens: 8191,
    max_input_tokens: 8191,
    input_cost_per_token: 2e-8,
    output_cost_per_token: 0.0
  },
  'azure/standard/1024-x-1024/dall-e-3': {
    output_cost_per_token: 0.0
  },
  'azure/hd/1024-x-1024/dall-e-3': {
    output_cost_per_token: 0.0
  },
  'azure/standard/1024-x-1792/dall-e-3': {
    output_cost_per_token: 0.0
  },
  'azure/standard/1792-x-1024/dall-e-3': {
    output_cost_per_token: 0.0
  },
  'azure/hd/1024-x-1792/dall-e-3': {
    output_cost_per_token: 0.0
  },
  'azure/hd/1792-x-1024/dall-e-3': {
    output_cost_per_token: 0.0
  },
  'azure/standard/1024-x-1024/dall-e-2': {
    output_cost_per_token: 0.0
  },
  'azure_ai/jamba-instruct': {
    max_tokens: 4096,
    max_input_tokens: 70000,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 7e-7
  },
  'azure_ai/mistral-large': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 4e-6,
    output_cost_per_token: 1.2e-5
  },
  'azure_ai/mistral-small': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 3e-6
  },
  'azure_ai/mistral-large-2407': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 2e-6,
    output_cost_per_token: 6e-6
  },
  'azure_ai/ministral-3b': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 4e-8,
    output_cost_per_token: 4e-8
  },
  'azure_ai/Llama-3.2-11B-Vision-Instruct': {
    max_tokens: 2048,
    max_input_tokens: 128000,
    max_output_tokens: 2048,
    input_cost_per_token: 3.7e-7,
    output_cost_per_token: 3.7e-7
  },
  'azure_ai/Llama-3.3-70B-Instruct': {
    max_tokens: 2048,
    max_input_tokens: 128000,
    max_output_tokens: 2048,
    input_cost_per_token: 7.1e-7,
    output_cost_per_token: 7.1e-7
  },
  'azure_ai/Llama-3.2-90B-Vision-Instruct': {
    max_tokens: 2048,
    max_input_tokens: 128000,
    max_output_tokens: 2048,
    input_cost_per_token: 2.04e-6,
    output_cost_per_token: 2.04e-6
  },
  'azure_ai/Meta-Llama-3-70B-Instruct': {
    max_tokens: 2048,
    max_input_tokens: 8192,
    max_output_tokens: 2048,
    input_cost_per_token: 1.1e-6,
    output_cost_per_token: 3.7e-7
  },
  'azure_ai/Meta-Llama-3.1-8B-Instruct': {
    max_tokens: 2048,
    max_input_tokens: 128000,
    max_output_tokens: 2048,
    input_cost_per_token: 3e-7,
    output_cost_per_token: 6.1e-7
  },
  'azure_ai/Meta-Llama-3.1-70B-Instruct': {
    max_tokens: 2048,
    max_input_tokens: 128000,
    max_output_tokens: 2048,
    input_cost_per_token: 2.68e-6,
    output_cost_per_token: 3.54e-6
  },
  'azure_ai/Meta-Llama-3.1-405B-Instruct': {
    max_tokens: 2048,
    max_input_tokens: 128000,
    max_output_tokens: 2048,
    input_cost_per_token: 5.33e-6,
    output_cost_per_token: 1.6e-5
  },
  'azure_ai/Phi-3.5-mini-instruct': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.3e-7,
    output_cost_per_token: 5.2e-7
  },
  'azure_ai/Phi-3.5-vision-instruct': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.3e-7,
    output_cost_per_token: 5.2e-7
  },
  'azure_ai/Phi-3.5-MoE-instruct': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.6e-7,
    output_cost_per_token: 6.4e-7
  },
  'azure_ai/Phi-3-mini-4k-instruct': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1.3e-7,
    output_cost_per_token: 5.2e-7
  },
  'azure_ai/Phi-3-mini-128k-instruct': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.3e-7,
    output_cost_per_token: 5.2e-7
  },
  'azure_ai/Phi-3-small-8k-instruct': {
    max_tokens: 4096,
    max_input_tokens: 8192,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 6e-7
  },
  'azure_ai/Phi-3-small-128k-instruct': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 6e-7
  },
  'azure_ai/Phi-3-medium-4k-instruct': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1.7e-7,
    output_cost_per_token: 6.8e-7
  },
  'azure_ai/Phi-3-medium-128k-instruct': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.7e-7,
    output_cost_per_token: 6.8e-7
  },
  'azure_ai/cohere-rerank-v3-multilingual': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'azure_ai/cohere-rerank-v3-english': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'azure_ai/Cohere-embed-v3-english': {
    max_tokens: 512,
    max_input_tokens: 512,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'azure_ai/Cohere-embed-v3-multilingual': {
    max_tokens: 512,
    max_input_tokens: 512,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'babbage-002': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 4096,
    input_cost_per_token: 4e-7,
    output_cost_per_token: 4e-7
  },
  'davinci-002': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 4096,
    input_cost_per_token: 2e-6,
    output_cost_per_token: 2e-6
  },
  'gpt-3.5-turbo-instruct': {
    max_tokens: 4096,
    max_input_tokens: 8192,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-6,
    output_cost_per_token: 2e-6
  },
  'gpt-3.5-turbo-instruct-0914': {
    max_tokens: 4097,
    max_input_tokens: 8192,
    max_output_tokens: 4097,
    input_cost_per_token: 1.5e-6,
    output_cost_per_token: 2e-6
  },
  'claude-instant-1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 1.63e-6,
    output_cost_per_token: 5.51e-6
  },
  'mistral/mistral-tiny': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 2.5e-7
  },
  'mistral/mistral-small': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 3e-6
  },
  'mistral/mistral-small-latest': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 3e-6
  },
  'mistral/mistral-medium': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 2.7e-6,
    output_cost_per_token: 8.1e-6
  },
  'mistral/mistral-medium-latest': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 2.7e-6,
    output_cost_per_token: 8.1e-6
  },
  'mistral/mistral-medium-2312': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 2.7e-6,
    output_cost_per_token: 8.1e-6
  },
  'mistral/mistral-large-latest': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 9e-6
  },
  'mistral/mistral-large-2402': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 4e-6,
    output_cost_per_token: 1.2e-5
  },
  'mistral/mistral-large-2407': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 9e-6
  },
  'mistral/pixtral-12b-2409': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 1.5e-7
  },
  'mistral/open-mistral-7b': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 2.5e-7
  },
  'mistral/open-mixtral-8x7b': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 7e-7,
    output_cost_per_token: 7e-7
  },
  'mistral/open-mixtral-8x22b': {
    max_tokens: 8191,
    max_input_tokens: 64000,
    max_output_tokens: 8191,
    input_cost_per_token: 2e-6,
    output_cost_per_token: 6e-6
  },
  'mistral/codestral-latest': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 3e-6
  },
  'mistral/codestral-2405': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 3e-6
  },
  'mistral/open-mistral-nemo': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 3e-7,
    output_cost_per_token: 3e-7
  },
  'mistral/open-mistral-nemo-2407': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 3e-7,
    output_cost_per_token: 3e-7
  },
  'mistral/open-codestral-mamba': {
    max_tokens: 256000,
    max_input_tokens: 256000,
    max_output_tokens: 256000,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 2.5e-7
  },
  'mistral/codestral-mamba-latest': {
    max_tokens: 256000,
    max_input_tokens: 256000,
    max_output_tokens: 256000,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 2.5e-7
  },
  'mistral/mistral-embed': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    input_cost_per_token: 1e-7
  },
  'deepseek-chat': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.4e-7,
    output_cost_per_token: 2.8e-7
  },
  'codestral/codestral-latest': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'codestral/codestral-2405': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'text-completion-codestral/codestral-latest': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'text-completion-codestral/codestral-2405': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'xai/grok-beta': {
    max_tokens: 131072,
    max_input_tokens: 131072,
    max_output_tokens: 131072,
    input_cost_per_token: 5e-6,
    output_cost_per_token: 1.5e-5
  },
  'deepseek-coder': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.4e-7,
    output_cost_per_token: 2.8e-7
  },
  'groq/llama-3.3-70b-versatile': {
    max_tokens: 8192,
    max_input_tokens: 128000,
    max_output_tokens: 8192,
    input_cost_per_token: 5.9e-7,
    output_cost_per_token: 7.9e-7
  },
  'groq/llama-3.3-70b-specdec': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 5.9e-7,
    output_cost_per_token: 9.9e-7
  },
  'groq/llama2-70b-4096': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 7e-7,
    output_cost_per_token: 8e-7
  },
  'groq/llama3-8b-8192': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 5e-8,
    output_cost_per_token: 8e-8
  },
  'groq/llama-3.2-1b-preview': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 4e-8,
    output_cost_per_token: 4e-8
  },
  'groq/llama-3.2-3b-preview': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 6e-8,
    output_cost_per_token: 6e-8
  },
  'groq/llama-3.2-11b-text-preview': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 1.8e-7,
    output_cost_per_token: 1.8e-7
  },
  'groq/llama-3.2-11b-vision-preview': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 1.8e-7,
    output_cost_per_token: 1.8e-7
  },
  'groq/llama-3.2-90b-text-preview': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 9e-7,
    output_cost_per_token: 9e-7
  },
  'groq/llama-3.2-90b-vision-preview': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 9e-7,
    output_cost_per_token: 9e-7
  },
  'groq/llama3-70b-8192': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 5.9e-7,
    output_cost_per_token: 7.9e-7
  },
  'groq/llama-3.1-8b-instant': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 5e-8,
    output_cost_per_token: 8e-8
  },
  'groq/llama-3.1-70b-versatile': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 5.9e-7,
    output_cost_per_token: 7.9e-7
  },
  'groq/llama-3.1-405b-reasoning': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 5.9e-7,
    output_cost_per_token: 7.9e-7
  },
  'groq/mixtral-8x7b-32768': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 32768,
    input_cost_per_token: 2.4e-7,
    output_cost_per_token: 2.4e-7
  },
  'groq/gemma-7b-it': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 7e-8,
    output_cost_per_token: 7e-8
  },
  'groq/gemma2-9b-it': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 2e-7
  },
  'groq/llama3-groq-70b-8192-tool-use-preview': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 8.9e-7,
    output_cost_per_token: 8.9e-7
  },
  'groq/llama3-groq-8b-8192-tool-use-preview': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 1.9e-7,
    output_cost_per_token: 1.9e-7
  },
  'cerebras/llama3.1-8b': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 1e-7
  },
  'cerebras/llama3.1-70b': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 6e-7,
    output_cost_per_token: 6e-7
  },
  'friendliai/mixtral-8x7b-instruct-v0-1': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 32768,
    input_cost_per_token: 4e-7,
    output_cost_per_token: 4e-7
  },
  'friendliai/meta-llama-3-8b-instruct': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 1e-7
  },
  'friendliai/meta-llama-3-70b-instruct': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 8e-7,
    output_cost_per_token: 8e-7
  },
  'claude-instant-1.2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 1.63e-7,
    output_cost_per_token: 5.51e-7
  },
  'claude-2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'claude-2.1': {
    max_tokens: 8191,
    max_input_tokens: 200000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'claude-3-haiku-20240307': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 1.25e-6
  },
  'claude-3-5-haiku-20241022': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 5e-6
  },
  'claude-3-opus-20240229': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 7.5e-5
  },
  'claude-3-sonnet-20240229': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'claude-3-5-sonnet-20240620': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'claude-3-5-sonnet-20241022': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'text-bison': {
    max_tokens: 2048,
    max_input_tokens: 8192,
    max_output_tokens: 2048
  },
  'text-bison@001': {
    max_tokens: 1024,
    max_input_tokens: 8192,
    max_output_tokens: 1024
  },
  'text-bison@002': {
    max_tokens: 1024,
    max_input_tokens: 8192,
    max_output_tokens: 1024
  },
  'text-bison32k': {
    max_tokens: 1024,
    max_input_tokens: 8192,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'text-bison32k@002': {
    max_tokens: 1024,
    max_input_tokens: 8192,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'text-unicorn': {
    max_tokens: 1024,
    max_input_tokens: 8192,
    max_output_tokens: 1024,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 2.8e-5
  },
  'text-unicorn@001': {
    max_tokens: 1024,
    max_input_tokens: 8192,
    max_output_tokens: 1024,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 2.8e-5
  },
  'chat-bison': {
    max_tokens: 4096,
    max_input_tokens: 8192,
    max_output_tokens: 4096,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'chat-bison@001': {
    max_tokens: 4096,
    max_input_tokens: 8192,
    max_output_tokens: 4096,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'chat-bison@002': {
    max_tokens: 4096,
    max_input_tokens: 8192,
    max_output_tokens: 4096,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'chat-bison-32k': {
    max_tokens: 8192,
    max_input_tokens: 32000,
    max_output_tokens: 8192,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'chat-bison-32k@002': {
    max_tokens: 8192,
    max_input_tokens: 32000,
    max_output_tokens: 8192,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'code-bison': {
    max_tokens: 1024,
    max_input_tokens: 6144,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'code-bison@001': {
    max_tokens: 1024,
    max_input_tokens: 6144,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'code-bison@002': {
    max_tokens: 1024,
    max_input_tokens: 6144,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'code-bison32k': {
    max_tokens: 1024,
    max_input_tokens: 6144,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'code-bison-32k@002': {
    max_tokens: 1024,
    max_input_tokens: 6144,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'code-gecko@001': {
    max_tokens: 64,
    max_input_tokens: 2048,
    max_output_tokens: 64,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'code-gecko@002': {
    max_tokens: 64,
    max_input_tokens: 2048,
    max_output_tokens: 64,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'code-gecko': {
    max_tokens: 64,
    max_input_tokens: 2048,
    max_output_tokens: 64,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'code-gecko-latest': {
    max_tokens: 64,
    max_input_tokens: 2048,
    max_output_tokens: 64,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'codechat-bison@latest': {
    max_tokens: 1024,
    max_input_tokens: 6144,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'codechat-bison': {
    max_tokens: 1024,
    max_input_tokens: 6144,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'codechat-bison@001': {
    max_tokens: 1024,
    max_input_tokens: 6144,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'codechat-bison@002': {
    max_tokens: 1024,
    max_input_tokens: 6144,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'codechat-bison-32k': {
    max_tokens: 8192,
    max_input_tokens: 32000,
    max_output_tokens: 8192,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'codechat-bison-32k@002': {
    max_tokens: 8192,
    max_input_tokens: 32000,
    max_output_tokens: 8192,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'gemini-pro': {
    max_tokens: 8192,
    max_input_tokens: 32760,
    max_output_tokens: 8192,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 1.5e-6
  },
  'gemini-1.0-pro': {
    max_tokens: 8192,
    max_input_tokens: 32760,
    max_output_tokens: 8192,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 1.5e-6
  },
  'gemini-1.0-pro-001': {
    max_tokens: 8192,
    max_input_tokens: 32760,
    max_output_tokens: 8192,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 1.5e-6
  },
  'gemini-1.0-ultra': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 2048,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 1.5e-6
  },
  'gemini-1.0-ultra-001': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 2048,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 1.5e-6
  },
  'gemini-1.0-pro-002': {
    max_tokens: 8192,
    max_input_tokens: 32760,
    max_output_tokens: 8192,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 1.5e-6
  },
  'gemini-1.5-pro': {
    max_tokens: 8192,
    max_input_tokens: 2097152,
    max_output_tokens: 8192,
    input_cost_per_token: 1.25e-6,
    output_cost_per_token: 5e-6
  },
  'gemini-1.5-pro-002': {
    max_tokens: 8192,
    max_input_tokens: 2097152,
    max_output_tokens: 8192,
    input_cost_per_token: 1.25e-6,
    output_cost_per_token: 5e-6
  },
  'gemini-1.5-pro-001': {
    max_tokens: 8192,
    max_input_tokens: 1000000,
    max_output_tokens: 8192,
    input_cost_per_token: 1.25e-6,
    output_cost_per_token: 5e-6
  },
  'gemini-1.5-pro-preview-0514': {
    max_tokens: 8192,
    max_input_tokens: 1000000,
    max_output_tokens: 8192,
    input_cost_per_token: 7.8125e-8,
    output_cost_per_token: 3.125e-7
  },
  'gemini-1.5-pro-preview-0215': {
    max_tokens: 8192,
    max_input_tokens: 1000000,
    max_output_tokens: 8192,
    input_cost_per_token: 7.8125e-8,
    output_cost_per_token: 3.125e-7
  },
  'gemini-1.5-pro-preview-0409': {
    max_tokens: 8192,
    max_input_tokens: 1000000,
    max_output_tokens: 8192,
    input_cost_per_token: 7.8125e-8,
    output_cost_per_token: 3.125e-7
  },
  'gemini-1.5-flash': {
    max_tokens: 8192,
    max_input_tokens: 1000000,
    max_output_tokens: 8192,
    input_cost_per_token: 7.5e-8,
    output_cost_per_token: 3e-7
  },
  'gemini-1.5-flash-exp-0827': {
    max_tokens: 8192,
    max_input_tokens: 1000000,
    max_output_tokens: 8192,
    input_cost_per_token: 4.688e-9,
    output_cost_per_token: 4.6875e-9
  },
  'gemini-1.5-flash-002': {
    max_tokens: 8192,
    max_input_tokens: 1048576,
    max_output_tokens: 8192,
    input_cost_per_token: 7.5e-8,
    output_cost_per_token: 3e-7
  },
  'gemini-1.5-flash-001': {
    max_tokens: 8192,
    max_input_tokens: 1000000,
    max_output_tokens: 8192,
    input_cost_per_token: 7.5e-8,
    output_cost_per_token: 3e-7
  },
  'gemini-1.5-flash-preview-0514': {
    max_tokens: 8192,
    max_input_tokens: 1000000,
    max_output_tokens: 8192,
    input_cost_per_token: 7.5e-8,
    output_cost_per_token: 4.6875e-9
  },
  'gemini-pro-experimental': {
    max_tokens: 8192,
    max_input_tokens: 1000000,
    max_output_tokens: 8192,
    input_cost_per_token: 0,
    output_cost_per_token: 0
  },
  'gemini-flash-experimental': {
    max_tokens: 8192,
    max_input_tokens: 1000000,
    max_output_tokens: 8192,
    input_cost_per_token: 0,
    output_cost_per_token: 0
  },
  'gemini-pro-vision': {
    max_tokens: 2048,
    max_input_tokens: 16384,
    max_output_tokens: 2048,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 5e-7
  },
  'gemini-1.0-pro-vision': {
    max_tokens: 2048,
    max_input_tokens: 16384,
    max_output_tokens: 2048,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 5e-7
  },
  'gemini-1.0-pro-vision-001': {
    max_tokens: 2048,
    max_input_tokens: 16384,
    max_output_tokens: 2048,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 5e-7
  },
  'medlm-medium': {
    max_tokens: 8192,
    max_input_tokens: 32768,
    max_output_tokens: 8192
  },
  'medlm-large': {
    max_tokens: 1024,
    max_input_tokens: 8192,
    max_output_tokens: 1024
  },
  'gemini-2.0-flash-exp': {
    max_tokens: 8192,
    max_input_tokens: 1048576,
    max_output_tokens: 8192,
    input_cost_per_token: 0,
    output_cost_per_token: 0
  },
  'gemini/gemini-2.0-flash-exp': {
    max_tokens: 8192,
    max_input_tokens: 1048576,
    max_output_tokens: 8192,
    input_cost_per_token: 0,
    output_cost_per_token: 0
  },
  'vertex_ai/claude-3-sonnet': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'vertex_ai/claude-3-sonnet@20240229': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'vertex_ai/claude-3-5-sonnet': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'vertex_ai/claude-3-5-sonnet@20240620': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'vertex_ai/claude-3-5-sonnet-v2': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'vertex_ai/claude-3-5-sonnet-v2@20241022': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'vertex_ai/claude-3-haiku': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 1.25e-6
  },
  'vertex_ai/claude-3-haiku@20240307': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 1.25e-6
  },
  'vertex_ai/claude-3-5-haiku': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 5e-6
  },
  'vertex_ai/claude-3-5-haiku@20241022': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 5e-6
  },
  'vertex_ai/claude-3-opus': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 7.5e-5
  },
  'vertex_ai/claude-3-opus@20240229': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 7.5e-5
  },
  'vertex_ai/meta/llama3-405b-instruct-maas': {
    max_tokens: 32000,
    max_input_tokens: 32000,
    max_output_tokens: 32000,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'vertex_ai/meta/llama3-70b-instruct-maas': {
    max_tokens: 32000,
    max_input_tokens: 32000,
    max_output_tokens: 32000,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'vertex_ai/meta/llama3-8b-instruct-maas': {
    max_tokens: 32000,
    max_input_tokens: 32000,
    max_output_tokens: 32000,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'vertex_ai/meta/llama-3.2-90b-vision-instruct-maas': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 2048,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'vertex_ai/mistral-large@latest': {
    max_tokens: 8191,
    max_input_tokens: 128000,
    max_output_tokens: 8191,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 9e-6
  },
  'vertex_ai/mistral-large@2407': {
    max_tokens: 8191,
    max_input_tokens: 128000,
    max_output_tokens: 8191,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 9e-6
  },
  'vertex_ai/mistral-nemo@latest': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 3e-6
  },
  'vertex_ai/jamba-1.5-mini@001': {
    max_tokens: 256000,
    max_input_tokens: 256000,
    max_output_tokens: 256000,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 4e-7
  },
  'vertex_ai/jamba-1.5-large@001': {
    max_tokens: 256000,
    max_input_tokens: 256000,
    max_output_tokens: 256000,
    input_cost_per_token: 2e-6,
    output_cost_per_token: 8e-6
  },
  'vertex_ai/jamba-1.5': {
    max_tokens: 256000,
    max_input_tokens: 256000,
    max_output_tokens: 256000,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 4e-7
  },
  'vertex_ai/jamba-1.5-mini': {
    max_tokens: 256000,
    max_input_tokens: 256000,
    max_output_tokens: 256000,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 4e-7
  },
  'vertex_ai/jamba-1.5-large': {
    max_tokens: 256000,
    max_input_tokens: 256000,
    max_output_tokens: 256000,
    input_cost_per_token: 2e-6,
    output_cost_per_token: 8e-6
  },
  'vertex_ai/mistral-nemo@2407': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 3e-6
  },
  'vertex_ai/codestral@latest': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 3e-6
  },
  'vertex_ai/codestral@2405': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 3e-6
  },
  'vertex_ai/imagegeneration@006': {},
  'vertex_ai/imagen-3.0-generate-001': {},
  'vertex_ai/imagen-3.0-fast-generate-001': {},
  'text-embedding-004': {
    max_tokens: 2048,
    max_input_tokens: 2048,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0
  },
  'text-embedding-005': {
    max_tokens: 2048,
    max_input_tokens: 2048,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0
  },
  'text-multilingual-embedding-002': {
    max_tokens: 2048,
    max_input_tokens: 2048,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0
  },
  'textembedding-gecko': {
    max_tokens: 3072,
    max_input_tokens: 3072,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0
  },
  'textembedding-gecko-multilingual': {
    max_tokens: 3072,
    max_input_tokens: 3072,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0
  },
  'textembedding-gecko-multilingual@001': {
    max_tokens: 3072,
    max_input_tokens: 3072,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0
  },
  'textembedding-gecko@001': {
    max_tokens: 3072,
    max_input_tokens: 3072,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0
  },
  'textembedding-gecko@003': {
    max_tokens: 3072,
    max_input_tokens: 3072,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0
  },
  'text-embedding-preview-0409': {
    max_tokens: 3072,
    max_input_tokens: 3072,
    input_cost_per_token: 6.25e-9,
    output_cost_per_token: 0
  },
  'text-multilingual-embedding-preview-0409': {
    max_tokens: 3072,
    max_input_tokens: 3072,
    input_cost_per_token: 6.25e-9,
    output_cost_per_token: 0
  },
  'palm/chat-bison': {
    max_tokens: 4096,
    max_input_tokens: 8192,
    max_output_tokens: 4096,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'palm/chat-bison-001': {
    max_tokens: 4096,
    max_input_tokens: 8192,
    max_output_tokens: 4096,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'palm/text-bison': {
    max_tokens: 1024,
    max_input_tokens: 8192,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'palm/text-bison-001': {
    max_tokens: 1024,
    max_input_tokens: 8192,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'palm/text-bison-safety-off': {
    max_tokens: 1024,
    max_input_tokens: 8192,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'palm/text-bison-safety-recitation-off': {
    max_tokens: 1024,
    max_input_tokens: 8192,
    max_output_tokens: 1024,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 1.25e-7
  },
  'gemini/gemini-1.5-flash-002': {
    max_tokens: 8192,
    max_input_tokens: 1048576,
    max_output_tokens: 8192,
    input_cost_per_token: 7.5e-8,
    output_cost_per_token: 3e-7
  },
  'gemini/gemini-1.5-flash-001': {
    max_tokens: 8192,
    max_input_tokens: 1048576,
    max_output_tokens: 8192,
    input_cost_per_token: 7.5e-8,
    output_cost_per_token: 3e-7
  },
  'gemini/gemini-1.5-flash': {
    max_tokens: 8192,
    max_input_tokens: 1048576,
    max_output_tokens: 8192,
    input_cost_per_token: 7.5e-8,
    output_cost_per_token: 3e-7
  },
  'gemini/gemini-1.5-flash-latest': {
    max_tokens: 8192,
    max_input_tokens: 1048576,
    max_output_tokens: 8192,
    input_cost_per_token: 7.5e-8,
    output_cost_per_token: 3e-7
  },
  'gemini/gemini-1.5-flash-8b': {
    max_tokens: 8192,
    max_input_tokens: 1048576,
    max_output_tokens: 8192,
    input_cost_per_token: 0,
    output_cost_per_token: 0
  },
  'gemini/gemini-1.5-flash-8b-exp-0924': {
    max_tokens: 8192,
    max_input_tokens: 1048576,
    max_output_tokens: 8192,
    input_cost_per_token: 0,
    output_cost_per_token: 0
  },
  'gemini/gemini-exp-1114': {
    max_tokens: 8192,
    max_input_tokens: 1048576,
    max_output_tokens: 8192,
    input_cost_per_token: 0,
    output_cost_per_token: 0
  },
  'gemini/gemini-exp-1206': {
    max_tokens: 8192,
    max_input_tokens: 2097152,
    max_output_tokens: 8192,
    input_cost_per_token: 0,
    output_cost_per_token: 0
  },
  'gemini/gemini-1.5-flash-exp-0827': {
    max_tokens: 8192,
    max_input_tokens: 1048576,
    max_output_tokens: 8192,
    input_cost_per_token: 0,
    output_cost_per_token: 0
  },
  'gemini/gemini-1.5-flash-8b-exp-0827': {
    max_tokens: 8192,
    max_input_tokens: 1000000,
    max_output_tokens: 8192,
    input_cost_per_token: 0,
    output_cost_per_token: 0
  },
  'gemini/gemini-pro': {
    max_tokens: 8192,
    max_input_tokens: 32760,
    max_output_tokens: 8192,
    input_cost_per_token: 3.5e-7,
    output_cost_per_token: 1.05e-6
  },
  'gemini/gemini-1.5-pro': {
    max_tokens: 8192,
    max_input_tokens: 2097152,
    max_output_tokens: 8192,
    input_cost_per_token: 3.5e-6,
    output_cost_per_token: 1.05e-5
  },
  'gemini/gemini-1.5-pro-002': {
    max_tokens: 8192,
    max_input_tokens: 2097152,
    max_output_tokens: 8192,
    input_cost_per_token: 3.5e-6,
    output_cost_per_token: 1.05e-5
  },
  'gemini/gemini-1.5-pro-001': {
    max_tokens: 8192,
    max_input_tokens: 2097152,
    max_output_tokens: 8192,
    input_cost_per_token: 3.5e-6,
    output_cost_per_token: 1.05e-5
  },
  'gemini/gemini-1.5-pro-exp-0801': {
    max_tokens: 8192,
    max_input_tokens: 2097152,
    max_output_tokens: 8192,
    input_cost_per_token: 3.5e-6,
    output_cost_per_token: 1.05e-5
  },
  'gemini/gemini-1.5-pro-exp-0827': {
    max_tokens: 8192,
    max_input_tokens: 2097152,
    max_output_tokens: 8192,
    input_cost_per_token: 0,
    output_cost_per_token: 0
  },
  'gemini/gemini-1.5-pro-latest': {
    max_tokens: 8192,
    max_input_tokens: 1048576,
    max_output_tokens: 8192,
    input_cost_per_token: 3.5e-6,
    output_cost_per_token: 1.05e-6
  },
  'gemini/gemini-pro-vision': {
    max_tokens: 2048,
    max_input_tokens: 30720,
    max_output_tokens: 2048,
    input_cost_per_token: 3.5e-7,
    output_cost_per_token: 1.05e-6
  },
  'gemini/gemini-gemma-2-27b-it': {
    max_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 3.5e-7,
    output_cost_per_token: 1.05e-6
  },
  'gemini/gemini-gemma-2-9b-it': {
    max_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 3.5e-7,
    output_cost_per_token: 1.05e-6
  },
  'command-r': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 6e-7
  },
  'command-r-08-2024': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 6e-7
  },
  'command-light': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-7,
    output_cost_per_token: 6e-7
  },
  'command-r-plus': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 2.5e-6,
    output_cost_per_token: 1e-5
  },
  'command-r-plus-08-2024': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 2.5e-6,
    output_cost_per_token: 1e-5
  },
  'command-nightly': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 2e-6
  },
  command: {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 2e-6
  },
  'rerank-v3.5': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'rerank-english-v3.0': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'rerank-multilingual-v3.0': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'rerank-english-v2.0': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'rerank-multilingual-v2.0': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'embed-english-light-v3.0': {
    max_tokens: 1024,
    max_input_tokens: 1024,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'embed-multilingual-v3.0': {
    max_tokens: 1024,
    max_input_tokens: 1024,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'embed-english-v2.0': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'embed-english-light-v2.0': {
    max_tokens: 1024,
    max_input_tokens: 1024,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'embed-multilingual-v2.0': {
    max_tokens: 768,
    max_input_tokens: 768,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'embed-english-v3.0': {
    max_tokens: 1024,
    max_input_tokens: 1024,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'replicate/meta/llama-2-13b': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 5e-7
  },
  'replicate/meta/llama-2-13b-chat': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 5e-7
  },
  'replicate/meta/llama-2-70b': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 6.5e-7,
    output_cost_per_token: 2.75e-6
  },
  'replicate/meta/llama-2-70b-chat': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 6.5e-7,
    output_cost_per_token: 2.75e-6
  },
  'replicate/meta/llama-2-7b': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-8,
    output_cost_per_token: 2.5e-7
  },
  'replicate/meta/llama-2-7b-chat': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-8,
    output_cost_per_token: 2.5e-7
  },
  'replicate/meta/llama-3-70b': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 6.5e-7,
    output_cost_per_token: 2.75e-6
  },
  'replicate/meta/llama-3-70b-instruct': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 6.5e-7,
    output_cost_per_token: 2.75e-6
  },
  'replicate/meta/llama-3-8b': {
    max_tokens: 8086,
    max_input_tokens: 8086,
    max_output_tokens: 8086,
    input_cost_per_token: 5e-8,
    output_cost_per_token: 2.5e-7
  },
  'replicate/meta/llama-3-8b-instruct': {
    max_tokens: 8086,
    max_input_tokens: 8086,
    max_output_tokens: 8086,
    input_cost_per_token: 5e-8,
    output_cost_per_token: 2.5e-7
  },
  'replicate/mistralai/mistral-7b-v0.1': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-8,
    output_cost_per_token: 2.5e-7
  },
  'replicate/mistralai/mistral-7b-instruct-v0.2': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-8,
    output_cost_per_token: 2.5e-7
  },
  'replicate/mistralai/mixtral-8x7b-instruct-v0.1': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-7,
    output_cost_per_token: 1e-6
  },
  'openrouter/deepseek/deepseek-coder': {
    max_tokens: 4096,
    max_input_tokens: 32000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.4e-7,
    output_cost_per_token: 2.8e-7
  },
  'openrouter/microsoft/wizardlm-2-8x22b:nitro': {
    max_tokens: 65536,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 1e-6
  },
  'openrouter/google/gemini-pro-1.5': {
    max_tokens: 8192,
    max_input_tokens: 1000000,
    max_output_tokens: 8192,
    input_cost_per_token: 2.5e-6,
    output_cost_per_token: 7.5e-6
  },
  'openrouter/mistralai/mixtral-8x22b-instruct': {
    max_tokens: 65536,
    input_cost_per_token: 6.5e-7,
    output_cost_per_token: 6.5e-7
  },
  'openrouter/cohere/command-r-plus': {
    max_tokens: 128000,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'openrouter/databricks/dbrx-instruct': {
    max_tokens: 32768,
    input_cost_per_token: 6e-7,
    output_cost_per_token: 6e-7
  },
  'openrouter/anthropic/claude-3-haiku': {
    max_tokens: 200000,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 1.25e-6
  },
  'openrouter/anthropic/claude-3-5-haiku': {
    max_tokens: 200000,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 5e-6
  },
  'openrouter/anthropic/claude-3-haiku-20240307': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 1.25e-6
  },
  'openrouter/anthropic/claude-3-5-haiku-20241022': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 5e-6
  },
  'openrouter/anthropic/claude-3.5-sonnet': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'openrouter/anthropic/claude-3.5-sonnet:beta': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'openrouter/anthropic/claude-3-sonnet': {
    max_tokens: 200000,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'openrouter/mistralai/mistral-large': {
    max_tokens: 32000,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'openrouter/cognitivecomputations/dolphin-mixtral-8x7b': {
    max_tokens: 32769,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 5e-7
  },
  'openrouter/google/gemini-pro-vision': {
    max_tokens: 45875,
    input_cost_per_token: 1.25e-7,
    output_cost_per_token: 3.75e-7
  },
  'openrouter/fireworks/firellava-13b': {
    max_tokens: 4096,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 2e-7
  },
  'openrouter/meta-llama/llama-3-8b-instruct:free': {
    max_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'openrouter/meta-llama/llama-3-8b-instruct:extended': {
    max_tokens: 16384,
    input_cost_per_token: 2.25e-7,
    output_cost_per_token: 2.25e-6
  },
  'openrouter/meta-llama/llama-3-70b-instruct:nitro': {
    max_tokens: 8192,
    input_cost_per_token: 9e-7,
    output_cost_per_token: 9e-7
  },
  'openrouter/meta-llama/llama-3-70b-instruct': {
    max_tokens: 8192,
    input_cost_per_token: 5.9e-7,
    output_cost_per_token: 7.9e-7
  },
  'openrouter/openai/o1-mini': {
    max_tokens: 65536,
    max_input_tokens: 128000,
    max_output_tokens: 65536,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.2e-5
  },
  'openrouter/openai/o1-mini-2024-09-12': {
    max_tokens: 65536,
    max_input_tokens: 128000,
    max_output_tokens: 65536,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.2e-5
  },
  'openrouter/openai/o1-preview': {
    max_tokens: 32768,
    max_input_tokens: 128000,
    max_output_tokens: 32768,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 6e-5
  },
  'openrouter/openai/o1-preview-2024-09-12': {
    max_tokens: 32768,
    max_input_tokens: 128000,
    max_output_tokens: 32768,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 6e-5
  },
  'openrouter/openai/gpt-4o': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-6,
    output_cost_per_token: 1.5e-5
  },
  'openrouter/openai/gpt-4o-2024-05-13': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-6,
    output_cost_per_token: 1.5e-5
  },
  'openrouter/openai/gpt-4-vision-preview': {
    max_tokens: 130000,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 3e-5
  },
  'openrouter/openai/gpt-3.5-turbo': {
    max_tokens: 4095,
    input_cost_per_token: 1.5e-6,
    output_cost_per_token: 2e-6
  },
  'openrouter/openai/gpt-3.5-turbo-16k': {
    max_tokens: 16383,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 4e-6
  },
  'openrouter/openai/gpt-4': {
    max_tokens: 8192,
    input_cost_per_token: 3e-5,
    output_cost_per_token: 6e-5
  },
  'openrouter/anthropic/claude-instant-v1': {
    max_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 1.63e-6,
    output_cost_per_token: 5.51e-6
  },
  'openrouter/anthropic/claude-2': {
    max_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 1.102e-5,
    output_cost_per_token: 3.268e-5
  },
  'openrouter/anthropic/claude-3-opus': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 7.5e-5
  },
  'openrouter/google/palm-2-chat-bison': {
    max_tokens: 25804,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 5e-7
  },
  'openrouter/google/palm-2-codechat-bison': {
    max_tokens: 20070,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 5e-7
  },
  'openrouter/meta-llama/llama-2-13b-chat': {
    max_tokens: 4096,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 2e-7
  },
  'openrouter/meta-llama/llama-2-70b-chat': {
    max_tokens: 4096,
    input_cost_per_token: 1.5e-6,
    output_cost_per_token: 1.5e-6
  },
  'openrouter/meta-llama/codellama-34b-instruct': {
    max_tokens: 8192,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 5e-7
  },
  'openrouter/nousresearch/nous-hermes-llama2-13b': {
    max_tokens: 4096,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 2e-7
  },
  'openrouter/mancer/weaver': {
    max_tokens: 8000,
    input_cost_per_token: 5.625e-6,
    output_cost_per_token: 5.625e-6
  },
  'openrouter/gryphe/mythomax-l2-13b': {
    max_tokens: 8192,
    input_cost_per_token: 1.875e-6,
    output_cost_per_token: 1.875e-6
  },
  'openrouter/jondurbin/airoboros-l2-70b-2.1': {
    max_tokens: 4096,
    input_cost_per_token: 1.3875e-5,
    output_cost_per_token: 1.3875e-5
  },
  'openrouter/undi95/remm-slerp-l2-13b': {
    max_tokens: 6144,
    input_cost_per_token: 1.875e-6,
    output_cost_per_token: 1.875e-6
  },
  'openrouter/pygmalionai/mythalion-13b': {
    max_tokens: 4096,
    input_cost_per_token: 1.875e-6,
    output_cost_per_token: 1.875e-6
  },
  'openrouter/mistralai/mistral-7b-instruct': {
    max_tokens: 8192,
    input_cost_per_token: 1.3e-7,
    output_cost_per_token: 1.3e-7
  },
  'openrouter/mistralai/mistral-7b-instruct:free': {
    max_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'openrouter/qwen/qwen-2.5-coder-32b-instruct': {
    max_tokens: 33792,
    max_input_tokens: 33792,
    max_output_tokens: 33792,
    input_cost_per_token: 1.8e-7,
    output_cost_per_token: 1.8e-7
  },
  'j2-ultra': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 1.5e-5
  },
  'jamba-1.5-mini@001': {
    max_tokens: 256000,
    max_input_tokens: 256000,
    max_output_tokens: 256000,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 4e-7
  },
  'jamba-1.5-large@001': {
    max_tokens: 256000,
    max_input_tokens: 256000,
    max_output_tokens: 256000,
    input_cost_per_token: 2e-6,
    output_cost_per_token: 8e-6
  },
  'jamba-1.5': {
    max_tokens: 256000,
    max_input_tokens: 256000,
    max_output_tokens: 256000,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 4e-7
  },
  'jamba-1.5-mini': {
    max_tokens: 256000,
    max_input_tokens: 256000,
    max_output_tokens: 256000,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 4e-7
  },
  'jamba-1.5-large': {
    max_tokens: 256000,
    max_input_tokens: 256000,
    max_output_tokens: 256000,
    input_cost_per_token: 2e-6,
    output_cost_per_token: 8e-6
  },
  'j2-mid': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 1e-5,
    output_cost_per_token: 1e-5
  },
  'j2-light': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 3e-6
  },
  dolphin: {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 16384,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 5e-7
  },
  chatdolphin: {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 16384,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 5e-7
  },
  'luminous-base': {
    max_tokens: 2048,
    input_cost_per_token: 3e-5,
    output_cost_per_token: 3.3e-5
  },
  'luminous-base-control': {
    max_tokens: 2048,
    input_cost_per_token: 3.75e-5,
    output_cost_per_token: 4.125e-5
  },
  'luminous-extended': {
    max_tokens: 2048,
    input_cost_per_token: 4.5e-5,
    output_cost_per_token: 4.95e-5
  },
  'luminous-extended-control': {
    max_tokens: 2048,
    input_cost_per_token: 5.625e-5,
    output_cost_per_token: 6.1875e-5
  },
  'luminous-supreme': {
    max_tokens: 2048,
    input_cost_per_token: 0.000175,
    output_cost_per_token: 0.0001925
  },
  'luminous-supreme-control': {
    max_tokens: 2048,
    input_cost_per_token: 0.00021875,
    output_cost_per_token: 0.000240625
  },
  'ai21.j2-mid-v1': {
    max_tokens: 8191,
    max_input_tokens: 8191,
    max_output_tokens: 8191,
    input_cost_per_token: 1.25e-5,
    output_cost_per_token: 1.25e-5
  },
  'ai21.j2-ultra-v1': {
    max_tokens: 8191,
    max_input_tokens: 8191,
    max_output_tokens: 8191,
    input_cost_per_token: 1.88e-5,
    output_cost_per_token: 1.88e-5
  },
  'ai21.jamba-instruct-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 70000,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 7e-7
  },
  'amazon.titan-text-lite-v1': {
    max_tokens: 4000,
    max_input_tokens: 42000,
    max_output_tokens: 4000,
    input_cost_per_token: 3e-7,
    output_cost_per_token: 4e-7
  },
  'amazon.titan-text-express-v1': {
    max_tokens: 8000,
    max_input_tokens: 42000,
    max_output_tokens: 8000,
    input_cost_per_token: 1.3e-6,
    output_cost_per_token: 1.7e-6
  },
  'amazon.titan-text-premier-v1:0': {
    max_tokens: 32000,
    max_input_tokens: 42000,
    max_output_tokens: 32000,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 1.5e-6
  },
  'amazon.titan-embed-text-v1': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'amazon.titan-embed-text-v2:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 0.0
  },
  'amazon.titan-embed-image-v1': {
    max_tokens: 128,
    max_input_tokens: 128,
    input_cost_per_token: 8e-7,
    output_cost_per_token: 0.0
  },
  'mistral.mistral-7b-instruct-v0:2': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 2e-7
  },
  'mistral.mixtral-8x7b-instruct-v0:1': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 4.5e-7,
    output_cost_per_token: 7e-7
  },
  'mistral.mistral-large-2402-v1:0': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'mistral.mistral-large-2407-v1:0': {
    max_tokens: 8191,
    max_input_tokens: 128000,
    max_output_tokens: 8191,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 9e-6
  },
  'mistral.mistral-small-2402-v1:0': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 3e-6
  },
  'bedrock/us-west-2/mistral.mixtral-8x7b-instruct-v0:1': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 4.5e-7,
    output_cost_per_token: 7e-7
  },
  'bedrock/us-east-1/mistral.mixtral-8x7b-instruct-v0:1': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 4.5e-7,
    output_cost_per_token: 7e-7
  },
  'bedrock/eu-west-3/mistral.mixtral-8x7b-instruct-v0:1': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 5.9e-7,
    output_cost_per_token: 9.1e-7
  },
  'bedrock/us-west-2/mistral.mistral-7b-instruct-v0:2': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 2e-7
  },
  'bedrock/us-east-1/mistral.mistral-7b-instruct-v0:2': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 2e-7
  },
  'bedrock/eu-west-3/mistral.mistral-7b-instruct-v0:2': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 2.6e-7
  },
  'bedrock/us-east-1/mistral.mistral-large-2402-v1:0': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/us-west-2/mistral.mistral-large-2402-v1:0': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/eu-west-3/mistral.mistral-large-2402-v1:0': {
    max_tokens: 8191,
    max_input_tokens: 32000,
    max_output_tokens: 8191,
    input_cost_per_token: 1.04e-5,
    output_cost_per_token: 3.12e-5
  },
  'amazon.nova-micro-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 300000,
    max_output_tokens: 4096,
    input_cost_per_token: 3.5e-8,
    output_cost_per_token: 1.4e-7
  },
  'amazon.nova-lite-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 6e-8,
    output_cost_per_token: 2.4e-7
  },
  'amazon.nova-pro-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 300000,
    max_output_tokens: 4096,
    input_cost_per_token: 8e-7,
    output_cost_per_token: 3.2e-6
  },
  'anthropic.claude-3-sonnet-20240229-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'anthropic.claude-3-5-sonnet-20240620-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'anthropic.claude-3-5-sonnet-20241022-v2:0': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'anthropic.claude-3-haiku-20240307-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 1.25e-6
  },
  'anthropic.claude-3-5-haiku-20241022-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 5e-6
  },
  'anthropic.claude-3-opus-20240229-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 7.5e-5
  },
  'us.anthropic.claude-3-sonnet-20240229-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'us.anthropic.claude-3-5-sonnet-20240620-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'us.anthropic.claude-3-5-sonnet-20241022-v2:0': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'us.anthropic.claude-3-haiku-20240307-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 1.25e-6
  },
  'us.anthropic.claude-3-5-haiku-20241022-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 5e-6
  },
  'us.anthropic.claude-3-opus-20240229-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 7.5e-5
  },
  'eu.anthropic.claude-3-sonnet-20240229-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'eu.anthropic.claude-3-5-sonnet-20240620-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'eu.anthropic.claude-3-5-sonnet-20241022-v2:0': {
    max_tokens: 8192,
    max_input_tokens: 200000,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'eu.anthropic.claude-3-haiku-20240307-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 1.25e-6
  },
  'eu.anthropic.claude-3-5-haiku-20241022-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 5e-6
  },
  'eu.anthropic.claude-3-opus-20240229-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-5,
    output_cost_per_token: 7.5e-5
  },
  'anthropic.claude-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/us-east-1/anthropic.claude-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/us-west-2/anthropic.claude-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/ap-northeast-1/anthropic.claude-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/eu-central-1/anthropic.claude-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/eu-central-1/1-month-commitment/anthropic.claude-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/eu-central-1/6-month-commitment/anthropic.claude-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-east-1/1-month-commitment/anthropic.claude-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-east-1/6-month-commitment/anthropic.claude-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-west-2/1-month-commitment/anthropic.claude-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-west-2/6-month-commitment/anthropic.claude-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'anthropic.claude-v2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/us-east-1/anthropic.claude-v2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/us-west-2/anthropic.claude-v2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/ap-northeast-1/anthropic.claude-v2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-v2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-v2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/eu-central-1/anthropic.claude-v2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/eu-central-1/1-month-commitment/anthropic.claude-v2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/eu-central-1/6-month-commitment/anthropic.claude-v2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-east-1/1-month-commitment/anthropic.claude-v2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-east-1/6-month-commitment/anthropic.claude-v2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-west-2/1-month-commitment/anthropic.claude-v2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-west-2/6-month-commitment/anthropic.claude-v2': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'anthropic.claude-v2:1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/us-east-1/anthropic.claude-v2:1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/us-west-2/anthropic.claude-v2:1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/ap-northeast-1/anthropic.claude-v2:1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-v2:1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-v2:1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/eu-central-1/anthropic.claude-v2:1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-6,
    output_cost_per_token: 2.4e-5
  },
  'bedrock/eu-central-1/1-month-commitment/anthropic.claude-v2:1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/eu-central-1/6-month-commitment/anthropic.claude-v2:1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-east-1/1-month-commitment/anthropic.claude-v2:1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-east-1/6-month-commitment/anthropic.claude-v2:1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-west-2/1-month-commitment/anthropic.claude-v2:1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-west-2/6-month-commitment/anthropic.claude-v2:1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'anthropic.claude-instant-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 1.63e-6,
    output_cost_per_token: 5.51e-6
  },
  'bedrock/us-east-1/anthropic.claude-instant-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-7,
    output_cost_per_token: 2.4e-6
  },
  'bedrock/us-east-1/1-month-commitment/anthropic.claude-instant-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-east-1/6-month-commitment/anthropic.claude-instant-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-west-2/1-month-commitment/anthropic.claude-instant-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-west-2/6-month-commitment/anthropic.claude-instant-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/us-west-2/anthropic.claude-instant-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 8e-7,
    output_cost_per_token: 2.4e-6
  },
  'bedrock/ap-northeast-1/anthropic.claude-instant-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 2.23e-6,
    output_cost_per_token: 7.55e-6
  },
  'bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-instant-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-instant-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/eu-central-1/anthropic.claude-instant-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191,
    input_cost_per_token: 2.48e-6,
    output_cost_per_token: 8.38e-6
  },
  'bedrock/eu-central-1/1-month-commitment/anthropic.claude-instant-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'bedrock/eu-central-1/6-month-commitment/anthropic.claude-instant-v1': {
    max_tokens: 8191,
    max_input_tokens: 100000,
    max_output_tokens: 8191
  },
  'cohere.command-text-v14': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-6,
    output_cost_per_token: 2e-6
  },
  'bedrock/*/1-month-commitment/cohere.command-text-v14': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096
  },
  'bedrock/*/6-month-commitment/cohere.command-text-v14': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096
  },
  'cohere.command-light-text-v14': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-7,
    output_cost_per_token: 6e-7
  },
  'bedrock/*/1-month-commitment/cohere.command-light-text-v14': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096
  },
  'bedrock/*/6-month-commitment/cohere.command-light-text-v14': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096
  },
  'cohere.command-r-plus-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 1.5e-5
  },
  'cohere.command-r-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 1.5e-6
  },
  'cohere.embed-english-v3': {
    max_tokens: 512,
    max_input_tokens: 512,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'cohere.embed-multilingual-v3': {
    max_tokens: 512,
    max_input_tokens: 512,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'meta.llama3-3-70b-instruct-v1:0': {
    max_tokens: 4096,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 7.2e-7,
    output_cost_per_token: 7.2e-7
  },
  'meta.llama2-13b-chat-v1': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 7.5e-7,
    output_cost_per_token: 1e-6
  },
  'meta.llama2-70b-chat-v1': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1.95e-6,
    output_cost_per_token: 2.56e-6
  },
  'meta.llama3-8b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-7,
    output_cost_per_token: 6e-7
  },
  'bedrock/us-east-1/meta.llama3-8b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-7,
    output_cost_per_token: 6e-7
  },
  'bedrock/us-west-1/meta.llama3-8b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 3e-7,
    output_cost_per_token: 6e-7
  },
  'bedrock/ap-south-1/meta.llama3-8b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 3.6e-7,
    output_cost_per_token: 7.2e-7
  },
  'bedrock/ca-central-1/meta.llama3-8b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 3.5e-7,
    output_cost_per_token: 6.9e-7
  },
  'bedrock/eu-west-1/meta.llama3-8b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 3.2e-7,
    output_cost_per_token: 6.5e-7
  },
  'bedrock/eu-west-2/meta.llama3-8b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 3.9e-7,
    output_cost_per_token: 7.8e-7
  },
  'bedrock/sa-east-1/meta.llama3-8b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 5e-7,
    output_cost_per_token: 1.01e-6
  },
  'meta.llama3-70b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 2.65e-6,
    output_cost_per_token: 3.5e-6
  },
  'bedrock/us-east-1/meta.llama3-70b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 2.65e-6,
    output_cost_per_token: 3.5e-6
  },
  'bedrock/us-west-1/meta.llama3-70b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 2.65e-6,
    output_cost_per_token: 3.5e-6
  },
  'bedrock/ap-south-1/meta.llama3-70b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 3.18e-6,
    output_cost_per_token: 4.2e-6
  },
  'bedrock/ca-central-1/meta.llama3-70b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 3.05e-6,
    output_cost_per_token: 4.03e-6
  },
  'bedrock/eu-west-1/meta.llama3-70b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 2.86e-6,
    output_cost_per_token: 3.78e-6
  },
  'bedrock/eu-west-2/meta.llama3-70b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 3.45e-6,
    output_cost_per_token: 4.55e-6
  },
  'bedrock/sa-east-1/meta.llama3-70b-instruct-v1:0': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 4.45e-6,
    output_cost_per_token: 5.88e-6
  },
  'meta.llama3-1-8b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 2048,
    input_cost_per_token: 2.2e-7,
    output_cost_per_token: 2.2e-7
  },
  'us.meta.llama3-1-8b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 2048,
    input_cost_per_token: 2.2e-7,
    output_cost_per_token: 2.2e-7
  },
  'meta.llama3-1-70b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 2048,
    input_cost_per_token: 9.9e-7,
    output_cost_per_token: 9.9e-7
  },
  'us.meta.llama3-1-70b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 2048,
    input_cost_per_token: 9.9e-7,
    output_cost_per_token: 9.9e-7
  },
  'meta.llama3-1-405b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 5.32e-6,
    output_cost_per_token: 1.6e-5
  },
  'us.meta.llama3-1-405b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 5.32e-6,
    output_cost_per_token: 1.6e-5
  },
  'meta.llama3-2-1b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 1e-7
  },
  'us.meta.llama3-2-1b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 1e-7
  },
  'eu.meta.llama3-2-1b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.3e-7,
    output_cost_per_token: 1.3e-7
  },
  'meta.llama3-2-3b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 1.5e-7
  },
  'us.meta.llama3-2-3b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 1.5e-7
  },
  'eu.meta.llama3-2-3b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.9e-7,
    output_cost_per_token: 1.9e-7
  },
  'meta.llama3-2-11b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 3.5e-7,
    output_cost_per_token: 3.5e-7
  },
  'us.meta.llama3-2-11b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 3.5e-7,
    output_cost_per_token: 3.5e-7
  },
  'meta.llama3-2-90b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 2e-6,
    output_cost_per_token: 2e-6
  },
  'us.meta.llama3-2-90b-instruct-v1:0': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 4096,
    input_cost_per_token: 2e-6,
    output_cost_per_token: 2e-6
  },
  '512-x-512/50-steps/stability.stable-diffusion-xl-v0': {
    max_tokens: 77,
    max_input_tokens: 77
  },
  '512-x-512/max-steps/stability.stable-diffusion-xl-v0': {
    max_tokens: 77,
    max_input_tokens: 77
  },
  'max-x-max/50-steps/stability.stable-diffusion-xl-v0': {
    max_tokens: 77,
    max_input_tokens: 77
  },
  'max-x-max/max-steps/stability.stable-diffusion-xl-v0': {
    max_tokens: 77,
    max_input_tokens: 77
  },
  '1024-x-1024/50-steps/stability.stable-diffusion-xl-v1': {
    max_tokens: 77,
    max_input_tokens: 77
  },
  '1024-x-1024/max-steps/stability.stable-diffusion-xl-v1': {
    max_tokens: 77,
    max_input_tokens: 77
  },
  'stability.sd3-large-v1:0': {
    max_tokens: 77,
    max_input_tokens: 77
  },
  'stability.stable-image-ultra-v1:0': {
    max_tokens: 77,
    max_input_tokens: 77
  },
  'sagemaker/meta-textgeneration-llama-2-7b': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'sagemaker/meta-textgeneration-llama-2-7b-f': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'sagemaker/meta-textgeneration-llama-2-13b': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'sagemaker/meta-textgeneration-llama-2-13b-f': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'sagemaker/meta-textgeneration-llama-2-70b': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'sagemaker/meta-textgeneration-llama-2-70b-b-f': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'together-ai-up-to-4b': {
    input_cost_per_token: 1e-7,
    output_cost_per_token: 1e-7
  },
  'together-ai-4.1b-8b': {
    input_cost_per_token: 2e-7,
    output_cost_per_token: 2e-7
  },
  'together-ai-8.1b-21b': {
    max_tokens: 1000,
    input_cost_per_token: 3e-7,
    output_cost_per_token: 3e-7
  },
  'together-ai-21.1b-41b': {
    input_cost_per_token: 8e-7,
    output_cost_per_token: 8e-7
  },
  'together-ai-41.1b-80b': {
    input_cost_per_token: 9e-7,
    output_cost_per_token: 9e-7
  },
  'together-ai-81.1b-110b': {
    input_cost_per_token: 1.8e-6,
    output_cost_per_token: 1.8e-6
  },
  'together-ai-embedding-up-to-150m': {
    input_cost_per_token: 8e-9,
    output_cost_per_token: 0.0
  },
  'together-ai-embedding-151m-to-350m': {
    input_cost_per_token: 1.6e-8,
    output_cost_per_token: 0.0
  },
  'together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo': {
    input_cost_per_token: 1.8e-7,
    output_cost_per_token: 1.8e-7
  },
  'together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo': {
    input_cost_per_token: 8.8e-7,
    output_cost_per_token: 8.8e-7
  },
  'together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo': {
    input_cost_per_token: 3.5e-6,
    output_cost_per_token: 3.5e-6
  },
  'together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1': {
    input_cost_per_token: 6e-7,
    output_cost_per_token: 6e-7
  },
  'together_ai/mistralai/Mistral-7B-Instruct-v0.1': {},
  'together_ai/togethercomputer/CodeLlama-34b-Instruct': {},
  'ollama/codegemma': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/codegeex4': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/deepseek-coder-v2-instruct': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/deepseek-coder-v2-base': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/deepseek-coder-v2-lite-instruct': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/deepseek-coder-v2-lite-base': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/internlm2_5-20b-chat': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/llama2': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/llama2:7b': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/llama2:13b': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/llama2:70b': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/llama2-uncensored': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/llama3': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/llama3:8b': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/llama3:70b': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/llama3.1': {
    max_tokens: 32768,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/mistral-large-instruct-2407': {
    max_tokens: 65536,
    max_input_tokens: 65536,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/mistral': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/mistral-7B-Instruct-v0.1': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/mistral-7B-Instruct-v0.2': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 32768,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/mixtral-8x7B-Instruct-v0.1': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 32768,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/mixtral-8x22B-Instruct-v0.1': {
    max_tokens: 65536,
    max_input_tokens: 65536,
    max_output_tokens: 65536,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/codellama': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/orca-mini': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'ollama/vicuna': {
    max_tokens: 2048,
    max_input_tokens: 2048,
    max_output_tokens: 2048,
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'deepinfra/lizpreciatior/lzlv_70b_fp16_hf': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 7e-7,
    output_cost_per_token: 9e-7
  },
  'deepinfra/Gryphe/MythoMax-L2-13b': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 2.2e-7,
    output_cost_per_token: 2.2e-7
  },
  'deepinfra/mistralai/Mistral-7B-Instruct-v0.1': {
    max_tokens: 8191,
    max_input_tokens: 32768,
    max_output_tokens: 8191,
    input_cost_per_token: 1.3e-7,
    output_cost_per_token: 1.3e-7
  },
  'deepinfra/meta-llama/Llama-2-70b-chat-hf': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 7e-7,
    output_cost_per_token: 9e-7
  },
  'deepinfra/cognitivecomputations/dolphin-2.6-mixtral-8x7b': {
    max_tokens: 8191,
    max_input_tokens: 32768,
    max_output_tokens: 8191,
    input_cost_per_token: 2.7e-7,
    output_cost_per_token: 2.7e-7
  },
  'deepinfra/codellama/CodeLlama-34b-Instruct-hf': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 6e-7,
    output_cost_per_token: 6e-7
  },
  'deepinfra/deepinfra/mixtral': {
    max_tokens: 4096,
    max_input_tokens: 32000,
    max_output_tokens: 4096,
    input_cost_per_token: 2.7e-7,
    output_cost_per_token: 2.7e-7
  },
  'deepinfra/Phind/Phind-CodeLlama-34B-v2': {
    max_tokens: 4096,
    max_input_tokens: 16384,
    max_output_tokens: 4096,
    input_cost_per_token: 6e-7,
    output_cost_per_token: 6e-7
  },
  'deepinfra/mistralai/Mixtral-8x7B-Instruct-v0.1': {
    max_tokens: 8191,
    max_input_tokens: 32768,
    max_output_tokens: 8191,
    input_cost_per_token: 2.7e-7,
    output_cost_per_token: 2.7e-7
  },
  'deepinfra/deepinfra/airoboros-70b': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 7e-7,
    output_cost_per_token: 9e-7
  },
  'deepinfra/01-ai/Yi-34B-Chat': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 6e-7,
    output_cost_per_token: 6e-7
  },
  'deepinfra/01-ai/Yi-6B-200K': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 1.3e-7,
    output_cost_per_token: 1.3e-7
  },
  'deepinfra/jondurbin/airoboros-l2-70b-gpt4-1.4.1': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 7e-7,
    output_cost_per_token: 9e-7
  },
  'deepinfra/meta-llama/Llama-2-13b-chat-hf': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 2.2e-7,
    output_cost_per_token: 2.2e-7
  },
  'deepinfra/amazon/MistralLite': {
    max_tokens: 8191,
    max_input_tokens: 32768,
    max_output_tokens: 8191,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 2e-7
  },
  'deepinfra/meta-llama/Llama-2-7b-chat-hf': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1.3e-7,
    output_cost_per_token: 1.3e-7
  },
  'deepinfra/meta-llama/Meta-Llama-3-8B-Instruct': {
    max_tokens: 8191,
    max_input_tokens: 8191,
    max_output_tokens: 4096,
    input_cost_per_token: 8e-8,
    output_cost_per_token: 8e-8
  },
  'deepinfra/meta-llama/Meta-Llama-3-70B-Instruct': {
    max_tokens: 8191,
    max_input_tokens: 8191,
    max_output_tokens: 4096,
    input_cost_per_token: 5.9e-7,
    output_cost_per_token: 7.9e-7
  },
  'deepinfra/meta-llama/Meta-Llama-3.1-405B-Instruct': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 32768,
    input_cost_per_token: 9e-7,
    output_cost_per_token: 9e-7
  },
  'deepinfra/01-ai/Yi-34B-200K': {
    max_tokens: 4096,
    max_input_tokens: 200000,
    max_output_tokens: 4096,
    input_cost_per_token: 6e-7,
    output_cost_per_token: 6e-7
  },
  'deepinfra/openchat/openchat_3.5': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1.3e-7,
    output_cost_per_token: 1.3e-7
  },
  'perplexity/codellama-34b-instruct': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 16384,
    input_cost_per_token: 3.5e-7,
    output_cost_per_token: 1.4e-6
  },
  'perplexity/codellama-70b-instruct': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 16384,
    input_cost_per_token: 7e-7,
    output_cost_per_token: 2.8e-6
  },
  'perplexity/llama-3.1-70b-instruct': {
    max_tokens: 131072,
    max_input_tokens: 131072,
    max_output_tokens: 131072,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 1e-6
  },
  'perplexity/llama-3.1-8b-instruct': {
    max_tokens: 131072,
    max_input_tokens: 131072,
    max_output_tokens: 131072,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 2e-7
  },
  'perplexity/llama-3.1-sonar-huge-128k-online': {
    max_tokens: 127072,
    max_input_tokens: 127072,
    max_output_tokens: 127072,
    input_cost_per_token: 5e-6,
    output_cost_per_token: 5e-6
  },
  'perplexity/llama-3.1-sonar-large-128k-online': {
    max_tokens: 127072,
    max_input_tokens: 127072,
    max_output_tokens: 127072,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 1e-6
  },
  'perplexity/llama-3.1-sonar-large-128k-chat': {
    max_tokens: 131072,
    max_input_tokens: 131072,
    max_output_tokens: 131072,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 1e-6
  },
  'perplexity/llama-3.1-sonar-small-128k-chat': {
    max_tokens: 131072,
    max_input_tokens: 131072,
    max_output_tokens: 131072,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 2e-7
  },
  'perplexity/llama-3.1-sonar-small-128k-online': {
    max_tokens: 127072,
    max_input_tokens: 127072,
    max_output_tokens: 127072,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 2e-7
  },
  'perplexity/pplx-7b-chat': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 7e-8,
    output_cost_per_token: 2.8e-7
  },
  'perplexity/pplx-70b-chat': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 7e-7,
    output_cost_per_token: 2.8e-6
  },
  'perplexity/pplx-7b-online': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 2.8e-7
  },
  'perplexity/pplx-70b-online': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 0.0,
    output_cost_per_token: 2.8e-6
  },
  'perplexity/llama-2-70b-chat': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 7e-7,
    output_cost_per_token: 2.8e-6
  },
  'perplexity/mistral-7b-instruct': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 7e-8,
    output_cost_per_token: 2.8e-7
  },
  'perplexity/mixtral-8x7b-instruct': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 7e-8,
    output_cost_per_token: 2.8e-7
  },
  'perplexity/sonar-small-chat': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 16384,
    input_cost_per_token: 7e-8,
    output_cost_per_token: 2.8e-7
  },
  'perplexity/sonar-small-online': {
    max_tokens: 12000,
    max_input_tokens: 12000,
    max_output_tokens: 12000,
    input_cost_per_token: 0,
    output_cost_per_token: 2.8e-7
  },
  'perplexity/sonar-medium-chat': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 16384,
    input_cost_per_token: 6e-7,
    output_cost_per_token: 1.8e-6
  },
  'perplexity/sonar-medium-online': {
    max_tokens: 12000,
    max_input_tokens: 12000,
    max_output_tokens: 12000,
    input_cost_per_token: 0,
    output_cost_per_token: 1.8e-6
  },
  'fireworks_ai/accounts/fireworks/models/llama-v3p2-1b-instruct': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 16384,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 1e-7
  },
  'fireworks_ai/accounts/fireworks/models/llama-v3p2-3b-instruct': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 16384,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 1e-7
  },
  'fireworks_ai/accounts/fireworks/models/llama-v3p2-11b-vision-instruct': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 16384,
    input_cost_per_token: 2e-7,
    output_cost_per_token: 2e-7
  },
  'accounts/fireworks/models/llama-v3p2-90b-vision-instruct': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 16384,
    input_cost_per_token: 9e-7,
    output_cost_per_token: 9e-7
  },
  'fireworks_ai/accounts/fireworks/models/firefunction-v2': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 9e-7,
    output_cost_per_token: 9e-7
  },
  'fireworks_ai/accounts/fireworks/models/mixtral-8x22b-instruct-hf': {
    max_tokens: 65536,
    max_input_tokens: 65536,
    max_output_tokens: 65536,
    input_cost_per_token: 1.2e-6,
    output_cost_per_token: 1.2e-6
  },
  'fireworks_ai/accounts/fireworks/models/qwen2-72b-instruct': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 32768,
    input_cost_per_token: 9e-7,
    output_cost_per_token: 9e-7
  },
  'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 9e-7,
    output_cost_per_token: 9e-7
  },
  'fireworks_ai/accounts/fireworks/models/yi-large': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 32768,
    input_cost_per_token: 3e-6,
    output_cost_per_token: 3e-6
  },
  'fireworks_ai/accounts/fireworks/models/deepseek-coder-v2-instruct': {
    max_tokens: 65536,
    max_input_tokens: 65536,
    max_output_tokens: 8192,
    input_cost_per_token: 1.2e-6,
    output_cost_per_token: 1.2e-6
  },
  'fireworks_ai/nomic-ai/nomic-embed-text-v1.5': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    input_cost_per_token: 8e-9,
    output_cost_per_token: 0.0
  },
  'fireworks_ai/nomic-ai/nomic-embed-text-v1': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    input_cost_per_token: 8e-9,
    output_cost_per_token: 0.0
  },
  'fireworks_ai/WhereIsAI/UAE-Large-V1': {
    max_tokens: 512,
    max_input_tokens: 512,
    input_cost_per_token: 1.6e-8,
    output_cost_per_token: 0.0
  },
  'fireworks_ai/thenlper/gte-large': {
    max_tokens: 512,
    max_input_tokens: 512,
    input_cost_per_token: 1.6e-8,
    output_cost_per_token: 0.0
  },
  'fireworks_ai/thenlper/gte-base': {
    max_tokens: 512,
    max_input_tokens: 512,
    input_cost_per_token: 8e-9,
    output_cost_per_token: 0.0
  },
  'fireworks-ai-up-to-16b': {
    input_cost_per_token: 2e-7,
    output_cost_per_token: 2e-7
  },
  'fireworks-ai-16.1b-to-80b': {
    input_cost_per_token: 9e-7,
    output_cost_per_token: 9e-7
  },
  'fireworks-ai-moe-up-to-56b': {
    input_cost_per_token: 5e-7,
    output_cost_per_token: 5e-7
  },
  'fireworks-ai-56b-to-176b': {
    input_cost_per_token: 1.2e-6,
    output_cost_per_token: 1.2e-6
  },
  'fireworks-ai-default': {
    input_cost_per_token: 0.0,
    output_cost_per_token: 0.0
  },
  'fireworks-ai-embedding-up-to-150m': {
    input_cost_per_token: 8e-9,
    output_cost_per_token: 0.0
  },
  'fireworks-ai-embedding-150m-to-350m': {
    input_cost_per_token: 1.6e-8,
    output_cost_per_token: 0.0
  },
  'anyscale/mistralai/Mistral-7B-Instruct-v0.1': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 16384,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 1.5e-7
  },
  'anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 16384,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 1.5e-7
  },
  'anyscale/mistralai/Mixtral-8x22B-Instruct-v0.1': {
    max_tokens: 65536,
    max_input_tokens: 65536,
    max_output_tokens: 65536,
    input_cost_per_token: 9e-7,
    output_cost_per_token: 9e-7
  },
  'anyscale/HuggingFaceH4/zephyr-7b-beta': {
    max_tokens: 16384,
    max_input_tokens: 16384,
    max_output_tokens: 16384,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 1.5e-7
  },
  'anyscale/google/gemma-7b-it': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 1.5e-7
  },
  'anyscale/meta-llama/Llama-2-7b-chat-hf': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 1.5e-7
  },
  'anyscale/meta-llama/Llama-2-13b-chat-hf': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 2.5e-7,
    output_cost_per_token: 2.5e-7
  },
  'anyscale/meta-llama/Llama-2-70b-chat-hf': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 1e-6
  },
  'anyscale/codellama/CodeLlama-34b-Instruct-hf': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 1e-6
  },
  'anyscale/codellama/CodeLlama-70b-Instruct-hf': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 1e-6
  },
  'anyscale/meta-llama/Meta-Llama-3-8B-Instruct': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 1.5e-7,
    output_cost_per_token: 1.5e-7
  },
  'anyscale/meta-llama/Meta-Llama-3-70B-Instruct': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 1e-6,
    output_cost_per_token: 1e-6
  },
  'cloudflare/@cf/meta/llama-2-7b-chat-fp16': {
    max_tokens: 3072,
    max_input_tokens: 3072,
    max_output_tokens: 3072,
    input_cost_per_token: 1.923e-6,
    output_cost_per_token: 1.923e-6
  },
  'cloudflare/@cf/meta/llama-2-7b-chat-int8': {
    max_tokens: 2048,
    max_input_tokens: 2048,
    max_output_tokens: 2048,
    input_cost_per_token: 1.923e-6,
    output_cost_per_token: 1.923e-6
  },
  'cloudflare/@cf/mistral/mistral-7b-instruct-v0.1': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 1.923e-6,
    output_cost_per_token: 1.923e-6
  },
  'cloudflare/@hf/thebloke/codellama-7b-instruct-awq': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 1.923e-6,
    output_cost_per_token: 1.923e-6
  },
  'voyage/voyage-01': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'voyage/voyage-lite-01': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'voyage/voyage-large-2': {
    max_tokens: 16000,
    max_input_tokens: 16000,
    input_cost_per_token: 1.2e-7,
    output_cost_per_token: 0.0
  },
  'voyage/voyage-law-2': {
    max_tokens: 16000,
    max_input_tokens: 16000,
    input_cost_per_token: 1.2e-7,
    output_cost_per_token: 0.0
  },
  'voyage/voyage-code-2': {
    max_tokens: 16000,
    max_input_tokens: 16000,
    input_cost_per_token: 1.2e-7,
    output_cost_per_token: 0.0
  },
  'voyage/voyage-2': {
    max_tokens: 4000,
    max_input_tokens: 4000,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'voyage/voyage-lite-02-instruct': {
    max_tokens: 4000,
    max_input_tokens: 4000,
    input_cost_per_token: 1e-7,
    output_cost_per_token: 0.0
  },
  'voyage/voyage-finance-2': {
    max_tokens: 4000,
    max_input_tokens: 4000,
    input_cost_per_token: 1.2e-7,
    output_cost_per_token: 0.0
  },
  'databricks/databricks-meta-llama-3-1-405b-instruct': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 5e-6,
    output_cost_per_token: 1.500002e-5
  },
  'databricks/databricks-meta-llama-3-1-70b-instruct': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 1.00002e-6,
    output_cost_per_token: 2.99999e-6
  },
  'databricks/meta-llama-3.3-70b-instruct': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 1.00002e-6,
    output_cost_per_token: 2.99999e-6
  },
  'databricks/databricks-dbrx-instruct': {
    max_tokens: 32768,
    max_input_tokens: 32768,
    max_output_tokens: 32768,
    input_cost_per_token: 7.4998e-7,
    output_cost_per_token: 2.24901e-6
  },
  'databricks/databricks-meta-llama-3-70b-instruct': {
    max_tokens: 128000,
    max_input_tokens: 128000,
    max_output_tokens: 128000,
    input_cost_per_token: 1.00002e-6,
    output_cost_per_token: 2.99999e-6
  },
  'databricks/databricks-llama-2-70b-chat': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 5.0001e-7,
    output_cost_per_token: 1.5e-6
  },
  'databricks/databricks-mixtral-8x7b-instruct': {
    max_tokens: 4096,
    max_input_tokens: 4096,
    max_output_tokens: 4096,
    input_cost_per_token: 5.0001e-7,
    output_cost_per_token: 9.9902e-7
  },
  'databricks/databricks-mpt-30b-instruct': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 9.9902e-7,
    output_cost_per_token: 9.9902e-7
  },
  'databricks/databricks-mpt-7b-instruct': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    max_output_tokens: 8192,
    input_cost_per_token: 5.0001e-7,
    output_cost_per_token: 0.0
  },
  'databricks/databricks-bge-large-en': {
    max_tokens: 512,
    max_input_tokens: 512,
    input_cost_per_token: 1.0003e-7,
    output_cost_per_token: 0.0
  },
  'databricks/databricks-gte-large-en': {
    max_tokens: 8192,
    max_input_tokens: 8192,
    input_cost_per_token: 1.2999e-7,
    output_cost_per_token: 0.0
  }
};



---
File: /burr/telemetry/ui/src/components/common/pagination.tsx
---

import clsx from 'clsx';
import type React from 'react';
import { Button } from './button';

export function Pagination({
  'aria-label': ariaLabel = 'Page navigation',
  className,
  ...props
}: React.ComponentPropsWithoutRef<'nav'>) {
  return <nav aria-label={ariaLabel} {...props} className={clsx(className, 'flex gap-x-2')} />;
}

export function PaginationPrevious({
  href = null,
  className,
  children = 'Previous'
}: React.PropsWithChildren<{ href?: string | null; className?: string }>) {
  return (
    <span className={clsx(className, 'grow basis-0')}>
      <Button {...(href === null ? { disabled: true } : { href })} plain aria-label="Previous page">
        <svg
          className="stroke-current"
          data-slot="icon"
          viewBox="0 0 16 16"
          fill="none"
          aria-hidden="true"
        >
          <path
            d="M2.75 8H13.25M2.75 8L5.25 5.5M2.75 8L5.25 10.5"
            strokeWidth={1.5}
            strokeLinecap="round"
            strokeLinejoin="round"
          />
        </svg>
        {children}
      </Button>
    </span>
  );
}

export function PaginationNext({
  href = null,
  className,
  children = 'Next'
}: React.PropsWithChildren<{ href?: string | null; className?: string }>) {
  return (
    <span className={clsx(className, 'flex grow basis-0 justify-end')}>
      <Button {...(href === null ? { disabled: true } : { href })} plain aria-label="Next page">
        {children}
        <svg
          className="stroke-current"
          data-slot="icon"
          viewBox="0 0 16 16"
          fill="none"
          aria-hidden="true"
        >
          <path
            d="M13.25 8L2.75 8M13.25 8L10.75 10.5M13.25 8L10.75 5.5"
            strokeWidth={1.5}
            strokeLinecap="round"
            strokeLinejoin="round"
          />
        </svg>
      </Button>
    </span>
  );
}

export function PaginationList({ className, ...props }: React.ComponentPropsWithoutRef<'span'>) {
  return <span {...props} className={clsx(className, 'hidden items-baseline gap-x-2 sm:flex')} />;
}

export function PaginationPage({
  href,
  className,
  current = false,
  children
}: React.PropsWithChildren<{ href: string; className?: string; current?: boolean }>) {
  return (
    <Button
      href={href}
      plain
      aria-label={`Page ${children}`}
      aria-current={current ? 'page' : undefined}
      className={clsx(
        className,
        'min-w-[2.25rem] before:absolute before:-inset-px before:rounded-lg',
        current && 'before:bg-zinc-950/5 dark:before:bg-white/10'
      )}
    >
      <span className="-mx-0.5">{children}</span>
    </Button>
  );
}

export function PaginationGap({
  className,
  children = <>&hellip;</>,
  ...props
}: React.ComponentPropsWithoutRef<'span'>) {
  return (
    <span
      aria-hidden="true"
      {...props}
      className={clsx(
        className,
        'w-[2.25rem] select-none text-center text-sm/6 font-semibold text-zinc-950 dark:text-white'
      )}
    >
      {children}
    </span>
  );
}
export const Paginator = (props: {
  currentPage: number;
  getPageURL: (page: number) => string;
  totalPages: number | undefined; // undefined if total pages is unknown
  hasNextPage: boolean;
}) => {
  const { currentPage, getPageURL, totalPages } = props;

  const renderPageNumbers = () => {
    const pages = [];

    if (totalPages !== undefined) {
      for (let page = currentPage - 2; page <= currentPage + 2; page++) {
        const classNames = page < 1 || page > totalPages ? 'invisible' : '';
        // TODO -- add pagination gap
        pages.push(
          <PaginationPage
            key={page}
            href={getPageURL(page)}
            current={page === currentPage}
            className={classNames}
          >
            {page}
          </PaginationPage>
        );
      }
    } else {
      pages.push(
        <PaginationPage key={currentPage} href={getPageURL(currentPage)} current>
          {currentPage}
        </PaginationPage>
      );

      if (currentPage > 1) {
        pages.unshift(
          <PaginationPage key={currentPage - 1} href={getPageURL(currentPage - 1)}>
            {currentPage - 1}
          </PaginationPage>
        );
      }

      if (currentPage > 2) {
        pages.unshift(
          <PaginationPage key={currentPage - 2} href={getPageURL(currentPage - 2)}>
            {currentPage - 2}
          </PaginationPage>
        );
        pages.push(<PaginationGap key="gap" />);
      }
    }

    return pages;
  };

  return (
    <Pagination className="flex flex-row justify-between bg-white w-full">
      <PaginationPrevious
        href={currentPage === 1 ? undefined : getPageURL(Math.max(1, currentPage - 1))}
      />
      <PaginationList className="w-2xl">{renderPageNumbers()}</PaginationList>
      <PaginationNext
        href={
          props.hasNextPage
            ? getPageURL(
                totalPages !== undefined ? Math.min(totalPages, currentPage + 1) : currentPage + 1
              )
            : undefined
        }
      />
    </Pagination>
  );
};



---
File: /burr/telemetry/ui/src/components/common/switch.tsx
---

/**
 * Tailwind catalyst component
 */

import {
  Field as HeadlessField,
  Switch as HeadlessSwitch,
  type FieldProps as HeadlessFieldProps,
  type SwitchProps as HeadlessSwitchProps
} from '@headlessui/react';
import { clsx } from 'clsx';
import type React from 'react';

export function SwitchGroup({ className, ...props }: React.ComponentPropsWithoutRef<'div'>) {
  return (
    <div
      data-slot="control"
      {...props}
      className={clsx(
        className,

        // Basic groups
        'space-y-3 [&_[data-slot=label]]:font-normal',

        // With descriptions
        'has-[[data-slot=description]]:space-y-6 [&_[data-slot=label]]:has-[[data-slot=description]]:font-medium'
      )}
    />
  );
}

export function SwitchField({ className, ...props }: HeadlessFieldProps) {
  return (
    <HeadlessField
      data-slot="field"
      {...props}
      className={clsx(
        className,

        // Base layout
        'grid grid-cols-[1fr_auto] items-center gap-x-8 gap-y-1 sm:grid-cols-[1fr_auto]',

        // Control layout
        '[&>[data-slot=control]]:col-start-2 [&>[data-slot=control]]:self-center',

        // Label layout
        '[&>[data-slot=label]]:col-start-1 [&>[data-slot=label]]:row-start-1 [&>[data-slot=label]]:justify-self-start',

        // Description layout
        '[&>[data-slot=description]]:col-start-1 [&>[data-slot=description]]:row-start-2',

        // With description
        '[&_[data-slot=label]]:has-[[data-slot=description]]:font-medium'
      )}
    />
  );
}

const colors = {
  'dark/zinc': [
    '[--switch-bg-ring:theme(colors.zinc.950/90%)] [--switch-bg:theme(colors.zinc.900)] dark:[--switch-bg-ring:transparent] dark:[--switch-bg:theme(colors.white/25%)]',
    '[--switch-ring:theme(colors.zinc.950/90%)] [--switch-shadow:theme(colors.black/10%)] [--switch:white] dark:[--switch-ring:theme(colors.zinc.700/90%)]'
  ],
  'dark/white': [
    '[--switch-bg-ring:theme(colors.zinc.950/90%)] [--switch-bg:theme(colors.zinc.900)] dark:[--switch-bg-ring:transparent] dark:[--switch-bg:theme(colors.white)]',
    '[--switch-ring:theme(colors.zinc.950/90%)] [--switch-shadow:theme(colors.black/10%)] [--switch:white] dark:[--switch-ring:transparent] dark:[--switch:theme(colors.zinc.900)]'
  ],
  dark: [
    '[--switch-bg-ring:theme(colors.zinc.950/90%)] [--switch-bg:theme(colors.zinc.900)] dark:[--switch-bg-ring:theme(colors.white/15%)]',
    '[--switch-ring:theme(colors.zinc.950/90%)] [--switch-shadow:theme(colors.black/10%)] [--switch:white]'
  ],
  zinc: [
    '[--switch-bg-ring:theme(colors.zinc.700/90%)] [--switch-bg:theme(colors.zinc.600)] dark:[--switch-bg-ring:transparent]',
    '[--switch-shadow:theme(colors.black/10%)] [--switch:white] [--switch-ring:theme(colors.zinc.700/90%)]'
  ],
  white: [
    '[--switch-bg-ring:theme(colors.black/15%)] [--switch-bg:white] dark:[--switch-bg-ring:transparent]',
    '[--switch-shadow:theme(colors.black/10%)] [--switch-ring:transparent] [--switch:theme(colors.zinc.950)]'
  ],
  red: [
    '[--switch-bg-ring:theme(colors.red.700/90%)] [--switch-bg:theme(colors.red.600)] dark:[--switch-bg-ring:transparent]',
    '[--switch:white] [--switch-ring:theme(colors.red.700/90%)] [--switch-shadow:theme(colors.red.900/20%)]'
  ],
  orange: [
    '[--switch-bg-ring:theme(colors.orange.600/90%)] [--switch-bg:theme(colors.orange.500)] dark:[--switch-bg-ring:transparent]',
    '[--switch:white] [--switch-ring:theme(colors.orange.600/90%)] [--switch-shadow:theme(colors.orange.900/20%)]'
  ],
  amber: [
    '[--switch-bg-ring:theme(colors.amber.500/80%)] [--switch-bg:theme(colors.amber.400)] dark:[--switch-bg-ring:transparent]',
    '[--switch-ring:transparent] [--switch-shadow:transparent] [--switch:theme(colors.amber.950)]'
  ],
  yellow: [
    '[--switch-bg-ring:theme(colors.yellow.400/80%)] [--switch-bg:theme(colors.yellow.300)] dark:[--switch-bg-ring:transparent]',
    '[--switch-ring:transparent] [--switch-shadow:transparent] [--switch:theme(colors.yellow.950)]'
  ],
  lime: [
    '[--switch-bg-ring:theme(colors.lime.400/80%)] [--switch-bg:theme(colors.lime.300)] dark:[--switch-bg-ring:transparent]',
    '[--switch-ring:transparent] [--switch-shadow:transparent] [--switch:theme(colors.lime.950)]'
  ],
  green: [
    '[--switch-bg-ring:theme(colors.green.700/90%)] [--switch-bg:theme(colors.green.600)] dark:[--switch-bg-ring:transparent]',
    '[--switch:white] [--switch-ring:theme(colors.green.700/90%)] [--switch-shadow:theme(colors.green.900/20%)]'
  ],
  emerald: [
    '[--switch-bg-ring:theme(colors.emerald.600/90%)] [--switch-bg:theme(colors.emerald.500)] dark:[--switch-bg-ring:transparent]',
    '[--switch:white] [--switch-ring:theme(colors.emerald.600/90%)] [--switch-shadow:theme(colors.emerald.900/20%)]'
  ],
  teal: [
    '[--switch-bg-ring:theme(colors.teal.700/90%)] [--switch-bg:theme(colors.teal.600)] dark:[--switch-bg-ring:transparent]',
    '[--switch:white] [--switch-ring:theme(colors.teal.700/90%)] [--switch-shadow:theme(colors.teal.900/20%)]'
  ],
  cyan: [
    '[--switch-bg-ring:theme(colors.cyan.400/80%)] [--switch-bg:theme(colors.cyan.300)] dark:[--switch-bg-ring:transparent]',
    '[--switch-ring:transparent] [--switch-shadow:transparent] [--switch:theme(colors.cyan.950)]'
  ],
  sky: [
    '[--switch-bg-ring:theme(colors.sky.600/80%)] [--switch-bg:theme(colors.sky.500)] dark:[--switch-bg-ring:transparent]',
    '[--switch:white] [--switch-ring:theme(colors.sky.600/80%)] [--switch-shadow:theme(colors.sky.900/20%)]'
  ],
  blue: [
    '[--switch-bg-ring:theme(colors.blue.700/90%)] [--switch-bg:theme(colors.blue.600)] dark:[--switch-bg-ring:transparent]',
    '[--switch:white] [--switch-ring:theme(colors.blue.700/90%)] [--switch-shadow:theme(colors.blue.900/20%)]'
  ],
  indigo: [
    '[--switch-bg-ring:theme(colors.indigo.600/90%)] [--switch-bg:theme(colors.indigo.500)] dark:[--switch-bg-ring:transparent]',
    '[--switch:white] [--switch-ring:theme(colors.indigo.600/90%)] [--switch-shadow:theme(colors.indigo.900/20%)]'
  ],
  violet: [
    '[--switch-bg-ring:theme(colors.violet.600/90%)] [--switch-bg:theme(colors.violet.500)] dark:[--switch-bg-ring:transparent]',
    '[--switch:white] [--switch-ring:theme(colors.violet.600/90%)] [--switch-shadow:theme(colors.violet.900/20%)]'
  ],
  purple: [
    '[--switch-bg-ring:theme(colors.purple.600/90%)] [--switch-bg:theme(colors.purple.500)] dark:[--switch-bg-ring:transparent]',
    '[--switch:white] [--switch-ring:theme(colors.purple.600/90%)] [--switch-shadow:theme(colors.purple.900/20%)]'
  ],
  fuchsia: [
    '[--switch-bg-ring:theme(colors.fuchsia.600/90%)] [--switch-bg:theme(colors.fuchsia.500)] dark:[--switch-bg-ring:transparent]',
    '[--switch:white] [--switch-ring:theme(colors.fuchsia.600/90%)] [--switch-shadow:theme(colors.fuchsia.900/20%)]'
  ],
  pink: [
    '[--switch-bg-ring:theme(colors.pink.600/90%)] [--switch-bg:theme(colors.pink.500)] dark:[--switch-bg-ring:transparent]',
    '[--switch:white] [--switch-ring:theme(colors.pink.600/90%)] [--switch-shadow:theme(colors.pink.900/20%)]'
  ],
  rose: [
    '[--switch-bg-ring:theme(colors.rose.600/90%)] [--switch-bg:theme(colors.rose.500)] dark:[--switch-bg-ring:transparent]',
    '[--switch:white] [--switch-ring:theme(colors.rose.600/90%)] [--switch-shadow:theme(colors.rose.900/20%)]'
  ]
};

type Color = keyof typeof colors;

export function Switch({
  color = 'dark/zinc',
  className,
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  children,
  ...props
}: {
  color?: Color;
  className?: string;
  children?: React.ReactNode;
} & Omit<HeadlessSwitchProps, 'children'>) {
  return (
    <HeadlessSwitch
      data-slot="control"
      className={clsx(
        className,

        // Base styles
        'group relative isolate inline-flex h-6 w-10 cursor-default rounded-full p-[3px] sm:h-5 sm:w-8',

        // Transitions
        'transition duration-0 ease-in-out data-[changing]:duration-200',

        // Outline and background color in forced-colors mode so switch is still visible
        'forced-colors:outline forced-colors:[--switch-bg:Highlight] dark:forced-colors:[--switch-bg:Highlight]',

        // Unchecked
        'bg-zinc-200 ring-1 ring-inset ring-black/5 dark:bg-white/5 dark:ring-white/15',

        // Checked
        'data-[checked]:bg-[--switch-bg] data-[checked]:ring-[--switch-bg-ring] dark:data-[checked]:bg-[--switch-bg] dark:data-[checked]:ring-[--switch-bg-ring]',

        // Focus
        'focus:outline-none data-[focus]:outline data-[focus]:outline-2 data-[focus]:outline-offset-2 data-[focus]:outline-blue-500',

        // Hover
        'data-[hover]:data-[checked]:ring-[--switch-bg-ring] data-[hover]:ring-black/15',
        'dark:data-[hover]:data-[checked]:ring-[--switch-bg-ring] dark:data-[hover]:ring-white/25',

        // Disabled
        'data-[disabled]:bg-zinc-200 data-[disabled]:data-[checked]:bg-zinc-200 data-[disabled]:opacity-50 data-[disabled]:data-[checked]:ring-black/5',
        'dark:data-[disabled]:bg-white/15 dark:data-[disabled]:data-[checked]:bg-white/15 dark:data-[disabled]:data-[checked]:ring-white/15',

        // Color specific styles
        colors[color]
      )}
      {...props}
    >
      <span
        aria-hidden="true"
        className={clsx(
          // Basic layout
          'pointer-events-none relative inline-block size-[1.125rem] rounded-full sm:size-3.5',

          // Transition
          'translate-x-0 transition duration-200 ease-in-out',

          // Invisible border so the switch is still visible in forced-colors mode
          'border border-transparent',

          // Unchecked
          'bg-white shadow ring-1 ring-black/5',

          // Checked
          'group-data-[checked]:bg-[--switch] group-data-[checked]:shadow-[--switch-shadow] group-data-[checked]:ring-[--switch-ring]',
          'group-data-[checked]:translate-x-4 sm:group-data-[checked]:translate-x-3',

          // Disabled
          'group-data-[disabled]:group-data-[checked]:bg-white group-data-[disabled]:group-data-[checked]:shadow group-data-[disabled]:group-data-[checked]:ring-black/5'
        )}
      />
    </HeadlessSwitch>
  );
}



---
File: /burr/telemetry/ui/src/components/common/table.tsx
---

/**
 * Tailwind catalyst component
 */
'use client';

import { clsx } from 'clsx';
import type React from 'react';
import { createContext, useContext, useState } from 'react';
import { Link } from './link';

const TableContext = createContext<{
  bleed: boolean;
  dense: number; // 0 is not dense, 1 is dense, 2 is very dense
  grid: boolean;
  striped: boolean;
}>({
  bleed: false,
  dense: 0,
  grid: false,
  striped: false
});

export function Table({
  bleed = false,
  dense = 0,
  grid = false,
  striped = false,
  className,
  children,
  ...props
}: {
  bleed?: boolean;
  dense?: number;
  grid?: boolean;
  striped?: boolean;
} & React.ComponentPropsWithoutRef<'div'>) {
  return (
    <TableContext.Provider
      value={{ bleed, dense, grid, striped } as React.ContextType<typeof TableContext>}
    >
      <div className="flow-root">
        <div
          {...props}
          // TODO - this is currently hidden cause of weird stuff, but we really shouldn't be, we should figure out why it's overflowing
          className={clsx(className, '-mx-[--gutter] overflow-x-hidden whitespace-nowrap')}
        >
          <div
            className={clsx('inline-block min-w-full align-middle', !bleed && 'sm:px-[--gutter]')}
          >
            <table className="min-w-full text-left text-sm/6">{children}</table>
          </div>
        </div>
      </div>
    </TableContext.Provider>
  );
}

export function TableHead({ className, ...props }: React.ComponentPropsWithoutRef<'thead'>) {
  return <thead className={clsx(className, 'text-zinc-500 dark:text-zinc-400')} {...props} />;
}

export function TableBody(props: React.ComponentPropsWithoutRef<'tbody'>) {
  return <tbody {...props} />;
}

const TableRowContext = createContext<{ href?: string; target?: string; title?: string }>({
  href: undefined,
  target: undefined,
  title: undefined
});

export function TableRow({
  href,
  target,
  title,
  className,
  children,
  ...props
}: { href?: string; target?: string; title?: string } & React.ComponentPropsWithoutRef<'tr'>) {
  const { striped } = useContext(TableContext);

  return (
    <TableRowContext.Provider
      value={{ href, target, title } as React.ContextType<typeof TableRowContext>}
    >
      <tr
        {...props}
        className={clsx(
          className,
          href &&
            'has-[[data-row-link][data-focus]]:outline has-[[data-row-link][data-focus]]:outline-2 has-[[data-row-link][data-focus]]:-outline-offset-2 has-[[data-row-link][data-focus]]:outline-blue-500 dark:focus-within:bg-white/[2.5%]',
          striped && 'even:bg-zinc-950/[2.5%] dark:even:bg-white/[2.5%]',
          href && striped && 'hover:bg-zinc-950/5 dark:hover:bg-white/5',
          href && !striped && 'hover:bg-zinc-950/[2.5%] dark:hover:bg-white/[2.5%]'
        )}
      >
        {children}
      </tr>
    </TableRowContext.Provider>
  );
}

export function TableHeader({ className, ...props }: React.ComponentPropsWithoutRef<'th'>) {
  const { bleed, grid } = useContext(TableContext);

  return (
    <th
      {...props}
      className={clsx(
        className,
        'border-b border-b-zinc-950/10 px-4 py-2 font-medium first:pl-[var(--gutter,theme(spacing.2))] last:pr-[var(--gutter,theme(spacing.2))] dark:border-b-white/10',
        grid && 'border-l border-l-zinc-950/5 first:border-l-0 dark:border-l-white/5',
        !bleed && 'sm:first:pl-2 sm:last:pr-2'
      )}
    />
  );
}

export function TableCell({ className, children, ...props }: React.ComponentPropsWithoutRef<'td'>) {
  const { bleed, dense, grid, striped } = useContext(TableContext);
  const { href, target, title } = useContext(TableRowContext);
  const [cellRef, setCellRef] = useState<HTMLElement | null>(null);

  return (
    <td
      ref={href ? setCellRef : undefined}
      {...props}
      className={clsx(
        className,
        'relative px-4 first:pl-[var(--gutter,theme(spacing.2))] last:pr-[var(--gutter,theme(spacing.2))]',
        !striped && 'border-b border-zinc-950/5 dark:border-white/5',
        grid && 'border-l border-l-zinc-950/5 first:border-l-0 dark:border-l-white/5',
        dense === 0 ? 'py-4' : dense === 1 ? 'py-2.5' : dense === 2 ? 'py-1' : 'py-0.2', //'py-2.5' : 'py-4',
        !bleed && 'sm:first:pl-2 sm:last:pr-2'
      )}
    >
      {href && (
        <Link
          data-row-link
          href={href}
          target={target}
          aria-label={title}
          tabIndex={cellRef?.previousElementSibling === null ? 0 : -1}
          className="absolute inset-0 focus:outline-none"
        />
      )}
      {children}
    </td>
  );
}



---
File: /burr/telemetry/ui/src/components/common/tabs.tsx
---

function classNames(...classes: string[]) {
  return classes.filter(Boolean).join(' ');
}
/**
 * Tabs component for displaying a list of tabs.
 */
export const Tabs = (props: {
  tabs: {
    id: string;
    displayName: string;
  }[];
  currentTab: string;
  setCurrentTab: (tab: string) => void;
}) => {
  // const MinimizeTableIcon = props.isMinimized ? ChevronRightIcon : ChevronLeftIcon;

  const { currentTab, setCurrentTab } = props;
  return (
    <div>
      <div className="sm:hidden">
        <label htmlFor="tabs" className="sr-only">
          Select a tab
        </label>
        {/* Use an "onChange" listener to redirect the user to the selected tab URL. */}
        <select
          id="tabs"
          name="tabs"
          className="block w-full rounded-md border-gray-300 focus:border-indigo-500 focus:ring-indigo-500"
          defaultValue={currentTab}
        >
          {props.tabs.map((tab) => (
            <option key={tab.id}>{tab.displayName}</option>
          ))}
        </select>
      </div>
      <div className="hidden sm:block">
        <nav className="flex space-x-4 flex-row pl-2" aria-label="Tabs">
          {props.tabs.map((tab) => (
            <span
              onClick={() => setCurrentTab(tab.id)}
              key={tab.displayName}
              className={classNames(
                tab.id === currentTab
                  ? 'bg-indigo-100 text-dwdarkblue'
                  : 'text-gray-500 hover:text-gray-700',
                'rounded-md px-3 py-2 text-sm font-medium hover:cursor-pointer'
              )}
              aria-current={tab.id === currentTab ? 'page' : undefined}
            >
              {tab.displayName}
            </span>
          ))}
        </nav>
      </div>
    </div>
  );
};



---
File: /burr/telemetry/ui/src/components/common/text.tsx
---

import { clsx } from 'clsx';
import { Link } from './link';

export function Text({ className, ...props }: React.ComponentPropsWithoutRef<'p'>) {
  return (
    <p
      {...props}
      data-slot="text"
      className={clsx(className, 'text-base/6 text-zinc-500 sm:text-sm/6 dark:text-zinc-400')}
    />
  );
}

export function TextLink({ className, ...props }: React.ComponentPropsWithoutRef<typeof Link>) {
  return (
    <Link
      {...props}
      className={clsx(
        className,
        'text-zinc-950 underline decoration-zinc-950/50 data-[hover]:decoration-zinc-950 dark:text-white dark:decoration-white/50 dark:data-[hover]:decoration-white'
      )}
    />
  );
}

export function Strong({ className, ...props }: React.ComponentPropsWithoutRef<'strong'>) {
  return (
    <strong {...props} className={clsx(className, 'font-medium text-zinc-950 dark:text-white')} />
  );
}

export function Code({ className, ...props }: React.ComponentPropsWithoutRef<'code'>) {
  return (
    <code
      {...props}
      className={clsx(
        className,
        'rounded border border-zinc-950/10 bg-zinc-950/[2.5%] px-0.5 text-sm font-medium text-zinc-950 sm:text-[0.8125rem] dark:border-white/20 dark:bg-white/5 dark:text-white'
      )}
    />
  );
}



---
File: /burr/telemetry/ui/src/components/common/textarea.tsx
---

import {
  Textarea as HeadlessTextarea,
  type TextareaProps as HeadlessTextareaProps
} from '@headlessui/react';
import { clsx } from 'clsx';
import { forwardRef } from 'react';

export const Textarea = forwardRef<
  HTMLTextAreaElement,
  { resizable?: boolean } & HeadlessTextareaProps
>(function Textarea({ className, resizable = true, ...props }, ref) {
  return (
    <span
      data-slot="control"
      className={clsx([
        className,

        // Basic layout
        'relative block w-full',

        // Background color + shadow applied to inset pseudo element, so shadow blends with border in light mode
        'before:absolute before:inset-px before:rounded-[calc(theme(borderRadius.lg)-1px)] before:bg-white before:shadow',

        // Background color is moved to control and shadow is removed in dark mode so hide `before` pseudo
        'dark:before:hidden',

        // Focus ring
        'after:pointer-events-none after:absolute after:inset-0 after:rounded-lg after:ring-inset after:ring-transparent sm:after:focus-within:ring-2 sm:after:focus-within:ring-blue-500',

        // Disabled state
        'has-[[data-disabled]]:opacity-50 before:has-[[data-disabled]]:bg-zinc-950/5 before:has-[[data-disabled]]:shadow-none'
      ])}
    >
      <HeadlessTextarea
        ref={ref}
        className={clsx([
          // Basic layout
          'relative block h-full w-full appearance-none rounded-lg px-[calc(theme(spacing[3.5])-1px)] py-[calc(theme(spacing[2.5])-1px)] sm:px-[calc(theme(spacing.3)-1px)] sm:py-[calc(theme(spacing[1.5])-1px)]',

          // Typography
          'text-base/6 text-zinc-950 placeholder:text-zinc-500 sm:text-sm/6 dark:text-white',

          // Border
          'border border-zinc-950/10 data-[hover]:border-zinc-950/20 dark:border-white/10 dark:data-[hover]:border-white/20',

          // Background color
          'bg-transparent dark:bg-white/5',

          // Hide default focus styles
          'focus:outline-none',

          // Invalid state
          'data-[invalid]:border-red-500 data-[invalid]:data-[hover]:border-red-500 data-[invalid]:dark:border-red-600 data-[invalid]:data-[hover]:dark:border-red-600',

          // Disabled state
          'disabled:border-zinc-950/20 disabled:dark:border-white/15 disabled:dark:bg-white/[2.5%] dark:data-[hover]:disabled:border-white/15',

          // Resizable
          resizable ? 'resize-y' : 'resize-none'
        ])}
        {...props}
      />
    </span>
  );
});



---
File: /burr/telemetry/ui/src/components/common/tooltip.tsx
---

import { ReactNode, useState } from 'react';

export const Tooltip: React.FC<{ text: string; children: ReactNode }> = ({ text, children }) => {
  const [isVisible, setIsVisible] = useState(false);
  const [tooltipStyle, setTooltipStyle] = useState<React.CSSProperties>({});

  const handleMouseEnter = (e: React.MouseEvent) => {
    const { clientX, clientY } = e;
    setTooltipStyle({
      left: `${clientX}px`,
      top: `${clientY - 5}px`, // 5px above the cursor
      position: 'absolute'
    });
    setIsVisible(true);
  };

  const handleMouseLeave = () => {
    setIsVisible(false);
  };

  return (
    <div className="relative inline-block">
      <div
        onMouseEnter={handleMouseEnter}
        onMouseLeave={handleMouseLeave}
        className="cursor-pointer"
      >
        {children}
      </div>
      {isVisible && (
        <div className="absolute bg-gray-700 text-white text-sm p-2 rounded" style={tooltipStyle}>
          {text}
        </div>
      )}
    </div>
  );
};

export default Tooltip;



---
File: /burr/telemetry/ui/src/components/nav/appcontainer.tsx
---

import { Fragment, useState } from 'react';
import { Dialog, Disclosure, Transition } from '@headlessui/react';
import {
  ComputerDesktopIcon,
  Square2StackIcon,
  QuestionMarkCircleIcon,
  XMarkIcon,
  ChatBubbleLeftEllipsisIcon,
  ChevronLeftIcon,
  ChevronRightIcon,
  ChevronDownIcon,
  FolderIcon
} from '@heroicons/react/24/outline';
import { ListBulletIcon } from '@heroicons/react/20/solid';
import { BreadCrumb } from './breadcrumb';
import { Link } from 'react-router-dom';
import { classNames } from '../../utils/tailwind';
import React from 'react';
import { DefaultService } from '../../api';
import { useQuery } from 'react-query';

// Define your GitHub logo SVG as a React component
const GithubLogo = () => (
  <svg
    style={{ width: 20 }}
    xmlns="http://www.w3.org/2000/svg"
    fill="none"
    viewBox="0 0 22 22"
    stroke="currentColor"
  >
    {/* SVG path for GitHub logo */}
    <path
      strokeLinecap="round"
      strokeLinejoin="round"
      strokeWidth={0.5}
      d="M12 2C6.477 2 2 6.477 2 12c0 4.418 2.865 8.166 6.839 9.489.5.09.682-.217.682-.482 0-.237-.009-.868-.014-1.703-2.782.6-3.369-1.34-3.369-1.34-.454-1.156-1.11-1.462-1.11-1.462-.908-.62.069-.608.069-.608 1.004.07 1.532 1.03 1.532 1.03.892 1.529 2.341 1.088 2.91.833.091-.646.349-1.086.635-1.337-2.22-.25-4.555-1.11-4.555-4.943 0-1.091.39-1.984 1.03-2.682-.103-.251-.447-1.265.098-2.634 0 0 .84-.27 2.75 1.02A9.564 9.564 0 0112 7.07c.85.004 1.705.115 2.502.337 1.909-1.29 2.747-1.02 2.747-1.02.547 1.37.203 2.383.1 2.634.64.698 1.028 1.59 1.028 2.682 0 3.842-2.337 4.687-4.565 4.932.359.31.678.92.678 1.852 0 1.335-.012 2.415-.012 2.741 0 .267.18.576.688.479C19.137 20.164 22 16.416 22 12c0-5.523-4.477-10-10-10z"
    />
  </svg>
);

/**
 * Toggles the sidebar open and closed.
 */
const ToggleOpenButton = (props: { open: boolean; toggleSidebar: () => void }) => {
  const MinimizeMaximizeIcon = props.open ? ChevronLeftIcon : ChevronRightIcon;
  return (
    <MinimizeMaximizeIcon
      className={classNames(
        'text-gray-400',
        'h-8 w-8 hover:bg-gray-50 rounded-md hover:cursor-pointer'
      )}
      aria-hidden="true"
      onClick={props.toggleSidebar}
    />
  );
};

/**
 * Container for the app. Contains three main parts:
 * 1. The sidebar
 * 2. The breadcrumb
 * 3. The main content
 *
 * TODO -- get this to work in small mode -- the sidebar is currently not there.
 */
export const AppContainer = (props: { children: React.ReactNode }) => {
  const [sidebarOpen, setSidebarOpen] = useState(true);
  const [smallSidebarOpen, setSmallSidebarOpen] = useState(false);
  const { data: backendSpec } = useQuery(['backendSpec'], () =>
    DefaultService.getAppSpecApiV0MetadataAppSpecGet().then((response) => {
      return response;
    })
  );
  const toggleSidebar = () => {
    setSidebarOpen(!sidebarOpen);
  };
  const navigation = [
    {
      name: 'Projects',
      href: '/projects',
      icon: Square2StackIcon,
      linkType: 'internal'
    },
    {
      name: 'Examples',
      href: 'https://github.com/DAGWorks-Inc/burr/tree/main/examples',
      icon: FolderIcon,
      linkType: 'external'
    },
    ...(backendSpec?.supports_demos
      ? [
          {
            name: 'Demos',
            href: '/demos',
            icon: ListBulletIcon,
            linkType: 'internal',
            children: [
              { name: 'counter', href: '/demos/counter', current: false, linkType: 'internal' },
              { name: 'chatbot', href: '/demos/chatbot', current: false, linkType: 'internal' },
              {
                name: 'email-assistant',
                href: '/demos/email-assistant',
                current: false,
                linkType: 'internal'
              },
              {
                name: 'streaming-chatbot',
                href: '/demos/streaming-chatbot',
                current: false,
                linkType: 'internal'
              },
              {
                name: 'deep-researcher',
                href: '/demos/deep-researcher',
                current: false,
                linkType: 'internal'
              }
            ]
          }
        ]
      : []),
    {
      name: 'Develop',
      href: 'https://github.com/dagworks-inc/burr',
      icon: ComputerDesktopIcon,
      linkType: 'external'
    },
    {
      name: 'Documentation',
      href: 'https://burr.dagworks.io',
      icon: QuestionMarkCircleIcon,
      linkType: 'external'
    },
    {
      name: 'GitHub Discussions',
      href: 'https://github.com/DAGWorks-Inc/burr/discussions',
      icon: GithubLogo,
      linkType: 'external'
    },
    {
      name: 'Get Help on Discord',
      href: 'https://discord.gg/emUEvxTb4D',
      icon: ChatBubbleLeftEllipsisIcon,
      linkType: 'external'
    }
  ];

  if (backendSpec?.indexing) {
    navigation.push({
      name: 'Admin',
      href: '/admin',
      icon: ListBulletIcon,
      linkType: 'internal'
    });
  }

  const isCurrent = (href: string, linkType: string) => {
    if (linkType === 'external') {
      return false;
    }
    return window.location.pathname.startsWith(href);
  };

  return (
    <>
      <div className="h-screen w-screen overflow-x-auto">
        <Transition.Root show={smallSidebarOpen} as={Fragment}>
          <Dialog as="div" className="relative z-50 lg:hidden" onClose={setSmallSidebarOpen}>
            <Transition.Child
              as={Fragment}
              enter="transition-opacity ease-linear duration-300"
              enterFrom="opacity-0"
              enterTo="opacity-100"
              leave="transition-opacity ease-linear duration-300"
              leaveFrom="opacity-100"
              leaveTo="opacity-0"
            >
              <div className="fixed inset-0 bg-gray-900/80" />
            </Transition.Child>

            <div className="fixed inset-0 flex">
              <Transition.Child
                as={Fragment}
                enter="transition ease-in-out duration-300 transform"
                enterFrom="-translate-x-full"
                enterTo="translate-x-0"
                leave="transition ease-in-out duration-300 transform"
                leaveFrom="translate-x-0"
                leaveTo="-translate-x-full"
              >
                <Dialog.Panel className="relative mr-16 flex w-full max-w-xs flex-1">
                  <Transition.Child
                    as={Fragment}
                    enter="ease-in-out duration-300"
                    enterFrom="opacity-0"
                    enterTo="opacity-100"
                    leave="ease-in-out duration-300"
                    leaveFrom="opacity-100"
                    leaveTo="opacity-0"
                  >
                    <div className="absolute left-full top-0 flex w-16 justify-center pt-5">
                      <button
                        type="button"
                        className="-m-2.5 p-2.5"
                        onClick={() => setSmallSidebarOpen(false)}
                      >
                        <span className="sr-only">Close sidebar</span>
                        <XMarkIcon className="h-6 w-6 text-white" aria-hidden="true" />
                      </button>
                    </div>
                  </Transition.Child>
                  {/* Sidebar component, swap this element with another sidebar if you like */}
                  <div className="flex grow flex-col gap-y-5 overflow-y-auto bg-white px-6 pb-2 py-2">
                    <div className="flex h-16 shrink-0 items-center">
                      <img className="h-10 w-auto" src={'/logo.png'} alt="Burr" />
                    </div>
                    <nav className="flex flex-1 flex-col">
                      <ul role="list" className="flex flex-1 flex-col gap-y-7">
                        <li>
                          <ul role="list" className="-mx-2 space-y-1">
                            {navigation.map((item) => (
                              <li key={item.name}>
                                <Link
                                  to={item.href}
                                  className={classNames(
                                    isCurrent(item.href, item.linkType)
                                      ? 'bg-gray-50 text-dwdarkblue'
                                      : item.linkType === 'external'
                                        ? 'text-gray-700 hover:text-dwdarkblue'
                                        : 'text-gray-700 hover:text-dwdarkblue hover:bg-gray-50',
                                    'group flex gap-x-3 rounded-md p-2 text-sm leading-6 font-semibold'
                                  )}
                                  target={item.linkType === 'external' ? '_blank' : undefined}
                                  rel={item.linkType === 'external' ? 'noreferrer' : undefined}
                                >
                                  <item.icon
                                    className={classNames(
                                      isCurrent(item.href, item.linkType)
                                        ? 'text-dwdarkblue'
                                        : 'text-gray-400 group-hover:text-dwdarkblue',
                                      'h-6 w-6 shrink-0'
                                    )}
                                    aria-hidden="true"
                                  />
                                  {item.name}
                                </Link>
                              </li>
                            ))}
                          </ul>
                        </li>
                      </ul>
                    </nav>
                  </div>
                </Dialog.Panel>
              </Transition.Child>
            </div>
          </Dialog>
        </Transition.Root>

        {/* Static sidebar for desktop */}
        <div
          className={`hidden ${
            sidebarOpen ? 'h-screen lg:fixed lg:inset-y-0 lg:z-50 lg:flex lg:w-72 lg:flex-col' : ''
          }`}
        >
          {/* Sidebar component, swap this element with another sidebar if you like */}
          <div className="flex grow flex-col gap-y-5 overflow-y-auto border-r border-gray-200 bg-white px-6 py-2">
            <div className="flex h-16 shrink-0 items-center">
              <img className="h-12 w-auto" src={'/public/logo.png'} alt="Burr" />
            </div>
            <nav className="flex flex-1 flex-col">
              <ul role="list" className="flex flex-1 flex-col gap-y-7">
                <li>
                  <ul role="list" className="-mx-2 space-y-1">
                    {navigation.map((item) => (
                      <li key={item.name}>
                        {!item?.children ? (
                          <Link
                            to={item.href}
                            className={classNames(
                              isCurrent(item.href, item.linkType)
                                ? 'bg-gray-50'
                                : 'hover:bg-gray-50',
                              'group flex gap-x-3 rounded-md p-2 text-sm leading-6 font-semibold text-gray-700'
                            )}
                            target={item.linkType === 'external' ? '_blank' : undefined}
                            rel={item.linkType === 'external' ? 'noreferrer' : undefined}
                          >
                            <item.icon
                              className="h-6 w-6 shrink-0 text-gray-400"
                              aria-hidden="true"
                            />
                            {item.name}
                          </Link>
                        ) : (
                          <Disclosure as="div">
                            {({ open }) => (
                              <>
                                <Disclosure.Button
                                  className={classNames(
                                    isCurrent(item.href, item.linkType)
                                      ? 'bg-gray-50'
                                      : 'hover:bg-gray-50',
                                    'flex items-center w-full text-left rounded-md p-2 gap-x-3 text-sm leading-6 font-semibold text-gray-700'
                                  )}
                                >
                                  <item.icon
                                    className="h-6 w-6 shrink-0 text-gray-400"
                                    aria-hidden="true"
                                  />
                                  {item.name}
                                  <ChevronDownIcon
                                    className={classNames(
                                      open ? 'rotate-180 text-gray-500' : 'text-gray-400',
                                      'ml-auto h-5 w-5 shrink-0'
                                    )}
                                    aria-hidden="true"
                                  />
                                </Disclosure.Button>
                                <Disclosure.Panel as="ul" className="mt-1 px-2">
                                  {item.children.map((subItem) => (
                                    <li key={subItem.name}>
                                      <Link
                                        to={subItem.href}
                                        className={classNames(
                                          isCurrent(subItem.href, subItem.linkType)
                                            ? 'bg-gray-50'
                                            : 'hover:bg-gray-50',
                                          'block rounded-md py-2 pr-2 pl-9 text-sm leading-6 text-gray-700'
                                        )}
                                        target={
                                          subItem.linkType === 'external' ? '_blank' : undefined
                                        }
                                        rel={
                                          subItem.linkType === 'external' ? 'noreferrer' : undefined
                                        }
                                      >
                                        {subItem.name}
                                      </Link>
                                    </li>
                                  ))}
                                </Disclosure.Panel>
                              </>
                            )}
                          </Disclosure>
                        )}
                      </li>
                    ))}
                  </ul>
                </li>
              </ul>
            </nav>
            <div className="flex justify-start -mx-5">
              <ToggleOpenButton open={sidebarOpen} toggleSidebar={toggleSidebar} />
            </div>
          </div>
        </div>
        <div
          className={`hidden h-screen ${
            !sidebarOpen
              ? 'lg:fixed lg:inset-y-0 lg:z-50 lg:flex lg:w-8 lg:flex-col justify-end lg:py-2 lg:px-1'
              : ''
          }`}
        >
          <ToggleOpenButton open={sidebarOpen} toggleSidebar={toggleSidebar} />
        </div>

        {/* This is a bit hacky -- just quickly prototyping and these margins were the ones that worked! */}
        <main className={`py-14 -my-1 ${sidebarOpen ? 'lg:pl-72' : 'lg:pl-5'} h-full`}>
          <div className="flex items-center px-5 sm:px-7 lg:px-9 pb-8 -my-4">
            <BreadCrumb />
          </div>
          <div className="flex h-full flex-col">
            <div className="px-4 sm:px-6 lg:px-2 max-h-full h-full flex-1"> {props.children}</div>
          </div>
        </main>
      </div>
    </>
  );
};



---
File: /burr/telemetry/ui/src/components/nav/breadcrumb.tsx
---

import { HomeIcon } from '@heroicons/react/20/solid';
import { Link, useLocation } from 'react-router-dom';

/**
 * Breadcrumb component
 * This isn't perfect as not all the sub-routes map to a name
 * (and you can click on any of them), but it works for now,
 * and no routes lead to nothing (the /project route leads
 * to /projects, which is the default...)
 */
export const BreadCrumb = () => {
  const location = useLocation();
  const pathnames = location.pathname.split('/').filter((x) => x);

  const pages = pathnames.map((value, index) => {
    const href = `/${pathnames.slice(0, index + 1).join('/')}`;
    const isCurrentPage = index === pathnames.length - 1;
    return {
      name: value,
      href,
      current: isCurrentPage
    };
  });
  {
    return (
      <nav className="flex" aria-label="Breadcrumb">
        <ol role="list" className="flex items-center space-x-4">
          <li>
            <div>
              <Link to="/" className="text-gray-400 hover:text-gray-500">
                <HomeIcon className="h-5 w-5 flex-shrink-0" aria-hidden="true" />
                <span className="sr-only">Home</span>
              </Link>
            </div>
          </li>
          {pages.map((page, index) => {
            // Quick trick to catch null primary keys
            const isNullPK = page.name === 'null' && index === 2;
            return (
              <li key={page.name}>
                <div className="flex items-center">
                  <svg
                    className="h-5 w-5 flex-shrink-0 text-gray-300"
                    fill="currentColor"
                    viewBox="0 0 20 20"
                    aria-hidden="true"
                  >
                    <path d="M5.555 17.776l8-16 .894.448-8 16-.894-.448z" />
                  </svg>
                  {isNullPK ? (
                    <span className="ml-4 text-sm font-medium text-gray-200">no primary key</span>
                  ) : (
                    <Link
                      to={page.href}
                      className="ml-4 text-sm font-medium text-gray-500 hover:text-gray-700"
                      aria-current={page.current ? 'page' : undefined}
                    >
                      {decodeURIComponent(page.name)}
                    </Link>
                  )}
                </div>
              </li>
            );
          })}
        </ol>
      </nav>
    );
  }
};



---
File: /burr/telemetry/ui/src/components/routes/app/ActionView.tsx
---

import { BookOpenIcon, PencilIcon } from '@heroicons/react/24/outline';
import { ActionModel } from '../../../api';
import { ChipGroup } from '../../common/chip';
import { Prism as SyntaxHighlighter } from 'react-syntax-highlighter';
import { base16AteliersulphurpoolLight } from 'react-syntax-highlighter/dist/esm/styles/prism';

/**
 * Renders the code for an action.
 * Note that if you turn on numbering the behavior can get really strange
 *
 */
export const CodeView = (props: { code: string }) => {
  return (
    <div className="h-full w-full pt-2 gap-2 flex flex-col max-w-full overflow-y-auto">
      <SyntaxHighlighter
        language="python"
        className="bg-dwdarkblue/100 hide-scrollbar"
        wrapLines={true}
        wrapLongLines={true}
        style={base16AteliersulphurpoolLight}
      >
        {props.code}
      </SyntaxHighlighter>
    </div>
  );
};
/**
 * Renders the view of the action -- this has some indications of reading/writing state.
 * Currently we don't have inputs, but we will add that in the future.
 *
 * TODO -- add inputs
 */
export const ActionView = (props: { currentAction: ActionModel | undefined }) => {
  if (props.currentAction === undefined) {
    return <div></div>;
  }
  const reads = props.currentAction.reads;
  const writes = props.currentAction.writes;
  const name = props.currentAction.name;

  return (
    <div className="h-full w-full pl-1 pt-2 gap-2 flex flex-col">
      <h1 className="text-2xl text-gray-600 font-semibold">{name}</h1>
      <ChipGroup
        chips={reads}
        type="stateRead"
        label={<BookOpenIcon className="h-6 w-6 text-gray-500" />}
      />
      <ChipGroup
        chips={writes}
        type="stateWrite"
        label={<PencilIcon className="h-6 w-6 text-gray-500" />}
      />
      <CodeView code={props.currentAction.code} />
    </div>
  );
};



---
File: /burr/telemetry/ui/src/components/routes/app/AnnotationsView.tsx
---

import { useContext, useEffect, useMemo, useState } from 'react';
import {
  AnnotationCreate,
  AnnotationDataPointer,
  AnnotationObservation,
  AnnotationOut,
  AnnotationUpdate,
  DefaultService,
  Step
} from '../../../api';
import { AppView, AppContext } from './AppView';

import Select, { components } from 'react-select';
import CreatableSelect from 'react-select/creatable';
import { FaClipboardList, FaExternalLinkAlt, FaThumbsDown, FaThumbsUp } from 'react-icons/fa';
import { Table, TableBody, TableCell, TableHead, TableHeader, TableRow } from '../../common/table';
import { Chip } from '../../common/chip';
import { Link } from 'react-router-dom';
import { useMutation, useQuery } from 'react-query';
import { Loading } from '../../common/loading';
import {
  ChevronDownIcon,
  PencilIcon,
  PencilSquareIcon,
  PlusIcon,
  XMarkIcon
} from '@heroicons/react/24/outline';
import { classNames } from '../../../utils/tailwind';
import { DateTimeDisplay } from '../../common/dates';
import { Drawer } from '../../common/drawer';
import { useLocationParams } from '../../../utils';

export const InlineAppView = (props: {
  projectId: string;
  partitionKey: string | null;
  appId: string;
  sequenceID: number;
}) => {
  return (
    <div className="w-full h-[40em]">
      <AppView
        projectId={props.projectId}
        appId={props.appId}
        orientation="stacked_horizontal"
        defaultAutoRefresh={false}
        enableFullScreenStepView={false}
        enableMinimizedStepView={false}
        allowAnnotations={false}
        restrictTabs={['data', 'code', 'reproduce', 'insights', 'graph']}
        disableNavigateSteps={false}
        forceCurrentActionIndex={{
          sequenceId: props.sequenceID,
          appId: props.appId,
          partitionKey: props.partitionKey
        }}
        partitionKey={props.partitionKey}
        forceFullScreen={true}
      />
    </div>
  );
};

const getAnnotationsTarget = (steps: Step[]) => {
  return steps.map((step) => ({
    sequenceId: step.step_start_log.sequence_id,
    spanId: null, // TODO -- allow annotations at the attribute/trace level!
    actionName: step.step_start_log.action
  }));
};

const getPossibleDataTargets = (step: Step) => {
  return [
    ...step.attributes.map((attribute) => ({
      type: AnnotationDataPointer.type.ATTRIBUTE,
      field_name: attribute.key,
      span_id: attribute.span_id,
      value: JSON.stringify(attribute.value)
    })),
    ...Object.keys(step.step_end_log?.state || {})
      .map((key) => ({
        type: AnnotationDataPointer.type.STATE_FIELD,
        field_name: key,
        span_id: null,
        value: JSON.stringify(step.step_end_log?.state[key])
      }))
      .filter((data) => !data.field_name.startsWith('__'))
  ];
};

export const AnnotationsView = (props: {
  currentStep: Step | undefined;
  appId: string;
  partitionKey: string | null;
  projectId: string;
  allAnnotations: AnnotationOut[];
  allSteps: Step[]; // List of steps so we can select the possible targets to annotate
}) => {
  const {
    currentEditingAnnotationContext,
    setCurrentEditingAnnotationContext,
    setCurrentHoverIndex,
    setCurrentSelectedIndex,
    createAnnotation,
    updateAnnotation,
    refreshAnnotationData
  } = useContext(AppContext);
  const tags = Array.from(new Set(props.allAnnotations.flatMap((annotation) => annotation.tags)));
  const annotationTargets = getAnnotationsTarget(props.allSteps);
  const selectedAnnotationTarget =
    currentEditingAnnotationContext === undefined
      ? undefined
      : {
          sequenceId: currentEditingAnnotationContext.sequenceId,
          spanId: null,
          actionName:
            props.allSteps.find(
              (step) =>
                step.step_start_log.sequence_id === currentEditingAnnotationContext.sequenceId
            )?.step_start_log.action || ''
        };
  const existingAnnotation = props.allAnnotations.find(
    (annotation) =>
      annotation.step_sequence_id === currentEditingAnnotationContext?.sequenceId &&
      annotation.app_id === props.appId &&
      annotation.span_id === currentEditingAnnotationContext.spanId
  );

  const step = existingAnnotation
    ? props.allSteps.find(
        (step) => step.step_start_log.sequence_id === existingAnnotation?.step_sequence_id
      )
    : props.allSteps.find(
        (step) => step.step_start_log.sequence_id === currentEditingAnnotationContext?.sequenceId
      );

  const allPossibleDataTargets: AnnnotationDataPointerWithValue[] = step
    ? getPossibleDataTargets(step)
    : [];

  return (
    <div>
      {currentEditingAnnotationContext && (
        <AnnotationEditCreateForm
          tagOptions={tags}
          allAnnotationTargets={annotationTargets}
          selectedAnnotationTarget={selectedAnnotationTarget}
          existingAnnotation={existingAnnotation}
          resetAnnotationContext={() => {
            setCurrentEditingAnnotationContext(undefined);
          }}
          createAnnotation={(annotation) => {
            createAnnotation(
              props.projectId,
              props.partitionKey,
              props.appId,
              currentEditingAnnotationContext.sequenceId,
              currentEditingAnnotationContext.spanId || undefined,
              annotation
            ).then(() => {
              refreshAnnotationData();
              setCurrentEditingAnnotationContext(undefined);
            });
          }}
          allPossibleDataTargets={allPossibleDataTargets}
          updateAnnotation={(annotationID, annotationUpdate) => {
            updateAnnotation(annotationID, annotationUpdate).then(() => {
              refreshAnnotationData();
              setCurrentEditingAnnotationContext(undefined);
            });
          }}
        />
      )}
      <AnnotationsTable
        annotations={props.allAnnotations}
        onClick={(annotation) => {
          // TODO -- ensure that the indices are aligned/set correctly
          setCurrentSelectedIndex({
            sequenceId: annotation.step_sequence_id,
            appId: props.appId,
            partitionKey: props.partitionKey
          });
        }}
        onHover={(annotation) => {
          setCurrentHoverIndex({
            sequenceId: annotation.step_sequence_id,
            appId: props.appId,
            partitionKey: props.partitionKey
          });
        }}
        displayProjectLevelAnnotationsLink={true} // we want to link back to the project level view
        projectId={props.projectId}
        highlightedSequence={
          currentEditingAnnotationContext ? currentEditingAnnotationContext.sequenceId : undefined
        }
      />
    </div>
  );
};

type DownloadButtonProps = {
  data: AnnotationOut[];
  fileName: string;
};
const annotationsToCSV = (annotations: AnnotationOut[]): string => {
  // Define the CSV headers
  const headers = [
    'id',
    'project_id',
    'app_id',
    'span_id',
    'step_sequence_id',
    'step_name',
    'tags',
    'observation_number',
    'note',
    'ground_truth',
    'thumbs_up_thumbs_down',
    'data_field_type',
    'data_field_name',
    'partition_key',
    'created',
    'updated'
  ];

  // Helper function to escape fields for CSV format
  const escapeCSV = (value: string) => {
    if (value === undefined) {
      return '';
    }
    if (value.includes(',') || value.includes('"') || value.includes('\n')) {
      return `"${value.replace(/"/g, '""')}"`;
    }
    return value;
  };

  // Convert the annotations to CSV format
  const rows = annotations.flatMap((annotation) => {
    return annotation.observations.map((observation, i) => {
      return {
        id: annotation.id.toString(),
        project_id: annotation.project_id,
        app_id: annotation.app_id,
        span_id: annotation.span_id || '',
        step_sequence_id: annotation.step_sequence_id.toString(),
        step_name: annotation.step_name,
        tags: annotation.tags.join(' '),
        observation_number: i.toString(),
        note: observation.data_fields['note'] || '',
        ground_truth: observation.data_fields['ground_truth'] || '',
        thumbs_up_thumbs_down:
          observation.thumbs_up_thumbs_down !== null
            ? observation.thumbs_up_thumbs_down.toString()
            : '',
        data_field_type: observation.data_pointers.map((dp) => dp.type).join(' '),
        data_field_name: observation.data_pointers.map((dp) => dp.field_name).join(' '),
        partition_key: annotation.partition_key || '',
        created: new Date(annotation.created).toISOString(),
        updated: new Date(annotation.updated).toISOString()
      };
    });
  });

  // Construct the CSV string -- TODO -- use a library for this
  const csvContent = [
    headers.join(','), // Add headers
    ...rows.map((row) =>
      headers.map((header) => escapeCSV(row[header as keyof typeof row])).join(',')
    ) // Add data rows
  ].join('\n');

  return csvContent;
};

const DownloadAnnotationsButton: React.FC<DownloadButtonProps> = ({ data, fileName }) => {
  const handleDownload = () => {
    const csvData = annotationsToCSV(data);
    const blob = new Blob([csvData], { type: 'text/csv;charset=utf-8;' });
    const url = URL.createObjectURL(blob);
    const link = document.createElement('a');
    link.href = url;
    link.setAttribute('download', fileName);
    document.body.appendChild(link);
    link.click();
    document.body.removeChild(link);
    URL.revokeObjectURL(url);
  };

  return (
    <button
      onClick={handleDownload}
      className="px-4 py-2 h-10 text-sm bg-dwlightblue text-white rounded-md hover:bg-dwlightblue/70 cursor-pointer"
    >
      Download
    </button>
  );
};

type TagOptionType = {
  value: string;
  label: string;
};

type AnnotationTarget = {
  sequenceId: number;
  actionName: string;
  spanId: string | null;
};

type TargetOptionType = {
  value: AnnotationTarget;
  label: string;
};

const getLabelForTarget = (target: AnnotationTarget) =>
  `${target.actionName}:${target.sequenceId}` + (target.spanId ? `-${target.spanId}` : '');

const ObservationsView = (props: { observations: AnnotationObservation[] }) => {
  // Observation -- this will be a list of notes with data attached
  // Data -- a chip with the type of the data (state, attribute) + key, with link
  // Thumbs up/down -- a thumbs up or thumbs down icon
  // Notes -- free-form text
  // This is going to expand to show the data and the notes, otherwise it'll just be truncated
  const [isExpanded, setIsExpanded] = useState(false);
  const observationsToShow = isExpanded ? props.observations : props.observations.slice(0, 1);
  return (
    <div
      className={`flex flex-col max-w-96 w-96 cursor-cell ${isExpanded ? 'whitespace-pre-line' : 'truncate'}`}
      onClick={(e) => {
        setIsExpanded((expanded) => !expanded);
        e.preventDefault();
      }}
    >
      {observationsToShow.map((observation, i) => {
        const Icon = observation.thumbs_up_thumbs_down ? FaThumbsUp : FaThumbsDown;
        const iconColor = observation.thumbs_up_thumbs_down ? 'text-green-500' : 'text-dwred';
        return (
          <div
            key={i}
            className={`flex flex-col gap-1 ${i === observationsToShow.length - 1 ? '' : 'border-b-2'}  border-gray-100`}
          >
            <div className={`flex flex-row gap-2 max-w-full items-baseline `}>
              {observation.thumbs_up_thumbs_down !== null && (
                <div className="translate-y-1">
                  <Icon className={iconColor} size={16} />
                </div>
              )}
              <div className="flex flex-row gap-2 pt-1">
                {observation.data_pointers.map((dataPointer, i) => (
                  <div key={i.toString()} className="flex flex-row gap-1">
                    <Chip
                      label={`${dataPointer.type === 'state_field' ? 'state' : 'attribute'}`}
                      chipType={`${dataPointer.type === 'state_field' ? 'annotateDataPointerState' : 'annotateDataPointerAttribute'}`}
                    />
                    <pre>.{dataPointer.field_name}</pre>
                  </div>
                ))}
              </div>
            </div>
            <div className="flex flex-grow justify-between flex-col">
              {observation.data_fields['note'] && (
                <div className="">
                  {' '}
                  <span className="text-gray-400 pr-2">Note</span>
                  {observation.data_fields['note']}
                </div>
              )}
              {observation.data_fields['ground_truth'] && (
                <div className="">
                  {' '}
                  <span className="text-gray-400 pr-2">Ground Truth:</span>
                  {observation.data_fields['ground_truth']}
                </div>
              )}
              {!isExpanded && props.observations.length > 1 && (
                <span className="text-gray-400 text-sm">
                  +{`${props.observations.length - 1} more`}
                </span>
              )}
            </div>
          </div>
        );
      })}
    </div>
  );
};

type Filters = {
  tags?: string[];
  actionNames?: string[];
};

type SearchBarProps = {
  filters: Filters;
  setFilters: (filters: Filters) => void;
  data: AnnotationOut[];
};

const SearchBar: React.FC<SearchBarProps> = ({ filters, setFilters, data }) => {
  // Options for react-select derived from the data
  const options = useMemo(() => {
    // Use Sets to ensure uniqueness
    const tagSet = new Set<string>();
    const actionNameSet = new Set<string>();

    // Populate the sets with unique tags and action names
    data.forEach((annotation) => {
      annotation.tags.forEach((tag) => tagSet.add(tag));
      actionNameSet.add(annotation.step_name);
    });

    // Convert sets to the format required for react-select
    const tagOptions = Array.from(tagSet).map((tag) => ({ value: tag, label: tag, type: 'tag' }));
    const actionNameOptions = Array.from(actionNameSet).map((name) => ({
      value: name,
      label: name,
      type: 'actionName'
    }));

    return [...tagOptions, ...actionNameOptions];
  }, [data]);

  // Handle selection from react-select
  // TODO -- remove anys here
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  const handleChange = (selectedOptions: any) => {
    const newFilters: Filters = {
      tags: selectedOptions

        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        .filter((option: any) => option.type === 'tag')

        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        .map((option: any) => option.value),
      actionNames: selectedOptions

        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        .filter((option: any) => option.type === 'actionName')

        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        .map((option: any) => option.value)
    };
    setFilters(newFilters);
  };
  const selectedOptions = options.filter((option) => {
    return (
      (filters.tags && filters.tags.includes(option.value)) ||
      (filters.actionNames && filters.actionNames.includes(option.value))
    );
  });

  const OptionType = (props: { type: 'actionName' | 'tag'; value: string }) => {
    const { type, value } = props;
    return (
      <div className="flex flex-row gap-2">
        <Chip
          chipType={type === 'tag' ? 'tag' : 'action'}
          label={type === 'tag' ? 'tag' : 'action'}
        />
        <span>{value}</span>
      </div>
    );
  };
  return (
    <Select
      options={options}
      onChange={handleChange}
      value={selectedOptions}
      isMulti
      placeholder="Search annotations by tag/action. Tags are *AND* queries, actions are *OR* queries."
      className="w-full"
      components={{
        Option: (props) => (
          <components.Option {...props}>
            <OptionType type={props.data.type as 'tag' | 'actionName'} value={props.data.value} />
          </components.Option>
        ),
        MultiValue: (props) => (
          <components.MultiValue {...props}>
            <OptionType type={props.data.type as 'tag' | 'actionName'} value={props.data.value} />
          </components.MultiValue>
        )
      }}
    />
  );
};

const filterData = (data: AnnotationOut[], filters: Filters) => {
  let filteredData = data;
  const tags = filters.tags;
  if (tags !== undefined && tags.length > 0) {
    filteredData = filteredData.filter((annotation) =>
      tags.every((tag) => annotation.tags.includes(tag))
    );
  }

  const actions = filters.actionNames;

  if (actions !== undefined && actions.length > 0) {
    filteredData = filteredData.filter((annotation) => actions.includes(annotation.step_name));
  }

  return filteredData;
};
export const AnnotationsTable = (props: {
  annotations: AnnotationOut[];
  projectId: string;
  onHover?: ((annotation: AnnotationOut) => void) | undefined;
  onClick?: ((annotation: AnnotationOut) => void) | undefined;
  highlightedSequence?: number;
  displayProjectLevelAnnotationsLink?: boolean;
  displayAppLevelIdentifiers?: boolean;
  displayInlineAppView?: boolean;
  displayTimestamps?: boolean;
  displaySearchBar?: boolean;
  allowInlineEdit?: boolean;
  refetchAnnotations?: () => void;
}) => {
  // Just in case we want to do live-updating, we need to pass it into the form...
  const updateAnnotationMutation = useMutation(
    (data: { annotationID: number; annotationData: AnnotationUpdate }) =>
      DefaultService.updateAnnotationApiV0ProjectIdAnnotationIdUpdateAnnotationsPut(
        props.projectId,
        data.annotationID,
        data.annotationData
      ),
    {
      onSuccess: () => {
        props.refetchAnnotations && props.refetchAnnotations();
        setCurrentlyEditingAnnotation(null); // We have to reset it somehow
      }
    }
  );
  const anyHavePartitionKey = props.annotations.some(
    (annotation) => annotation.partition_key !== null
  );
  const displayPartitionKeyColumn = anyHavePartitionKey && props.displayAppLevelIdentifiers;
  const displayAppIdColumn = props.displayAppLevelIdentifiers;
  const [expandedAnnotation, setExpandedAnnotation] = useState<AnnotationOut | null>(null);

  const [filters, setFilters] = useState<Filters>({});

  const sortedFilteredAnnotations = filterData(
    [...props.annotations].sort((a, b) => (a.step_sequence_id > b.step_sequence_id ? 1 : -1)),
    filters
  );

  // When this is open a modal will be open too for editing
  const [currentlyEditingAnnotation, setCurrentlyEditingAnnotation] =
    useState<AnnotationOut | null>(null);

  const annotationTarget = {
    sequenceId: currentlyEditingAnnotation?.step_sequence_id || 0,
    actionName: currentlyEditingAnnotation?.step_name || '',
    spanId: currentlyEditingAnnotation?.span_id || null
  };
  const { data: appData } = useQuery(
    ['steps', currentlyEditingAnnotation?.app_id],
    () =>
      DefaultService.getApplicationLogsApiV0ProjectIdAppIdPartitionKeyAppsGet(
        props.projectId,
        currentlyEditingAnnotation?.app_id as string,
        currentlyEditingAnnotation?.partition_key as string
      ),
    {
      enabled: currentlyEditingAnnotation !== null
    }
  );
  // Either we're editing one that exists, or we're creating a new one...
  // TODO -- simplify the logic here, we should have one path...
  const annotationStep = currentlyEditingAnnotation
    ? // Editing an existing annotation
      appData?.steps.find(
        (step) => step.step_start_log.sequence_id === currentlyEditingAnnotation?.step_sequence_id
      )
    : // Creating a new annotation
      appData?.steps.find(
        (step) => step.step_start_log.sequence_id === annotationTarget.sequenceId
      );
  return (
    <div className="w-full h-full">
      <Drawer
        title="Edit Annotation"
        open={currentlyEditingAnnotation !== null}
        close={() => {
          setCurrentlyEditingAnnotation(null);
        }}
      >
        {/* gate for type-checking, really this won't be open unless there is one */}
        {currentlyEditingAnnotation && (
          <AnnotationEditCreateForm
            tagOptions={Array.from(
              new Set(props.annotations.flatMap((annotation) => annotation.tags))
            )}
            allAnnotationTargets={[annotationTarget]}
            selectedAnnotationTarget={annotationTarget}
            existingAnnotation={currentlyEditingAnnotation as AnnotationOut}
            resetAnnotationContext={function (): void {
              setCurrentlyEditingAnnotation(null);
            }}
            createAnnotation={function (): void {}}
            updateAnnotation={function (annotationID: number, annotation: AnnotationUpdate): void {
              updateAnnotationMutation.mutate({
                annotationID: annotationID,
                annotationData: annotation
              });
            }}
            allPossibleDataTargets={annotationStep ? getPossibleDataTargets(annotationStep) : []}
          />
        )}
      </Drawer>
      {props.displaySearchBar && (
        <div className="w-full px-4 py-2 flex flex-row gap-2">
          <SearchBar filters={filters} setFilters={setFilters} data={props.annotations} />
          <DownloadAnnotationsButton data={sortedFilteredAnnotations} fileName="annotations.csv" />
        </div>
      )}
      <Table dense={1}>
        <TableHead>
          <TableRow>
            {props.displayInlineAppView && <TableHeader className=""></TableHeader>}
            {displayPartitionKeyColumn && <TableHeader className="">Partition Key</TableHeader>}
            {props.displayTimestamps && <TableHeader className="">Updated at</TableHeader>}
            {props.displayAppLevelIdentifiers && <TableHeader className="">App ID</TableHeader>}

            <TableHeader className="">Step</TableHeader>
            {/* <TableHeader className=""></TableHeader> */}
            {/* <TableHeader className="">Span</TableHeader> */}
            <TableHeader className="">Tags</TableHeader>
            <TableHeader className="">Observations</TableHeader>
            <TableHeader colSpan={1}>
              {props.displayProjectLevelAnnotationsLink ? (
                <Link to={`/annotations/${props.projectId}`}>
                  <FaExternalLinkAlt className="text-gray-500 w-3 h-3 hover:scale-125" />
                </Link>
              ) : (
                <></>
              )}
            </TableHeader>
            <TableHeader />
            {props.allowInlineEdit && <TableHeader className="w-6"></TableHeader>}
          </TableRow>
        </TableHead>
        <TableBody>
          {sortedFilteredAnnotations.map((annotation) => {
            const isExpanded = expandedAnnotation?.id === annotation.id;
            // const ThumbsUpIcon = annotation.thumbs_up_thumbs_down ? FaThumbsUp : FaThumbsDown;
            const selected = annotation.step_sequence_id === props.highlightedSequence;
            // const thumbsUpColor = annotation.thumbs_up_thumbs_down
            //   ? 'text-green-500'
            //   : 'text-dwred';
            return (
              <>
                <TableRow
                  key={annotation.id}
                  className={`cursor-pointer ${selected ? 'bg-gray-200' : 'hover:bg-gray-50'}`}
                  onClick={() => {
                    if (props.onClick) {
                      props.onClick(annotation);
                    }
                  }}
                  onMouseEnter={() => {
                    if (props.onHover) {
                      props.onHover(annotation);
                    }
                  }}
                >
                  {props.displayInlineAppView && (
                    <TableCell className="w-6 align-top">
                      {' '}
                      <ChevronDownIcon
                        className={classNames(
                          isExpanded ? 'rotate-180 text-gray-500' : 'text-gray-400',
                          'ml-auto h-5 w-5 shrink-0'
                        )}
                        aria-hidden="true"
                        onClick={() => {
                          setExpandedAnnotation(isExpanded ? null : annotation);
                        }}
                      />
                    </TableCell>
                  )}
                  {props.displayTimestamps && (
                    <TableCell className="align-top">
                      <DateTimeDisplay date={annotation.updated} mode={'short'} />
                    </TableCell>
                  )}
                  {displayPartitionKeyColumn && (
                    <TableCell>
                      <Link
                        className="hover:underline text-dwlightblue cursor-pointer"
                        to={`/project/${props.projectId}/${annotation.partition_key}`}
                      >
                        {annotation.partition_key}
                      </Link>
                    </TableCell>
                  )}
                  {displayAppIdColumn && (
                    <TableCell className="align-top">
                      <Link
                        className="hover:underline text-dwlightblue cursor-pointer"
                        to={`/project/${props.projectId}/${annotation.partition_key}/${annotation.app_id}`}
                      >
                        {annotation.app_id}
                      </Link>
                    </TableCell>
                  )}
                  <TableCell className="align-top">
                    <div className="align-top flex flex-row gap-1 items-baseline">
                      <Chip label={`action: ${annotation.step_sequence_id}`} chipType="action" />
                      <Link
                        className="flex flex-row gap-1 hover:underline text-dwlightblue cursor-pointer"
                        to={`/project/${props.projectId}/${annotation.partition_key}/${annotation.app_id}?sequence_id=${annotation.step_sequence_id}&tab=annotations`}
                      >
                        {/* <span>{annotation.step_sequence_id}</span> */}
                        {annotation.span_id !== null && <span>{`:${annotation.span_id}`}</span>}
                        <span>({annotation.step_name})</span>
                      </Link>
                    </div>
                  </TableCell>
                  <TableCell className="align-top">
                    <div className="flex flex-row gap-1 max-w-96 flex-wrap">
                      {annotation.tags.map((tag, i) => (
                        <Chip key={i} label={tag} chipType="tag" />
                      ))}
                    </div>
                  </TableCell>
                  <TableCell className="align-top">
                    <ObservationsView observations={annotation.observations} />
                  </TableCell>
                  <TableCell />
                  {props.allowInlineEdit && (
                    <TableCell className="align-top">
                      <AnnotateButton
                        appID={annotation.app_id}
                        partitionKey={annotation.partition_key}
                        sequenceID={annotation.step_sequence_id}
                        spanID={annotation.span_id || undefined}
                        existingAnnotation={annotation}
                        setCurrentEditingAnnotationContext={(context) => {
                          setCurrentlyEditingAnnotation(
                            context.existingAnnotation as AnnotationOut
                          );
                        }}
                      />
                    </TableCell>
                  )}
                </TableRow>
                <TableRow className="w-full">
                  {isExpanded && (
                    <TableCell className="" colSpan={16}>
                      <div className="px-10">
                        <InlineAppView
                          projectId={props.projectId}
                          partitionKey={annotation.partition_key}
                          appId={annotation.app_id}
                          sequenceID={annotation.step_sequence_id}
                        ></InlineAppView>
                      </div>
                    </TableCell>
                  )}
                </TableRow>
              </>
            );
          })}
        </TableBody>
      </Table>
      {props.annotations.length === 0 && (
        <div className="flex flex-row justify-center items-center h-96">
          <div className="text-gray-400 text-lg flex flex-row gap-1">
            No annotations found -- go to an application run and click on an entry in the
            annnotations column <PencilIcon className="h-6 w-6" /> to add annotations.
          </div>
        </div>
      )}
    </div>
  );
};

const getEmptyObservation = (): AnnotationObservation => ({
  data_fields: {
    note: '',
    ground_truth: ''
  },
  thumbs_up_thumbs_down: null,
  data_pointers: []
});

const ObservationForm = (props: {
  observation: AnnotationObservation;
  addObservation: () => void;
  removeObservation: () => void;
  setObservation: (observation: AnnotationObservation) => void;
  allowDelete: boolean;
  possibleDataTargets: AnnnotationDataPointerWithValue[];
}) => {
  const observation = props.observation;
  const thumbsUp = observation.thumbs_up_thumbs_down;
  const [showGroundTruth, setShowGroundTruth] = useState(
    Boolean(observation.data_fields['ground_truth'])
  );
  const toggleThumbsUp = (clickedThumbsUp: boolean) => {
    if (thumbsUp === clickedThumbsUp) {
      observation.thumbs_up_thumbs_down = null;
    } else {
      observation.thumbs_up_thumbs_down = clickedThumbsUp;
    }
    props.setObservation(observation);
  };

  const possibleDataTargetValues =
    props.possibleDataTargets.map((dataPointer) => ({
      value: dataPointer,
      label: `${dataPointer.type}:${dataPointer.field_name}`
    })) || [];
  return (
    <div className="flex flex-col">
      <div className="flex flex-row gap-2">
        <div className="flex space-x-2">
          <button>
            <FaThumbsUp
              className={`text-green-500 hover:scale-110 ${thumbsUp !== true ? 'opacity-50' : ''}`}
              size={16}
              onClick={() => toggleThumbsUp(true)}
            />
          </button>
          <button>
            <FaThumbsDown
              className={`text-dwred hover:scale-110 ${thumbsUp !== false ? 'opacity-50' : ''}`}
              size={16}
              onClick={() => toggleThumbsUp(false)}
            />
          </button>
        </div>
        <div className="flex flex-grow">
          <Select
            options={possibleDataTargetValues}
            value={
              possibleDataTargetValues.find(
                (option) =>
                  option.value.field_name === observation.data_pointers[0]?.field_name &&
                  option.value.type === observation.data_pointers[0]?.type
              ) || null
            }
            onChange={(selectedOption) => {
              if (selectedOption === null) return;
              observation.data_pointers = [selectedOption.value];
              props.setObservation(observation);
            }}
            placeholder="Select data (fields in state/attributes) associated with your observation"
            styles={{
              placeholder: (provided) => ({
                ...provided,
                whiteSpace: 'nowrap',
                overflow: 'hidden',
                textOverflow: 'ellipsis'
              }),
              control: (provided) => ({
                ...provided,
                resize: 'horizontal'
              })
            }}
            className="w-full"
            // TODO: Create and use a custom component for the menu items
            components={{
              Option: (props) => {
                // TODO: Customize the menu item component here
                return (
                  <components.Option {...props}>
                    {' '}
                    <div className="flex flex-row gap-2">
                      <Chip
                        chipType={
                          props.data.value.type === 'state_field'
                            ? 'annotateDataPointerState'
                            : 'annotateDataPointerAttribute'
                        }
                        label={props.data.value.type}
                      />
                      <span>{props.data.value.field_name}</span>
                      <pre>{props.data.value.value}</pre>
                    </div>
                  </components.Option>
                );
              },
              SingleValue: (props) => {
                // TODO: Customize the selected item component here
                return (
                  <components.SingleValue {...props}>
                    <div className="flex flex-row gap-2">
                      <Chip
                        chipType={
                          props.data.value.type === 'state_field'
                            ? 'annotateDataPointerState'
                            : 'annotateDataPointerAttribute'
                        }
                        label={props.data.value.type}
                      />
                      <span>{props.data.value.field_name}</span>
                      {/* <pre>{props.data.value.value}</pre> */}
                    </div>
                  </components.SingleValue>
                );
              }
            }}
          />
        </div>
        <button
          className="w-10 inline-flex justify-center items-center rounded-md bg-green-600 px-3 py-2 text-sm font-semibold text-white shadow-sm hover:bg-green-600/70 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-700/70"
          disabled={observation.data_fields['ground_truth']}
          onClick={() => {
            setShowGroundTruth((show) => !show);
          }}
        >
          <FaClipboardList className="h-4 w-4" />
        </button>

        {props.allowDelete && (
          <button
            className="w-10 inline-flex justify-center items-center rounded-md bg-dwred/80 px-3 py-2 text-sm font-semibold text-white shadow-sm hover:bg-gray-600/70 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-700/70"
            onClick={() => {
              props.removeObservation();
            }}
          >
            <XMarkIcon className="h-6 w-6" />
          </button>
        )}
        <button
          className="w-10 inline-flex justify-center items-center rounded-md bg-dwdarkblue/80 px-3 py-2 text-sm font-semibold text-white shadow-sm hover:bg-gray-600/70 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-gray-700/70"
          onClick={() => {
            props.addObservation();
          }}
        >
          {'+'}
        </button>
      </div>
      {observation.data_pointers.length > 0 && (
        <pre className=" text-sm mt-2 whitespace-pre-wrap">
          {
            possibleDataTargetValues.find(
              (option) =>
                option.value.field_name === observation.data_pointers[0]?.field_name &&
                option.value.type === observation.data_pointers[0]?.type
            )?.value.value
          }
        </pre>
      )}
      <div className="flex flex-col mt-2 items-baseline gap-2">
        <div className="text text-gray-500 text-md min-w-max">Notes</div>
        <textarea
          placeholder="Notes about the annotation go here..."
          value={observation.data_fields['note']}
          rows={4}
          className="block w-full rounded-md border-0 p-2 text-gray-900 shadow-sm ring-1 ring-inset ring-gray-300 placeholder:text-gray-400 focus:ring-2 focus:ring-inset focus:ring-dwlightblue/30 sm:text-sm sm:leading-6"
          defaultValue={''}
          onChange={(e) => {
            observation.data_fields['note'] = e.target.value;
            props.setObservation(observation);
          }}
        />
      </div>
      {(showGroundTruth || observation.data_fields['ground_truth']) && (
        <div className="flex flex-col mt-2 items-baseline gap-2">
          <div className="text text-gray-500 text-md min-w-max">Ground truth </div>
          <textarea
            placeholder="Enter a ground-truth (optional)"
            value={observation.data_fields['ground_truth']}
            rows={4}
            className="resize-none block w-full hide-scrollbar rounded-md border-0 p-2 text-gray-900 shadow-sm ring-1 ring-inset ring-gray-300 placeholder:text-gray-400 focus:ring-2 focus:ring-inset focus:ring-dwlightblue/30 sm:text-sm sm:leading-6"
            defaultValue={''}
            onChange={(e) => {
              observation.data_fields['ground_truth'] = e.target.value;
              props.setObservation(observation);
            }}
          ></textarea>
        </div>
      )}
    </div>
  );
};
type AnnnotationDataPointerWithValue = AnnotationDataPointer & { value: string };

export const AnnotateButton = (props: {
  appID: string;
  partitionKey: string | null;
  sequenceID: number;
  spanID?: string;
  attribute?: string; // TODO -- consider whether we want to remove, we generally annotate at the step level
  // But we might want to prepopulate the attribute if we are annotating a specific attribute (in the observations field)
  existingAnnotation: AnnotationOut | undefined;
  setCurrentEditingAnnotationContext: (context: {
    appId: string;
    partitionKey: string | null;
    sequenceId: number;
    attributeName: string | undefined;
    spanId: string | null;
    existingAnnotation: AnnotationOut | undefined;
  }) => void;
  // setTab: (tab: string) => void; // used if we want to change tab for view
}) => {
  const Icon = props.existingAnnotation ? PencilSquareIcon : PlusIcon;
  return (
    <Icon
      className="hover:scale-125 h-4 w-4"
      onClick={(e) => {
        props.setCurrentEditingAnnotationContext({
          appId: props.appID,
          partitionKey: props.partitionKey,
          sequenceId: props.sequenceID,
          attributeName: props.attribute,
          spanId: props.spanID || null,
          existingAnnotation: props.existingAnnotation
        });
        e.stopPropagation();
        e.preventDefault();
      }}
    />
  );
};

const DEFAULT_TAG_OPTIONS = [
  'to-review',
  'hallucination',
  'incomplete',
  'incorrect',
  'correct',
  'ambiguous',
  'user-error',
  'intentional-user-error'
];
const AnnotationEditCreateForm = (props: {
  tagOptions: string[];
  allAnnotationTargets: AnnotationTarget[];
  selectedAnnotationTarget: AnnotationTarget | undefined;
  existingAnnotation: AnnotationOut | undefined; // Only there if we are editing an existing annotation
  resetAnnotationContext: () => void;
  createAnnotation: (annotation: AnnotationCreate) => void;
  updateAnnotation: (annotationID: number, annotation: AnnotationUpdate) => void;
  allPossibleDataTargets: AnnnotationDataPointerWithValue[];
}) => {
  const [targetValue, setTargetValue] = useState<TargetOptionType | null>(null);

  const [tags, setTags] = useState<TagOptionType[]>([]);

  // const [attribute, setAttribute] = useState<string>('');
  const [observations, setObservations] = useState<AnnotationObservation[]>([
    getEmptyObservation()
  ]);

  // Define options for the select components

  const tagOptions: TagOptionType[] = [...DEFAULT_TAG_OPTIONS, ...props.tagOptions].map((tag) => ({
    value: tag,
    label: tag
  }));

  const allTargets = props.allAnnotationTargets.map((target) => ({
    value: target,
    label: getLabelForTarget(target)
  }));

  useEffect(() => {
    // Reset to the selected annotation if it exists
    if (props.existingAnnotation) {
      setTargetValue({
        value: {
          sequenceId: props.existingAnnotation.step_sequence_id,
          actionName: props.existingAnnotation.step_name,
          spanId: props.existingAnnotation.span_id
        },
        label: getLabelForTarget({
          sequenceId: props.existingAnnotation.step_sequence_id,
          actionName: props.existingAnnotation.step_name,
          spanId: props.existingAnnotation.span_id
        })
      });
      setObservations(props.existingAnnotation.observations);
      setTags(
        props.existingAnnotation.tags.map((tag) => ({
          value: tag,
          label: tag
        }))
      );
      // Otherwise, create a new one
      // } else if (props.selectedAnnotationTarget) {
      //   setTargetValue({
      //     value: props.selectedAnnotationTarget,
      //     label: getLabelForTarget(props.selectedAnnotationTarget)
      //   });
    }
  }, [props.existingAnnotation]);

  useEffect(() => {
    if (props.selectedAnnotationTarget && !props.existingAnnotation) {
      setTargetValue({
        value: props.selectedAnnotationTarget,
        label: getLabelForTarget(props.selectedAnnotationTarget)
      });
      setTags([]);
      setObservations([getEmptyObservation()]);
    }
  }, [props.selectedAnnotationTarget?.sequenceId]);
  const TagChipOption = (props: { label: string; chipType: string }) => {
    return (
      <div className="flex flex-row gap-2">
        <Chip label={props.label} chipType={'tag'} />
      </div>
    );
  };
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  const handleTagChange = (selectedOptions: any) => {
    setTags(selectedOptions ? selectedOptions.map((option: TagOptionType) => option) : []);
  };
  const mode = props.existingAnnotation !== undefined ? 'edit' : 'create';

  return (
    <div className="p-4 space-y-4">
      <Select
        options={allTargets}
        value={targetValue}
        onChange={(selectedOption) => setTargetValue(selectedOption)}
        placeholder="Select an option..."
        className="basic-single"
        classNamePrefix="select"
      />
      <CreatableSelect
        options={tagOptions}
        isMulti
        value={tags}
        onChange={handleTagChange}
        placeholder="Select tags/create new"
        className="basic-multi-select"
        classNamePrefix="select"
        components={{
          Option: (props) => (
            <components.Option {...props}>
              <TagChipOption label={props.data.value} chipType="tag" />
            </components.Option>
          ),
          MultiValue: (props) => (
            <components.MultiValue {...props}>
              <TagChipOption label={props.data.value} chipType="tag" />
            </components.MultiValue>
          )
        }}
      />
      <h1 className="text-md font-semibold text-gray-600">Observations </h1>
      {observations.map((observation, i) => {
        const allowDelete = observations.length > 1;
        return (
          <ObservationForm
            key={i}
            observation={observation}
            addObservation={() => {
              setObservations([...observations, getEmptyObservation()]);
            }}
            removeObservation={() => {
              setObservations(observations.filter((_, index) => index !== i));
            }}
            allowDelete={allowDelete}
            setObservation={(observation) => {
              setObservations(observations.map((obs, index) => (index === i ? observation : obs)));
            }}
            possibleDataTargets={props.allPossibleDataTargets}
          />
        );
      })}
      <div className="flex flex-row justify-end items-center gap-2">
        {/* <div className="flex-grow"> */}
        {/* <div className="text-sm min-w-max text-gray-500">Ground truth (optional) </div> */}

        {/* </div> */}
        <div className="flex flex-row gap-2">
          <button
            onClick={props.resetAnnotationContext}
            className="w-fullinline-flex items-center rounded-md bg-dwred/80 px-3 py-2 text-sm font-semibold text-white shadow-sm hover:bg-dwred/30 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-dwred/70"
          >
            {'Cancel'}
          </button>
          <button
            onClick={() => {
              if (mode === 'create') {
                props.createAnnotation({
                  observations: observations,
                  tags: tags.map((tag) => tag.value),
                  span_id: targetValue?.value.spanId || null,
                  step_name: targetValue?.value.actionName || ''
                });
              } else {
                props.updateAnnotation(props.existingAnnotation?.id as number, {
                  observations: observations,
                  tags: tags.map((tag) => tag.value),
                  span_id: targetValue?.value.spanId || null,
                  step_name: targetValue?.value.actionName || ''
                });
              }
            }}
            type="submit"
            className="w-fullinline-flex items-center rounded-md bg-dwlightblue/80 px-3 py-2 text-sm font-semibold text-white shadow-sm hover:bg-dwlightblue/30 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-dwlightblue/30"
          >
            {mode === 'edit' ? 'Update' : 'Create'}
          </button>
        </div>
      </div>
    </div>
  );
};
/**
 * Annotations view for the application -- this is the top-level project view
 * @returns
 */
export const AnnotationsViewContainer = () => {
  const { projectId } = useLocationParams();
  const { data: backendSpec } = useQuery(['backendSpec'], () =>
    DefaultService.getAppSpecApiV0MetadataAppSpecGet().then((response) => {
      return response;
    })
  );

  // TODO -- use a skiptoken to bypass annotation loading if we don't need them
  const { data, refetch } = useQuery(['annotations', projectId], () =>
    DefaultService.getAnnotationsApiV0ProjectIdAnnotationsGet(projectId as string)
  );
  // dummy value as this will not be linked to if annotations are not supported

  if (data === undefined || backendSpec === undefined) return <Loading />;
  return (
    <AnnotationsTable
      annotations={data}
      projectId={projectId as string}
      displayAppLevelIdentifiers
      displayInlineAppView
      displayTimestamps
      displaySearchBar
      allowInlineEdit
      refetchAnnotations={refetch}
    />
  );
};



---
File: /burr/telemetry/ui/src/components/routes/app/AppView.tsx
---

import { Navigate } from 'react-router';
import {
  AnnotationCreate,
  AnnotationOut,
  AnnotationUpdate,
  AttributeModel,
  DefaultService
} from '../../../api';
import { useMutation, useQuery } from 'react-query';
import { Loading } from '../../common/loading';
import { ApplicationTable } from './StepList';
import { TwoColumnLayout, TwoRowLayout } from '../../common/layout';
import { AppStateView } from './StateMachine';
import { createContext, useEffect, useState } from 'react';
import { Status, useLocationParams } from '../../../utils';
import { GraphView } from './GraphView';
import { useSearchParams } from 'react-router-dom';

export const REFRESH_INTERVAL = 500;

export const backgroundColorsForStatus = (status: Status) => {
  const colorsByStatus = {
    success: 'bg-green-500/80',
    failure: 'bg-dwred/80',
    running: 'bg-dwlightblue/80'
  };
  return colorsByStatus[status];
};

/**
 * Some tailwind mappings for bg colors -- we get progressively lighter the further back in
 * time we go
 * @param index Index away from the current step
 * @param status Status of the step at that index
 * @returns A tailwind class for the background color
 */
export const backgroundColorsForIndex = (index: number, status: Status) => {
  const colorsByStatus = {
    success: [
      'bg-green-500/80',
      'bg-green-500/70',
      'bg-green-500/60',
      'bg-green-500/50',
      'bg-green-500/30',
      'bg-green-500/10',
      'bg-green-500/5'
    ],
    failure: [
      'bg-dwred/80',
      'bg-dwred/70',
      'bg-dwred/60',
      'bg-dwred/50',
      'bg-dwred/30',
      'bg-dwred/10',
      'bg-dwred/5'
    ],
    // We likely will not need the prior running colors but :shrug: best to keep it consistent
    running: [
      'bg-dwlightblue/80',
      'bg-dwlightblue/70',
      'bg-dwlightblue/60',
      'bg-dwlightblue/50',
      'bg-dwlightblue/30',
      'bg-dwlightblue/10',
      'bg-dwlightblue/5'
    ]
  };
  const colors = colorsByStatus[status];
  if (index < colors.length) {
    return colors[index];
  }
  return colors[colors.length - 1];
};

// Default number of previous actions to show
const NUM_PREVIOUS_ACTIONS = 0;

export type SequenceLocation = {
  appId: string;
  partitionKey: string | null;
  sequenceId: number;
};

export type AnnotationEditingContext = SequenceLocation & {
  spanId: string | null;
  attributeName?: string;
  existingAnnotation: AnnotationOut | undefined;
};

// TODO -- break this out into multiple pieces -- it's a bit of a monolith of data passed down
// Classic react problem of using context when data should be wired through -- it inherently gets super complicated...
export type HighlightState = {
  attributesHighlighted: AttributeModel[];
  setAttributesHighlighted: (attributes: AttributeModel[]) => void;
  setTab: (tab: string) => void;
  tab: string;
  setCurrentSelectedIndex: (index: SequenceLocation | undefined) => void;
  currentSelectedIndex?: SequenceLocation;
  setCurrentHoverIndex: (index: SequenceLocation | undefined) => void;
  currentHoverIndex?: SequenceLocation;
  currentEditingAnnotationContext?: AnnotationEditingContext;
  setCurrentEditingAnnotationContext: (
    annotationContext: AnnotationEditingContext | undefined
  ) => void;
  createAnnotation: (
    projectId: string,
    partitionKey: string | null,
    appId: string,
    sequenceId: number,
    spanId: string | undefined,
    annotation: AnnotationCreate
  ) => Promise<AnnotationOut>;
  updateAnnotation: (
    annotationID: number,
    annotationUpdate: AnnotationUpdate
  ) => Promise<AnnotationOut>;
  refreshAnnotationData: () => void;
};

export const AppContext = createContext<HighlightState>({
  attributesHighlighted: [],
  setAttributesHighlighted: () => {},
  setTab: () => {},
  tab: 'data',
  setCurrentSelectedIndex: () => {},
  currentSelectedIndex: undefined,
  setCurrentHoverIndex: () => {},
  currentHoverIndex: undefined,
  currentEditingAnnotationContext: undefined,
  setCurrentEditingAnnotationContext: () => {},
  createAnnotation: () => {
    throw new Error('Not to be used with default context values');
  },
  updateAnnotation: () => {
    throw new Error('Not to be used with default context values');
  },
  refreshAnnotationData: () => {}
});

/**
 * View for the "Application" page. This has two columns:
 * 1. A list of steps
 * 2. A view of the state machine/data/action
 */
export const AppView = (props: {
  projectId: string;
  appId: string;
  partitionKey: string | null;
  orientation: 'stacked_vertical' | 'stacked_horizontal';
  defaultAutoRefresh?: boolean;
  enableFullScreenStepView: boolean;
  enableMinimizedStepView: boolean;
  allowAnnotations: boolean;
  restrictTabs?: string[];
  disableNavigateSteps?: boolean;
  forceCurrentActionIndex?: SequenceLocation;
  forceFullScreen?: boolean;
}) => {
  const [searchParams, setSearchParams] = useSearchParams();
  const [topToBottomChronological, setTopToBottomChronological] = useState(true);
  const [inspectViewOpen, setInspectViewOpen] = useState(false);

  const currentSequenceLocation = (
    searchParams.get('sequence_location')
      ? JSON.parse(searchParams.get('sequence_location')!)
      : undefined
  ) as SequenceLocation | undefined;

  // we want to use the current App ID passed in
  const appID = props.appId;
  // But if we want to focus on sub-applications we need to load those steps as well
  // These will get passed to the data views
  const currentFocusAppID =
    currentSequenceLocation?.appId !== undefined ? currentSequenceLocation?.appId : appID;
  // Ditto with partition key
  const partitionKey = props.partitionKey;
  const currentFocusPartitionKey =
    currentSequenceLocation?.partitionKey !== undefined
      ? currentSequenceLocation?.partitionKey
      : props.partitionKey;
  const _setCurrentSequenceLocation = (location: SequenceLocation | undefined) => {
    const newSearchParams = new URLSearchParams(searchParams); // Clone the searchParams
    if (location !== undefined) {
      newSearchParams.set('sequence_location', JSON.stringify(location));
    } else {
      newSearchParams.delete('sequence_location');
    }
    setSearchParams(newSearchParams); // Update the searchParams with the new object
  };

  if (
    props.forceCurrentActionIndex !== undefined &&
    JSON.stringify(currentSequenceLocation) !== JSON.stringify(props.forceCurrentActionIndex)
  ) {
    _setCurrentSequenceLocation(props.forceCurrentActionIndex);
  }

  const setCurrentSequenceLocation = (location: SequenceLocation | undefined) => {
    if (!props.disableNavigateSteps) {
      _setCurrentSequenceLocation(location);
    }
  };
  const { projectId } = props;
  // const [currentActionIndex, setCurrentActionIndex] = useState<number | undefined>(undefined);
  const [hoverIndex, setHoverIndex] = useState<SequenceLocation | undefined>(undefined);
  const [autoRefresh, setAutoRefresh] = useState(props.defaultAutoRefresh || false);
  const shouldQuery = projectId !== undefined && appID !== undefined;
  const [minimizedTable, setMinimizedTable] = useState(false);
  const [highlightedAttributes, setHighlightedAttributes] = useState<AttributeModel[]>([]);
  const fullScreen =
    props.forceFullScreen ||
    (searchParams.get('full') === 'true' && props.enableFullScreenStepView);
  const displayGraphAsTabs = props.orientation === 'stacked_vertical' || fullScreen;
  const defaultTab = displayGraphAsTabs ? 'graph' : 'data';
  // const [currentTab, setCurrentTab] = useState(defaultTab);
  const currentTab = searchParams.get('tab') || defaultTab;
  const setCurrentTab = (tab: string) => {
    const newSearchParams = new URLSearchParams(searchParams); // Clone the searchParams
    newSearchParams.set('tab', tab);
    setSearchParams(newSearchParams); // Update the searchParams with the new object
  };
  const setFullScreen = (full: boolean) => {
    const newSearchParams = new URLSearchParams(searchParams); // Clone the searchParams
    if (full) {
      newSearchParams.set('full', 'true');
    } else {
      newSearchParams.delete('full');
    }
    setSearchParams(newSearchParams); // Update the searchParams with the new object
  };
  const [currentEditingAnnotationContext, setCurrentEditingAnnotationContext] = useState<
    AnnotationEditingContext | undefined
  >(undefined);
  const { data: backendSpec } = useQuery(['backendSpec'], () =>
    DefaultService.getAppSpecApiV0MetadataAppSpecGet().then((response) => {
      return response;
    })
  );
  const { data, error } = useQuery(
    ['steps', appID, partitionKey],
    () =>
      DefaultService.getApplicationLogsApiV0ProjectIdAppIdPartitionKeyAppsGet(
        projectId as string,
        appID as string,
        props.partitionKey !== null ? props.partitionKey : '__none__'
      ),
    {
      refetchInterval: autoRefresh ? REFRESH_INTERVAL : false,
      enabled: shouldQuery
    }
  );

  const { data: currentFocusStepsData } = useQuery(
    ['steps', currentFocusAppID, currentFocusPartitionKey],
    () =>
      DefaultService.getApplicationLogsApiV0ProjectIdAppIdPartitionKeyAppsGet(
        projectId as string,
        currentFocusAppID as string,
        currentFocusPartitionKey !== null ? currentFocusPartitionKey : '__none__'
      ),
    {
      refetchInterval: autoRefresh ? REFRESH_INTERVAL : false,
      enabled: currentFocusAppID !== appID && currentFocusAppID !== undefined
    }
  );
  // TODO -- use a skiptoken to bypass annotation loading if we don't need them
  const { data: annotationsData, refetch: refetchAnnotationsData } = useQuery(
    ['annotations', appID, partitionKey],
    () =>
      DefaultService.getAnnotationsApiV0ProjectIdAnnotationsGet(
        projectId as string,
        appID as string,
        partitionKey !== null ? partitionKey : '__none__'
      ),
    {
      refetchInterval: autoRefresh ? REFRESH_INTERVAL : false,
      enabled: shouldQuery && props.allowAnnotations && backendSpec?.supports_annotations
    }
  );

  const { data: currentFocusAnnotationsData } = useQuery(
    ['annotations', currentFocusAppID, currentFocusPartitionKey],
    () =>
      DefaultService.getAnnotationsApiV0ProjectIdAnnotationsGet(
        projectId as string,
        currentFocusAppID as string,
        currentFocusPartitionKey !== null ? partitionKey : '__none__'
      ),
    {
      enabled:
        shouldQuery &&
        props.allowAnnotations &&
        backendSpec?.supports_annotations &&
        currentFocusAppID !== appID &&
        currentFocusAppID !== undefined
    }
  );

  const createAnnotationMutation = useMutation(
    (data: {
      projectId: string;
      annotationData: AnnotationCreate;
      appID: string;
      partitionKey: string | null;
      sequenceID: number;
    }) =>
      DefaultService.createAnnotationApiV0ProjectIdAppIdPartitionKeySequenceIdAnnotationsPost(
        projectId,
        appID,
        data.partitionKey !== null ? data.partitionKey : '__none__',
        data.sequenceID,
        data.annotationData
      )
  );

  const updateAnnotationMutation = useMutation(
    (data: { annotationID: number; annotationData: AnnotationUpdate }) =>
      DefaultService.updateAnnotationApiV0ProjectIdAnnotationIdUpdateAnnotationsPut(
        projectId,
        data.annotationID,
        data.annotationData
      )
  );

  useEffect(() => {
    const steps = data?.steps || [];
    const maxSequenceID = Math.max(...steps.map((step) => step.step_start_log.sequence_id));
    const minSequenceID = Math.min(...steps.map((step) => step.step_start_log.sequence_id));
    const handleKeyDown = (event: KeyboardEvent) => {
      switch (event.key) {
        case topToBottomChronological ? 'ArrowUp' : 'ArrowDown':
          if (
            currentSequenceLocation === undefined ||
            currentSequenceLocation.sequenceId <= minSequenceID
          ) {
            setCurrentSequenceLocation({
              appId: appID,
              partitionKey: partitionKey,
              sequenceId: minSequenceID
            });
          } else {
            setCurrentSequenceLocation({
              ...currentSequenceLocation,
              sequenceId: currentSequenceLocation.sequenceId - 1
            });
            // semtCurrentActionIndex(currentActionIndex - 1);
          }
          break;
        case topToBottomChronological ? 'ArrowDown' : 'ArrowUp':
          // case 'ArrowUp':
          if (currentSequenceLocation === undefined) {
            setCurrentSequenceLocation({
              appId: appID,
              partitionKey: partitionKey,
              sequenceId: maxSequenceID
            });
          } else if (currentSequenceLocation.sequenceId >= maxSequenceID) {
            // setCurrentActionIndex(currentActionIndex);
          } else {
            setCurrentSequenceLocation({
              ...currentSequenceLocation,
              sequenceId: currentSequenceLocation.sequenceId + 1
            });
            // setCurrentActionIndex(currentActionIndex + 1);
          }
          break;
        default:
          break;
      }
    };

    document.addEventListener('keydown', handleKeyDown);

    return () => {
      document.removeEventListener('keydown', handleKeyDown);
    };
  }, [data?.steps, currentSequenceLocation, setCurrentSequenceLocation]);
  useEffect(() => {
    if (autoRefresh) {
      const maxSequenceID = Math.max(
        ...(data?.steps.map((step) => step.step_start_log.sequence_id) || [])
      );
      setCurrentSequenceLocation({
        appId: appID,
        partitionKey: partitionKey,
        sequenceId: maxSequenceID
      });
    }
  }, [data, autoRefresh]);
  if (!shouldQuery) {
    return <Navigate to={'/projects'} />;
  }

  if (error) return <div>Error loading steps</div>;
  if (data === undefined || backendSpec === undefined) {
    return <Loading />;
  }
  const displayAnnotations = props.allowAnnotations && backendSpec.supports_annotations;
  // TODO -- re-enable this if I want...
  // const previousActions =
  //   currentActionIndex !== undefined
  //     ? getPriorActions(currentActionIndex, data.steps, NUM_PREVIOUS_ACTIONS)
  //     : currentActionIndex;
  const stepsCopied = [...data.steps];
  const stepsSorted = stepsCopied.sort((a, b) => {
    // Parse dates to get time in milliseconds
    const sequenceA = a.step_start_log.sequence_id;
    const sequenceB = b.step_start_log.sequence_id;
    if (sequenceA !== sequenceB) {
      return sequenceB - sequenceA;
    }
    const timeA = new Date(a.step_start_log.start_time).getTime();
    const timeB = new Date(b.step_start_log.start_time).getTime();

    // If times are equal down to milliseconds, compare microseconds
    if (timeA === timeB) {
      const microA = parseInt(a.step_start_log.start_time.slice(-6));
      const microB = parseInt(b.step_start_log.start_time.slice(-6));

      // Compare microseconds
      return microB - microA;
    }

    // Otherwise, compare milliseconds
    return timeB - timeA;
  });
  const Layout = props.orientation === 'stacked_horizontal' ? TwoColumnLayout : TwoRowLayout;
  const currentStep = stepsSorted.find(
    (step) =>
      step.step_start_log.sequence_id === currentSequenceLocation?.sequenceId &&
      appID === currentSequenceLocation?.appId &&
      partitionKey === currentSequenceLocation?.partitionKey
  );
  const hoverAction = hoverIndex
    ? stepsSorted.find(
        (step) =>
          step.step_start_log.sequence_id === hoverIndex.sequenceId &&
          appID === hoverIndex.appId &&
          partitionKey === hoverIndex.partitionKey
      )
    : undefined;

  return (
    <AppContext.Provider
      value={{
        attributesHighlighted: highlightedAttributes,
        setAttributesHighlighted: setHighlightedAttributes,
        setTab: setCurrentTab,
        tab: currentTab,
        setCurrentSelectedIndex: (loc) => {
          setCurrentSequenceLocation(loc);
          setInspectViewOpen(loc !== undefined);
        },
        currentSelectedIndex: currentSequenceLocation,
        setCurrentHoverIndex: setHoverIndex,
        currentHoverIndex: hoverIndex,
        currentEditingAnnotationContext: currentEditingAnnotationContext,
        setCurrentEditingAnnotationContext: setCurrentEditingAnnotationContext,
        // TODO -- handle span ID
        createAnnotation: async (
          projectId,
          partitionKey,
          appId,
          sequenceId,
          spanId,
          annotation
        ) => {
          const response = await createAnnotationMutation.mutateAsync({
            projectId: projectId,
            partitionKey: partitionKey,
            annotationData: annotation,
            appID: appId as string,
            sequenceID: sequenceId
          });
          await refetchAnnotationsData();
          return response;
        },
        updateAnnotation: async (annotationID, annotation) => {
          const response = await updateAnnotationMutation.mutateAsync({
            annotationID: annotationID,
            annotationData: annotation
          });
          await refetchAnnotationsData();
          return response;
        },
        refreshAnnotationData: refetchAnnotationsData
      }}
    >
      <Layout
        mode={fullScreen ? 'expanding-second' : minimizedTable ? 'first-minimal' : 'half'}
        firstItem={
          <div className="w-full h-full flex flex-col">
            <div
              className={`w-full ${fullScreen ? 'h-full' : props.orientation === 'stacked_vertical' ? 'h-full' : 'h-1/2'}`}
            >
              <ApplicationTable
                steps={stepsSorted}
                appID={appID}
                partitionKey={partitionKey}
                numPriorIndices={NUM_PREVIOUS_ACTIONS}
                autoRefresh={autoRefresh}
                setAutoRefresh={setAutoRefresh}
                minimized={minimizedTable}
                setMinimized={setMinimizedTable}
                projectId={projectId}
                parentPointer={data?.parent_pointer || undefined}
                spawningParentPointer={data?.spawning_parent_pointer || undefined}
                links={data.children || []}
                fullScreen={fullScreen}
                setFullScreen={setFullScreen}
                allowFullScreen={props.enableFullScreenStepView}
                allowMinimized={props.enableMinimizedStepView}
                topToBottomChronological={topToBottomChronological}
                setTopToBottomChronological={setTopToBottomChronological}
                toggleInspectViewOpen={() => setInspectViewOpen(!inspectViewOpen)}
                displayAnnotations={displayAnnotations}
                annotations={annotationsData || []}
              />
            </div>
            {!fullScreen && props.orientation === 'stacked_horizontal' && (
              <div className="h-1/2 w-[full]">
                <GraphView
                  stateMachine={data.application}
                  currentAction={currentStep}
                  // highlightedActions={previousActions}
                  highlightedActions={[]}
                  hoverAction={hoverAction}
                />
              </div>
            )}
          </div>
        }
        secondItem={
          <AppStateView
            steps={currentFocusStepsData?.steps || stepsSorted}
            stateMachine={currentFocusStepsData?.application || data.application}
            // stateMachine={data.application}
            // highlightedActions={previousActions}
            highlightedActions={[]}
            hoverAction={hoverAction}
            currentActionLocation={currentSequenceLocation}
            displayGraphAsTab={displayGraphAsTabs} // in this case we want the graph as a tab
            setMinimized={(min: boolean) => setInspectViewOpen(!min)}
            isMinimized={!inspectViewOpen}
            allowMinimized={inspectViewOpen && fullScreen}
            // TODO -- render better if this is undefined -- this is an odd way to fall back
            annotations={currentFocusAnnotationsData || annotationsData}
            restrictTabs={props.restrictTabs}
            allowAnnotations={displayAnnotations}
          />
        }
        animateSecondPanel={inspectViewOpen}
      />
    </AppContext.Provider>
  );
};

export const AppViewContainer = () => {
  const { projectId, appId, partitionKey } = useLocationParams();
  return (
    <AppView
      projectId={projectId}
      appId={appId}
      partitionKey={partitionKey}
      orientation={'stacked_horizontal'}
      enableFullScreenStepView={true}
      enableMinimizedStepView={true}
      allowAnnotations={true}
    />
  );
};



---
File: /burr/telemetry/ui/src/components/routes/app/DataView.tsx
---

import React, { useContext, useEffect, useState } from 'react';
import { AttributeModel, Step } from '../../../api';
import JsonView from '@uiw/react-json-view';
import { Button } from '../../common/button';
import { Switch, SwitchField } from '../../common/switch';
import { Label } from '../../common/fieldset';
import { classNames } from '../../../utils/tailwind';
import { ChevronDownIcon, ChevronUpIcon } from '@heroicons/react/20/solid';
import { getUniqueAttributeID } from '../../../utils';
import { AppContext } from './AppView';
import { MinusIcon } from '@heroicons/react/24/outline';

/**
 * Common JSON view so we can make everything look the same.
 * We override icons to keep them consistent.
 * @param props The value/how deep we want it to be collapsed (defaults to 2)
 * @returns JSON view
 */
const CommonJsonView = (props: { value: object; collapsed?: number }) => {
  const collapsed = props.collapsed || 2;
  return (
    <JsonView
      value={props.value}
      collapsed={collapsed}
      enableClipboard={true}
      displayDataTypes={false}
    >
      <JsonView.Arrow
        // @ts-ignore
        render={({ 'data-expanded': isExpanded }) => {
          if (isExpanded) {
            return (
              <svg
                xmlns="http://www.w3.org/2000/svg"
                viewBox="0 0 16 16"
                fill="currentColor"
                className="size-4 hover:cursor-pointer"
              >
                <path
                  fillRule="evenodd"
                  d="M4.22 6.22a.75.75 0 0 1 1.06 0L8 8.94l2.72-2.72a.75.75 0 1 1 1.06 1.06l-3.25 3.25a.75.75 0 0 1-1.06 0L4.22 7.28a.75.75 0 0 1 0-1.06Z"
                  clipRule="evenodd"
                />
              </svg>
            );
          }
          return (
            <svg
              xmlns="http://www.w3.org/2000/svg"
              viewBox="0 0 16 16"
              fill="currentColor"
              className="size-4 hover:cursor-pointer"
            >
              <path
                fillRule="evenodd"
                d="M11.78 9.78a.75.75 0 0 1-1.06 0L8 7.06 5.28 9.78a.75.75 0 0 1-1.06-1.06l3.25-3.25a.75.75 0 0 1 1.06 0l3.25 3.25a.75.75 0 0 1 0 1.06Z"
                clipRule="evenodd"
              />
            </svg>
          );
        }}
      ></JsonView.Arrow>
    </JsonView>
  );
};
const StateButton = (props: { label: string; selected: boolean; setSelected: () => void }) => {
  const color = props.selected ? 'zinc' : 'light';
  return (
    <Button className="w-min cursor-pointer" color={color} onClick={props.setSelected}>
      {props.label}
    </Button>
  );
};

export const ErrorView = (props: { error: string }) => {
  return (
    <>
      <pre className="text-dwred rounded-sm p-2  text-wrap text-xs">{props.error}</pre>
    </>
  );
};

type DUAL_TOGGLE = 'default_expanded' | 'all_hidden';
type EXPANDED_TOGGLE = DUAL_TOGGLE | 'default_collapsed';
// type EXPANDED_TOGGLE = 'default_expanded' | 'default_collapsed' | 'all_hidden';
const cycleExpanded = (expanded: EXPANDED_TOGGLE): EXPANDED_TOGGLE => {
  switch (expanded) {
    case 'default_expanded':
      return 'default_collapsed';
    case 'default_collapsed':
      return 'all_hidden';
    case 'all_hidden':
      return 'default_expanded';
  }
};

const cycleExpandedDual = (expanded: DUAL_TOGGLE): DUAL_TOGGLE => {
  switch (expanded) {
    case 'default_expanded':
      return 'all_hidden';
    case 'all_hidden':
      return 'default_expanded';
  }
};

/**
 * Section header that allows for exapnsion/contraction of all subcomponents
 */
const SectionHeaderWithExpand = (props: {
  name: string;
  defaultExpanded?: EXPANDED_TOGGLE;
  setDefaultExpanded?: (expanded: EXPANDED_TOGGLE) => void;
  dualToggle?: boolean;
  hideTopBorder?: boolean;
}) => {
  let expandedState = props.defaultExpanded || 'default_expanded';
  const cycle = props.dualToggle ? cycleExpandedDual : cycleExpanded;
  if (props.dualToggle) {
    if (expandedState === 'default_collapsed') {
      expandedState = 'all_hidden';
    }
  }
  const MinimizeMaximizeIcon =
    props.defaultExpanded === 'default_expanded'
      ? ChevronUpIcon
      : props.defaultExpanded === 'default_collapsed'
        ? MinusIcon
        : ChevronDownIcon;
  return (
    <div className={`flex flex-row items-center gap-1 ${props.hideTopBorder ? '' : 'border-t'}`}>
      <h1 className="text-2xl text-gray-900 font-semibold">{props.name}</h1>
      <MinimizeMaximizeIcon
        className={classNames(
          'text-gray-500',
          'h-5 w-5 rounded-md hover:cursor-pointer hover:scale-105'
        )}
        aria-hidden="true"
        onClick={() => {
          if (props.setDefaultExpanded) {
            // @ts-ignore
            props.setDefaultExpanded(cycle(expandedState || 'default_expanded'));
          }
        }}
      />
    </div>
  );
};

export const DataView = (props: { currentStep: Step | undefined; priorStep: Step | undefined }) => {
  const [whichState, setWhichState] = useState<'after' | 'before' | 'compare'>('after');
  const stepToExamine = whichState !== 'before' ? props.currentStep : props.priorStep;
  const stateData = stepToExamine?.step_end_log?.state;
  const resultData = stepToExamine?.step_end_log?.result || undefined;
  const inputs = stepToExamine?.step_start_log?.inputs;
  const compareStateData =
    whichState === 'compare' ? props.priorStep?.step_end_log?.state : undefined;
  const error = props.currentStep?.step_end_log?.exception;
  const [viewRawData, setViewRawData] = useState<'raw' | 'render'>('render');

  const [allStateExpanded, setAllStateExpanded] = useState<EXPANDED_TOGGLE>('default_expanded');
  const [allResultExpanded, setAllResultExpanded] = useState<EXPANDED_TOGGLE>('default_expanded');
  const [allInputExpanded, setAllInputExpanded] = useState<EXPANDED_TOGGLE>('default_expanded');
  const [allAttributeExpanded, setAllAttributeExpanded] =
    useState<EXPANDED_TOGGLE>('default_expanded');

  const attributes = stepToExamine?.attributes || [];

  return (
    <div className="pl-1 flex flex-col gap-2 hide-scrollbar">
      <div className="flex flex-row justify-between sticky top-0 z-20 bg-white">
        <SectionHeaderWithExpand
          name="State"
          defaultExpanded={allStateExpanded}
          setDefaultExpanded={setAllStateExpanded}
          dualToggle={viewRawData === 'raw'}
          hideTopBorder={true}
        />
        <div className="flex flex-row justify-end gap-2 pr-2">
          <SwitchField>
            <Switch
              name="test"
              checked={viewRawData === 'raw'}
              onChange={(checked) => {
                setViewRawData(checked ? 'raw' : 'render');
              }}
            ></Switch>
            <Label className="-mx-2">Raw</Label>
          </SwitchField>

          {stateData !== undefined && (
            <StateButton
              label="after"
              selected={whichState === 'after'}
              setSelected={() => {
                setWhichState('after');
              }}
            />
          )}

          {
            <StateButton
              label="before"
              selected={whichState === 'before'}
              setSelected={() => {
                setWhichState('before');
              }}
            />
          }
          {stateData !== undefined && (
            <StateButton
              label="difference"
              selected={whichState === 'compare'}
              setSelected={() => {
                setWhichState('compare');
              }}
            />
          )}
        </div>
      </div>

      <div className={`${allStateExpanded === 'all_hidden' ? 'hidden' : ''}`}>
        <StateView
          stateData={stateData}
          viewRawData={viewRawData}
          isExpanded={allStateExpanded === 'default_expanded'}
          compareStateData={compareStateData}
        />
      </div>
      {error && (
        <>
          <h1 className="text-2xl text-gray-900 font-semibold">Error</h1>
          <ErrorView error={error} />
        </>
      )}
      {resultData && Object.keys(resultData).length > 0 && (
        <>
          <SectionHeaderWithExpand
            name="Result"
            defaultExpanded={allResultExpanded}
            setDefaultExpanded={setAllResultExpanded}
            dualToggle={viewRawData === 'raw'}
          />
          <div className={`${allResultExpanded === 'all_hidden' ? 'hidden' : ''}`}>
            <ResultView
              resultData={resultData}
              viewRawData={viewRawData}
              isExpanded={allResultExpanded === 'default_expanded'}
            />
          </div>
        </>
      )}
      {inputs && Object.keys(inputs).length > 0 && (
        <>
          <SectionHeaderWithExpand
            name="Inputs"
            defaultExpanded={allInputExpanded}
            setDefaultExpanded={setAllInputExpanded}
            dualToggle={viewRawData === 'raw'}
          />
          <div className={`${allInputExpanded === 'all_hidden' ? 'hidden' : ''}`}>
            {
              <InputsView
                inputs={inputs}
                isExpanded={allInputExpanded === 'default_expanded'}
                viewRawData={viewRawData}
              />
            }
          </div>
        </>
      )}
      {attributes && attributes.length > 0 && (
        <>
          <SectionHeaderWithExpand
            name="Attributes"
            defaultExpanded={allAttributeExpanded}
            setDefaultExpanded={setAllAttributeExpanded}
            dualToggle={viewRawData === 'raw'}
          />
          <div className={`${allAttributeExpanded === 'all_hidden' ? 'hidden' : ''}`}>
            {attributes.map((attribute, i) => (
              <AttributeView
                key={i}
                attribute={attribute}
                isExpanded={allAttributeExpanded === 'default_expanded'}
                viewRawData={viewRawData}
              />
            ))}
          </div>
        </>
      )}
    </div>
  );
};
// eslint-disable-next-line @typescript-eslint/no-explicit-any
const primitive = (value: any): boolean => {
  return (typeof value !== 'object' && typeof value !== 'function') || value === null;
};
/**
 * Basic json diff -- will return the json object representing the diffs.
 * Assumes arrays are compared by index -- nothing smarter than that.
 *
 * This has an entry for each difference -- e.g. everything that
 * is in current and not prior or different in current and prior.
 *
 * @param current -- current object/primitive/array
 * @param prior -- prior object/primitive/array
 * @returns -- object representing the diff
 */
// eslint-disable-next-line @typescript-eslint/no-explicit-any
const diffJSON = (current: any, prior: any): any | undefined => {
  // If the type doesn't match, they're not equal
  if (typeof prior !== typeof current) {
    return current;
  }
  // If they're both primitives, we can compare them directly
  if (primitive(prior) || primitive(current)) {
    return prior !== current ? current : undefined;
  }
  // If they're both arrays, we need to compare each element (recursively)
  if (Array.isArray(prior) && Array.isArray(current)) {
    const currentLength = current.length;
    const arrayDiff = [];
    for (let i = 0; i < currentLength; i++) {
      if (i > prior.length - 1) {
        arrayDiff[i] = current[i];
        continue;
      }
      const result = diffJSON(current[i], prior[i]);
      if (result !== undefined && (Object.keys(result).length !== 0 || primitive(result))) {
        arrayDiff[i] = result;
      }
    }
    if (arrayDiff.length > 0) {
      return arrayDiff.filter((item) => item !== undefined);
    } else {
      return undefined;
    }
  }
  // If they're both objects, we need to compare each key (recursively)
  const keys = new Set([...Object.keys(prior), ...Object.keys(current)]);
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  const diff: any = {};
  keys.forEach((key) => {
    // key not in prior but in current
    if (prior[key] === undefined && current[key] !== undefined) {
      diff[key] = current[key];
    } else {
      // diff the objects
      const result = diffJSON(current[key], prior[key]);
      if (result !== undefined && (Object.keys(result).length !== 0 || primitive(result))) {
        diff[key] = result;
      }
    }
  });
  return diff;
};

export const StateView = (props: {
  stateData: DataType | undefined;
  compareStateData: DataType | undefined;
  viewRawData: 'render' | 'raw';
  isExpanded: boolean;
}) => {
  const { stateData, viewRawData, isExpanded } = props;
  const stateToView =
    props.stateData === undefined || props.compareStateData === undefined
      ? stateData
      : diffJSON(props.stateData, props.compareStateData);
  return (
    <>
      {stateData !== undefined && viewRawData === 'render' && (
        <FormRenderer data={stateToView || {}} isDefaultExpanded={isExpanded} />
      )}
      {stateData !== undefined && viewRawData === 'raw' && (
        <CommonJsonView value={stateToView || {}} />
      )}
    </>
  );
};

export const ResultView = (props: {
  resultData: DataType | undefined;
  viewRawData: 'render' | 'raw';
  isExpanded: boolean;
}) => {
  const { resultData, viewRawData, isExpanded } = props;
  return (
    <>
      {resultData && viewRawData === 'render' && (
        <>
          <FormRenderer data={resultData} isDefaultExpanded={isExpanded} />
        </>
      )}
      {resultData && viewRawData === 'raw' && (
        <>
          <CommonJsonView value={resultData} />{' '}
        </>
      )}
    </>
  );
};

export const AttributeView = (props: {
  attribute: AttributeModel;
  viewRawData: 'render' | 'raw';
  isExpanded: boolean;
  hideHighlight?: boolean;
}) => {
  const { attribute, viewRawData, isExpanded } = props;
  const attributeAsObject = { [attribute.key]: attribute.value };
  const { attributesHighlighted } = useContext(AppContext);
  const uniqueID = getUniqueAttributeID(attribute);
  const attributeHighlighted = attributesHighlighted
    .map((item) => getUniqueAttributeID(item))
    .includes(uniqueID);
  return (
    <div
      id={getUniqueAttributeID(attribute)}
      className={`${attributeHighlighted && !props.hideHighlight ? 'bg-pink-100' : ''}`}
    >
      {viewRawData === 'render' && (
        <>
          <FormRenderer data={attributeAsObject} isDefaultExpanded={isExpanded} />
        </>
      )}
      {viewRawData === 'raw' && (
        <>
          <CommonJsonView value={attributeAsObject} />{' '}
        </>
      )}
    </div>
  );
};

export const InputsView = (props: {
  inputs: object;
  isExpanded: boolean;
  viewRawData: 'render' | 'raw';
}) => {
  const { inputs, viewRawData, isExpanded } = props;
  return (
    <>
      {inputs && viewRawData === 'render' ? (
        <>
          <FormRenderer data={inputs as DataType} isDefaultExpanded={isExpanded} />
        </>
      ) : (
        (inputs && viewRawData) === 'raw' && (
          <>
            <CommonJsonView value={inputs} />
          </>
        )
      )}
    </>
  );
};

type DataType = Record<string, string | number | boolean | object>;

const Header = (props: {
  name: string;
  isExpanded: boolean;
  setExpanded: (expanded: boolean) => void;
}) => {
  const MinimizeMaximizeIcon = props.isExpanded ? ChevronUpIcon : ChevronDownIcon;

  return (
    <div className="flex flex-row gap-1 z-10 pb-2 items-center">
      <h1 className="text-lg text-gray-900 font-semibold text-under">{props.name}</h1>
      <MinimizeMaximizeIcon
        className={classNames(
          'text-gray-500',
          'h-6 w-6 hover:bg-gray-50 rounded-md hover:cursor-pointer hover:scale-105'
        )}
        aria-hidden="true"
        onClick={() => {
          props.setExpanded(!props.isExpanded);
        }}
      />
    </div>
  );
};
export const RenderedField = (props: {
  value: string | number | boolean | object | null;
  keyName?: string;
  level?: number;
  defaultExpanded: boolean;
}) => {
  const [isExpanded, setExpanded] = useState(true);
  useEffect(() => {
    setExpanded(props.defaultExpanded || false);
  }, [props.defaultExpanded, props.value, props.keyName]);
  // TODO: have max level depth.
  const { value, keyName: key, level } = props;
  const bodyClassNames =
    'border-gray-100 border-l-[8px] pl-1 hover:bg-gray-100 text-sm text-gray-700';
  if ((key || '').startsWith('__')) {
    return null;
  }
  return (
    <div className="">
      {key && <Header name={key} isExpanded={isExpanded} setExpanded={setExpanded} />}
      {isExpanded &&
        (props.value instanceof Array &&
        props.value.length > 0 &&
        typeof props.value[0] === 'number' ? (
          <div key={key + '-' + String(level)}>
            <CommonJsonView value={props.value} />
          </div>
        ) : typeof value === 'string' ? (
          <div key={key + '-' + String(level)}>
            <pre
              className={`${bodyClassNames} whitespace-pre-wrap word-wrap-break-word max-w-[1000px]`}
            >
              {value}
            </pre>
          </div>
        ) : Array.isArray(value) ? (
          <div key={key + String(level)}>
            <div>
              {value.map((v, i) => {
                return (
                  <div key={key + '-' + i.toString()} className={bodyClassNames}>
                    <RenderedField
                      value={v}
                      keyName={key + '[' + i.toString() + ']'}
                      level={(level || 0) + 1}
                      defaultExpanded={props.defaultExpanded}
                    />
                  </div>
                );
              })}
            </div>
          </div>
        ) : typeof value === 'object' ? (
          <div key={key}>
            <div>
              {value === null ? (
                <span>NULL</span>
              ) : (
                Object.entries(value).map(([k, v]) => {
                  // if (v instanceof Array && v.length > 0 && typeof v[0] === 'number') {
                  //   // we want to display arrays of numbers as a single string.
                  //   v = v.toString();
                  // }
                  return (
                    <div key={key + '-' + k} className={bodyClassNames}>
                      <RenderedField
                        value={v}
                        keyName={k}
                        level={(level || 0) + 1}
                        defaultExpanded={props.defaultExpanded}
                      />
                    </div>
                  );
                })
              )}
            </div>
          </div>
        ) : value === null ? (
          <div key={key + '-' + String(level)}>
            <pre className={bodyClassNames}>NULL</pre>
          </div>
        ) : (
          <div key={key + '-' + String(level)} className="">
            <pre>{value.toString()}</pre>
          </div>
        ))}
    </div>
  );
};

interface FormRendererProps {
  data: Record<string, string | number | boolean | object | null>;
  isDefaultExpanded: boolean;
}

// This component is used to render the form data in a structured way
const FormRenderer: React.FC<FormRendererProps> = ({ data, isDefaultExpanded: isExpanded }) => {
  if (data !== null) {
    return (
      <>
        {Object.entries(data).map(([key, value]) => {
          return (
            <RenderedField
              keyName={key}
              value={value}
              level={0}
              key={key}
              defaultExpanded={isExpanded}
            />
          );
        })}
      </>
    );
  }
  return null;
};

export default FormRenderer;



---
File: /burr/telemetry/ui/src/components/routes/app/GraphView.tsx
---

import { ActionModel, ApplicationModel, Step } from '../../../api';

import ELK from 'elkjs/lib/elk.bundled.js';
import React, { createContext, useCallback, useLayoutEffect, useRef, useState } from 'react';
import ReactFlow, {
  BaseEdge,
  Controls,
  EdgeProps,
  Handle,
  MarkerType,
  Position,
  ReactFlowProvider,
  getBezierPath,
  useNodes,
  useReactFlow
} from 'reactflow';

import 'reactflow/dist/style.css';
import { backgroundColorsForIndex } from './AppView';
import { getActionStatus } from '../../../utils';
import { getSmartEdge } from '@tisoap/react-flow-smart-edge';

const elk = new ELK();

const elkOptions = {
  'elk.algorithm': 'layered',
  'elk.layered.spacing.nodeNodeBetweenLayers': '100',
  'elk.spacing.nodeNode': '80',
  'org.eclipse.elk.alg.layered.options.CycleBreakingStrategy': 'GREEDY',
  'org.eclipse.elk.layered.nodePlacement.strategy': 'BRANDES_KOEPF',
  // 'org.eclipse.elk.layered.feedbackEdges': 'true',
  'org.eclipse.elk.layered.crossingMinimization.strategy': 'LAYER_SWEEP'
};

type ActionNodeData = {
  action: ActionModel;
  label: string;
};

type InputNodeData = {
  input: string;
  label: string;
};

type NodeData = ActionNodeData | InputNodeData;

type NodeType = {
  id: string;
  type: string;
  data: NodeData;
  position: {
    x: number;
    y: number;
  };
};

type EdgeData = {
  from: string;
  to: string;
  condition: string;
};
type EdgeType = {
  id: string;
  source: string;
  target: string;
  markerEnd: {
    type: MarkerType;
    width: number;
    height: number;
  };
  data: EdgeData;
};

const ActionNode = (props: { data: NodeData }) => {
  const {
    highlightedActions: previousActions,
    hoverAction,
    currentAction
  } = React.useContext(NodeStateProvider);
  const highlightedActions = [currentAction, ...(previousActions || [])].reverse();
  const data = props.data as ActionNodeData;
  const name = data.action.name;
  const indexOfAction = highlightedActions.findIndex(
    (step) => step?.step_start_log.action === data.action.name
  );
  const shouldHighlight = indexOfAction !== -1;
  const step = highlightedActions[indexOfAction];
  const isCurrentAction = currentAction?.step_start_log.action === name;
  const bgColor =
    isCurrentAction && step !== undefined
      ? backgroundColorsForIndex(0, getActionStatus(step))
      : shouldHighlight
        ? 'bg-gray-100'
        : '';
  const opacity = hoverAction?.step_start_log.action === name ? 'opacity-50' : '';
  const additionalClasses = isCurrentAction
    ? 'border-dwlightblue/50 text-white border-2'
    : shouldHighlight
      ? 'border-dwlightblue/50 border-2'
      : 'border-dwlightblue/20 border-2';
  return (
    <>
      <Handle type="target" position={Position.Top} />
      <div
        className={`${bgColor} ${opacity} ${additionalClasses} text-xl font-sans p-4 rounded-md border`}
      >
        {name}
      </div>
      <Handle type="source" position={Position.Bottom} id="a" />
    </>
  );
};

const InputNode = (props: { data: NodeData }) => {
  return (
    <>
      <div className=" text-xl font-sans p-4 rounded-md  bg-white bg-opacity-0">
        {props.data.label}
      </div>
      <Handle type="source" position={Position.Bottom} id="a" className="w-0" />
    </>
  );
};
// TODO -- separate out into different edge types
export const ActionActionEdge = ({
  sourceX,
  sourceY,
  targetX,
  targetY,
  sourcePosition,
  targetPosition,
  markerEnd,
  data
}: EdgeProps) => {
  const nodes = useNodes();
  data = data as EdgeData;
  const { highlightedActions: previousActions, currentAction } =
    React.useContext(NodeStateProvider);
  const allActionsInPath = [...(previousActions || []), ...(currentAction ? [currentAction] : [])];
  const containsFrom = allActionsInPath.some(
    (action) => action.step_start_log.action === data.from
  );
  const containsTo = allActionsInPath.some((action) => action.step_start_log.action === data.to);
  const shouldHighlight = containsFrom && containsTo;
  const getSmartEdgeResponse = getSmartEdge({
    sourcePosition,
    targetPosition,
    sourceX,
    sourceY,
    targetX,
    targetY,
    nodes
  });
  let edgePath = null;
  if (getSmartEdgeResponse !== null) {
    edgePath = getSmartEdgeResponse.svgPathString;
  } else {
    edgePath = getBezierPath({
      sourceX,
      sourceY,
      sourcePosition,
      targetX,
      targetY,
      targetPosition
    })[0];
  }

  const style = {
    markerColor: shouldHighlight ? 'black' : 'gray',
    strokeWidth: shouldHighlight ? 2 : 0.5
  };
  return (
    <>
      <BaseEdge path={edgePath} markerEnd={markerEnd} style={style} label={'test'} />
    </>
  );
};

const getLayoutedElements = (
  nodes: NodeType[],
  edges: EdgeType[],
  options: { [key: string]: string } = {}
) => {
  const isHorizontal = options?.['elk.direction'] === 'RIGHT';
  const nodeNameMap = nodes.reduce(
    (acc, node) => {
      acc[node.id] = node;
      return acc;
    },
    {} as { [key: string]: NodeType }
  );
  const edgeNameMap = edges.reduce(
    (acc, edge) => {
      acc[edge.id] = edge;
      return acc;
    },
    {} as { [key: string]: EdgeType }
  );
  const graph = {
    id: 'root',
    layoutOptions: options,
    children: nodes.map((node) => ({
      ...node,
      // Adjust the target and source handle positions based on the layout
      // direction.
      targetPosition: isHorizontal ? 'left' : 'top',
      sourcePosition: isHorizontal ? 'right' : 'bottom',

      // Hardcode a width and height for elk to use when layouting.
      width: 150,
      height: 100
    })),
    edges: edges.map((edge) => {
      return {
        ...edge,
        sources: [edge.source],
        targets: [edge.target]
      };
    })
  };
  return elk.layout(graph).then((layoutedGraph) => ({
    nodes: (layoutedGraph.children || []).map((node) => {
      const originalNode = nodeNameMap[node.id];
      return {
        ...originalNode,
        position: {
          x: node.x as number,
          y: node.y as number
        }
      };
    }),
    edges: (layoutedGraph?.edges || []).map((edge) => {
      return {
        ...edge,
        markerEnd: { type: MarkerType.Arrow, width: 20, height: 20 },
        source: edge.sources[0],
        target: edge.targets[0],
        data: {
          from: edge.sources[0],
          to: edge.targets[0],
          condition: edgeNameMap[edge.id].data.condition
        }
      };
    })
  }));
};

const convertApplicationToGraph = (stateMachine: ApplicationModel): [NodeType[], EdgeType[]] => {
  const shouldDisplayInput = (input: string) => !input.startsWith('__');
  const inputUniqueID = (action: ActionModel, input: string) => `${action.name}:${input}`; // Currently they're distinct by name

  const allActionNodes = stateMachine.actions.map((action) => ({
    id: action.name,
    type: 'action',
    data: { action, label: action.name },
    position: { x: 0, y: 0 }
  }));
  // TODO -- consider displaying optional inputs
  const allInputNodes = stateMachine.actions.flatMap((action) =>
    (action.inputs || []).filter(shouldDisplayInput).map((input) => ({
      id: inputUniqueID(action, input),
      type: 'externalInput',
      data: { input, label: input },
      position: { x: 0, y: 0 }
    }))
  );
  const allInputTransitions = stateMachine.actions.flatMap((action) =>
    (action.inputs || []).filter(shouldDisplayInput).map((input) => ({
      id: `${action.name}:${input}-${action.name}`,
      source: inputUniqueID(action, input),
      target: action.name,
      markerEnd: { type: MarkerType.ArrowClosed, width: 20, height: 20 },
      data: { from: inputUniqueID(action, input), condition: input, to: action.name }
    }))
  );
  const allTransitionEdges = stateMachine.transitions.map((transition) => ({
    id: `${transition.from_}-${transition.to}`,
    source: transition.from_,
    target: transition.to,
    markerEnd: { type: MarkerType.ArrowClosed, width: 20, height: 20 },
    data: { from: transition.from_, to: transition.to, condition: transition.condition }
  }));
  return [
    [...allActionNodes, ...allInputNodes],
    [...allInputTransitions, ...allTransitionEdges]
  ];
};

const nodeTypes = {
  action: ActionNode,
  externalInput: InputNode // if this is "input" it is reserved...
};

const edgeTypes = {
  default: ActionActionEdge
};

type NodeState = {
  highlightedActions: Step[] | undefined; // one for each highlighted action, in order from most recent to least recent
  hoverAction: Step | undefined; // the action currently being hovered over
  currentAction: Step | undefined; // the action currently being viewed
};
const NodeStateProvider = createContext<NodeState>({
  highlightedActions: undefined,
  hoverAction: undefined,
  currentAction: undefined
});

export const _Graph = (props: {
  stateMachine: ApplicationModel;
  currentAction: Step | undefined;
  previousActions: Step[] | undefined;
  hoverAction: Step | undefined;
}) => {
  const [initialNodes, initialEdges] = React.useMemo(() => {
    return convertApplicationToGraph(props.stateMachine);
  }, [props.stateMachine]);

  const [nodes, setNodes] = useState<NodeType[]>([]);
  const [edges, setEdges] = useState<EdgeType[]>([]);

  const { fitView } = useReactFlow();

  const onLayout = useCallback(
    ({ direction = 'UP', useInitialNodes = false }): void => {
      const opts = { 'elk.direction': direction, ...elkOptions };
      const ns = useInitialNodes ? initialNodes : nodes;
      const es = useInitialNodes ? initialEdges : edges;

      getLayoutedElements(ns, es, opts).then(({ nodes: layoutedNodes, edges: layoutedEdges }) => {
        setNodes(layoutedNodes);
        setEdges(layoutedEdges);

        window.requestAnimationFrame(() => fitView());
      });
    },
    [nodes, edges]
  );

  useLayoutEffect(() => {
    onLayout({ direction: 'DOWN', useInitialNodes: true });
  }, []);

  return (
    <NodeStateProvider.Provider
      value={{
        highlightedActions: props.previousActions,
        hoverAction: props.hoverAction,
        currentAction: props.currentAction
      }}
    >
      <div className="h-full w-full relative">
        <ReactFlow
          nodes={nodes}
          edges={edges}
          edgesUpdatable={false}
          nodesDraggable={false}
          nodeTypes={nodeTypes}
          edgeTypes={edgeTypes}
          fitView
          maxZoom={100}
          minZoom={0.1}
        />
        <Controls position="bottom-right" className=" relative" />
      </div>
    </NodeStateProvider.Provider>
  );
};

export const GraphView = (props: {
  stateMachine: ApplicationModel;
  currentAction: Step | undefined;
  highlightedActions: Step[] | undefined;
  hoverAction: Step | undefined;
}) => {
  const parentRef = useRef<HTMLDivElement | null>(null);
  const childRef = useRef<HTMLDivElement | null>(null);

  return (
    <div ref={parentRef} className="h-full w-full flex-1">
      <div ref={childRef} className="h-full w-full">
        <ReactFlowProvider>
          <_Graph
            stateMachine={props.stateMachine}
            currentAction={props.currentAction}
            previousActions={props.highlightedActions}
            hoverAction={props.hoverAction}
          />
        </ReactFlowProvider>
      </div>
    </div>
  );
};



---
File: /burr/telemetry/ui/src/components/routes/app/InsightsView.tsx
---

import { AttributeModel, Span, Step } from '../../../api';
import { Table, TableBody, TableCell, TableHead, TableHeader, TableRow } from '../../common/table';
import React, { useContext, useState } from 'react';
import { ChevronDownIcon, ChevronUpIcon } from '@heroicons/react/24/outline';
import { AppContext } from './AppView';
import { Chip } from '../../common/chip';
import { modelCosts } from '../../common/modelCost';

/**
 * Insights allow us to visualize and surface attributes stored in steps.
 * The common use-case is LLM calls -- we want to summarize tokens, prompts, etc...
 * That said, we could also use it for loss functions on ML training, whatnot
 * Eventually we will be moving this to the server side and make it at a project level, but this is
 * a good start (at the app level).
 *
 * Insights are an aggregation over attributes
 */
type InsightBase = {
  // Tells the category
  category: 'llm' | 'metric';
  // Tells whether or not we have the insight, meaning whether it should register or not
  hasInsight: (allAttributes: AttributeModel[]) => boolean;
  // Name of the insight
  insightName: string;
  // The insight value
  RenderInsightValue: React.FC<{ attributes: AttributeModel[] }>;
};

/**
 * We want to allow them to expose individual values, but we may change the types here
 * later. Basically we have a simple flag on whether they show individual values (E.g if they're an aggregation),
 * and if they do, we have a function to capture them and a function to render them.
 *
 * These are undefined if we do not.
 *
 * Code will likely be repeated between this and the hasInsight/RenderInsightValue, but we'll keep it separate
 * for now.
 */

type InsightWithIndividualValues = InsightBase & {
  // Captures individual values for the insight -- allows you to expand out
  captureIndividualValues: (allAttributes: AttributeModel[]) => AttributeModel[];
  // Render the individual values
  RenderIndividualValue: React.FC<{ attribute: AttributeModel }>;
};

type InsightWithoutIndividualValues = InsightBase & {
  captureIndividualValues?: never;
  RenderIndividualValue?: never;
};

type Insight = InsightWithIndividualValues | InsightWithoutIndividualValues;

// eslint-disable-next-line @typescript-eslint/no-unused-vars
const getCostPerTokenModelDollars = (model: string) => {
  // TODO -- unhardcode this!
  const promptCost = modelCosts[model]?.input_cost_per_token || 0;
  const completionCost = modelCosts[model]?.output_cost_per_token || 0;
  return { promptCost, completionCost };
  // return { promptCost: 500 / 1_000_000, completionCost: 500 / 1_000_000 };
};

type CostProps = {
  totalCost: number;
  costDetails?: {
    costPerPromptToken: number;
    costPerCompletionToken: number;
    promptTokens: number;
    completionTokens: number;
  };
  currency?: string;
};
const Cost: React.FC<CostProps> = ({ currency = 'USD', totalCost, costDetails }) => {
  const [showDetails, setShowDetails] = useState(false);

  // Determine the number of decimal places needed, up to a maximum of 5

  // Inline function to apply decimal places logic
  const applyDecimalPlaces = (num: number) => {
    const decimalPlaces = Math.min((num.toString().split('.')[1] || '').length, 5);
    return num.toFixed(decimalPlaces);
  };
  const decimalPlaces = Math.min((totalCost.toString().split('.')[1] || '').length, 5);

  // Format the number with dynamic decimal places
  const formattedCost = new Intl.NumberFormat('en-US', {
    style: 'currency',
    currency: currency,
    minimumFractionDigits: decimalPlaces,
    maximumFractionDigits: decimalPlaces
  }).format(totalCost);

  return (
    <div className="relative bg-dwlightblue/10 text-dwdarkblue text-sm font-medium px-2 py-1 rounded-sm">
      <div
        onMouseLeave={() => setShowDetails(false)}
        onMouseEnter={() => setShowDetails((prev) => !prev)}
        className="cursor-pointer"
      >
        {formattedCost}
      </div>
      {costDetails && showDetails && (
        <div className="absolute right-full top-0 h-full w-min min-w-max p-3 bg-white  border-gray-200 z-50 rounded-sm flex flex-row items-center">
          <p className="flex flex-row gap-0 justify-center items-center p-2">
            {'$'}
            {applyDecimalPlaces(totalCost)} = {costDetails.promptTokens} prompt tokens * {'$'}
            {applyDecimalPlaces(costDetails.costPerPromptToken)} + {costDetails.completionTokens}{' '}
            completion tokens * {'$'}
            {applyDecimalPlaces(costDetails.costPerCompletionToken)}
          </p>
        </div>
      )}
    </div>
  );
};

type CostAttributeModel = AttributeModel & {
  value: {
    promptTokens: number;
    completionTokens: number;
    costPerPromptToken: number;
    costPerCompletionToken: number;
  };
};

const gatherCostAttributes = (allAttributes: AttributeModel[]): CostAttributeModel[] => {
  const modelAttributesBySpanID = allAttributes.reduce((acc, attribute) => {
    if (attribute.key === 'gen_ai.response.model') {
      acc.set(attribute.span_id || '', attribute);
    }
    return acc;
  }, new Map<string, AttributeModel>());

  const tokensUsedBySpanID = allAttributes.reduce((acc, attribute) => {
    if (attribute.key === 'gen_ai.usage.completion_tokens') {
      const currentValue = acc.get(attribute.span_id || '') || { prompt: 0, completion: 0 };
      acc.set(attribute.span_id || '', {
        prompt: currentValue.prompt,
        completion: currentValue.completion + (attribute.value as number)
      });
    }
    if (attribute.key === 'gen_ai.usage.prompt_tokens') {
      const currentValue = acc.get(attribute.span_id || '') || { prompt: 0, completion: 0 };
      acc.set(attribute.span_id || '', {
        prompt: currentValue.prompt + (attribute.value as number),
        completion: currentValue.completion
      });
    }
    return acc;
  }, new Map<string, { prompt: number; completion: number }>());
  const costDataBySpanID = new Map<
    string,
    {
      promptTokens: number;
      completionTokens: number;
      costPerPromptToken: number;
      costPerCompletionToken: number;
    }
  >();
  tokensUsedBySpanID.forEach((tokensUsed, spanID) => {
    const modelAttribute = modelAttributesBySpanID.get(spanID);
    if (modelAttribute) {
      const { promptCost, completionCost } = getCostPerTokenModelDollars(
        modelAttribute.value as string
      );
      // console.log('promptCost', promptCost, 'completionCost', completionCost, 'tokensUsed', tokensUsed);
      costDataBySpanID.set(
        spanID,
        {
          promptTokens: tokensUsed.prompt,
          completionTokens: tokensUsed.completion,
          costPerPromptToken: promptCost,
          costPerCompletionToken: completionCost
        }
        // tokensUsed.prompt * promptCost + tokensUsed.completion * completionCost
      );
    }
  });
  return Array.from(costDataBySpanID.entries()).map(([spanID, cost]) => {
    const modelAttribute = modelAttributesBySpanID.get(spanID);
    return {
      key: spanID,
      action_sequence_id: modelAttribute?.action_sequence_id || 0,
      value: cost,
      span_id: spanID,
      timestamp: modelAttribute?.time_logged || 0,
      tags: {}
    };
  });
};

const REGISTERED_INSIGHTS: Insight[] = [
  {
    category: 'llm',
    hasInsight: (allAttributes) => {
      return allAttributes.some(
        (attribute) => attribute.key.endsWith('prompt_tokens') && attribute.key.startsWith('gen_ai')
      );
    },
    insightName: 'Total Prompt Tokens',
    RenderInsightValue: (props) => {
      let totalPromptTokens = 0;
      props.attributes.forEach((attribute) => {
        if (attribute.key.endsWith('prompt_tokens') && attribute.key.startsWith('gen_ai')) {
          totalPromptTokens += attribute.value as number;
        }
      });
      return <p>{totalPromptTokens}</p>;
    },
    captureIndividualValues: (allAttributes) => {
      return allAttributes.filter(
        (attribute) => attribute.key.endsWith('prompt_tokens') && attribute.key.startsWith('gen_ai')
      );
    },
    RenderIndividualValue: (props: { attribute: AttributeModel }) => {
      return <p>{props.attribute.value?.toString()}</p>;
    }
  },
  {
    category: 'llm',
    hasInsight: (allAttributes) => {
      return allAttributes.some(
        (attribute) =>
          attribute.key.endsWith('completion_tokens') && attribute.key.startsWith('gen_ai')
      );
    },
    insightName: 'Total Completion Tokens',
    RenderInsightValue: (props) => {
      let totalCompletionTokens = 0;
      props.attributes.forEach((attribute) => {
        if (attribute.key.endsWith('completion_tokens')) {
          totalCompletionTokens += attribute.value as number;
        }
      });
      return <p>{totalCompletionTokens}</p>;
    },
    captureIndividualValues: (allAttributes) => {
      return allAttributes.filter(
        (attribute) =>
          attribute.key.endsWith('completion_tokens') && attribute.key.startsWith('gen_ai')
      );
    },
    RenderIndividualValue: (props: { attribute: AttributeModel }) => {
      return <p>{props.attribute.value?.toString()}</p>;
    }
  },
  {
    category: 'llm',
    hasInsight: (allAttributes) => {
      return allAttributes.some(
        (attribute) => attribute.key.endsWith('prompt_tokens') && attribute.key.startsWith('gen_ai')
      );
    },
    insightName: 'Total LLM Calls',
    RenderInsightValue: (props) => {
      let totalLLMCalls = 0;
      props.attributes.forEach((attribute) => {
        if (attribute.key.endsWith('prompt_tokens') && attribute.key.startsWith('gen_ai')) {
          totalLLMCalls += 1;
        }
      });
      return <p>{totalLLMCalls}</p>;
    },
    captureIndividualValues: (allAttributes) => {
      const spanIDToLLMCalls = new Map<string, number>();
      allAttributes.forEach((attribute) => {
        if (attribute.key.endsWith('prompt_tokens') && attribute.key.startsWith('gen_ai')) {
          spanIDToLLMCalls.set(
            attribute.span_id || '',
            (spanIDToLLMCalls.get(attribute.span_id || '') || 0) + 1
          );
        }
      });
      return Array.from(spanIDToLLMCalls.entries()).map(([spanID, count]) => {
        return {
          key: 'llm_calls',
          action_sequence_id: 0,
          value: count,
          span_id: spanID,
          timestamp: 0,
          tags: {}
        };
      });
      // return allAttributes.filter((attribute) => attribute.key.endsWith('prompt_tokens')).map;
    },
    RenderIndividualValue: (props: { attribute: AttributeModel }) => {
      return <p>{props.attribute.value?.toString()}</p>;
    }
  },
  {
    category: 'llm',
    hasInsight: (allAttributes) => {
      return allAttributes.some(
        (attribute) => attribute.key.endsWith('prompt_tokens') && attribute.key.startsWith('gen_ai')
      );
    },
    insightName: 'Total LLM Cost',
    RenderInsightValue: (props) => {
      const costAttributes = gatherCostAttributes(props.attributes);
      let totalCost = 0;
      costAttributes.forEach((attribute) => {
        totalCost +=
          attribute.value.promptTokens * attribute.value.costPerPromptToken +
          attribute.value.completionTokens * attribute.value.costPerCompletionToken;
      });
      return <Cost totalCost={totalCost} />;
    },
    captureIndividualValues: (allAttributes) => {
      return gatherCostAttributes(allAttributes);
    },
    RenderIndividualValue: (props: { attribute: AttributeModel }) => {
      const cast = props.attribute as CostAttributeModel;
      return (
        <Cost
          totalCost={
            cast.value.promptTokens * cast.value.costPerPromptToken +
            cast.value.completionTokens * cast.value.costPerCompletionToken
          }
          costDetails={{
            costPerPromptToken: cast.value.costPerPromptToken,
            costPerCompletionToken: cast.value.costPerCompletionToken,
            promptTokens: cast.value.promptTokens,
            completionTokens: cast.value.completionTokens
          }}
        />
      );
      // return <p>{props.attribute.value?.toString()}</p>;
    }
  }
];

// TODO -- get anonymous insights
const InsightSubTable = (props: {
  attributes: AttributeModel[];
  insight: Insight;
  allSpans: Span[];
  allSteps: Step[];
  appID: string;
  partitionKey: string | null;
}) => {
  const individualValues = props.insight.captureIndividualValues?.(props.attributes);
  const [isExpanded, setIsExpanded] = React.useState(false);
  const ExpandIcon = isExpanded ? ChevronUpIcon : ChevronDownIcon;
  const canExpand = props.insight.captureIndividualValues !== undefined;
  const spansBySpanID = props.allSpans.reduce((acc, span) => {
    acc.set(span.begin_entry.span_id, span);
    return acc;
  }, new Map<string, Span>());
  const stepsByStepID = props.allSteps.reduce((acc, step) => {
    acc.set(step.step_start_log.sequence_id, step);
    return acc;
  }, new Map<number, Step>());

  const { currentSelectedIndex, setCurrentSelectedIndex, currentHoverIndex, setCurrentHoverIndex } =
    useContext(AppContext);

  return (
    <>
      <TableRow
        className="hover:bg-gray-50 cursor-pointer "
        onClick={() => {
          if (canExpand) {
            setIsExpanded(!isExpanded);
          }
        }}
      >
        <TableCell>
          <div className="flex flex-row gap-1 items-center">
            <Chip label={props.insight.category} chipType={props.insight.category}></Chip>
            {props.insight.insightName}
          </div>
        </TableCell>
        <TableCell className=""></TableCell>
        <TableCell className=""></TableCell>

        <TableCell className="flex flex-row justify-end gap-2">
          <props.insight.RenderInsightValue attributes={props.attributes} />
          {canExpand ? (
            <button>
              <ExpandIcon className="h-5 w-5" />
            </button>
          ) : (
            <></>
          )}
        </TableCell>
      </TableRow>
      {isExpanded
        ? individualValues?.map((attribute, i) => {
            const span = spansBySpanID.get(attribute.span_id || '');
            const step = stepsByStepID.get(span?.begin_entry.action_sequence_id || 0);
            const insightCasted = props.insight as InsightWithIndividualValues;
            const isHovered = currentHoverIndex === span?.begin_entry.action_sequence_id;
            const isCurrentSelected = currentSelectedIndex === span?.begin_entry.action_sequence_id;
            return (
              <TableRow
                key={attribute.key + i}
                className={` ${isCurrentSelected ? 'bg-gray-200' : isHovered ? 'bg-gray-50' : 'hover:bg-gray-50'} cursor-pointer`}
                onMouseEnter={() => {
                  setCurrentHoverIndex(
                    span
                      ? {
                          sequenceId: span.begin_entry.action_sequence_id,
                          appId: props.appID,
                          partitionKey: props.partitionKey
                        }
                      : undefined
                  );
                }}
                onMouseLeave={() => {
                  setCurrentHoverIndex(undefined);
                }}
                onClick={() => {
                  setCurrentSelectedIndex(
                    span
                      ? {
                          sequenceId: span.begin_entry.action_sequence_id,
                          appId: props.appID,
                          partitionKey: props.partitionKey
                        }
                      : undefined
                  );
                }}
              >
                <TableCell></TableCell>
                <TableCell className="">
                  {step && (
                    <div className="flex gap-2">
                      <span>{step.step_start_log.action}</span>
                      <span>({span?.begin_entry.action_sequence_id})</span>
                    </div>
                  )}
                </TableCell>
                <TableCell className="">
                  <div className="flex gap">
                    <div>{span?.begin_entry.span_name || ''}</div>
                    <div className="pl-2">({attribute.span_id?.split(':')})</div>
                  </div>
                </TableCell>
                <TableCell className="flex justify-end mr-7">
                  <insightCasted.RenderIndividualValue attribute={attribute} />
                </TableCell>
              </TableRow>
            );
          })
        : null}
    </>
  );
};

export const InsightsView = (props: {
  steps: Step[];
  appId: string;
  partitionKey: string | null;
}) => {
  const allAttributes: AttributeModel[] = props.steps.flatMap((step) => step.attributes);

  const allSpans = props.steps.flatMap((step) => step.spans);

  const out = (
    <div className="pt-0 flex flex-col hide-scrollbar">
      <Table dense={1}>
        <TableHead>
          <TableRow className="hover:bg-gray-100">
            <TableHeader className="w-72">Name </TableHeader>
            <TableHeader className="w-48">Step</TableHeader>
            <TableHeader className="w-48">Span</TableHeader>
            <TableHeader className="flex justify-end">Value</TableHeader>
            {/* <TableHeader colSpan={1}></TableHeader> */}
          </TableRow>
        </TableHead>
        <TableBody>
          {REGISTERED_INSIGHTS.map((insight) => {
            if (insight.hasInsight(allAttributes)) {
              return (
                <InsightSubTable
                  key={insight.insightName}
                  attributes={allAttributes}
                  insight={insight}
                  allSpans={allSpans}
                  allSteps={props.steps}
                  appID={props.appId}
                  partitionKey={props.partitionKey}
                />
              );
            }
          })}
        </TableBody>
      </Table>
      <div>
        <h2 className="text-gray-500 pl-2 pt-4">
          Use this tab to view summaries of your application. This automatically picks up on a
          variety of attributes, including those populated by{' '}
          <a
            className="text-dwlightblue"
            href={'https://www.traceloop.com/docs/openllmetry/tracing/without-sdk'}
          >
            opentelemetry instrumentation.
          </a>{' '}
          -- E.G. LLM call data. To instrument, and start collecting, see{' '}
          <a
            className="text-dwlightblue"
            href={'https://burr.dagworks.io/concepts/additional-visibility/#quickstart'}
          >
            see docs.
          </a>
        </h2>
      </div>
    </div>
  );
  return out;
};



---
File: /burr/telemetry/ui/src/components/routes/app/ReproduceView.tsx
---

import { ClipboardIcon } from '@heroicons/react/24/outline';
import { Step } from '../../../api';
import { useEffect, useState } from 'react';

const FlashMessage = ({
  message,
  duration,
  onClose
}: {
  message: string;
  duration: number;
  onClose: () => void;
}) => {
  useEffect(() => {
    const timer = setTimeout(onClose, duration);
    return () => clearTimeout(timer);
  }, [duration, onClose]);

  return (
    <div className="fixed bottom-4 right-4 bg-dwlightblue text-white p-2 rounded shadow-lg">
      {message}
    </div>
  );
};

export const ReproduceView = (props: {
  step: Step | undefined;
  appId: string;
  partitionKey: string;
  projectID: string;
}) => {
  const cmd =
    'burr-test-case create  \\\n' +
    `  --project-name "${props.projectID}" \\\n` +
    `  --partition-key "${props.partitionKey}" \\\n` +
    `  --app-id "${props.appId}" \\\n` +
    `  --sequence-id ${props.step ? props.step.step_start_log.sequence_id : '?'} \\\n` +
    '  --target-file-name YOUR_FIXTURE_FILE.json \n';

  const [isFlashVisible, setIsFlashVisible] = useState(false);

  return (
    <div className="pt-2 flex flex-col gap-4">
      {isFlashVisible && (
        <FlashMessage
          message="Copied to clipboard!"
          duration={2000}
          onClose={() => setIsFlashVisible(false)}
        />
      )}
      <div className="flex flex-row justify-between text-gray-700">
        <p>
          To generate a test case for this step, run the following command.
          <a
            href="https://burr.dagworks.io/examples/guardrails/creating_tests/"
            target="_blank"
            rel="noreferrer"
            className="hover:underline text-dwlightblue"
          >
            {' '}
            Further reading
          </a>
          .
        </p>{' '}
        <ClipboardIcon
          className="h-5 w-5 min-h-5 min-w-5 cursor-pointer hover:scale-110"
          onClick={() => {
            navigator.clipboard.writeText(cmd);
            setIsFlashVisible(true);
          }}
        />
      </div>
      <pre className="text-white bg-gray-800 p-2 rounded-md text-sm">{cmd}</pre>
    </div>
  );
};



---
File: /burr/telemetry/ui/src/components/routes/app/StateMachine.tsx
---

import { AnnotationOut, ApplicationModel, Step } from '../../../api';
import { Tabs } from '../../common/tabs';
import { DataView } from './DataView';
import { ActionView } from './ActionView';
import { GraphView } from './GraphView';
import { InsightsView } from './InsightsView';
import { ReproduceView } from './ReproduceView';
import { useContext } from 'react';
import { AppContext, SequenceLocation } from './AppView';
import { ChevronLeftIcon, ChevronRightIcon } from '@heroicons/react/24/outline';
import { AnnotationsView } from './AnnotationsView';
import { useLocationParams } from '../../../utils';

const NoStepSelected = () => {
  return (
    <div className="flex flex-col items-center justify-center h-full">
      <p className="text-xl text-gray-400">Please select a step from the table on the left</p>
    </div>
  );
};

export const AppStateView = (props: {
  steps: Step[];
  stateMachine: ApplicationModel;
  highlightedActions: Step[] | undefined;
  hoverAction: Step | undefined;
  currentActionLocation: SequenceLocation | undefined;
  displayGraphAsTab: boolean;
  setMinimized: (minimized: boolean) => void;
  isMinimized: boolean;
  allowMinimized: boolean;
  annotations?: AnnotationOut[];
  restrictTabs?: string[];
  allowAnnotations?: boolean;
}) => {
  const { tab, setTab } = useContext(AppContext);
  const { projectId, appId, partitionKey } = useLocationParams();
  const currentStep = props.steps.find(
    (step) =>
      // We don't get the global App ID -- we assume the steps are from the right one
      step.step_start_log.sequence_id === props.currentActionLocation?.sequenceId
  );
  const priorStep =
    currentStep &&
    props.steps.find(
      (step) =>
        step.step_start_log.sequence_id === (props.currentActionLocation?.sequenceId || 0) - 1
    );

  const actionModel = props.stateMachine.actions.find(
    (action) => action.name === currentStep?.step_start_log.action
  );
  const tabs = [
    { id: 'data', displayName: 'Data' },
    { id: 'code', displayName: 'Code' },
    { id: 'reproduce', displayName: 'Reproduce' },
    { id: 'insights', displayName: 'Insights' },
    ...(props.allowAnnotations ? [{ id: 'annotations', displayName: 'Annotations' }] : [])
  ].filter((tab) => !props.restrictTabs || props.restrictTabs.includes(tab.id));
  if (
    props.displayGraphAsTab &&
    (props.restrictTabs === undefined || props.restrictTabs.includes('graph'))
  ) {
    tabs.push({ id: 'graph', displayName: 'Graph' });
  }
  const MinimizeTableIcon = props.isMinimized ? ChevronLeftIcon : ChevronRightIcon;
  return (
    <>
      <div className="flex flex-row items-center pl-4">
        {props.allowMinimized && (
          <button onClick={() => props.setMinimized(!props.isMinimized)}>
            <MinimizeTableIcon className="h-4 w-4 hover:scale-110 cursor-pointer text-gray-600" />
          </button>
        )}
        <Tabs tabs={tabs} currentTab={tab} setCurrentTab={setTab} />
      </div>
      <div className="px-4 h-full w-full hide-scrollbar overflow-y-auto">
        {tab === 'data' &&
          (currentStep ? (
            <DataView currentStep={currentStep} priorStep={priorStep} />
          ) : (
            <NoStepSelected />
          ))}
        {tab === 'code' &&
          (currentStep ? <ActionView currentAction={actionModel} /> : <NoStepSelected />)}
        {tab === 'graph' && (
          <GraphView
            stateMachine={props.stateMachine}
            currentAction={currentStep}
            highlightedActions={props.highlightedActions}
            hoverAction={props.hoverAction}
          />
        )}
        {tab === 'insights' && (
          <InsightsView
            steps={props.steps}
            appId={appId as string}
            partitionKey={partitionKey as string}
          />
        )}
        {tab === 'reproduce' &&
          (currentStep ? (
            <ReproduceView
              step={currentStep}
              appId={appId as string}
              partitionKey={partitionKey as string}
              projectID={projectId as string}
            />
          ) : (
            <NoStepSelected />
          ))}
        {tab === 'annotations' && (
          <AnnotationsView
            currentStep={currentStep}
            allSteps={props.steps}
            appId={appId as string}
            partitionKey={partitionKey === 'null' ? null : partitionKey || null}
            projectId={projectId as string}
            allAnnotations={props.annotations || []}
          />
        )}
      </div>
    </>
  );
};



---
File: /burr/telemetry/ui/src/components/routes/app/StepList.tsx
---

import {
  AnnotationOut,
  AttributeModel,
  ChildApplicationModel,
  DefaultService,
  EndStreamModel,
  FirstItemStreamModel,
  InitializeStreamModel,
  PointerModel,
  Span,
  Step
} from '../../../api';
import { Table, TableBody, TableCell, TableHead, TableHeader, TableRow } from '../../common/table';
import { DateTimeDisplay, DurationDisplay } from '../../common/dates';
import {
  AppContext,
  backgroundColorsForIndex,
  backgroundColorsForStatus,
  REFRESH_INTERVAL,
  SequenceLocation
} from './AppView';
import { Status, getActionStatus } from '../../../utils';
import { Chip } from '../../common/chip';
import { useContext, useEffect, useRef, useState } from 'react';
import { TbGrillFork } from 'react-icons/tb';
import {
  FiArrowDown,
  FiArrowUp,
  FiChevronDown,
  FiChevronUp,
  FiClock,
  FiDatabase
} from 'react-icons/fi';

import {
  ChevronLeftIcon,
  ChevronRightIcon,
  MinusIcon,
  PlayIcon,
  PlusIcon
} from '@heroicons/react/24/outline';

import { PauseIcon } from '@heroicons/react/24/solid';
import { TiFlowChildren } from 'react-icons/ti';

import { Link, useNavigate } from 'react-router-dom';
import { AiOutlineFullscreenExit, AiOutlineFullscreen } from 'react-icons/ai';
import { RiCornerDownRightLine } from 'react-icons/ri';

import { RxActivityLog } from 'react-icons/rx';
import { RenderedField } from './DataView';
import { FaPencilAlt } from 'react-icons/fa';
import { AnnotateButton } from './AnnotationsView';
import { useQuery } from 'react-query';

const StatusChip = (props: { status: Status }) => {
  return (
    <Chip
      label={props.status}
      chipType={props.status}
      className="w-16 flex flex-row justify-center"
    />
  );
};

// The threshold between steps for a "Pause"
// E.G. that control flow moves out of Burr for a bit
// Really we should be recording this using the client, but this is fine for now
const PAUSE_TIME_THRESHOLD_MILLIS = 1_000;

/**
 * Quick auto-refresh switch.
 * TODO -- remove this once we get websockets working.
 */
const AutoRefreshSwitch = (props: {
  autoRefresh: boolean;
  setAutoRefresh: (b: boolean) => void;
  textColor?: string;
}) => {
  const AutoRefreshIcon = props.autoRefresh ? PauseIcon : PlayIcon;
  const textColor = props.textColor || 'text-gray-600';

  return (
    <AutoRefreshIcon
      className={`h-4 w-4 ${textColor} hover:scale-110 cursor-pointer`}
      onClick={(e) => {
        props.setAutoRefresh(!props.autoRefresh);
        e.stopPropagation();
      }}
    />
  );
};

const RecursionDepthPadding = (props: { depth: number; children: React.ReactNode }) => {
  return (
    <div className="flex flex-row items-center h-full">
      {new Array(props.depth).fill(0).map((i) => (
        <div className="w-5 h-5 flex-row flex justify-start items-center" key={i}>
          <div className="h-8 translate-x-1 w-0.5 bg-gray-200 opacity-75"> </div>
        </div>
      ))}
      {props.children}
    </div>
  );
};
/**
 * Quick component to make the table row common between
 * the action and span rows
 * @param props
 * @returns
 */
const CommonTableRow = (props: {
  children: React.ReactNode;
  sequenceID: number;
  isHovered: boolean;
  shouldBeHighlighted: boolean;
  currentSelectedIndex: SequenceLocation | undefined;
  step: Step;
  setCurrentHoverIndex: (index?: SequenceLocation) => void;
  setCurrentSelectedIndex: (index?: SequenceLocation) => void;
  appID: string;
  partitionKey: string | null;
}) => {
  return (
    <TableRow
      key={props.sequenceID}
      className={`h-full ${props.isHovered ? 'opacity-80' : ''} cursor-pointer 
            ${
              props.shouldBeHighlighted && props.currentSelectedIndex !== undefined
                ? backgroundColorsForIndex(
                    props.currentSelectedIndex.sequenceId - props.sequenceID,
                    getActionStatus(props.step)
                  )
                : ''
            }`}
      onMouseEnter={() => {
        props.setCurrentHoverIndex({
          sequenceId: props.sequenceID,
          appId: props.appID,
          partitionKey: props.partitionKey
        });
      }}
      onMouseLeave={() => {
        props.setCurrentHoverIndex(undefined);
      }}
      onClick={(e) => {
        if (
          props.currentSelectedIndex?.sequenceId === props.sequenceID &&
          props.currentSelectedIndex?.appId === props.appID &&
          props.currentSelectedIndex?.partitionKey === props.partitionKey
        ) {
          props.setCurrentSelectedIndex(undefined);
        } else {
          props.setCurrentSelectedIndex({
            sequenceId: props.sequenceID,
            appId: props.appID,
            partitionKey: props.partitionKey
          });
        }
        e.stopPropagation();
      }}
    >
      {props.children}
    </TableRow>
  );
};

const ActionTableRow = (props: {
  step: StepWithEllapsedTime;
  numPriorIndices: number;
  isTracesExpanded: boolean;
  toggleTraceExpanded: (index: number) => void;
  isLinksExpanded: boolean;
  toggleLinksExpanded: (index: number) => void;
  minimized: boolean;
  links: ChildApplicationModel[];
  displaySpansCol: boolean;
  displayLinksCol: boolean;
  earliestTimeSeen: Date;
  latestTimeSeen: Date;
  isExpanded: boolean;
  allowExpand: boolean;
  setExpanded: (b: boolean) => void;
  expandNonSpanAttributes: boolean;
  setExpandNonSpanAttributes: (b: boolean) => void;
  streamingEvents: Array<InitializeStreamModel | FirstItemStreamModel | EndStreamModel>;
  displayAnnotations: boolean;
  existingAnnotation: AnnotationOut | undefined;
  appID: string;
  partitionKey: string | null;
  depth: number;
}) => {
  const sequenceID = props.step.step_start_log.sequence_id;
  const {
    setCurrentEditingAnnotationContext,
    setTab,
    currentHoverIndex,
    setCurrentHoverIndex,
    currentSelectedIndex,
    setCurrentSelectedIndex
  } = useContext(AppContext);

  const isHovered =
    currentHoverIndex?.sequenceId === sequenceID &&
    currentHoverIndex?.appId === props.appID &&
    currentHoverIndex?.partitionKey === props.partitionKey;
  // const spanCount = props.step.spans.length;
  const childCount = props.links.length;
  const shouldBeHighlighted =
    currentSelectedIndex !== undefined &&
    currentSelectedIndex.appId === props.appID &&
    currentSelectedIndex.partitionKey === props.partitionKey &&
    sequenceID <= currentSelectedIndex.sequenceId &&
    sequenceID >= currentSelectedIndex.sequenceId - props.numPriorIndices;
  // const shouldBeHighlighted =
  //   currentSelectedIndex !== undefined &&
  //   sequenceID <= currentSelectedIndex &&
  //   sequenceID >= currentSelectedIndex - props.numPriorIndices;
  // const TraceExpandIcon = props.isTracesExpanded ? MinusIcon : PlusIcon;
  const LinkExpandIcon = props.isLinksExpanded ? MinusIcon : PlusIcon;
  const attributes = props.step.attributes || [];
  const allSpansRecorded = props.step.spans.map((span) => span.begin_entry.span_id);
  const nonSpanAttributes = attributes.filter(
    (attr) => !allSpansRecorded.includes(attr.span_id || '')
  );
  const isStreaming = props.step.streaming_events.length > 0;
  return (
    <CommonTableRow
      sequenceID={sequenceID}
      isHovered={isHovered}
      shouldBeHighlighted={shouldBeHighlighted}
      currentSelectedIndex={currentSelectedIndex}
      step={props.step}
      setCurrentHoverIndex={setCurrentHoverIndex}
      setCurrentSelectedIndex={setCurrentSelectedIndex}
      appID={props.appID}
      partitionKey={props.partitionKey}
    >
      <TableCell className="text-gray-500 w-12 max-w-12 min-w-12">
        <div className="flex flex-row items-center">
          <RecursionDepthPadding depth={props.depth}>
            <span>{sequenceID}</span>{' '}
          </RecursionDepthPadding>
        </div>
      </TableCell>
      <TableCell>
        <div className="flex flex-row gap-1 items-center">
          <div className={`${props.allowExpand ? '' : 'invisible'}`}>
            <ToggleButton
              isExpanded={props.isExpanded}
              toggle={() => props.setExpanded(!props.isExpanded)}
              disabled={false}
            />
          </div>
          <div
            className={`${props.minimized ? 'w-32' : 'w-72 max-w-72'} flex flex-row justify-start gap-1 items-center`}
          >
            <Chip
              label={isStreaming ? 'stream' : 'action'}
              chipType={isStreaming ? 'stream' : 'action'}
              className="w-16 flex flex-row justify-center"
            />
            {props.step.step_start_log.action}
            {props.isExpanded && nonSpanAttributes.length > 0 ? (
              <ToggleButton
                isExpanded={nonSpanAttributes.length > 0 ? props.expandNonSpanAttributes : true}
                toggle={
                  nonSpanAttributes.length > 0
                    ? () => props.setExpandNonSpanAttributes(!props.expandNonSpanAttributes)
                    : () => {}
                }
                disabled={false}
                IconExpanded={MinusIcon}
                IconCollapsed={FiDatabase}
              />
            ) : (
              <></>
            )}
          </div>
        </div>
      </TableCell>
      {props.minimized && <TableCell colSpan={3} />}
      {!props.minimized && (
        <>
          <TableCell className="h-[1px] px-0" colSpan={1}>
            <WaterfallPiece
              step={props.step}
              earliestStartTime={props.earliestTimeSeen}
              latestEndTime={props.latestTimeSeen}
              endTime={
                props.step.step_end_log ? new Date(props.step.step_end_log?.end_time) : undefined
              }
              startTime={new Date(props.step.step_start_log.start_time)}
              bgColor={backgroundColorsForStatus(getActionStatus(props.step))}
              isHighlighted={shouldBeHighlighted}
              kind="span"
              globalEllapsedTimeMillis={props.step.latestGlobalEllapsedTime}
            />
          </TableCell>
          {props.displayLinksCol && (
            <TableCell className="w-5">
              {childCount > 0 ? (
                <div className="flex gap-1 items-center">
                  <LinkExpandIcon
                    className="h-4 w-4 text-gray-600 hover:scale-110 cursor-pointer"
                    onClick={(e) => {
                      props.toggleLinksExpanded(sequenceID);
                      e.stopPropagation();
                    }}
                  />
                  <span className="text-gray-600">{childCount}</span>
                </div>
              ) : (
                <span></span>
              )}
            </TableCell>
          )}
          <TableCell className="w-16">
            <div className="max-w-min">
              <StatusChip status={getActionStatus(props.step)} />
            </div>
          </TableCell>
          <TableCell className="w-10">
            {props.displayAnnotations ? (
              <AnnotateButton
                appID={props.appID}
                partitionKey={props.partitionKey}
                sequenceID={sequenceID}
                existingAnnotation={props.existingAnnotation}
                setCurrentEditingAnnotationContext={(context) => {
                  setCurrentEditingAnnotationContext(context);
                  setTab('annotations');
                }}
              />
            ) : (
              <></>
            )}
          </TableCell>
        </>
      )}
    </CommonTableRow>
  );
};
/**
 * Loads the data for the StepList + anything else
 * @param props
 * @returns
 */
const SelfLoadingSubApplicationContainer = (props: {
  appID: string;
  partitionKey: string | null;
  projectID: string;
  minimized: boolean;
  projectId: string;
  topToBottomChronological: boolean;
  displayAnnotations: boolean;
  // traceExpandedActions: number[];
  displaySpansCol: boolean;
  displayLinksCol: boolean;
  earliestTimeSeen: Date;
  latestTimeSeen: Date;
  // links: ChildApplicationModel[];
  autoRefresh: boolean;
  depth: number;
}) => {
  const { data } = useQuery(
    ['steps', props.projectID, props.appID, props.partitionKey],
    () =>
      DefaultService.getApplicationLogsApiV0ProjectIdAppIdPartitionKeyAppsGet(
        props.projectId as string,
        props.appID as string,
        props.partitionKey !== null ? props.partitionKey : '__none__'
      ),
    {
      // TODO -- decide how we want to auto-refresh with lots of nested stuff?
      // Really, we'll want a bulk API but this is OK for now...
      refetchInterval: props.autoRefresh ? REFRESH_INTERVAL : false,
      enabled: true
    }
  );
  // TODO -- use a skiptoken to bypass annotation loading if we don't need them
  const { data: annotationsData } = useQuery(
    ['annotations', props.projectID, props.appID, props.partitionKey],
    () =>
      DefaultService.getAnnotationsApiV0ProjectIdAnnotationsGet(
        props.projectId as string,
        props.appID as string,
        props.partitionKey !== null ? props.partitionKey : '__none__'
      )
  );
  const [traceExpandedActions, setTraceExpandedActions] = useState<number[]>([]);
  const [linksExpandedActions, setLinksExpandedActions] = useState<number[]>([]);

  const toggleTraceExpandedActions = (index: number) => {
    if (traceExpandedActions.includes(index)) {
      setTraceExpandedActions(traceExpandedActions.filter((i) => i !== index));
    } else {
      setTraceExpandedActions([...traceExpandedActions, index]);
    }
  };
  const isLinksExpanded = (index: number) => {
    return linksExpandedActions.includes(index);
  };
  const toggleLinksExpanded = (index: number) => {
    if (isLinksExpanded(index)) {
      setLinksExpandedActions(linksExpandedActions.filter((i) => i !== index));
    } else {
      setLinksExpandedActions([...linksExpandedActions, index]);
    }
  };

  if (data === undefined || annotationsData === undefined) {
    return <></>;
  }
  const steps = data.steps;
  const stepsWithEllapsedTime = collapseTimestampsToEllapsedTime(steps);
  const annotations = annotationsData;
  return (
    <StepList
      appID={props.appID}
      partitionKey={props.partitionKey}
      stepsWithEllapsedTime={stepsWithEllapsedTime}
      annotations={annotations}
      numPriorIndices={0}
      minimized={props.minimized}
      projectId={props.projectId}
      topToBottomChronological={props.topToBottomChronological}
      displayAnnotations={false} // TODO -- get annotations to work propertly for sub-applications
      traceExpandedActions={traceExpandedActions}
      setTraceExpandedActions={setTraceExpandedActions}
      linksExpandedActions={linksExpandedActions}
      links={data.children}
      toggleTraceExpandedActions={toggleTraceExpandedActions}
      toggleLinksExpanded={toggleLinksExpanded}
      displaySpansCol={props.displaySpansCol}
      displayLinksCol={props.displayLinksCol}
      earliestTimeSeen={props.earliestTimeSeen}
      latestTimeSeen={props.latestTimeSeen}
      depth={props.depth + 1}
    />
  );
};

const LinkSubTable = (props: {
  appID: string;
  partitionKey: string | null;
  step: StepWithEllapsedTime;
  numPriorIndices: number;
  minimized: boolean;
  links: ChildApplicationModel[];
  projectId: string;
  displaySpansCol: boolean;
  displayLinksCol: boolean;
  // TODO -- consider pushing down into this component, we probably don't need it in the container
  expandedSubApplicationIDs: string[];
  setExpandedSubApplicationIDs: (appIDs: string[]) => void;
  topToBottomChronological: boolean;
  earliestTimeSeen: Date;
  latestTimeSeen: Date;
  depth: number;
}) => {
  const { currentHoverIndex, setCurrentHoverIndex, currentSelectedIndex, setCurrentSelectedIndex } =
    useContext(AppContext);
  const sequenceID = props.step.step_start_log.sequence_id;
  const isHovered =
    currentHoverIndex?.appId === props.appID &&
    currentHoverIndex?.partitionKey === props.partitionKey &&
    currentHoverIndex?.sequenceId === sequenceID;
  const shouldBeHighlighted =
    currentSelectedIndex !== undefined &&
    currentSelectedIndex.appId === props.appID &&
    currentSelectedIndex.partitionKey === props.partitionKey &&
    sequenceID <= currentSelectedIndex.sequenceId &&
    sequenceID >= currentSelectedIndex.sequenceId - props.numPriorIndices;
  const normalText = shouldBeHighlighted ? 'text-gray-100' : 'text-gray-600';
  const iconColor = shouldBeHighlighted ? 'text-gray-100' : 'text-gray-400';
  const navigate = useNavigate();
  return (
    <>
      {props.links.map((subApp) => {
        const subApplicationIsExpanded = props.expandedSubApplicationIDs.includes(
          subApp.child.app_id
        );
        const toggleExpanded = (appID: string) => {
          if (props.expandedSubApplicationIDs.includes(appID)) {
            props.setExpandedSubApplicationIDs(
              props.expandedSubApplicationIDs.filter((id) => id !== appID)
            );
          } else {
            props.setExpandedSubApplicationIDs([...props.expandedSubApplicationIDs, appID]);
          }
        };
        const childType = subApp.event_type;
        const Icon = childType === 'fork' ? TbGrillFork : TiFlowChildren;
        return (
          <>
            <CommonTableRow
              appID={props.appID}
              partitionKey={props.partitionKey}
              key={`${subApp.child.app_id}-link-table-row`}
              sequenceID={sequenceID}
              isHovered={isHovered}
              shouldBeHighlighted={shouldBeHighlighted}
              currentSelectedIndex={currentSelectedIndex}
              step={props.step}
              setCurrentHoverIndex={setCurrentHoverIndex}
              setCurrentSelectedIndex={setCurrentSelectedIndex}
            >
              <TableCell colSpan={1} className="">
                <RecursionDepthPadding depth={props.depth}>
                  <Icon className={`h-5 w-5 ${iconColor} -ml-1`} />
                </RecursionDepthPadding>
              </TableCell>
              <TableCell
                colSpan={1}
                className={`${normalText} w-48 min-w-48 max-w-48 truncate pl-9`}
              >
                <div
                  className="z-50 truncate"
                  onClick={(e) => {
                    navigate(
                      `/project/${props.projectId}/${subApp.child.partition_key || 'null'}/${subApp.child.app_id}`
                    );
                    e.stopPropagation();
                  }}
                >
                  <span className="hover:underline">{subApp.child.app_id}</span>
                </div>
              </TableCell>
              <TableCell colSpan={1} className="relative" id="placeholder1">
                <WaterfallPiece
                  step={props.step}
                  startTime={new Date(props.step.step_start_log.start_time)}
                  endTime={new Date(props.step.step_end_log?.end_time || new Date())}
                  earliestStartTime={props.earliestTimeSeen}
                  latestEndTime={props.latestTimeSeen}
                  globalEllapsedTimeMillis={0}
                  bgColor={props.step.step_end_log?.exception ? 'bg-dwred' : 'bg-green-500'}
                  isHighlighted={false}
                  kind="subaction"
                  setSubActionExpanded={() => {
                    toggleExpanded(subApp.child.app_id);
                  }}
                  isSubActionExpanded={subApplicationIsExpanded}
                />
              </TableCell>
              <TableCell colSpan={1} className="w-5" />
              {!props.minimized && (
                <TableCell colSpan={1} className="text-gray-500">
                  <Chip
                    label={subApp.event_type === 'fork' ? 'forked' : 'spawned'}
                    chipType={subApp.event_type === 'fork' ? 'fork' : 'spawn'}
                    className="w-16 flex flex-row justify-center"
                  />
                </TableCell>
              )}
              <TableCell colSpan={1} />
            </CommonTableRow>
            {subApplicationIsExpanded ? (
              <SelfLoadingSubApplicationContainer
                minimized={props.minimized}
                projectId={props.projectId}
                topToBottomChronological={props.topToBottomChronological}
                displayAnnotations={false}
                // traceExpandedActions={[]}
                // links={[]}
                displaySpansCol={props.displaySpansCol}
                displayLinksCol={props.displayLinksCol}
                earliestTimeSeen={props.earliestTimeSeen}
                latestTimeSeen={props.latestTimeSeen}
                appID={subApp.child.app_id}
                partitionKey={subApp.child.partition_key}
                projectID={props.projectId}
                autoRefresh={false} // TODO -- make this configurable/cascade down from the container
                depth={props.depth}
              />
            ) : (
              <></>
            )}
          </>
        );
      })}
    </>
  );
};

const StepSubTableRow = (props: {
  appID: string;
  partitionKey: string | null;
  spanID: string | null; // undefined if it is not associated with a span, E.G a free-standing attribute
  name: string;
  minimized: boolean;
  sequenceID: number;
  isHovered: boolean;
  shouldBeHighlighted: boolean;
  step: StepWithEllapsedTime;
  startTime: Date;
  endTime: Date | undefined;
  model: Span | AttributeModel | InitializeStreamModel | FirstItemStreamModel | EndStreamModel;
  modelType: 'span' | 'attribute' | 'first_item_stream' | 'end_stream';
  earliestTimeSeen: Date;
  latestTimeSeen: Date;
  displayFullAppWaterfall: boolean;
  displaySpanID: boolean;
  setDisplayAttributes?: (b: boolean) => void;
  displayAttributes?: boolean;
  displayAnnotations: boolean;
  depth: number; // app-depth with recursion
}) => {
  const {
    setCurrentHoverIndex,
    currentSelectedIndex,
    setCurrentSelectedIndex,
    setTab,
    setAttributesHighlighted
  } = useContext(AppContext);
  const lightText = 'text-gray-300';
  const normalText = props.shouldBeHighlighted ? 'text-gray-100' : 'text-gray-600';
  const attrsForSpan = props.step.attributes.filter((attr) => attr.span_id === props.spanID);
  const spanIDUniqueToAction = props.spanID?.split(':')[1] || '';
  const Icon = props.modelType === 'span' ? RiCornerDownRightLine : RxActivityLog;
  const [displayAttributeValue, setDisplayAttributeValue] = useState(
    props.modelType === 'attribute'
  );
  // This is a quick implementation for prototyping -- we will likely switch this up
  // This assumes that the span UID is of the form "actionID:spanID.spanID.spanID..."
  // Which is currently the case
  let depth = spanIDUniqueToAction.split('.').length;
  if (props.modelType === 'attribute') {
    depth += 1;
  }
  if (props.modelType === 'first_item_stream' || props.modelType === 'end_stream') {
    depth += 1;
  }
  const onClick = () => {
    if (props.modelType !== 'attribute') {
      return;
    }
    setAttributesHighlighted([props.model as AttributeModel]);
    // TODO -- this is not setting in the URL, we need to figure out why
    setTab('data');
  };
  return (
    <CommonTableRow
      appID={props.appID}
      partitionKey={props.partitionKey}
      sequenceID={props.sequenceID}
      isHovered={props.isHovered}
      shouldBeHighlighted={props.shouldBeHighlighted}
      currentSelectedIndex={currentSelectedIndex}
      step={props.step}
      setCurrentHoverIndex={setCurrentHoverIndex}
      setCurrentSelectedIndex={setCurrentSelectedIndex}
    >
      <TableCell
        className={` ${lightText} w-10 min-w-10 ${props.displaySpanID ? '' : 'text-opacity-0'}`}
      >
        <RecursionDepthPadding depth={props.depth}>
          <span>{spanIDUniqueToAction}</span>
        </RecursionDepthPadding>
      </TableCell>
      {!props.minimized ? (
        <>
          <TableCell
            onClick={onClick}
            className={`${normalText} ${props.minimized ? 'w-32 min-w-32' : 'w-72 max-w-72'} flex flex-col`}
          >
            <div className="flex flex-row gap-1 items-center">
              {[...Array(depth).keys()].map((i) => (
                <Icon
                  key={i}
                  className={`${i === depth - 1 ? 'opacity-0' : 'opacity-0'} text-lg text-gray-600 w-4 flex-shrink-0`}
                ></Icon>
              ))}
              <Chip
                label={
                  props.modelType === 'end_stream'
                    ? 'end'
                    : props.modelType === 'first_item_stream'
                      ? 'start'
                      : props.modelType
                }
                chipType={props.modelType}
                className="w-16 min-w-16 justify-center"
              />
              {props.modelType === 'attribute' ? (
                <ToggleButton
                  isExpanded={displayAttributeValue}
                  toggle={() => setDisplayAttributeValue(!displayAttributeValue)}
                  disabled={false}
                  IconExpanded={FiChevronUp}
                  IconCollapsed={FiChevronDown}
                />
              ) : (
                <></>
              )}
              <span>{props.name}</span>
              {props.setDisplayAttributes && attrsForSpan.length > 0 ? (
                <ToggleButton
                  isExpanded={props.displayAttributes || false}
                  toggle={() => props.setDisplayAttributes?.(!props.displayAttributes)}
                  disabled={false}
                  IconExpanded={MinusIcon}
                  IconCollapsed={FiDatabase}
                />
              ) : (
                <></>
              )}
            </div>
          </TableCell>
          <TableCell className="h-[1px] pl-0">
            {!displayAttributeValue ? (
              <WaterfallPiece
                step={props.step}
                earliestStartTime={props.earliestTimeSeen}
                latestEndTime={props.latestTimeSeen}
                endTime={props.endTime}
                startTime={props.startTime}
                bgColor={backgroundColorsForStatus(getActionStatus(props.step))}
                isHighlighted={props.shouldBeHighlighted}
                kind={'span'}
                globalEllapsedTimeMillis={props.step.latestGlobalEllapsedTime}
              />
            ) : (
              <div
                className="flex justify-start overflow-hidden pl-20"
                onClick={(e) => {
                  e.stopPropagation();
                }}
              >
                <RenderedField
                  value={(props.model as AttributeModel).value}
                  defaultExpanded={true} // no real need for default expanded
                />
              </div>
            )}
          </TableCell>
          {<TableCell className="" colSpan={2} />}
        </>
      ) : (
        <TableCell colSpan={5}></TableCell>
      )}
    </CommonTableRow>
  );
};

const StepSubTable = (props: {
  appID: string;
  partitionKey: string | null;
  spans: Span[];
  attributes: AttributeModel[];
  streamingEvents: Array<InitializeStreamModel | FirstItemStreamModel | EndStreamModel>;
  step: StepWithEllapsedTime;
  numPriorIndices: number;
  minimized: boolean;
  displaySpansCol: boolean;
  displayLinksCol: boolean;
  earliestTimeSeen: Date;
  latestTimeSeen: Date;
  expandNonSpanAttributes: boolean;
  topToBottomChronological: boolean;
  displayAnnotations: boolean;
  depth: number;
}) => {
  const { currentHoverIndex, currentSelectedIndex } = useContext(AppContext);
  const attributesBySpanID = props.attributes.reduce((acc, attr) => {
    const existing = acc.get(attr.span_id) || [];
    existing.push(attr);
    acc.set(attr.span_id, existing);
    return acc;
  }, new Map<string | null, AttributeModel[]>());
  // TODO -- display
  // const streamingEventsBySpanID = props.streamingEvents.reduce((acc, event) => {
  //   const existing = acc.get(event.span_id) || [];
  //   existing.push(event);
  //   acc.set(event.span_id, existing);
  //   return acc;
  // }, new Map<string | null, Array<InitializeStreamModel | FirstItemStreamModel | EndStreamModel>>());
  const sequenceID = props.step.step_start_log.sequence_id;
  const isHovered =
    currentHoverIndex?.sequenceId === sequenceID &&
    currentHoverIndex?.appId === props.appID &&
    currentHoverIndex?.partitionKey === props.partitionKey;
  // const spanCount = props.step.spans.length;
  const shouldBeHighlighted =
    currentSelectedIndex !== undefined &&
    currentSelectedIndex.appId === props.appID &&
    currentSelectedIndex.partitionKey === props.partitionKey &&
    sequenceID <= currentSelectedIndex.sequenceId &&
    sequenceID >= currentSelectedIndex.sequenceId - props.numPriorIndices;
  const displayFullAppWaterfall = true; // TODO -- configure if we zoom on a step
  const allSpanIds = props.spans.map((span) => span.begin_entry.span_id);
  const [spanIdsWithAttributesDisplayed, setSpanIdsWithAttributesDisplayed] = useState<string[]>(
    []
  );
  // N^2
  // This is because we create span references without logging them -- using the trace at the root level
  // E.G. 15:0 -- this is just the root. No need to display it, we'll just show it on the action
  const attributesNoSpanInfo = props.attributes.filter(
    (attr) => !allSpanIds.includes(attr.span_id || '')
  );
  const nonSpanAttributes = attributesBySpanID.get(null) || [];
  const stepStartTime = new Date(props.step.step_start_log.start_time);
  nonSpanAttributes.sort((a, b) => {
    return (
      (new Date(a.time_logged || stepStartTime) > new Date(b.time_logged || stepStartTime)
        ? 1
        : -1) * (props.topToBottomChronological ? -1 : 1)
    );
  });
  const spans = [...props.spans];
  spans.sort((a, b) => {
    if (a.begin_entry.span_id > b.begin_entry.span_id) {
      return 1;
    }
    return -1;
  });

  const seenNonSpanAttributes = new Set<string | null>();

  const firstStream = props.streamingEvents.find((event) => event.type === 'first_item_stream') as
    | FirstItemStreamModel
    | undefined;

  const streamingEventsToRender = props.streamingEvents.filter(
    (item) => item.type === 'first_item_stream' || item.type === 'end_stream'
  );
  streamingEventsToRender.sort((a, b) => {
    return (
      (new Date((a as FirstItemStreamModel).first_item_time) >
      new Date((b as FirstItemStreamModel).first_item_time)
        ? 1
        : -1) * (props.topToBottomChronological ? -1 : 1)
    );
  });

  return (
    <>
      {streamingEventsToRender.map((event, i) => {
        if (event.type === 'first_item_stream') {
          const streamModel = event as FirstItemStreamModel;
          const ttfs =
            new Date(streamModel.first_item_time).getTime() -
            new Date(props.step.step_start_log.start_time).getTime();
          return (
            <StepSubTableRow
              key={`streaming-${i}`}
              appID={props.appID}
              partitionKey={props.partitionKey}
              spanID={null}
              name={`delay: ${ttfs}ms`}
              minimized={props.minimized}
              sequenceID={sequenceID}
              isHovered={isHovered}
              shouldBeHighlighted={shouldBeHighlighted}
              step={props.step}
              startTime={new Date(streamModel.first_item_time)}
              endTime={new Date(streamModel.first_item_time)}
              model={streamModel}
              modelType={event.type}
              earliestTimeSeen={props.earliestTimeSeen}
              latestTimeSeen={props.latestTimeSeen}
              displayFullAppWaterfall={displayFullAppWaterfall}
              displaySpanID={true}
              displayAnnotations={props.displayAnnotations}
              depth={props.depth}
            />
          );
        } else {
          const streamModel = event as EndStreamModel;
          const ellapsedStreamTime = firstStream
            ? new Date(streamModel.end_time).getTime() -
              new Date(firstStream?.first_item_time).getTime()
            : undefined;
          const numStreamed = streamModel.items_streamed;
          // const name = ellapsedStreamTime ? `last token (throughput=${ellapsedStreamTime/streamModel.items_streamed} ms/token)` : 'last token';
          // const name = `throughput: ${(ellapsedStreamTime || 0) / numStreamed} ms/token (${numStreamed} tokens/${ellapsedStreamTime}ms)`;
          const name = `throughput: ${((ellapsedStreamTime || 0) / numStreamed).toFixed(1)} ms/token (${numStreamed}/${ellapsedStreamTime}ms)`;
          return (
            <StepSubTableRow
              key={`streaming-${i}`}
              appID={props.appID}
              partitionKey={props.partitionKey}
              spanID={null}
              name={name}
              minimized={props.minimized}
              sequenceID={sequenceID}
              isHovered={isHovered}
              shouldBeHighlighted={shouldBeHighlighted}
              step={props.step}
              startTime={new Date(streamModel.end_time)}
              endTime={new Date(streamModel.end_time)}
              model={streamModel}
              modelType={'end_stream'}
              earliestTimeSeen={props.earliestTimeSeen}
              latestTimeSeen={props.latestTimeSeen}
              displayFullAppWaterfall={displayFullAppWaterfall}
              displaySpanID={true}
              displayAnnotations={props.displayAnnotations}
              depth={props.depth}
            />
          );
        }
      })}
      {props.expandNonSpanAttributes ? (
        attributesNoSpanInfo.map((attr, i) => {
          const displaySpanID = !seenNonSpanAttributes.has(attr.span_id) && attr.span_id !== null;
          seenNonSpanAttributes.add(attr.span_id);
          return (
            <StepSubTableRow
              key={`${attr.key}-${i}`}
              appID={props.appID}
              partitionKey={props.partitionKey}
              spanID={attr.span_id}
              name={attr.key}
              minimized={props.minimized}
              sequenceID={sequenceID}
              isHovered={isHovered}
              shouldBeHighlighted={shouldBeHighlighted}
              step={props.step}
              startTime={new Date(attr.time_logged || stepStartTime)}
              endTime={new Date(attr.time_logged || stepStartTime)}
              model={attr}
              modelType="attribute"
              earliestTimeSeen={props.earliestTimeSeen}
              latestTimeSeen={props.latestTimeSeen}
              displayFullAppWaterfall={displayFullAppWaterfall}
              displaySpanID={displaySpanID}
              displayAnnotations={props.displayAnnotations}
              depth={props.depth}
            />
          );
        })
      ) : (
        <></>
      )}
      {spans.flatMap((span, i) => {
        const spanID = span.begin_entry.span_id;
        // if (!openSpanIDs.includes(spanID)) {
        //   return [];
        // }
        // TODO -- add null
        const hasAttributes = attributesBySpanID.has(spanID);
        const attributesToDisplay =
          hasAttributes && spanIdsWithAttributesDisplayed.includes(spanID)
            ? attributesBySpanID.get(spanID) || []
            : [];

        return [
          <StepSubTableRow
            key={i}
            spanID={spanID}
            appID={props.appID}
            partitionKey={props.partitionKey}
            name={span.begin_entry.span_name}
            minimized={props.minimized}
            sequenceID={sequenceID}
            isHovered={isHovered}
            shouldBeHighlighted={shouldBeHighlighted}
            step={props.step}
            startTime={new Date(span.begin_entry.start_time)}
            endTime={span.end_entry?.end_time ? new Date(span.end_entry.end_time) : undefined}
            model={span}
            modelType="span"
            earliestTimeSeen={props.earliestTimeSeen}
            latestTimeSeen={props.latestTimeSeen}
            displayFullAppWaterfall={displayFullAppWaterfall}
            displaySpanID={true}
            setDisplayAttributes={(b) => {
              if (b) {
                setSpanIdsWithAttributesDisplayed([...spanIdsWithAttributesDisplayed, spanID]);
              } else {
                setSpanIdsWithAttributesDisplayed(
                  spanIdsWithAttributesDisplayed.filter((id) => id !== spanID)
                );
              }
            }}
            displayAttributes={spanIdsWithAttributesDisplayed.includes(spanID)}
            displayAnnotations={props.displayAnnotations}
            depth={props.depth}
          />,
          ...attributesToDisplay.map((attr, i) => {
            return (
              <StepSubTableRow
                key={`${attr.key}-${i}`}
                appID={props.appID}
                partitionKey={props.partitionKey}
                spanID={attr.span_id}
                name={attr.key}
                minimized={props.minimized}
                sequenceID={sequenceID}
                isHovered={isHovered}
                shouldBeHighlighted={shouldBeHighlighted}
                step={props.step}
                startTime={new Date(attr.time_logged || new Date(span.begin_entry.start_time))}
                endTime={new Date(attr.time_logged || new Date(span.begin_entry.start_time))}
                model={attr}
                modelType="attribute"
                earliestTimeSeen={props.earliestTimeSeen}
                latestTimeSeen={props.latestTimeSeen}
                displayFullAppWaterfall={displayFullAppWaterfall}
                displaySpanID={false}
                displayAnnotations={props.displayAnnotations}
                depth={props.depth}
              />
            );
          })
        ];
      })}
    </>
  );
};

const WaterfallPiece: React.FC<{
  step: StepWithEllapsedTime;
  startTime: Date;
  endTime: Date | undefined;
  earliestStartTime: Date;
  latestEndTime: Date | undefined;
  globalEllapsedTimeMillis: number; // Total time of active trace
  bgColor: string;
  isHighlighted: boolean;
  kind?: 'span' | 'event' | 'subaction';
  // Whether to display just ellapsed (clock) time
  displayAbsoluteOrEllapsedTime?: 'absolute' | 'ellapsed';
  // In the case that we have the subaction type -- otherwise these will be undefined
  setSubActionExpanded?: (b: boolean) => void;
  isSubActionExpanded?: boolean;
}> = (props) => {
  const useAbsoluteTime = (props.displayAbsoluteOrEllapsedTime || 'absolute') === 'absolute';
  const containerRef = useRef<HTMLDivElement>(null);

  const stepStartTimeAbsolute = new Date(props.step.step_start_log.start_time).getTime();
  // Subtract the duration of the step to get the start time
  const stepStartTimeRelative = props.step.cumulativeTimeMillis - props.step.stepDurationMillis;
  const eventStartTimeSinceStepBegan = props.startTime.getTime() - stepStartTimeAbsolute;
  const eventEndTimeSinceStepBegan =
    (props.endTime || new Date()).getTime() - stepStartTimeAbsolute;

  const earliestStartTimeMilliseconds = useAbsoluteTime ? props.earliestStartTime.getTime() : 0;

  const latestEndTimeMilliseconds = useAbsoluteTime
    ? (props.latestEndTime || new Date()).getTime()
    : props.globalEllapsedTimeMillis;

  const stepBeganReferencePoint = useAbsoluteTime ? stepStartTimeAbsolute : stepStartTimeRelative;

  const startTimeMilliseconds = stepBeganReferencePoint + eventStartTimeSinceStepBegan;
  const endTimeMilliseconds = stepBeganReferencePoint + eventEndTimeSinceStepBegan;
  const earliestToLatestMilliseconds = latestEndTimeMilliseconds - earliestStartTimeMilliseconds;

  const bgColor = props.isHighlighted ? 'bg-white' : props.bgColor;
  const [isHovered, setIsHovered] = useState(false);
  return (
    <div ref={containerRef} className="w-full px-2 h-full">
      {(() => {
        const leftPositionPercentage =
          ((startTimeMilliseconds - earliestStartTimeMilliseconds) / earliestToLatestMilliseconds) *
          100;
        const widthPercentage =
          Math.max(
            Math.max(
              (endTimeMilliseconds - startTimeMilliseconds) / earliestToLatestMilliseconds,
              0.005
            )
            // 0.1
          ) * 100;
        const isCloseToEnd = leftPositionPercentage + widthPercentage > 80; // Threshold for "close to the end"
        // TODO -- unhack these, we're converting back and forth cause we already have interfaces for strings and
        // don't want to change
        const SubActionIcon = props.isSubActionExpanded ? MinusIcon : PlusIcon;
        const hoverItem =
          props.kind === 'event' || props.kind === 'subaction' ? (
            <></>
          ) : (
            <div className="flex flex-row gap-1 items-center">
              <DurationDisplay
                startDate={new Date(startTimeMilliseconds).toISOString()}
                endDate={new Date(endTimeMilliseconds).toISOString()}
              />
              <DateTimeDisplay
                date={props.startTime.toISOString()}
                mode={'short'}
                displayMillis={true}
              />
            </div>
          );

        return (
          <>
            {props.kind === 'span' ? (
              <div
                className={`${bgColor} opacity-50 h-12 px-0 rounded-sm`}
                onMouseEnter={() => setIsHovered(true)}
                onMouseLeave={() => setIsHovered(false)}
                style={{
                  width: `${widthPercentage}%`,
                  position: 'absolute',
                  bottom: '5%',
                  height: '90%',
                  left: `${leftPositionPercentage}%`
                }}
              />
            ) : props.kind === 'event' ? (
              <div
                className={`h-5 w-5 ${bgColor} opacity-50 absolute`}
                style={{
                  left: `${leftPositionPercentage}%`,
                  top: '50%',
                  transform: 'translate(-50%, -50%) rotate(45deg)'
                }}
              />
            ) : (
              // Sub-action
              <div className="h-full flex flex-col justify-center">
                <div
                  className="w-full h-full flex-col flex justify-center z-50 hover:scale-y-150"
                  style={{
                    width: `${widthPercentage}%`,
                    position: 'absolute',
                    left: `${leftPositionPercentage}%`
                  }}
                  onClick={(e) => {
                    props.setSubActionExpanded?.(!props.isSubActionExpanded);
                    e.stopPropagation();
                  }}
                >
                  <div
                    className={`${props.isHighlighted ? 'bg-transparent' : props.bgColor} opacity-50 h-1 px-0 rounded-sm flex flex-row justify-center items-center`}
                    onMouseEnter={() => setIsHovered(true)}
                    onMouseLeave={() => setIsHovered(false)}
                  >
                    <SubActionIcon
                      className={`hover:scale-125 transform-none cursor-pointer h-4 w-4 rounded-full text-gray-700 ${props.isHighlighted ? props.bgColor : 'bg-white'}`}
                    />
                  </div>
                </div>
              </div>
            )}

            {/* Hoverable zone with buffer to avoid flicker */}
            <div
              className="absolute h-full"
              onMouseEnter={() => setIsHovered(true)}
              onMouseLeave={() => setIsHovered(false)}
              style={{
                left: `calc(${leftPositionPercentage}% - 20px)`,
                width: `calc(${widthPercentage}% + 40px)` // 20px buffer on each side
              }}
            ></div>

            {
              <div
                className={`backdrop-blur-lg rounded-md px-4 transition-opacity duration-500 ${
                  isHovered ? 'opacity-100' : 'opacity-0 pointer-events-none'
                }`}
                style={{
                  position: 'absolute',
                  top: 5,
                  right: isCloseToEnd ? `calc(100% - ${leftPositionPercentage}%)` : `auto`,
                  left: isCloseToEnd
                    ? `auto`
                    : `calc(${leftPositionPercentage}% + ${widthPercentage}%)`
                }}
              >
                {hoverItem}
              </div>
            }
          </>
        );
      })()}
    </div>
  );
};

const ToggleButton = (props: {
  isExpanded: boolean;
  toggle: () => void;
  disabled: boolean;
  IconExpanded?: React.FC;
  IconCollapsed?: React.FC;
}) => {
  const IconExpanded = props.IconExpanded || MinusIcon;
  const IconCollapsed = props.IconCollapsed || PlusIcon;
  const ToggleIcon = props.isExpanded ? IconExpanded : IconCollapsed;
  const textColor = props.disabled ? 'text-gray-300' : 'text-gray-600';
  const hoverScale = props.disabled ? '' : 'hover:scale-110 cursor-pointer';
  return (
    <ToggleIcon
      className={`h-4 w-4 min-w-4 cursor-pointer ${textColor} ${hoverScale}`}
      onClick={(e) => {
        if (!props.disabled) {
          props.toggle();
        }
        e.stopPropagation();
        e.preventDefault();
      }}
    />
  );
};

type StepWithEllapsedTime = Step & {
  cumulativeTimeMillis: number;
  stepDurationMillis: number;
  pauseAfterLastStepMillis: number;
  latestGlobalEllapsedTime: number;
};

/**
 * Quick function to collapse to ellapsed time
 * Note this may break if steps are not in a perfectly linear order.
 * This is not the case now, but as we add parallelism it very well may be.
 * @param steps
 * @param globalStartTimeMillis -- optional, if not provided we assume the first step is the start
 * @param globalEndTimeMillis - optional, if not provided we assume the last step is the end
 * @returns
 */
const collapseTimestampsToEllapsedTime = (
  steps: Step[],
  globalStartTimeMillis?: number,
  globalEndTimeMillis?: number
): StepWithEllapsedTime[] => {
  if (steps.length === 0) {
    return [];
  }

  const sortedSteps = steps.sort((a, b) => {
    return new Date(a.step_start_log.start_time) > new Date(b.step_start_log.start_time) ? 1 : -1;
  });

  // If global start time is provided, calculate the offset to shift all times
  const firstStepStartTime = new Date(sortedSteps[0].step_start_log.start_time).getTime();
  const timeOffset = globalStartTimeMillis ? globalStartTimeMillis - firstStepStartTime : 0;

  let lastTime = globalStartTimeMillis || firstStepStartTime;
  let cumulativeTimeMillis = 0;

  return sortedSteps
    .map((step) => {
      const stepStartTime = new Date(step.step_start_log.start_time).getTime() + timeOffset;
      const stepEndTime =
        new Date(step.step_end_log?.end_time || new Date()).getTime() + timeOffset;

      const pauseAfterLastStepMillis = stepStartTime - lastTime;
      const stepDurationMillis = stepEndTime - stepStartTime;

      cumulativeTimeMillis += stepDurationMillis;
      lastTime = stepEndTime;

      return {
        ...step,
        cumulativeTimeMillis,
        pauseAfterLastStepMillis,
        stepDurationMillis,
        latestGlobalEllapsedTime: 0
      };
    })
    .map((step) => ({
      ...step,
      latestGlobalEllapsedTime: globalEndTimeMillis ? globalEndTimeMillis : cumulativeTimeMillis
    }));
};

const PauseRow = (props: { pauseMillis: number }) => {
  return (
    <TableRow className=" bg-gray-50 text-sm text-gray-300">
      <TableCell colSpan={1}>
        <FiClock className="w-5 h-t" />
        {/* <span className="text-gray-500">pause</span> */}
      </TableCell>
      <TableCell colSpan={1}>
        <DurationDisplay startDate={0} endDate={props.pauseMillis} />
      </TableCell>
      <TableCell colSpan={5}></TableCell>
    </TableRow>
  );
};

const ActionSubTable = (props: {
  step: StepWithEllapsedTime;
  appID: string;
  partitionKey: string | null;
  numPriorIndices: number;
  isTraceExpanded: boolean;
  toggleTraceExpandedActions: (index: number) => void;
  isLinksExpanded: boolean;
  toggleLinksExpanded: (index: number) => void;
  minimized: boolean;
  links: ChildApplicationModel[];
  displaySpansCol: boolean;
  displayLinksCol: boolean;
  earliestTimeSeen: Date;
  latestTimeSeen: Date;
  setTraceExpanded: (b: boolean) => void;
  projectId: string;
  pauseLocation?: 'top' | 'bottom'; // If it's top then we put one above, if it's bottom then below, otherwise no pause
  pauseTime?: number;
  topToBottomChronological: boolean;
  displayAnnotations: boolean;
  annotationsForStep: AnnotationOut[];
  depth: number;
}) => {
  const {
    step,
    latestTimeSeen,
    isTraceExpanded,
    toggleTraceExpandedActions,
    isLinksExpanded,
    toggleLinksExpanded,
    links,
    displaySpansCol,
    displayLinksCol,
    earliestTimeSeen,
    setTraceExpanded,
    pauseTime,
    topToBottomChronological,
    depth
  } = props;
  const [expandNonSpanAttributes, setExpandNonSpanAttributes] = useState<boolean>(false); // attributes that are associated with the action, not the span...
  const [highlightedSubApplicationIDs, setHighlightedSubApplicationIDs] = useState<string[]>([]);
  // TODO -- lower a lot of this state into this component
  // This was pulled out quickly and we need to put anyting that's not read above into this
  return (
    <>
      {props.pauseLocation === 'top' && pauseTime !== undefined && (
        <PauseRow pauseMillis={pauseTime} />
      )}
      <ActionTableRow
        key={`${step.step_start_log.sequence_id}-action-table-row`}
        appID={props.appID}
        partitionKey={props.partitionKey}
        step={step}
        numPriorIndices={props.numPriorIndices}
        isTracesExpanded={isTraceExpanded}
        isLinksExpanded={isLinksExpanded}
        toggleTraceExpanded={toggleTraceExpandedActions}
        toggleLinksExpanded={toggleLinksExpanded}
        minimized={props.minimized}
        links={links}
        displaySpansCol={displaySpansCol}
        displayLinksCol={displayLinksCol}
        earliestTimeSeen={earliestTimeSeen}
        isExpanded={isTraceExpanded}
        setExpanded={setTraceExpanded}
        allowExpand={
          step.spans.length > 0 || step.streaming_events.length > 0 || step.attributes.length > 0
        }
        latestTimeSeen={latestTimeSeen}
        expandNonSpanAttributes={expandNonSpanAttributes}
        setExpandNonSpanAttributes={setExpandNonSpanAttributes}
        streamingEvents={step.streaming_events}
        displayAnnotations={props.displayAnnotations}
        existingAnnotation={props.annotationsForStep.find(
          (annotation) =>
            annotation.step_sequence_id === step.step_start_log.sequence_id &&
            annotation.span_id === null
        )}
        depth={depth}
      />
      {isTraceExpanded && (
        <StepSubTable
          spans={step.spans}
          appID={props.appID}
          partitionKey={props.partitionKey}
          attributes={step.attributes}
          step={step}
          numPriorIndices={props.numPriorIndices}
          minimized={props.minimized}
          displaySpansCol={displaySpansCol}
          displayLinksCol={displayLinksCol}
          earliestTimeSeen={earliestTimeSeen}
          latestTimeSeen={latestTimeSeen}
          expandNonSpanAttributes={expandNonSpanAttributes}
          streamingEvents={step.streaming_events}
          topToBottomChronological={topToBottomChronological}
          displayAnnotations={props.displayAnnotations}
          depth={depth}
          // forceCollapsed={!intentionExpandAll}
        />
      )}
      {isLinksExpanded && (
        <LinkSubTable
          step={step}
          appID={props.appID}
          partitionKey={props.partitionKey}
          links={links}
          numPriorIndices={props.numPriorIndices}
          minimized={props.minimized}
          projectId={props.projectId}
          displaySpansCol={displaySpansCol}
          displayLinksCol={displayLinksCol}
          setExpandedSubApplicationIDs={setHighlightedSubApplicationIDs}
          expandedSubApplicationIDs={highlightedSubApplicationIDs}
          earliestTimeSeen={earliestTimeSeen}
          latestTimeSeen={latestTimeSeen}
          topToBottomChronological={topToBottomChronological}
          depth={depth}
        />
      )}
      {props.pauseLocation === 'bottom' && pauseTime !== undefined && (
        <PauseRow pauseMillis={pauseTime} />
      )}
    </>
  );
};

export const StepList = (props: {
  stepsWithEllapsedTime: StepWithEllapsedTime[];
  annotations: AnnotationOut[];
  numPriorIndices: number;
  minimized: boolean;
  projectId: string;
  topToBottomChronological: boolean;
  displayAnnotations: boolean;
  traceExpandedActions: number[];
  setTraceExpandedActions: (indices: number[]) => void;
  linksExpandedActions: number[];
  toggleTraceExpandedActions: (index: number) => void;
  toggleLinksExpanded: (index: number) => void;
  displaySpansCol: boolean;
  displayLinksCol: boolean;
  earliestTimeSeen: Date;
  latestTimeSeen: Date;
  links: ChildApplicationModel[];
  appID: string;
  partitionKey: string | null;
  depth: number;
}) => {
  const linksBySequenceID = props.links.reduce((acc, child) => {
    const existing = acc.get(child.sequence_id || -1) || [];
    existing.push(child);
    acc.set(child.sequence_id || -1, existing);
    return acc;
  }, new Map<number, ChildApplicationModel[]>());
  return (
    <>
      {props.stepsWithEllapsedTime.map((step) => {
        // TODO -- make more efficient with a map
        const annotationsForStep = props.annotations.filter(
          (annotation) => annotation.step_sequence_id === step.step_start_log.sequence_id
        );
        const isTraceExpanded = props.traceExpandedActions.includes(
          step.step_start_log.sequence_id
        );
        const setTraceExpanded = (b: boolean) => {
          if (b) {
            props.setTraceExpandedActions([
              ...props.traceExpandedActions,
              step.step_start_log.sequence_id
            ]);
          } else {
            props.setTraceExpandedActions(
              props.traceExpandedActions.filter((i) => i !== step.step_start_log.sequence_id)
            );
          }
        };
        const isLinksExpanded = props.linksExpandedActions.includes(
          step.step_start_log.sequence_id
        );
        const links = linksBySequenceID.get(step.step_start_log.sequence_id) || [];
        const beforePause = step.pauseAfterLastStepMillis > PAUSE_TIME_THRESHOLD_MILLIS;
        return (
          <ActionSubTable
            key={step.step_start_log.sequence_id}
            step={step}
            appID={props.appID}
            partitionKey={props.partitionKey}
            numPriorIndices={props.numPriorIndices}
            isTraceExpanded={isTraceExpanded}
            toggleTraceExpandedActions={props.toggleTraceExpandedActions}
            isLinksExpanded={isLinksExpanded}
            toggleLinksExpanded={props.toggleLinksExpanded}
            minimized={props.minimized}
            links={links}
            displaySpansCol={props.displaySpansCol}
            displayLinksCol={props.displayLinksCol}
            earliestTimeSeen={props.earliestTimeSeen}
            latestTimeSeen={props.latestTimeSeen}
            setTraceExpanded={setTraceExpanded}
            projectId={props.projectId}
            pauseLocation={
              beforePause ? (props.topToBottomChronological ? 'top' : 'bottom') : undefined
            }
            pauseTime={step.pauseAfterLastStepMillis}
            topToBottomChronological={props.topToBottomChronological}
            displayAnnotations={props.displayAnnotations}
            annotationsForStep={annotationsForStep}
            depth={props.depth}
          />
        );
      })}
    </>
  );
};

// const TableWithRef = forwardRef<HTMLDivElement, React.ComponentProps<typeof Table>>(
//   (props, ref) => {
//     return <Table {...props} ref={ref as React.Ref<HTMLDivElement>} />;
//   }
// );

/**
 * Table with a list of steps.
 * The indexing is off here -- as it updates the index stays the same.
 * We need to think through the UI a bit, but for now this works cleanly.
 * This also handles setting hover/clicking state through fields/variables
 * passed in.
 *
 * TODO -- add pagination.
 * TODO -- fix up indexing
 */
export const ApplicationTable = (props: {
  steps: Step[];
  appID: string;
  partitionKey: string | null;
  annotations: AnnotationOut[];
  numPriorIndices: number;
  autoRefresh: boolean;
  setAutoRefresh: (b: boolean) => void;
  minimized: boolean;
  setMinimized: (b: boolean) => void;
  allowMinimized: boolean;
  projectId: string;
  parentPointer: PointerModel | undefined;
  spawningParentPointer: PointerModel | undefined;
  links: ChildApplicationModel[];
  fullScreen: boolean;
  allowFullScreen: boolean;
  setFullScreen: (b: boolean) => void;
  topToBottomChronological: boolean;
  setTopToBottomChronological: (b: boolean) => void;
  toggleInspectViewOpen: () => void;
  displayAnnotations: boolean;
}) => {
  // This is a quick way of expanding the actions all at once
  const [traceExpandedActions, setTraceExpandedActions] = useState<number[]>([]);

  const toggleTraceExpandedActions = (index: number) => {
    if (traceExpandedActions.includes(index)) {
      setTraceExpandedActions(traceExpandedActions.filter((i) => i !== index));
    } else {
      setTraceExpandedActions([...traceExpandedActions, index]);
    }
  };
  const [intentionExpandAll, setIntentionExpandAll] = useState(false);

  const [linksExpandedActions, setLinksExpandedActions] = useState<number[]>([]);
  const stepsWithEllapsedTime = collapseTimestampsToEllapsedTime(props.steps);
  const tableRef = useRef<HTMLTableElement>(null);
  const tableScrollRef = useRef<HTMLDivElement>(null);

  const uniqueStepView = props.steps.flatMap((step) => {
    return [
      step.step_start_log.sequence_id,
      step.step_end_log?.sequence_id || '-',
      ...step.attributes.map((attr) => attr.key || '-'),
      ...step.spans.map((span) => span.begin_entry.span_id || '-')
    ];
  });

  uniqueStepView.sort();
  useEffect(() => {
    if (props.autoRefresh && tableScrollRef.current && props.topToBottomChronological) {
      //TODO -- scroll to the end if we we're on auto-refresh
      tableScrollRef.current.scrollIntoView({ behavior: 'smooth', block: 'end' });
    }
  }, [uniqueStepView.map((i) => i.toString()).join(''), props.autoRefresh]);

  stepsWithEllapsedTime
    .sort((a, b) => {
      const naturalOrder = b.step_start_log.sequence_id - a.step_start_log.sequence_id;
      if (naturalOrder !== 0) {
        return props.topToBottomChronological ? naturalOrder : -naturalOrder;
      }
      return 0;
    })
    .reverse();

  const toggleExpandAllTraces = () => {
    if (intentionExpandAll) {
      setTraceExpandedActions([]);
    } else {
      const allIndices = stepsWithEllapsedTime.map((step) => step.step_start_log.sequence_id);
      setTraceExpandedActions(allIndices);
    }
    setIntentionExpandAll(!intentionExpandAll);
  };
  const isLinksExpanded = (index: number) => {
    return linksExpandedActions.includes(index);
  };
  const toggleLinksExpanded = (index: number) => {
    if (isLinksExpanded(index)) {
      setLinksExpandedActions(linksExpandedActions.filter((i) => i !== index));
    } else {
      setLinksExpandedActions([...linksExpandedActions, index]);
    }
  };
  const earliestTimeSeen =
    stepsWithEllapsedTime.length > 0
      ? new Date(
          Math.min(
            ...stepsWithEllapsedTime.map((step) =>
              new Date(step.step_start_log.start_time).getTime()
            )
          )
        )
      : new Date();

  const endTimes = stepsWithEllapsedTime.map((step) => step.step_end_log?.end_time || new Date());
  const latestTimeSeen =
    endTimes.length > 0
      ? new Date(Math.max(...endTimes.map((date) => new Date(date).getTime())))
      : new Date();
  const MinimizeTableIcon = props.minimized ? ChevronRightIcon : ChevronLeftIcon;
  const FullScreenIcon = props.fullScreen ? AiOutlineFullscreenExit : AiOutlineFullscreen;
  const displaySpansCol = stepsWithEllapsedTime.some(
    (step) => step.spans.length > 0 || step.streaming_events.length > 0
  );
  const displayLinksCol = props.links.length > 0;

  const [tableHeight, setTableHeight] = useState('auto');

  const updateTableHeight = () => {
    if (tableRef.current) {
      const parentElement = tableRef.current.parentElement;
      if (parentElement) {
        const parentHeight = parentElement.clientHeight;
        const tableTop =
          tableRef.current.getBoundingClientRect().top - parentElement.getBoundingClientRect().top;
        const availableHeight = parentHeight - tableTop;
        setTableHeight(`${availableHeight}px`);
      }
    }
  };

  useEffect(() => {
    updateTableHeight(); // Initial calculation on mount
    window.addEventListener('resize', updateTableHeight); // Recalculate on window resize

    // Set up a ResizeObserver to listen to changes in the parent element's size
    const observer = new ResizeObserver(updateTableHeight);
    if (tableRef.current?.parentElement) {
      observer.observe(tableRef.current.parentElement);
    }

    return () => {
      window.removeEventListener('resize', updateTableHeight);
      observer.disconnect(); // Cleanup observer on unmount
    };
  }, []);

  const parentRows = (
    <>
      {props.parentPointer ? (
        <ParentLink
          parentPointer={props.parentPointer}
          projectId={props.projectId}
          type="fork"
          minimized={props.minimized}
        />
      ) : (
        <></>
      )}
      {props.spawningParentPointer ? (
        <ParentLink
          parentPointer={props.spawningParentPointer}
          projectId={props.projectId}
          type="spawn"
          displayLinksCol={displayLinksCol}
          displaySpansCol={displaySpansCol}
          minimized={props.minimized}
        />
      ) : (
        <></>
      )}
    </>
  );

  return (
    <div ref={tableRef} style={{ height: tableHeight }}>
      <Table dense={2} style={{ maxHeight: tableHeight }} className="hide-scrollbar">
        <TableHead className=" bg-white sticky top-0 z-50">
          <TableRow className="">
            <TableHeader className="" colSpan={1}>
              <div className="py-1 flex flex-row gap-2 items-center">
                <MinimizeTableIcon
                  className={`h-4 w-4 ${props.fullScreen ? 'text-gray-300' : 'hover:scale-110 cursor-pointer text-gray-600'} ${props.allowMinimized ? '' : 'invisible'}`}
                  onClick={(e) => {
                    props.setMinimized(!props.minimized);
                    e.stopPropagation();
                  }}
                />
                <FullScreenIcon
                  className={`h-4 w-4 text-white hover:scale-125 cursor-pointer bg-dwdarkblue/70 rounded-sm ${!props.allowFullScreen ? 'invisible' : ''}`}
                  onClick={(e) => {
                    props.setFullScreen(!props.fullScreen);
                    props.setMinimized(false);
                    e.stopPropagation();
                  }}
                />
                {props.minimized ? (
                  <ToggleButton
                    disabled={!displaySpansCol}
                    isExpanded={intentionExpandAll}
                    toggle={toggleExpandAllTraces}
                  />
                ) : (
                  <></>
                )}
                {props.minimized ? (
                  <AutoRefreshSwitch
                    setAutoRefresh={props.setAutoRefresh}
                    autoRefresh={props.autoRefresh}
                  />
                ) : (
                  <></>
                )}
              </div>
            </TableHeader>
            <TableHeader className="w-72">
              <div className="flex gap-1 items-center justify-start">
                <ToggleButton
                  disabled={!displaySpansCol}
                  isExpanded={intentionExpandAll}
                  toggle={toggleExpandAllTraces}
                />
                Action
                <ToggleButton
                  disabled={false}
                  isExpanded={props.topToBottomChronological}
                  toggle={() => props.setTopToBottomChronological(!props.topToBottomChronological)}
                  IconCollapsed={FiArrowUp}
                  IconExpanded={FiArrowDown}
                />
              </div>
            </TableHeader>

            {!props.minimized && (
              <>
                {displayLinksCol && (
                  <TableHeader>
                    <div className="flex flex-row items-center gap-2">
                      <ToggleButton
                        disabled={!displaySpansCol}
                        isExpanded={intentionExpandAll}
                        toggle={toggleExpandAllTraces}
                      />
                      Links
                    </div>
                  </TableHeader>
                )}
                <TableHeader></TableHeader>
                <TableHeader className="flex justify-end items-center">
                  <div className="flex flex-row items-center gap-2 bg-dwdarkblue/70 text-white px-1 rounded-md hover:opacity-75">
                    <AutoRefreshSwitch
                      setAutoRefresh={props.setAutoRefresh}
                      autoRefresh={props.autoRefresh}
                      textColor="text-white"
                    />
                    <span className="">Live</span>
                  </div>
                </TableHeader>
                {props.displayAnnotations && (
                  <TableHeader>
                    <FaPencilAlt />
                  </TableHeader>
                )}
              </>
            )}
          </TableRow>
        </TableHead>
        {props.topToBottomChronological ? parentRows : <></>}
        <TableBody className="pt-10">
          <StepList
            appID={props.appID}
            partitionKey={props.partitionKey}
            stepsWithEllapsedTime={stepsWithEllapsedTime}
            annotations={props.annotations}
            numPriorIndices={props.numPriorIndices}
            minimized={props.minimized}
            projectId={props.projectId}
            topToBottomChronological={props.topToBottomChronological}
            displayAnnotations={props.displayAnnotations}
            traceExpandedActions={traceExpandedActions}
            setTraceExpandedActions={setTraceExpandedActions}
            linksExpandedActions={linksExpandedActions}
            toggleTraceExpandedActions={toggleTraceExpandedActions}
            toggleLinksExpanded={toggleLinksExpanded}
            displaySpansCol={displaySpansCol}
            displayLinksCol={displayLinksCol}
            earliestTimeSeen={earliestTimeSeen}
            latestTimeSeen={latestTimeSeen}
            links={props.links}
            depth={0}
          />
        </TableBody>

        {!props.topToBottomChronological ? parentRows : <></>}
        <div ref={tableScrollRef}></div>
      </Table>
    </div>
  );
};

const ParentLink = (props: {
  parentPointer: PointerModel;
  projectId: string;
  displayLinksCol?: boolean;
  displaySpansCol?: boolean;
  type: 'spawn' | 'fork';
  minimized: boolean;
}) => {
  const Icon = props.type === 'fork' ? TbGrillFork : TiFlowChildren;
  return (
    <TableRow className="text-gray-500 cursor-pointer bg-gray-100">
      <TableCell colSpan={1} className="items-center justify-center flex max-w-20 -ml-5">
        <Icon className="h-5 w-5 -ml-1" />
      </TableCell>
      <TableCell colSpan={2} className="text-gray-500">
        <div className="flex flex-row gap-1 items-center pl-5">
          <Link
            to={`/project/${props.projectId}/${props.parentPointer.partition_key}/${props.parentPointer.app_id}`}
          >
            <span className="hover:underline">{props.parentPointer.app_id}</span>
          </Link>
          <span>@</span>
          <span>{props.parentPointer.sequence_id}</span>
        </div>
      </TableCell>
      {!props.minimized && (
        <>
          {/* <TableCell></TableCell>â€º */}
          <TableCell colSpan={1} className="text-gray-500">
            <Chip
              label={'parent'}
              chipType={props.type}
              className="w-16 flex flex-row justify-center"
            />
          </TableCell>
          <TableCell colSpan={1} />
        </>
      )}
    </TableRow>
  );
};



---
File: /burr/telemetry/ui/src/components/routes/AdminView.tsx
---

import { useQuery } from 'react-query';
import { Table, TableBody, TableCell, TableHead, TableHeader, TableRow } from '../common/table';
import { DefaultService } from '../../api';
import { Loading } from '../common/loading';
import { DateTimeDisplay, DurationDisplay } from '../common/dates';
import JsonView from '@uiw/react-json-view';
import { useState } from 'react';
import { FunnelIcon } from '@heroicons/react/24/outline';

const RecordsHeader = (props: {
  displayZeroCount: boolean;
  setDisplayZeroCount: (displayZeroCount: boolean) => void;
}) => {
  const fillColor = props.displayZeroCount ? 'fill-gray-300' : 'fill-gray-700';
  const borderColor = props.displayZeroCount ? 'text-gray-300' : 'text-gray-700';
  return (
    <div className="flex flex-row items-center gap-1">
      <FunnelIcon
        className={`h-5 w-5 hover:scale-125 cursor-pointer ${fillColor} ${borderColor}`}
        onClick={() => {
          props.setDisplayZeroCount(!props.displayZeroCount);
        }}
      />
      <span>Records Processed</span>
    </div>
  );
};

/**
 * Currently just shows indexing jobs, but we'll likely
 * want to add more depending on whether the backend supports it.
 * @returns A rendered admin view object
 */
export const AdminView = () => {
  const [displayZeroCount, setDisplayZeroCount] = useState(false);

  const { data, isLoading } = useQuery(['indexingJobs', displayZeroCount], () =>
    DefaultService.getIndexingJobsApiV0IndexingJobsGet(
      0, // TODO -- add pagination
      100,
      !displayZeroCount
    )
  );
  if (isLoading) {
    return <Loading />;
  }

  return (
    <Table dense={1}>
      <TableHead>
        <TableRow>
          <TableHeader>ID</TableHeader>
          <TableHeader>Start Time</TableHeader>
          <TableHeader>Duration</TableHeader>
          <TableHeader>Status</TableHeader>
          <TableHeader>
            <RecordsHeader
              displayZeroCount={displayZeroCount}
              setDisplayZeroCount={setDisplayZeroCount}
            />
          </TableHeader>
          <TableHeader>Metadata</TableHeader>
        </TableRow>
      </TableHead>
      <TableBody>
        {data?.map((job) => {
          return (
            <TableRow key={job.id} className="hover:bg-gray-50">
              <TableCell>{job.id}</TableCell>
              <TableCell>
                {<DateTimeDisplay date={job.start_time} mode={'short'}></DateTimeDisplay>}
              </TableCell>
              <TableCell>
                {job.end_time ? (
                  <DurationDisplay
                    startDate={job.start_time}
                    endDate={job.end_time}
                  ></DurationDisplay>
                ) : (
                  <></>
                )}
              </TableCell>
              <TableCell>{job.status.toLowerCase()}</TableCell>
              <TableCell>{job.records_processed}</TableCell>
              <TableCell>
                <JsonView value={job.metadata} displayDataTypes enableClipboard={false}></JsonView>
              </TableCell>
            </TableRow>
          );
        })}
      </TableBody>
    </Table>
  );
};



---
File: /burr/telemetry/ui/src/components/routes/AppList.tsx
---

import { useQuery } from 'react-query';
import { Navigate, useParams } from 'react-router';
import { Loading } from '../common/loading';
import { ApplicationSummary, DefaultService } from '../../api';
import { Table, TableBody, TableCell, TableHead, TableHeader, TableRow } from '../common/table';
import { DateTimeDisplay } from '../common/dates';
import { useEffect, useState } from 'react';
import { FunnelIcon, MinusIcon, PlusIcon } from '@heroicons/react/24/outline';
import { Link, useNavigate, useSearchParams } from 'react-router-dom';
import { MdForkRight } from 'react-icons/md';
import { RiCornerDownRightLine } from 'react-icons/ri';
import React from 'react';
import { Paginator } from '../common/pagination';

const StepCountHeader = (props: {
  displayZeroCount: boolean;
  setDisplayZeroCount: (displayZeroCount: boolean) => void;
}) => {
  const fillColor = props.displayZeroCount ? 'fill-gray-300' : 'fill-gray-700';
  const borderColor = props.displayZeroCount ? 'text-gray-300' : 'text-gray-700';
  return (
    <div className="flex flex-row items-center gap-1">
      <FunnelIcon
        className={`h-5 w-5 hover:scale-125 cursor-pointer ${fillColor} ${borderColor}`}
        onClick={() => {
          props.setDisplayZeroCount(!props.displayZeroCount);
        }}
      />
      <span>Seq ID</span>
    </div>
  );
};

const isNullPartitionKey = (partitionKey: string | null) => {
  return partitionKey === null || partitionKey === '__none__';
};

const getForkID = (app: ApplicationSummary) => {
  if (app.parent_pointer) {
    return app.parent_pointer.app_id;
  } else {
    return null;
  }
};

const getParentPartitionKey = (app: ApplicationSummary) => {
  if (app.parent_pointer) {
    return app.parent_pointer.partition_key;
  } else {
    return null;
  }
};

/**
 * Sub-application list -- handles spaned applications.
 * This contains a list of applications that correspond to a parent application.
 * It is either an individual application or more when recursively applied
 * @param props
 * @returns
 */
const AppSubList = (props: {
  app: ApplicationSummary;
  navigate: (url: string) => void;
  projectId: string;
  spawningParentMap: Map<string, ApplicationSummary[]>;
  parentHovered?: boolean;
  depth?: number;
  displayPartitionKey: boolean;
}) => {
  const [subAppsExpanded, setSubAppsExpanded] = useState(false);

  const forkID = getForkID(props.app);
  const { app } = props;
  const ExpandIcon = subAppsExpanded ? MinusIcon : PlusIcon;

  const [isHovered, setIsHovered] = useState(true);
  const isHighlighted = props.parentHovered || isHovered;
  const depth = props.depth || 0;
  return (
    <>
      <TableRow
        onMouseEnter={() => {
          setIsHovered(true);
        }}
        onMouseLeave={() => {
          setIsHovered(false);
        }}
        key={props.app.app_id}
        className={`cursor-pointer ${isHighlighted ? 'bg-gray-50' : ''}`}
        onClick={() => {
          props.navigate(`/project/${props.projectId}/${app.partition_key}/${app.app_id}`);
        }}
      >
        {props.displayPartitionKey && (
          <TableCell className="text-gray-600 font-sans">
            {isNullPartitionKey(app.partition_key) ? (
              <></>
            ) : (
              <Link
                to={`/project/${props.projectId}/${app.partition_key}`}
                className="hover:underline"
                onClick={(e) => {
                  props.navigate(`/project/${props.projectId}/${app.partition_key}`);
                  e.stopPropagation();
                }}
              >
                {app.partition_key}
              </Link>
            )}
          </TableCell>
        )}
        <TableCell className="font-semibold text-gray-700">
          <div className="flex flex-row gap-1 items-center md:min-w-[21rem] md:max-w-none max-w-24">
            {[...Array(depth).keys()].map((i) => (
              <RiCornerDownRightLine
                key={i}
                className={`${i === depth - 1 ? 'opacity-100' : 'opacity-0'} text-lg text-gray-600`}
              ></RiCornerDownRightLine>
            ))}
            {app.app_id}
          </div>
        </TableCell>
        <TableCell>
          <DateTimeDisplay date={app.last_written} mode="long" />
        </TableCell>
        <TableCell className="z-50">
          {forkID ? (
            <MdForkRight
              className=" hover:scale-125 h-5 w-5 text-gray-600 "
              onClick={(e) => {
                props.navigate(
                  `/project/${props.projectId}/${getParentPartitionKey(app) || 'null'}/${forkID}`
                );
                e.stopPropagation();
              }}
            />
          ) : (
            <></>
          )}
        </TableCell>
        <TableCell>
          {props.spawningParentMap.has(props.app.app_id) ? (
            <div className="flex flex-row items-center gap-1">
              <ExpandIcon
                className="h-4 w-4 text-gray-600 hover:scale-110 cursor-pointer"
                onClick={(e) => {
                  setSubAppsExpanded(!subAppsExpanded);
                  e.stopPropagation();
                }}
              />
              <span className="text-gray-600">
                ({props.spawningParentMap.get(props.app.app_id)?.length})
              </span>
            </div>
          ) : (
            <></>
          )}
        </TableCell>
        <TableCell className="text-gray-600">{app.num_steps}</TableCell>
      </TableRow>
      {props.spawningParentMap.has(props.app.app_id) && subAppsExpanded
        ? props.spawningParentMap.get(props.app.app_id)?.map((subApp) => {
            return (
              <AppSubList
                key={subApp.app_id}
                displayPartitionKey={props.displayPartitionKey}
                app={subApp}
                navigate={props.navigate}
                projectId={props.projectId}
                spawningParentMap={props.spawningParentMap}
                parentHovered={isHighlighted}
                depth={depth + 1}
              />
            );
          })
        : null}
    </>
  );
};
/**
 * List of applications. Purely rendering a list, also sorts them.
 */
export const AppListTable = (props: { apps: ApplicationSummary[]; projectId: string }) => {
  const appsCopy = [...props.apps];
  const [displayZeroCount, setDisplayZeroCount] = useState(true);
  const navigate = useNavigate();
  const appsToDisplay = appsCopy
    .sort((a, b) => {
      return new Date(a.last_written) > new Date(b.last_written) ? -1 : 1;
    })
    .filter((app) => {
      return app.num_steps > 0 || displayZeroCount;
    });

  // Maps from parents -> children
  const spawningParentMap = appsToDisplay.reduce((acc, app) => {
    if (app.spawning_parent_pointer) {
      if (acc.has(app.spawning_parent_pointer.app_id)) {
        acc.get(app.spawning_parent_pointer.app_id)!.push(app);
      } else {
        acc.set(app.spawning_parent_pointer.app_id, [app]);
      }
    }
    return acc;
  }, new Map<string, ApplicationSummary[]>());

  // Display the parents no matter what
  // const rootAppsToDisplay = appsToDisplay.filter((app) => app.spawning_parent_pointer === null);
  const rootAppsToDisplay = appsToDisplay;
  const anyHavePartitionKey = rootAppsToDisplay.some(
    (app) => !isNullPartitionKey(app.partition_key)
  );

  const tableRef = React.createRef<HTMLDivElement>();
  const [tableHeight, setTableHeight] = useState('auto');

  const updateTableHeight = () => {
    if (tableRef.current) {
      const parentElement = tableRef.current.parentElement;
      if (parentElement) {
        const parentHeight = parentElement.clientHeight;
        const tableTop =
          tableRef.current.getBoundingClientRect().top - parentElement.getBoundingClientRect().top;
        const availableHeight = parentHeight - tableTop;
        setTableHeight(`${availableHeight}px`);
      }
    }
  };

  useEffect(() => {
    updateTableHeight(); // Initial calculation on mount
    window.addEventListener('resize', updateTableHeight); // Recalculate on window resize

    // Set up a ResizeObserver to listen to changes in the parent element's size
    const observer = new ResizeObserver(updateTableHeight);
    if (tableRef.current?.parentElement) {
      observer.observe(tableRef.current.parentElement);
    }

    return () => {
      window.removeEventListener('resize', updateTableHeight);
      observer.disconnect(); // Cleanup observer on unmount
    };
  }, []);

  return (
    <div ref={tableRef} style={{ height: tableHeight }} className="flex flex-col justify-between">
      <Table dense={1} style={{ maxHeight: tableHeight }} className="hide-scrollbar">
        <TableHead className=" bg-white sticky top-0 z-50">
          <TableRow>
            {anyHavePartitionKey && <TableHeader>Partition Key</TableHeader>}
            <TableHeader>ID</TableHeader>
            <TableHeader>Last Run</TableHeader>
            <TableHeader>Forked</TableHeader>
            <TableHeader>Sub Apps</TableHeader>
            <TableHeader>
              <StepCountHeader
                displayZeroCount={displayZeroCount}
                setDisplayZeroCount={setDisplayZeroCount}
              />
            </TableHeader>
          </TableRow>
        </TableHead>
        <TableBody>
          {appsToDisplay.map((app) => {
            return (
              <AppSubList
                key={app.app_id}
                app={app}
                projectId={props.projectId}
                navigate={navigate}
                spawningParentMap={spawningParentMap}
                displayPartitionKey={anyHavePartitionKey}
              />
            );
          })}
        </TableBody>
      </Table>
    </div>
  );
};

const DEFAULT_LIMIT = 100;

/**
 * List of applications. This fetches data from the BE and passes it to the table
 */
export const AppList = () => {
  const { projectId, partitionKey } = useParams();
  const [searchParams] = useSearchParams();
  const currentOffset = searchParams.get('offset') ? parseInt(searchParams.get('offset')!) : 0;
  const pageSize = DEFAULT_LIMIT;
  const { data, error } = useQuery(
    ['apps', projectId, partitionKey, pageSize, currentOffset],
    () =>
      DefaultService.getAppsApiV0ProjectIdPartitionKeyAppsGet(
        projectId as string,
        partitionKey ? partitionKey : '__none__',
        pageSize,
        currentOffset
      ),
    { enabled: projectId !== undefined }
  );

  const [queriedData, setQueriedData] = useState(data);

  useEffect(() => {
    if (data !== undefined) {
      setQueriedData(data);
    }
  }, [data]);

  if (projectId === undefined) {
    return <Navigate to={'/projects'} />;
  }

  if (error) return <div>Error loading apps</div>;
  const shouldDisplayPagination = queriedData ? queriedData.total > pageSize : false;
  return (
    <div className="h-full flex flex-col gap-1 justify-between">
      {queriedData !== undefined ? (
        <AppListTable apps={queriedData.applications} projectId={projectId} />
      ) : (
        <Loading />
      )}
      <div className="flex flex-row w-full justify-center">
        {shouldDisplayPagination && (
          <Paginator
            currentPage={currentOffset / pageSize + 1}
            getPageURL={(page) => {
              // TODO -- make this more robust to URL changes
              return `/project/${projectId}/${partitionKey || '__none__'}?offset=${(page - 1) * pageSize}`;
            }}
            totalPages={queriedData ? Math.ceil(queriedData?.total / pageSize) : undefined}
            // TODO -- add query result back
            hasNextPage={queriedData ? currentOffset + pageSize < queriedData.total : false}
          ></Paginator>
        )}
      </div>
    </div>
  );
};



---
File: /burr/telemetry/ui/src/components/routes/ProjectList.tsx
---

import { useQuery } from 'react-query';
import { DefaultService, Project } from '../../api';
import { Table, TableBody, TableCell, TableHead, TableHeader, TableRow } from '../common/table';
import { Loading } from '../common/loading';
import { DateDisplay } from '../common/dates';
import { Chip } from '../common/chip';
import { LinkText } from '../common/href';
import { Link, useNavigate } from 'react-router-dom';
import { FaRegEdit } from 'react-icons/fa';

/**
 * Table of a project list. Uses the tailwind catalyst component.
 * This does not load or fetch any data, just renders it
 */
export const ProjectListTable = (props: { projects: Project[]; includeAnnotations: boolean }) => {
  const navigate = useNavigate();
  const projectsCopy = [...props.projects];
  const projectsSorted = projectsCopy.sort((a, b) => {
    return a.last_written > b.last_written ? -1 : 1;
  });
  return (
    <Table>
      <TableHead>
        <TableRow>
          <TableHeader>Name</TableHeader>
          <TableHeader>Created</TableHeader>
          <TableHeader>Last Run</TableHeader>
          <TableHeader>Link</TableHeader>
          <TableHeader>App Runs</TableHeader>
          {props.includeAnnotations && <TableHeader className="w-10">Annotations</TableHeader>}
          <TableHeader></TableHeader>
        </TableRow>
      </TableHead>
      <TableBody>
        {projectsSorted.map((project) => {
          let projectName = project.name;
          const chipType =
            projectName.startsWith('demo:') || projectName.startsWith('demo_')
              ? 'demo'
              : projectName.startsWith('test:') || projectName.startsWith('test_')
                ? 'test'
                : undefined;
          if (chipType) {
            projectName = projectName.slice(5);
          }

          return (
            <TableRow
              key={project.id}
              className="hover:bg-gray-50 cursor-pointer"
              onClick={() => navigate(`/project/${project.id}`)}
            >
              <TableCell className="font-semibold text-gray-700">
                <div className="flex flex-row gap-2">
                  {chipType !== undefined && <Chip label={chipType} chipType={chipType}></Chip>}
                  {projectName}
                </div>
              </TableCell>
              <TableCell>
                <DateDisplay date={project.created} />
              </TableCell>
              <TableCell>
                <DateDisplay date={project.last_written} />
              </TableCell>
              <TableCell>
                <LinkText
                  href={project.uri}
                  text={project.uri.replace('https://github.com/DAGWorks-Inc/burr/tree/main/', '')}
                />
              </TableCell>
              <TableCell>{project.num_apps}</TableCell>
              {props.includeAnnotations && (
                <TableCell>
                  <Link to={`/annotations/${project.id}`}>
                    <FaRegEdit
                      className="hover:underline hover:scale-125"
                      onClick={(e) => {
                        navigate(`/annotations/${project.id}`);
                        e.stopPropagation();
                        e.preventDefault();
                      }}
                    ></FaRegEdit>
                  </Link>
                </TableCell>
              )}
            </TableRow>
          );
        })}
      </TableBody>
    </Table>
  );
};

/**
 * Container for the table -- fetches the data and passes it to the table
 */
export const ProjectList = () => {
  const { data, error } = useQuery('projects', DefaultService.getProjectsApiV0ProjectsGet);
  const { data: backendSpec } = useQuery(['backendSpec'], () =>
    DefaultService.getAppSpecApiV0MetadataAppSpecGet().then((response) => {
      return response;
    })
  );
  if (error) return <div>Error loading projects</div>;
  if (data === undefined || backendSpec === undefined) return <Loading />;
  return (
    <div className="">
      <ProjectListTable projects={data} includeAnnotations={backendSpec?.supports_annotations} />
    </div>
  );
};



---
File: /burr/telemetry/ui/src/examples/Chatbot.tsx
---

import { ComputerDesktopIcon, UserIcon } from '@heroicons/react/24/outline';
import { classNames } from '../utils/tailwind';
import { Button } from '../components/common/button';
import { TwoColumnLayout } from '../components/common/layout';
import { ApplicationSummary, ChatItem, DefaultService } from '../api';
import { KeyboardEvent, useEffect, useState } from 'react';
import { useMutation, useQuery } from 'react-query';
import { Loading } from '../components/common/loading';
import Markdown from 'react-markdown';
import remarkGfm from 'remark-gfm';
import { TelemetryWithSelector } from './Common';

type Role = 'assistant' | 'user';

const DEFAULT_CHAT_HISTORY: ChatItem[] = [
  {
    role: ChatItem.role.ASSISTANT,
    content:
      'ðŸ“– Select a conversation from the list to get started! ' +
      'The left side of this is a simple chatbot. The right side is the same' +
      ' Burr Telemetry app you can see if you click through the [chatbot demo](/projects/demo_chatbot) project. Note that images ' +
      "will likely stop displaying after a while due to OpenAI's persistence policy. So generate some new ones! ðŸ“–",
    type: ChatItem.type.TEXT
  },
  {
    role: ChatItem.role.ASSISTANT,
    content:
      ' \n\nðŸ’¡ This is meant to demonstrate the power of the Burr model -- ' +
      'chat on the left while examining the internals of the chatbot on the right.ðŸ’¡',
    type: ChatItem.type.TEXT
  }
];

const getCharacter = (role: Role) => {
  return role === 'assistant' ? 'AI' : 'You';
};

const RoleIcon = (props: { role: Role }) => {
  const Icon = props.role === 'assistant' ? ComputerDesktopIcon : UserIcon;
  return (
    <Icon className={classNames('text-gray-400', 'ml-auto h-6 w-6 shrink-0')} aria-hidden="true" />
  );
};

const LAST_MESSAGE_ID = 'last-message';

const ImageWithBackup = (props: { src: string; alt: string }) => {
  const [caption, setCaption] = useState<string | undefined>(undefined);
  return (
    <div>
      <img
        src={props.src}
        alt={props.alt}
        onError={(e) => {
          const img = e.target as HTMLImageElement;
          img.src = 'https://via.placeholder.com/500x500?text=Image+Unavailable';
          img.alt =
            'Image unavailable as OpenAI does not persist images -- generate a new one, or modify the code to save it for you.';
          setCaption(img.alt);
        }}
      />
      {caption && <span className="italic text-gray-300">{caption}</span>}
    </div>
  );
};
const ChatMessage = (props: { message: ChatItem; id?: string }) => {
  return (
    <div className="flex gap-3 my-4  text-gray-600 text-sm flex-1 w-full" id={props.id}>
      <span className="relative flex shrink-0">
        <RoleIcon role={props.message.role} />
      </span>
      <p className="leading-relaxed w-full">
        <span className="block font-bold text-gray-700">{getCharacter(props.message.role)} </span>
        {props.message.type === ChatItem.type.TEXT ||
        props.message.type === ChatItem.type.CODE ||
        props.message.type === ChatItem.type.ERROR ? (
          <Markdown
            components={{
              // Custom rendering for links
              a: ({ ...props }) => <a className="text-dwlightblue hover:underline" {...props} />
            }}
            remarkPlugins={[remarkGfm]}
            className={`whitespace-pre-wrap break-lines max-w-full ${props.message.type === ChatItem.type.ERROR ? 'bg-dwred/10' : ''} p-0.5`}
          >
            {props.message.content}
          </Markdown>
        ) : (
          <ImageWithBackup src={props.message.content} alt="chatbot image" />
        )}
      </p>
    </div>
  );
};

const scrollToLatest = () => {
  const lastMessage = document.getElementById(LAST_MESSAGE_ID);
  if (lastMessage) {
    const scroll = () => {
      lastMessage.scrollIntoView({ behavior: 'smooth', block: 'start', inline: 'nearest' });
    };
    scroll();
    const observer = new ResizeObserver(() => {
      scroll();
    });
    observer.observe(lastMessage);
    setTimeout(() => observer.disconnect(), 1000); // Adjust timeout as needed
  }
};

export const Chatbot = (props: { projectId: string; appId: string | undefined }) => {
  const [currentPrompt, setCurrentPrompt] = useState<string>('');
  const [displayedChatHistory, setDisplayedChatHistory] = useState(DEFAULT_CHAT_HISTORY);
  const { isLoading } = useQuery(
    // TODO -- handle errors
    ['chat', props.projectId, props.appId],
    () =>
      DefaultService.chatHistoryApiV0ChatbotResponseProjectIdAppIdGet(
        props.projectId,
        props.appId || '' // TODO -- find a cleaner way of doing a skip-token like thing here
      ),
    {
      enabled: props.appId !== undefined,
      onSuccess: (data) => {
        setDisplayedChatHistory(data); // when its succesful we want to set the displayed chat history
      }
    }
  );

  // Scroll to the latest message when the chat history changes
  useEffect(() => {
    scrollToLatest();
  }, [displayedChatHistory]);

  const mutation = useMutation(
    (message: string) => {
      return DefaultService.chatResponseApiV0ChatbotResponseProjectIdAppIdPost(
        props.projectId,
        props.appId || '',
        message
      );
    },
    {
      onSuccess: (data) => {
        setDisplayedChatHistory(data);
      }
    }
  );

  if (isLoading) {
    return <Loading />;
  }
  const sendPrompt = () => {
    if (currentPrompt !== '') {
      mutation.mutate(currentPrompt);
      setCurrentPrompt('');
      setDisplayedChatHistory([
        ...displayedChatHistory,
        {
          role: ChatItem.role.USER,
          content: currentPrompt,
          type: ChatItem.type.TEXT
        }
      ]);
    }
  };
  const isChatWaiting = mutation.isLoading;
  return (
    <div className="mr-4 bg-white  w-full flex flex-col h-full">
      <h1 className="text-2xl font-bold px-4 text-gray-600">{'Learn Burr '}</h1>
      <h2 className="text-lg font-normal px-4 text-gray-500 flex flex-row">
        The chatbot below is implemented using Burr. Watch the Burr UI (on the right) change as you
        chat...
      </h2>
      <div className="flex-1 overflow-y-auto p-4 hide-scrollbar">
        {displayedChatHistory.map((message, i) => (
          <ChatMessage
            message={message}
            key={i}
            id={i === displayedChatHistory.length - 1 ? LAST_MESSAGE_ID : undefined}
          ></ChatMessage>
        ))}
      </div>
      <div className="flex items-center pt-4 gap-2">
        <input
          className="flex h-10 w-full rounded-md border border-[#e5e7eb] px-3 py-2 text-sm placeholder-[#6b7280] focus:outline-none focus:ring-2 focus:ring-[#9ca3af] disabled:cursor-not-allowed disabled:opacity-50 text-[#030712] focus-visible:ring-offset-2"
          placeholder="Ask me how tall the Eifel tower is!"
          value={currentPrompt}
          onChange={(e) => setCurrentPrompt(e.target.value)}
          onKeyDown={(e: KeyboardEvent<HTMLInputElement>) => {
            if (e.key === 'Enter' && !e.shiftKey) {
              e.preventDefault();
              sendPrompt();
            }
          }}
          disabled={isChatWaiting || props.appId === undefined}
        />
        <Button
          className="w-min cursor-pointer h-full"
          color="dark"
          disabled={isChatWaiting || props.appId === undefined}
          onClick={() => {
            sendPrompt();
          }}
        >
          Send
        </Button>
      </div>
    </div>
  );
};

export const ChatbotWithTelemetry = () => {
  const currentProject = 'demo_chatbot';
  const [currentApp, setCurrentApp] = useState<ApplicationSummary | undefined>(undefined);

  return (
    <TwoColumnLayout
      firstItem={<Chatbot projectId={currentProject} appId={currentApp?.app_id} />}
      secondItem={
        <TelemetryWithSelector
          projectId={currentProject}
          currentApp={currentApp}
          setCurrentApp={setCurrentApp}
          createNewApp={DefaultService.createNewApplicationApiV0ChatbotCreateProjectIdAppIdPost}
        />
      }
      mode={'third'}
    ></TwoColumnLayout>
  );
};



---
File: /burr/telemetry/ui/src/examples/Common.tsx
---

import { useMutation, useQuery } from 'react-query';
import { ApplicationSummary } from '../api/models/ApplicationSummary';
import { DateTimeDisplay } from '../components/common/dates';
import { MiniTelemetry } from './MiniTelemetry';
import { DefaultService } from '../api';
import AsyncCreatableSelect from 'react-select/async-creatable';

type CreateNewApp = typeof DefaultService.createNewApplicationApiV0ChatbotCreateProjectIdAppIdPost;

export const TelemetryWithSelector = (props: {
  projectId: string;
  currentApp: ApplicationSummary | undefined;
  setCurrentApp: (app: ApplicationSummary) => void;
  createNewApp: CreateNewApp;
}) => {
  return (
    <div className="w-full h-[90%]">
      <div className="w-full">
        <ChatbotAppSelector
          projectId={props.projectId}
          setApp={props.setCurrentApp}
          currentApp={props.currentApp}
          placeholder={'Select a conversation or create a new one by typing...'}
          createNewApp={props.createNewApp}
        />
      </div>
      <MiniTelemetry
        projectId={props.projectId}
        appId={props.currentApp?.app_id}
        partitionKey={null}
      ></MiniTelemetry>
    </div>
  );
};

const Label = (props: { application: ApplicationSummary }) => {
  return (
    <div className="flex flex-row gap-2 items-center justify-between">
      <div className="flex flex-row gap-2 items-center">
        <span className="text-gray-400 w-10">{props.application.num_steps}</span>
        <span>{props.application.app_id}</span>
      </div>
      <DateTimeDisplay date={props.application.first_written} mode="short" />
    </div>
  );
};

export const ChatbotAppSelector = (props: {
  projectId: string;
  setApp: (app: ApplicationSummary) => void;
  currentApp: ApplicationSummary | undefined;
  placeholder: string;
  createNewApp: CreateNewApp;
}) => {
  const { projectId, setApp } = props;
  const { data, refetch } = useQuery(
    ['apps', projectId],
    // TODO - use the right partition key
    () => DefaultService.getAppsApiV0ProjectIdPartitionKeyAppsGet(projectId as string, '__none__'),
    { enabled: projectId !== undefined }
  );
  const createAndUpdateMutation = useMutation(
    (app_id: string) => props.createNewApp(projectId, app_id),
    {
      onSuccess: (appID) => {
        refetch().then((data) => {
          const appSummaries = data.data?.applications || [];
          const app = appSummaries.find((app) => app.app_id === appID);
          if (app) {
            setApp(app);
          }
        });
      }
    }
  );
  const appSummaries = data?.applications || [];
  const appSetter = (appID: string) => createAndUpdateMutation.mutate(appID);
  const dataOrEmpty = Array.from(appSummaries || []);
  const options = dataOrEmpty
    .sort((a, b) => {
      return new Date(a.last_written) > new Date(b.last_written) ? -1 : 1;
    })
    .map((app) => {
      return {
        value: app.app_id,
        label: <Label application={app} />
      };
    });
  return (
    <AsyncCreatableSelect
      placeholder={props.placeholder}
      cacheOptions
      defaultOptions={options}
      options={options}
      onCreateOption={appSetter}
      value={options.find((option) => option.value === props.currentApp?.app_id) || null}
      onChange={(choice) => {
        const app = dataOrEmpty.find((app) => app.app_id === choice?.value);
        if (app) {
          setApp(app);
        }
      }}
      loadOptions={(inputValue: string) => {
        return Promise.resolve(
          options.filter((option) => {
            return option.value.startsWith(inputValue);
          })
        );
      }}
    />
  );
};



---
File: /burr/telemetry/ui/src/examples/Counter.tsx
---

import { Link } from 'react-router-dom';

export const Counter = () => {
  return (
    <div className="flex justify-center items-center h-full w-full">
      <p className="text-gray-700">
        {' '}
        This is a WIP! Please check back soon or comment/vote/contribute at the{' '}
        <Link
          className="hover:underline text-dwlightblue"
          to="https://github.com/DAGWorks-Inc/burr/issues/69"
        >
          github issue
        </Link>
        .
      </p>
    </div>
  );
};



---
File: /burr/telemetry/ui/src/examples/DeepResearcher.tsx
---

import { KeyboardEvent, useState } from 'react';
import { ApplicationSummary, DefaultService, ResearchSummary } from '../api';
import { TwoColumnLayout } from '../components/common/layout';
import { TelemetryWithSelector } from './Common';
import { Button } from '../components/common/button';
import { useMutation, useQuery } from 'react-query';
import { Text } from '../components/common/text';
import { Field, Label } from '../components/common/fieldset';

const DEFAULT_RESEARCH_SUMMARY: ResearchSummary = {
  running_summary: ''
};

export const ResearchSummaryView = (props: { summary: string }) => {
  return (
    <>
      <Text>
        <pre className="whitespace-pre-wrap text-xs">{props.summary}</pre>
      </Text>
    </>
  );
};

export const DeepResearcher = (props: { projectId: string; appId: string | undefined }) => {
  const [currentTopic, setCurrentTopic] = useState<string>('');
  const [displayedReport, setDisplayedReport] = useState(DEFAULT_RESEARCH_SUMMARY);
  const { data: validationData, isLoading: isValidationLoading } = useQuery(
    ['valid'],
    DefaultService.validateEnvironmentApiV0DeepResearcherValidateGet
  );

  const mutation = useMutation(
    (topic: string) => {
      return DefaultService.researchResponseApiV0DeepResearcherResponseProjectIdAppIdPost(
        props.projectId,
        props.appId || '',
        topic
      );
    },
    {
      onSuccess: (data) => {
        setDisplayedReport(data);
      }
    }
  );
  const sendTopic = () => {
    if (currentTopic !== '') {
      mutation.mutate(currentTopic);
      setCurrentTopic('');
      setDisplayedReport(DEFAULT_RESEARCH_SUMMARY);
    }
  };
  const isResearcherWaiting = mutation.isLoading || isValidationLoading;
  const displayValidationError = validationData !== null;

  return (
    <div className="px-4 bg-white  w-full flex flex-col  h-full gap-5 overflow-y-scroll">
      <h1 className="text-2xl font-bold text-gray-600">{'Learn Burr '}</h1>
      <h2 className="text-lg font-normal text-gray-500 flex flex-row">
        The research assistant below is implemented using Burr. Watch the Burr UI (on the right)
        change as you chat...
      </h2>
      <div className="flex flex-col">
        {displayValidationError && (
          <p className="text-lg font-normal text-dwred">{validationData}</p>
        )}
        {!displayValidationError && (
          <Field>
            <Label className="text-lg font-bold text-gray-600">Report</Label>
          </Field>
        )}
        <ResearchSummaryView summary={displayedReport.running_summary} />
      </div>
      <div className="flex items-center pt-4 gap-2">
        <input
          className="flex h-10 w-full rounded-md border border-[#e5e7eb] px-3 py-2 text-sm placeholder-[#6b7280] focus:outline-none focus:ring-2 focus:ring-[#9ca3af] disabled:cursor-not-allowed disabled:opacity-50 text-[#030712] focus-visible:ring-offset-2"
          placeholder="how to get a job in data science"
          value={currentTopic}
          onChange={(e) => setCurrentTopic(e.target.value)}
          onKeyDown={(e: KeyboardEvent<HTMLInputElement>) => {
            if (e.key === 'Enter' && !e.shiftKey) {
              e.preventDefault();
              sendTopic();
            }
          }}
          disabled={isResearcherWaiting || props.appId === undefined || displayValidationError}
        />
        <Button
          className="w-min cursor-pointer h-full"
          color="dark"
          disabled={isResearcherWaiting || props.appId === undefined || displayValidationError}
          onClick={() => {
            sendTopic();
          }}
        >
          Send
        </Button>
      </div>
    </div>
  );
};

export const DeepResearcherWithTelemetry = () => {
  const currentProject = 'demo_deep_researcher';
  const [currentApp, setCurrentApp] = useState<ApplicationSummary | undefined>(undefined);
  return (
    <TwoColumnLayout
      firstItem={<DeepResearcher projectId={currentProject} appId={currentApp?.app_id} />}
      secondItem={
        <TelemetryWithSelector
          projectId={currentProject}
          currentApp={currentApp}
          setCurrentApp={setCurrentApp}
          createNewApp={
            DefaultService.createNewApplicationApiV0DeepResearcherCreateProjectIdAppIdPost
          }
        />
      }
      mode={'third'}
    ></TwoColumnLayout>
  );
};



---
File: /burr/telemetry/ui/src/examples/EmailAssistant.tsx
---

import { TwoColumnLayout } from '../components/common/layout';
import { MiniTelemetry } from './MiniTelemetry';
import {
  ApplicationSummary,
  DefaultService,
  DraftInit,
  EmailAssistantState,
  Feedback,
  QuestionAnswers
} from '../api';
import { useEffect, useState } from 'react';
import { useMutation, useQuery } from 'react-query';
import { Loading } from '../components/common/loading';
import { Field, Label } from '../components/common/fieldset';
import { Textarea } from '../components/common/textarea';
import { Input } from '../components/common/input';
import { Text } from '../components/common/text';
import { Button } from '../components/common/button';
import { DateTimeDisplay } from '../components/common/dates';
import AsyncCreatableSelect from 'react-select/async-creatable';

export const InitialDraftView = (props: {
  submitInitial: (initial: DraftInit) => void;
  isLoading: boolean;
  responseInstructions: string | null;
  emailToRespond: string | null;
}) => {
  const [userProvidedEmailToRespond, setUserProvidedEmailToRespond] = useState<string>(
    props.emailToRespond || ''
  );
  const [userProvidedResponseInstructions, setUserProvidedResponseInstructions] = useState<string>(
    props.responseInstructions || ''
  );
  const editMode = props.emailToRespond === null;

  return (
    <div className="w-full flex flex-col gap-2">
      <Field>
        <Label>Email to respond to</Label>
        <Textarea
          name="email_to_respond"
          value={userProvidedEmailToRespond}
          onChange={(e) => {
            setUserProvidedEmailToRespond(e.target.value);
          }}
          disabled={!editMode}
        />
      </Field>
      <Field>
        <Label>Response Instructions</Label>
        <Input
          name="response_instructions"
          value={userProvidedResponseInstructions}
          onChange={(e) => {
            setUserProvidedResponseInstructions(e.target.value);
          }}
          disabled={!editMode}
        />
      </Field>
      {editMode && (
        <Button
          color="white"
          className="cursor-pointer w-full"
          disabled={props.isLoading}
          onClick={() =>
            props.submitInitial({
              email_to_respond: userProvidedEmailToRespond,
              response_instructions: userProvidedResponseInstructions
            })
          }
        >
          {'Submit'}
        </Button>
      )}
    </div>
  );
};

export const SubmitAnswersView = (props: {
  state: EmailAssistantState;
  submitAnswers: (questions: QuestionAnswers) => void;
  questions: string[] | null;
  answers: string[] | null;
  isLoading: boolean;
}) => {
  const questions = props.state.questions || [];
  const [answers, setAnswers] = useState<string[]>(props.answers || questions.map(() => ''));
  const editMode = props.isLoading || props.answers === null;
  return (
    <div className="w-full flex flex-col gap-2">
      <div className="flex flex-col gap-2">
        {(props.state.questions || []).map((question, index) => {
          return (
            <Field key={index}>
              <Label>{question}</Label>
              <Input
                disabled={!editMode}
                value={answers[index]}
                onChange={(e) => {
                  const newAnswers = [...answers];
                  newAnswers[index] = e.target.value;
                  setAnswers(newAnswers);
                }}
              />
            </Field>
          );
        })}
      </div>
      {editMode && (
        <Button
          color="white"
          className="cursor-pointer w-full"
          disabled={props.isLoading}
          onClick={() => props.submitAnswers({ answers: answers })}
        >
          {'Submit'}
        </Button>
      )}
    </div>
  );
};

export const SubmitFeedbackView = (props: {
  state: EmailAssistantState;
  submitFeedback: (feedbacks: Feedback) => void;
  drafts: string[] | null;
  feedbacks: string[] | null;
  isLoading: boolean;
}) => {
  const editMode = props.feedbacks === null || props.feedbacks.length !== props.drafts?.length; // if its not equal we are one step behind
  const [feedback, setFeedback] = useState<string>(props.feedbacks?.[-1] || '');
  return (
    <div className="w-full flex flex-col gap-2">
      <Field>
        <Label className="text-lg font-bold text-gray-600">Drafts</Label>
      </Field>
      <div className="flex flex-col gap-2">
        {(props.drafts || []).map((draft, index) => {
          const providedFeedback = props.feedbacks?.[index];
          return (
            <>
              <Text>
                <pre className="whitespace-pre-wrap text-xs" key={index}>
                  {draft}
                </pre>
              </Text>
              <h3 className="text-sm font-semibold text-gray-600"></h3>
              <Input
                disabled={!editMode || props.isLoading || index !== (props.drafts || []).length - 1}
                placeholder="Provide feedback here..."
                value={providedFeedback || feedback}
                onChange={(e) => {
                  setFeedback(e.target.value);
                }}
              />
            </>
          );
        })}
        {editMode && (
          <div className="flex flex-row gap-1">
            <Button
              color="white"
              className="cursor-pointer w-full"
              disabled={props.isLoading}
              onClick={() => {
                props.submitFeedback({ feedback: feedback }), setFeedback('');
              }}
            >
              {'Submit Feedback'}
            </Button>
            <Button
              color="white"
              className="cursor-pointer w-full"
              disabled={props.isLoading}
              onClick={() => props.submitFeedback({ feedback: '' })}
            >
              {'Looks good!'}
            </Button>
          </div>
        )}
      </div>
    </div>
  );
};

export const EmailAssistant = (props: { projectId: string; appId: string | undefined }) => {
  // starts off as null
  const [emailAssistantState, setEmailAssistantState] = useState<EmailAssistantState | null>(null);
  const { data: validationData, isLoading: isValidationLoading } = useQuery(
    ['valid', props.projectId, props.appId],
    DefaultService.validateEnvironmentApiV0EmailAssistantValidateProjectIdAppIdGet
  );

  useEffect(() => {
    if (props.appId !== undefined) {
      // TODO -- handle errors
      DefaultService.getStateApiV0EmailAssistantStateProjectIdAppIdGet(props.projectId, props.appId)
        .then((data) => {
          setEmailAssistantState(data); // we want to initialize the chat history
        })
        .catch((e) => {
          // eslint-disable-next-line
          console.error(e); // TODO -- handle errors
        });
    }
  }, [props.appId, props.projectId]); // This will only get called when the appID or projectID changes, which will be at the beginning

  const { isLoading: isGetInitialStateLoading } = useQuery(
    // TODO -- handle errors
    ['emailAssistant', props.projectId, props.appId],
    () =>
      DefaultService.getStateApiV0EmailAssistantStateProjectIdAppIdGet(
        props.projectId,
        props.appId || '' // TODO -- find a cleaner way of doing a skip-token like thing here
        // This is skipped if the appId is undefined so this is just to make the type-checker happy
      ),
    {
      enabled: props.appId !== undefined,
      onSuccess: (data) => {
        setEmailAssistantState(data); // when its succesful we want to set the displayed chat history
      }
    }
  );

  const submitInitialMutation = useMutation(
    (draftData: DraftInit) =>
      DefaultService.initializeDraftApiV0EmailAssistantCreateProjectIdAppIdPost(
        props.projectId,
        props.appId || 'create_new',
        draftData
      ),
    {
      onSuccess: (data) => {
        setEmailAssistantState(data);
      }
    }
  );

  const submitAnswersMutation = useMutation(
    (answers: QuestionAnswers) =>
      DefaultService.answerQuestionsApiV0EmailAssistantAnswerQuestionsProjectIdAppIdPost(
        props.projectId,
        props.appId || '',
        answers
      ),
    {
      onSuccess: (data) => {
        setEmailAssistantState(data);
      }
    }
  );
  const submitFeedbackMutation = useMutation(
    (feedbacks: Feedback) =>
      DefaultService.provideFeedbackApiV0EmailAssistantProvideFeedbackProjectIdAppIdPost(
        props.projectId,
        props.appId || '',
        feedbacks
      ),
    {
      onSuccess: (data) => {
        setEmailAssistantState(data);
      }
    }
  );

  const isLoading = isGetInitialStateLoading;
  const anyMutationLoading =
    submitInitialMutation.isLoading ||
    submitAnswersMutation.isLoading ||
    submitFeedbackMutation.isLoading;

  if (isLoading || isValidationLoading) {
    return <Loading />;
  }
  const displayValidationError = validationData !== null;
  const displayInstructions = emailAssistantState === null && !displayValidationError;
  const displayInitialDraft = emailAssistantState !== null;
  const displaySubmitAnswers =
    displayInitialDraft && emailAssistantState.next_step !== 'process_input';
  const displaySubmitFeedback =
    displaySubmitAnswers && emailAssistantState.next_step !== 'clarify_instructions';
  const displayFinalDraft = displaySubmitFeedback && emailAssistantState.next_step === null;
  return (
    <div className="px-4 bg-white  w-full flex flex-col  h-full gap-5 overflow-y-scroll">
      <h1 className="text-2xl font-bold  text-gray-600">{'Learn Burr '}</h1>
      <h2 className="text-lg font-normal text-gray-500 flex flex-row">
        The email assistant below is implemented using Burr. Copy/paste the email you want to
        respond to and provide directions, it will ask you questions, generate a response, and
        adjust for your feedback.
      </h2>
      <div className="flex flex-col">
        {displayInstructions && (
          <p className="text-lg font-normal text-gray-500">
            Please click &apos;create new&apos; on the right to get started!
          </p>
        )}
        {displayValidationError && (
          <p className="text-lg font-normal text-dwred">{validationData}</p>
        )}
        {displayInitialDraft && (
          <InitialDraftView
            isLoading={anyMutationLoading}
            submitInitial={(initial) => {
              submitInitialMutation.mutate(initial);
            }}
            responseInstructions={emailAssistantState?.response_instructions}
            emailToRespond={emailAssistantState?.email_to_respond}
          />
        )}
      </div>
      {displaySubmitAnswers && (
        <SubmitAnswersView
          state={emailAssistantState}
          submitAnswers={(answers) => {
            submitAnswersMutation.mutate(answers);
          }}
          questions={emailAssistantState.questions}
          answers={emailAssistantState.answers}
          isLoading={anyMutationLoading}
        />
      )}
      {displaySubmitFeedback && (
        <SubmitFeedbackView
          state={emailAssistantState}
          submitFeedback={(feedbacks) => {
            submitFeedbackMutation.mutate(feedbacks);
          }}
          drafts={emailAssistantState.drafts}
          feedbacks={emailAssistantState.feedback_history}
          isLoading={anyMutationLoading}
        />
      )}
      {displayFinalDraft && (
        <div className="flex flex-col gap-2">
          <h2 className="text-lg font-bold text-gray-600">Final Draft</h2>
          <Text>
            <pre className="whitespace-pre-wrap text-xs">
              {emailAssistantState.drafts?.[emailAssistantState.drafts.length - 1]}
            </pre>
          </Text>
        </div>
      )}
    </div>
  );
};

export const TelemetryWithSelector = (props: {
  projectId: string;
  currentApp: ApplicationSummary | undefined;
  setCurrentApp: (app: ApplicationSummary) => void;
}) => {
  return (
    <div className="w-full h-[90%]">
      <div className="w-full">
        <EmailAssistantAppSelector
          projectId={props.projectId}
          setApp={props.setCurrentApp}
          currentApp={props.currentApp}
          placeholder={'Select a conversation or create a new one by typing...'}
        ></EmailAssistantAppSelector>
      </div>
      <MiniTelemetry
        projectId={props.projectId}
        appId={props.currentApp?.app_id}
        partitionKey={null}
      ></MiniTelemetry>
    </div>
  );
};

const EmailAssistantLabel = (props: { application: ApplicationSummary }) => {
  return (
    <div className="flex flex-row gap-2 items-center justify-between">
      <div className="flex flex-row gap-2 items-center">
        <span className="text-gray-400 w-10">{props.application.num_steps}</span>
        <span>{props.application.app_id}</span>
      </div>
      <DateTimeDisplay date={props.application.first_written} mode="short" />
    </div>
  );
};

export const EmailAssistantAppSelector = (props: {
  projectId: string;
  setApp: (app: ApplicationSummary) => void;
  currentApp: ApplicationSummary | undefined;
  placeholder: string;
}) => {
  const { projectId, setApp } = props;
  const { data, refetch } = useQuery(
    ['apps', projectId],
    () => DefaultService.getAppsApiV0ProjectIdPartitionKeyAppsGet(projectId as string, '__none__'),
    { enabled: projectId !== undefined }
  );
  const createAndUpdateMutation = useMutation(
    (app_id: string) =>
      DefaultService.createNewApplicationApiV0EmailAssistantCreateNewProjectIdAppIdPost(
        projectId,
        app_id
      ),
    {
      onSuccess: (appID) => {
        refetch().then((data) => {
          const appSummaries = data.data?.applications || [];
          const app = appSummaries.find((app) => app.app_id === appID);
          if (app) {
            setApp(app);
          }
        });
      }
    }
  );
  const appSetter = (appID: string) => createAndUpdateMutation.mutate(appID);
  const dataOrEmpty = Array.from(data?.applications || []);
  const options = dataOrEmpty
    .sort((a, b) => {
      return new Date(a.last_written) > new Date(b.last_written) ? -1 : 1;
    })
    .map((app) => {
      return {
        value: app.app_id,
        label: <EmailAssistantLabel application={app} />
      };
    });
  return (
    <AsyncCreatableSelect
      placeholder={props.placeholder}
      cacheOptions
      defaultOptions={options}
      onCreateOption={appSetter}
      value={options.find((option) => option.value === props.currentApp?.app_id) || null}
      onChange={(choice) => {
        const app = dataOrEmpty.find((app) => app.app_id === choice?.value);
        if (app) {
          setApp(app);
        }
      }}
    />
  );
};

export const EmailAssistantWithTelemetry = () => {
  const currentProject = 'demo_email-assistant';
  const [currentApp, setCurrentApp] = useState<ApplicationSummary | undefined>(undefined);

  return (
    <TwoColumnLayout
      firstItem={<EmailAssistant projectId={currentProject} appId={currentApp?.app_id} />}
      secondItem={
        <TelemetryWithSelector
          projectId={currentProject}
          currentApp={currentApp}
          setCurrentApp={setCurrentApp}
        />
      }
      mode={'third'}
    ></TwoColumnLayout>
  );
};



---
File: /burr/telemetry/ui/src/examples/MiniTelemetry.tsx
---

import { AppView } from '../components/routes/app/AppView';

export const MiniTelemetry = (props: {
  projectId: string;
  partitionKey: string | null;
  appId: string | undefined;
}) => {
  //TODO -- put this upstream
  const { projectId, appId, partitionKey } = props;
  if (appId === undefined) {
    return <div></div>;
  }
  return (
    <AppView
      projectId={projectId}
      appId={appId}
      orientation="stacked_vertical"
      defaultAutoRefresh={true}
      enableFullScreenStepView={false}
      enableMinimizedStepView={false}
      allowAnnotations={false}
      partitionKey={partitionKey === 'null' ? null : partitionKey}
    />
  );
};



---
File: /burr/telemetry/ui/src/examples/StreamingChatbot.tsx
---

import { ComputerDesktopIcon, UserIcon } from '@heroicons/react/24/outline';
import { classNames } from '../utils/tailwind';
import { Button } from '../components/common/button';
import { TwoColumnLayout } from '../components/common/layout';
import { ApplicationSummary, ChatItem, DefaultService } from '../api';
import { KeyboardEvent, useEffect, useState } from 'react';
import { useQuery } from 'react-query';
import { Loading } from '../components/common/loading';
import Markdown from 'react-markdown';
import remarkGfm from 'remark-gfm';
import { TelemetryWithSelector } from './Common';

type Role = 'assistant' | 'user';

/**
 * Simple chat history with instructions to start -- constant across everything.
 */
const DEFAULT_CHAT_HISTORY: ChatItem[] = [
  {
    role: ChatItem.role.ASSISTANT,
    content:
      'ðŸ“– Select a conversation from the list to get started! ' +
      'The left side of this is a simple chatbot. The right side is the same' +
      ' Burr Telemetry app you can see if you click through the [chatbot demo](/projects/demo_chatbot) project. Note that images ' +
      "will likely stop displaying after a while due to OpenAI's persistence policy. So generate some new ones! ðŸ“–",
    type: ChatItem.type.TEXT
  },
  {
    role: ChatItem.role.ASSISTANT,
    content:
      ' \n\nðŸ’¡ This is meant to demonstrate the power of the Burr streaming capabilities! ' +
      'chat on the left while examining the internals of the chatbot on the right.ðŸ’¡',
    type: ChatItem.type.TEXT
  }
];

/**
 * Helper function to get the character based on the role -- this just selects how it renders
 * @param role Role of the character
 * @returns AI or You based on the role
 */
const getCharacter = (role: Role) => {
  return role === 'assistant' ? 'AI' : 'You';
};

/**
 * Component for the Icon based on the role
 */
const RoleIcon = (props: { role: Role }) => {
  const Icon = props.role === 'assistant' ? ComputerDesktopIcon : UserIcon;
  return (
    <Icon className={classNames('text-gray-400', 'ml-auto h-6 w-6 shrink-0')} aria-hidden="true" />
  );
};

// Constant so we can scroll to the latest
const VIEW_END_ID = 'end-view';

/**
 * React component to render a chat message. Can handle markdown.
 */
const ChatMessage = (props: { message: ChatItem; id?: string }) => {
  return (
    <div className="flex gap-3 my-4  text-gray-600 text-sm flex-1 w-full" id={props.id}>
      <span className="relative flex shrink-0">
        <RoleIcon role={props.message.role} />
      </span>
      <div className="leading-relaxed w-full">
        <span className="block font-bold text-gray-700">{getCharacter(props.message.role)} </span>
        {props.message.type === ChatItem.type.TEXT ||
        props.message.type === ChatItem.type.CODE ||
        props.message.type === ChatItem.type.ERROR ? (
          <Markdown
            components={{
              // Custom rendering for links
              a: ({ ...props }) => <a className="text-dwlightblue hover:underline" {...props} />
            }}
            remarkPlugins={[remarkGfm]}
            className={`whitespace-pre-wrap break-lines max-w-full ${props.message.type === ChatItem.type.ERROR ? 'bg-dwred/10' : ''} p-0.5`}
          >
            {props.message.content}
          </Markdown>
        ) : (
          <></>
        )}
      </div>
    </div>
  );
};

/**
 * Convenience function to scroll to the latest message in the chat.
 * This just sets the div to scroll to the bottom and sets up an observer to keep it there.
 */
const scrollToLatest = () => {
  const lastMessage = document.getElementById(VIEW_END_ID);
  if (lastMessage) {
    // Directly scroll to the bottom
    lastMessage.scrollTop = lastMessage.scrollHeight;

    // Setup an observer to auto-scroll on size changes
    const observer = new ResizeObserver(() => {
      lastMessage.scrollTop = lastMessage.scrollHeight;
    });
    observer.observe(lastMessage);

    // Cleanup observer on component unmount or updates
    return () => observer.disconnect();
  }
};

// Types for the chatbot stream -- we can't use react-query/openapi as they don't support streaming
// See
type Event = {
  type: 'delta' | 'chat_history';
};

type ChatMessageEvent = Event & {
  value: string;
};

type ChatHistoryEvent = Event & {
  value: ChatItem[];
};

/**
 * Streaming chatbot component -- renders chat + fetches from API
 */
export const StreamingChatbot = (props: { projectId: string; appId: string | undefined }) => {
  const [currentPrompt, setCurrentPrompt] = useState<string>('');
  const [displayedChatHistory, setDisplayedChatHistory] = useState(DEFAULT_CHAT_HISTORY);
  const [currentResponse, setCurrentResponse] = useState<string>('');
  const [isChatWaiting, setIsChatWaiting] = useState<boolean>(false);

  const { isLoading: isInitialLoading } = useQuery(
    // TODO -- handle errors
    ['chat', props.projectId, props.appId],
    () =>
      DefaultService.chatHistoryApiV0StreamingChatbotHistoryProjectIdAppIdGet(
        props.projectId,
        props.appId || '' // TODO -- find a cleaner way of doing a skip-token like thing here
      ),
    {
      enabled: props.appId !== undefined,
      onSuccess: (data) => {
        setDisplayedChatHistory(data); // when its succesful we want to set the displayed chat history
      },
      onError: (error: Error) => {
        setDisplayedChatHistory([
          ...DEFAULT_CHAT_HISTORY,
          {
            content: `Unable to load from server: ${error.message}`,
            role: ChatItem.role.ASSISTANT,
            type: ChatItem.type.ERROR
          }
        ]);
      }
    }
  );

  const submitPrompt = async () => {
    setCurrentResponse(''); // Reset it
    setIsChatWaiting(true);
    const response = await fetch(
      `/api/v0/streaming_chatbot/response/${props.projectId}/${props.appId}`,
      {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ prompt: currentPrompt })
      }
    );
    const reader = response.body?.getReader();
    let chatResponse = '';
    if (reader) {
      const decoder = new TextDecoder('utf-8');
      // eslint-disable-next-line no-constant-condition
      while (true) {
        const result = await reader.read();
        if (result.done) {
          break;
        }
        const message = decoder.decode(result.value, { stream: true });
        message
          .split('data: ')
          .slice(1)
          .forEach((item) => {
            const event: Event = JSON.parse(item);
            if (event.type === 'chat_history') {
              const chatMessageEvent = event as ChatHistoryEvent;
              setDisplayedChatHistory(chatMessageEvent.value);
            }
            if (event.type === 'delta') {
              const chatMessageEvent = event as ChatMessageEvent;
              chatResponse += chatMessageEvent.value;
              setCurrentResponse(chatResponse);
            }
          });
      }
      setDisplayedChatHistory((chatHistory) => [
        ...chatHistory,
        {
          role: ChatItem.role.USER,
          content: currentPrompt,
          type: ChatItem.type.TEXT
        },
        {
          role: ChatItem.role.ASSISTANT,
          content: chatResponse,
          type: ChatItem.type.TEXT
        }
      ]);
      setCurrentPrompt('');
      setCurrentResponse('');
      setIsChatWaiting(false);
    }
  };

  // Scroll to the latest message when the chat history changes
  useEffect(() => {
    scrollToLatest();
  }, [displayedChatHistory, currentResponse, isChatWaiting]);

  if (isInitialLoading) {
    return <Loading />;
  }
  return (
    <div className="mr-4 bg-white  w-full flex flex-col h-full">
      <h1 className="text-2xl font-bold px-4 text-gray-600">{'Learn Burr '}</h1>
      <h2 className="text-lg font-normal px-4 text-gray-500 flex flex-row">
        The chatbot below is implemented using Burr. Watch the Burr UI (on the right) change as you
        chat...
      </h2>
      <div className="flex-1 overflow-y-auto p-4 hide-scrollbar" id={VIEW_END_ID}>
        {displayedChatHistory.map((message, i) => (
          <ChatMessage message={message} key={i} />
        ))}
        {isChatWaiting && (
          <ChatMessage
            message={{
              role: ChatItem.role.USER,
              content: currentPrompt,
              type: ChatItem.type.TEXT
            }}
          />
        )}
        {isChatWaiting && (
          <ChatMessage
            message={{
              content: currentResponse,
              type: ChatItem.type.TEXT,
              role: ChatItem.role.ASSISTANT
            }}
          />
        )}
      </div>
      <div className="flex items-center pt-4 gap-2">
        <input
          className="flex h-10 w-full rounded-md border border-[#e5e7eb] px-3 py-2 text-sm placeholder-[#6b7280] focus:outline-none focus:ring-2 focus:ring-[#9ca3af] disabled:cursor-not-allowed disabled:opacity-50 text-[#030712] focus-visible:ring-offset-2"
          placeholder="Ask me how tall the Eifel tower is!"
          value={currentPrompt}
          onChange={(e) => setCurrentPrompt(e.target.value)}
          onKeyDown={(e: KeyboardEvent<HTMLInputElement>) => {
            if (e.key === 'Enter' && !e.shiftKey) {
              e.preventDefault();
              submitPrompt();
            }
          }}
          disabled={isChatWaiting || props.appId === undefined}
        />
        <Button
          className="w-min cursor-pointer h-full"
          color="dark"
          disabled={isChatWaiting || props.appId === undefined}
          onClick={() => {
            submitPrompt();
          }}
        >
          Send
        </Button>
      </div>
    </div>
  );
};

export const StreamingChatbotWithTelemetry = () => {
  const currentProject = 'demo_streaming_chatbot';
  const [currentApp, setCurrentApp] = useState<ApplicationSummary | undefined>(undefined);

  return (
    <TwoColumnLayout
      firstItem={<StreamingChatbot projectId={currentProject} appId={currentApp?.app_id} />}
      secondItem={
        <TelemetryWithSelector
          projectId={currentProject}
          currentApp={currentApp}
          setCurrentApp={setCurrentApp}
          createNewApp={
            DefaultService.createNewApplicationApiV0StreamingChatbotCreateProjectIdAppIdPost
          }
        />
      }
      mode={'third'}
    ></TwoColumnLayout>
  );
};



---
File: /burr/telemetry/ui/src/utils/tailwind.ts
---

export const classNames = (...classes: string[]): string => {
  return classes.filter(Boolean).join(' ');
};



---
File: /burr/telemetry/ui/src/App.css
---

.App {
  text-align: center;
}

.App-logo {
  height: 40vmin;
  pointer-events: none;
}

@media (prefers-reduced-motion: no-preference) {
  .App-logo {
    animation: App-logo-spin infinite 20s linear;
  }
}

.App-header {
  background-color: #282c34;
  min-height: 100vh;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  font-size: calc(10px + 2vmin);
  color: white;
}

.App-link {
  color: #61dafb;
}

@keyframes App-logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}



---
File: /burr/telemetry/ui/src/App.test.tsx
---

import React from 'react';
import { render, screen } from '@testing-library/react';
import App from './App';

test('renders learn react link', () => {
  render(<App />);
  const linkElement = screen.getByText(/learn react/i);
  expect(linkElement).toBeInTheDocument();
});



---
File: /burr/telemetry/ui/src/App.tsx
---

import { BrowserRouter as Router, Routes, Route, Navigate } from 'react-router-dom';
import './App.css';
import { ProjectList } from './components/routes/ProjectList';
import { AppList } from './components/routes/AppList';
import { AppViewContainer } from './components/routes/app/AppView';
import { QueryClient, QueryClientProvider } from 'react-query';
import { AppContainer } from './components/nav/appcontainer';
import { ChatbotWithTelemetry } from './examples/Chatbot';
import { Counter } from './examples/Counter';
import { EmailAssistantWithTelemetry } from './examples/EmailAssistant';
import { StreamingChatbotWithTelemetry } from './examples/StreamingChatbot';
import { AdminView } from './components/routes/AdminView';
import { AnnotationsViewContainer } from './components/routes/app/AnnotationsView';
import { DeepResearcherWithTelemetry } from './examples/DeepResearcher';

/**
 * Basic application. We have an AppContainer -- this has a breadcrumb and a sidebar.
 * We refer to route paths to gather parameters, as its simple to wire through. We may
 * want to consider passing parameters in to avoid overly coupled dependencies/ensure
 * more reusable top-level components.
 *
 * Note that you can run this in one of two modes:
 * 1. As an asset served by the backend
 * 2. Standalone, using npm run
 *
 * npm run will proxy to port 7241, versus the asset which will
 * hit the backend (on the same port/server as the FE, its just a static route).
 *
 * @returns A rendered application object
 */
const App = () => {
  return (
    <QueryClientProvider client={new QueryClient()}>
      <Router>
        <AppContainer>
          <Routes>
            <Route path="/" element={<Navigate to="/projects" />} />
            <Route path="/projects" element={<ProjectList />} />
            <Route path="/project/:projectId" element={<AppList />} />
            <Route path="/project/:projectId/:partitionKey" element={<AppList />} />
            <Route path="/project/:projectId/:partitionKey/:appId" element={<AppViewContainer />} />
            <Route path="/annotations/:projectId/" element={<AnnotationsViewContainer />} />
            <Route path="/demos/counter" element={<Counter />} />
            <Route path="/demos/chatbot" element={<ChatbotWithTelemetry />} />
            <Route path="/demos/streaming-chatbot" element={<StreamingChatbotWithTelemetry />} />
            <Route path="/demos/email-assistant" element={<EmailAssistantWithTelemetry />} />
            <Route path="/demos/deep-researcher" element={<DeepResearcherWithTelemetry />} />
            <Route path="/admin" element={<AdminView />} />
            <Route path="*" element={<Navigate to="/projects" />} />
          </Routes>
        </AppContainer>
      </Router>
    </QueryClientProvider>
  );
};

export default App;



---
File: /burr/telemetry/ui/src/index.css
---

/* Tailwind imports */
@tailwind base;
@tailwind components;
@tailwind utilities;

/* Non-tailwind CSS */
.hide-scrollbar::-webkit-scrollbar {
  display: none;
}

/* Hide scrollbar for IE, Edge, and Firefox */
.hide-scrollbar {
  -ms-overflow-style: none; /* IE and Edge */
  scrollbar-width: none; /* Firefox */
}



---
File: /burr/telemetry/ui/src/index.tsx
---

import React from 'react';
import ReactDOM from 'react-dom/client';
import './index.css';
import App from './App';
import reportWebVitals from './reportWebVitals';

const root = ReactDOM.createRoot(document.getElementById('root') as HTMLElement);
root.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);

// If you want to start measuring performance in your app, pass a function
// to log results (for example: reportWebVitals(console.log))
// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals
reportWebVitals();



---
File: /burr/telemetry/ui/src/react-app-env.d.ts
---

/// <reference types="react-scripts" />



---
File: /burr/telemetry/ui/src/reportWebVitals.ts
---

import { ReportHandler } from 'web-vitals';

const reportWebVitals = (onPerfEntry?: ReportHandler) => {
  if (onPerfEntry && onPerfEntry instanceof Function) {
    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {
      getCLS(onPerfEntry);
      getFID(onPerfEntry);
      getFCP(onPerfEntry);
      getLCP(onPerfEntry);
      getTTFB(onPerfEntry);
    });
  }
};

export default reportWebVitals;



---
File: /burr/telemetry/ui/src/setupTests.ts
---

// jest-dom adds custom jest matchers for asserting on DOM nodes.
// allows you to do things like:
// expect(element).toHaveTextContent(/react/i)
// learn more: https://github.com/testing-library/jest-dom
import '@testing-library/jest-dom';



---
File: /burr/telemetry/ui/src/utils.tsx
---

import { useParams } from 'react-router-dom';
import { AttributeModel, Step } from './api';

export type Status = 'success' | 'failure' | 'running';
/*
 * Gets the action given a step.
 * TODO -- put this in the BE
 */
export const getActionStatus = (action: Step) => {
  if (action.step_end_log === null) {
    return 'running';
  }
  if (action?.step_end_log?.exception) {
    return 'failure';
  }
  return 'success';
};

export const getUniqueAttributeID = (attribute: AttributeModel) => {
  return `${attribute.action_sequence_id}-${attribute.span_id}`;
};

export const useLocationParams = () => {
  const { projectId, appId, partitionKey } = useParams();
  return {
    projectId: projectId as string,
    appId: appId as string,
    partitionKey: (partitionKey as string) === 'null' ? null : (partitionKey as string)
  };
};



---
File: /burr/telemetry/ui/.eslintrc.js
---

module.exports = {
  env: {
    browser: true,
    es2021: true
  },
  extends: [
    'eslint:recommended',
    'plugin:react/recommended',
    'plugin:@typescript-eslint/recommended',
    'plugin:prettier/recommended'
  ],
  overrides: [],
  parser: '@typescript-eslint/parser',
  parserOptions: {
    ecmaVersion: 'latest',
    sourceType: 'module'
  },
  plugins: ['react', '@typescript-eslint', 'react-hooks'],
  rules: {
    'react-hooks/rules-of-hooks': 'error', // Checks rules of Hooks
    'react-hooks/exhaustive-deps': 'off', // Checks effect dependencies
    'react/react-in-jsx-scope': 'off',
    '@typescript-eslint/ban-ts-comment': 'off',
    'react/prop-types': 'off', //Appears to be busted: see https://stackoverflow.com/questions/38684925/react-eslint-error-missing-in-props-validation
    eqeqeq: 'error',
    'no-console': 'warn'
  },
  settings: {
    react: {
      version: 'detect'
    }
  }
};



---
File: /burr/telemetry/ui/tailwind.config.js
---

/** @type {import('tailwindcss').Config} */
module.exports = {
  content: ["./src/**/*.{js,jsx,ts,tsx}"],
  theme: {
    extend: {
      colors: {
        dwdarkblue: "rgb(43,49,82)",
        dwred: "rgb(234,85,86)",
        dwlightblue: "rgb(66,157,188)",
        dwwhite: "white",
        dwblack: "black",
    },},
  },
  // Only use this for debugging!
  plugins: [
    require("tailwindcss-question-mark"),
  ],
  darkMode: 'false',
};



---
File: /burr/tests/common/test_async_utils.py
---

from typing import AsyncGenerator, Generator

from burr.common.async_utils import arealize, asyncify_generator


async def test_asyncify_generator_with_sync_generator():
    def sync_gen() -> Generator[int, None, None]:
        for i in range(5):
            yield i

    generator = sync_gen()
    async_generator = asyncify_generator(generator)

    result = [item async for item in async_generator]
    assert result == [0, 1, 2, 3, 4], "Should convert sync generator to async generator"


async def test_asyncify_generator_with_async_generator():
    async def async_gen() -> AsyncGenerator[int, None]:
        for i in range(5):
            yield i

    generator = async_gen()
    async_generator = asyncify_generator(generator)

    result = [item async for item in async_generator]
    assert result == [0, 1, 2, 3, 4], "Should pass through async generator unchanged"


# Test cases for arealize
async def test_arealize_with_sync_generator():
    def sync_gen() -> Generator[int, None, None]:
        for i in range(5):
            yield i

    generator = sync_gen()
    result = await arealize(generator)
    assert result == [0, 1, 2, 3, 4], "Should convert sync generator to list"


async def test_arealize_with_async_generator():
    async def async_gen() -> AsyncGenerator[int, None]:
        for i in range(5):
            yield i

    generator = async_gen()
    result = await arealize(generator)
    assert result == [0, 1, 2, 3, 4], "Should realize async generator to list"


# Edge cases
async def test_asyncify_generator_empty_sync_generator():
    def sync_gen() -> Generator[int, None, None]:
        if False:  # No items to yield
            yield

    generator = sync_gen()
    async_generator = asyncify_generator(generator)

    result = [item async for item in async_generator]
    assert result == [], "Should handle empty sync generator"


async def test_arealize_empty_sync_generator():
    def sync_gen() -> Generator[int, None, None]:
        if False:  # No items to yield
            yield

    generator = sync_gen()
    result = await arealize(generator)
    assert result == [], "Should handle empty sync generator to list"


async def test_asyncify_generator_empty_async_generator():
    async def async_gen() -> AsyncGenerator[int, None]:
        if False:  # No items to yield
            yield

    generator = async_gen()
    async_generator = asyncify_generator(generator)

    result = [item async for item in async_generator]
    assert result == [], "Should handle empty async generator"


async def test_arealize_empty_async_generator():
    async def async_gen() -> AsyncGenerator[int, None]:
        if False:  # No items to yield
            yield

    generator = async_gen()
    result = await arealize(generator)
    assert result == [], "Should handle empty async generator to list"



---
File: /burr/tests/core/test_action.py
---

import asyncio
from typing import AsyncGenerator, Generator, Optional, Tuple, cast

import pytest

from burr.core import State
from burr.core.action import (
    Action,
    AsyncStreamingAction,
    AsyncStreamingResultContainer,
    Condition,
    Function,
    Input,
    Result,
    SingleStepAction,
    SingleStepStreamingAction,
    StreamingAction,
    StreamingResultContainer,
    action,
    create_action,
    default,
    derive_inputs_from_fn,
    streaming_action,
)


def test_is_async_true():
    class AsyncFunction(Function):
        @property
        def inputs(self) -> list[str]:
            return []

        @property
        def reads(self) -> list[str]:
            return []

        async def run(self, state: State, **run_kwargs) -> dict:
            return {}

    func = AsyncFunction()
    assert func.is_async()


def test_is_async_false():
    class SyncFunction(Function):
        def reads(self) -> list[str]:
            return []

        def run(self, state: State, **run_kwargs) -> dict:
            return {}

    func = SyncFunction()
    assert not func.is_async()


class BasicAction(Action):
    @property
    def reads(self) -> list[str]:
        return ["input_variable"]

    def run(self, state: State) -> dict:
        return {"output_variable": state["input_variable"]}

    @property
    def writes(self) -> list[str]:
        return ["output_variable"]

    def update(self, result: dict, state: State) -> State:
        return state.update(**result)

    @property
    def tags(self) -> list[str]:
        return ["tag1", "tag2"]


def test_with_name():
    action = BasicAction()
    assert action.name is None  # Nothing set initially
    with_name = action.with_name("my_action")
    assert with_name.name == "my_action"  # Name set on copy
    assert with_name.reads == action.reads
    assert with_name.writes == action.writes
    assert with_name.tags == action.tags


def test_condition():
    cond = Condition(["foo"], lambda state: state["foo"] == "bar", name="foo")
    assert cond.name == "foo"
    assert cond.reads == ["foo"]
    assert cond.run(State({"foo": "bar"})) == {Condition.KEY: True}
    assert cond.run(State({"foo": "baz"})) == {Condition.KEY: False}


def test_condition_when():
    cond = Condition.when(foo="bar")
    assert cond.name == "foo=bar"
    assert cond.reads == ["foo"]
    assert cond.run(State({"foo": "bar"})) == {Condition.KEY: True}
    assert cond.run(State({"foo": "baz"})) == {Condition.KEY: False}


def test_condition_when_complex():
    cond = Condition.when(foo="bar", baz="qux")
    assert cond.name == "baz=qux, foo=bar"
    assert sorted(cond.reads) == ["baz", "foo"]
    assert cond.run(State({"foo": "bar", "baz": "qux"})) == {Condition.KEY: True}
    assert cond.run(State({"foo": "baz", "baz": "qux"})) == {Condition.KEY: False}
    assert cond.run(State({"foo": "bar", "baz": "corge"})) == {Condition.KEY: False}
    assert cond.run(State({"foo": "baz", "baz": "corge"})) == {Condition.KEY: False}


def test_condition_default():
    cond = default
    assert cond.name == "default"
    assert cond.reads == []
    assert cond.run(State({"foo": "bar"})) == {Condition.KEY: True}


def test_condition_expr():
    cond = Condition.expr("foo == 'bar'")
    assert cond.name == "foo == 'bar'"
    assert cond.reads == ["foo"]
    assert cond.run(State({"foo": "bar"})) == {Condition.KEY: True}
    assert cond.run(State({"foo": "baz"})) == {Condition.KEY: False}


def test_condition_expr_complex():
    cond = Condition.expr("foo == 'bar' and len(baz) == 3")
    assert cond.name == "foo == 'bar' and len(baz) == 3"
    assert sorted(cond.reads) == ["baz", "foo"]
    assert cond.run(State({"foo": "bar", "baz": "qux"})) == {Condition.KEY: True}
    assert cond.run(State({"foo": "baz", "baz": "qux"})) == {Condition.KEY: False}
    assert cond.run(State({"foo": "bar", "baz": "corge"})) == {Condition.KEY: False}
    assert cond.run(State({"foo": "baz", "baz": "corge"})) == {Condition.KEY: False}


def test_condition__validate_success():
    cond = Condition.when(foo="bar")
    cond._validate(State({"foo": "bar"}))


def test_condition__validate_failure():
    cond = Condition.when(foo="bar")
    with pytest.raises(ValueError, match="foo"):
        cond._validate(State({"baz": "baz"}))


def test_condition_invert():
    cond = Condition(
        ["foo"],
        lambda state: state["foo"] == "bar",
        name="foo == 'bar'",
    )
    cond_inverted = ~cond
    assert cond_inverted.name == "~foo == 'bar'"
    assert cond_inverted.reads == ["foo"]
    assert cond_inverted.run(State({"foo": "bar"})) == {Condition.KEY: False}
    assert cond_inverted.run(State({"foo": "baz"})) == {Condition.KEY: True}


def test_condition_and():
    cond1 = Condition.when(foo="bar")
    cond2 = Condition.when(baz="qux")
    cond_and = cond1 & cond2
    assert cond_and.name == "foo=bar & baz=qux"
    assert sorted(cond_and.reads) == ["baz", "foo"]
    assert cond_and.run(State({"foo": "bar", "baz": "qux"})) == {Condition.KEY: True}
    assert cond_and.run(State({"foo": "baz", "baz": "qux"})) == {Condition.KEY: False}
    assert cond_and.run(State({"foo": "bar", "baz": "corge"})) == {Condition.KEY: False}
    assert cond_and.run(State({"foo": "baz", "baz": "corge"})) == {Condition.KEY: False}


def test_condition_or():
    cond1 = Condition.when(foo="bar")
    cond2 = Condition.when(baz="qux")
    cond_or = cond1 | cond2
    assert cond_or.name == "foo=bar | baz=qux"
    assert sorted(cond_or.reads) == ["baz", "foo"]
    assert cond_or.run(State({"foo": "bar", "baz": "qux"})) == {Condition.KEY: True}
    assert cond_or.run(State({"foo": "baz", "baz": "qux"})) == {Condition.KEY: True}
    assert cond_or.run(State({"foo": "bar", "baz": "corge"})) == {Condition.KEY: True}
    assert cond_or.run(State({"foo": "baz", "baz": "corge"})) == {Condition.KEY: False}


def test_condition_lmda():
    cond = Condition.lmda(lambda state: state["foo"] == "bar", ["foo"])
    assert cond.reads == ["foo"]
    assert cond.run(State({"foo": "bar"})) == {Condition.KEY: True}
    assert cond.run(State({"foo": "baz"})) == {Condition.KEY: False}


# TODO -- add this in once we decide what to do with optional keys...
# def test_condition_exists_single():
#     cond = Condition.exists("foo")
#     assert cond.name == "exists_foo"
#     assert cond.reads == ["foo"]
#     assert cond.run(State({"foo": "baz", "bar": "baz"})) == {Condition.KEY: True}
#     assert cond.run(State({})) == {Condition.KEY: False}
#
#
# def test_condition_exists_double():
#     cond = Condition.exists("foo", "bar")
#     assert cond.name == "exists_bar_and_foo"
#     assert cond.reads == ["foo"]
#     assert cond.run(State({"foo": "baz", "bar": "baz"})) == {Condition.KEY: True}
#     assert cond.run(State({"foo" : "bar"})) == {Condition.KEY: False}


def test_result():
    result = Result("foo", "bar")
    assert result.run(State({"foo": "baz", "bar": "qux", "baz": "quux"})) == {
        "foo": "baz",
        "bar": "qux",
    }
    assert result.writes == []  # doesn't write anything
    assert result.reads == ["foo", "bar"]
    # no results
    assert result.update(
        {"foo": "baz", "bar": "qux"}, State({"foo": "baz", "bar": "qux", "baz": "quux"})
    ) == State(
        {"foo": "baz", "bar": "qux", "baz": "quux"}
    )  # no impact


def test_input():
    input_action = Input("foo", "bar")
    assert input_action.reads == []
    assert input_action.writes == ["foo", "bar"]
    assert input_action.inputs == ["foo", "bar"]
    assert (result := input_action.run(State({}), foo="baz", bar="qux")) == {
        "foo": "baz",
        "bar": "qux",
    }
    assert input_action.update(result, State({})).get_all() == {"foo": "baz", "bar": "qux"}


def test_function_based_action():
    @action(reads=["input_variable"], writes=["output_variable"], tags=["tag1", "tag2"])
    def my_action(state: State) -> Tuple[dict, State]:
        return {"output_variable": state["input_variable"]}, state.update(
            output_variable=state["input_variable"]
        )

    fn_based_action = create_action(my_action, name="my_action")
    assert fn_based_action.single_step
    assert fn_based_action.name == "my_action"
    assert fn_based_action.reads == ["input_variable"]
    assert fn_based_action.writes == ["output_variable"]
    assert fn_based_action.tags == ["tag1", "tag2"]
    result, state = fn_based_action.run_and_update(State({"input_variable": "foo"}))
    assert result == {"output_variable": "foo"}
    assert state.get_all() == {"input_variable": "foo", "output_variable": "foo"}


def test_function_based_action_with_inputs():
    @action(reads=["input_variable"], writes=["output_variable"], tags=["tag1", "tag2"])
    def my_action(state: State, bound_input: int, unbound_input: int) -> Tuple[dict, State]:
        res = state["input_variable"] + bound_input + unbound_input
        return {"output_variable": res}, state.update(output_variable=res)

    fn_based_action = cast(
        SingleStepAction, create_action(my_action.bind(bound_input=10), name="my_action")
    )
    assert fn_based_action.inputs == (["unbound_input"], [])
    assert fn_based_action.tags == ["tag1", "tag2"]
    result, state = fn_based_action.run_and_update(State({"input_variable": 1}), unbound_input=100)
    assert state.get_all() == {"input_variable": 1, "output_variable": 111}
    assert result == {"output_variable": 111}


def test_function_based_action_multiple_binds():
    # This ensures that bind doesn't impact the original function
    @action(reads=["input_variable"], writes=["output_variable"])
    def my_action(state: State, bound_input: int, unbound_input: int) -> Tuple[dict, State]:
        res = state["input_variable"] + bound_input + unbound_input
        return {"output_variable": res}, state.update(output_variable=res)

    # The binding has to happen before the actions are created
    # Otherwise it'll get the right value
    # This simulates how the ApplicationBuilder creates them
    bound_1 = my_action.bind(bound_input=10)
    bound_2 = my_action.bind(bound_input=20)

    fn_based_action = cast(SingleStepAction, create_action(bound_1, name="my_action"))
    fn_based_action_2 = cast(SingleStepAction, create_action(bound_2, name="my_action"))
    assert fn_based_action.inputs == (["unbound_input"], [])
    result, state = fn_based_action.run_and_update(State({"input_variable": 1}), unbound_input=100)
    assert state.get_all() == {"input_variable": 1, "output_variable": 111}
    assert result == {"output_variable": 111}

    assert fn_based_action_2.inputs == (["unbound_input"], [])
    result, state = fn_based_action_2.run_and_update(
        State({"input_variable": 1}), unbound_input=100
    )
    assert state.get_all() == {"input_variable": 1, "output_variable": 121}
    assert result == {"output_variable": 121}


def test_function_based_action_with_defaults():
    @action(reads=["input_variable"], writes=["output_variable"])
    def my_action(
        state: State, bound_input: int, unbound_input: int, unbound_default_input: int = 1000
    ) -> Tuple[dict, State]:
        res = state["input_variable"] + bound_input + unbound_input + unbound_default_input
        return {"output_variable": res}, state.update(output_variable=res)

    fn_based_action: SingleStepAction = create_action(
        my_action.bind(bound_input=10), name="my_action"
    )
    assert fn_based_action.inputs == (["unbound_input"], ["unbound_default_input"])
    result, state = fn_based_action.run_and_update(State({"input_variable": 1}), unbound_input=100)
    assert state.get_all() == {"input_variable": 1, "output_variable": 1111}
    assert result == {"output_variable": 1111}


def test_function_based_action_with_defaults_unbound():
    # inputs can have defaults -- this tests that we treat them propertly
    @action(reads=["input_variable"], writes=["output_variable"])
    def my_action(
        state: State, unbound_input_1: int, unbound_input_2: int, unbound_default_input: int = 1000
    ) -> Tuple[dict, State]:
        res = state["input_variable"] + unbound_input_1 + unbound_input_2 + unbound_default_input
        return {"output_variable": res}, state.update(output_variable=res)

    fn_based_action: SingleStepAction = create_action(my_action, name="my_action")
    assert fn_based_action.inputs == (
        ["unbound_input_1", "unbound_input_2"],
        ["unbound_default_input"],
    )
    result, state = fn_based_action.run_and_update(
        State({"input_variable": 1}), unbound_input_1=10, unbound_input_2=100
    )
    assert state.get_all() == {"input_variable": 1, "output_variable": 1111}
    assert result == {"output_variable": 1111}


async def test_function_based_action_async():
    @action(reads=["input_variable"], writes=["output_variable"], tags=["tag1", "tag2"])
    async def my_action(state: State) -> Tuple[dict, State]:
        await asyncio.sleep(0.01)
        return {"output_variable": state["input_variable"]}, state.update(
            output_variable=state["input_variable"]
        )

    fn_based_action = create_action(my_action, name="my_action")
    assert fn_based_action.is_async()
    assert fn_based_action.single_step is True
    assert fn_based_action.name == "my_action"
    assert fn_based_action.reads == ["input_variable"]
    assert fn_based_action.writes == ["output_variable"]
    assert fn_based_action.tags == ["tag1", "tag2"]
    result, state = await fn_based_action.run_and_update(State({"input_variable": "foo"}))
    assert result == {"output_variable": "foo"}
    assert state.get_all() == {"input_variable": "foo", "output_variable": "foo"}


async def test_function_based_action_with_inputs_async():
    @action(reads=["input_variable"], writes=["output_variable"])
    async def my_action(state: State, bound_input: int, unbound_input: int) -> Tuple[dict, State]:
        await asyncio.sleep(0.01)
        res = state["input_variable"] + bound_input + unbound_input
        return {"output_variable": res}, state.update(output_variable=res)

    fn_based_action: SingleStepAction = create_action(
        my_action.bind(bound_input=10), name="my_action"
    )
    assert fn_based_action.is_async()
    assert fn_based_action.inputs == (["unbound_input"], [])
    result, state = await fn_based_action.run_and_update(
        State({"input_variable": 1}), unbound_input=100
    )
    assert state.get_all() == {"input_variable": 1, "output_variable": 111}
    assert result == {"output_variable": 111}


async def test_function_based_action_with_defaults_async():
    @action(reads=["input_variable"], writes=["output_variable"])
    async def my_action(
        state: State, bound_input: int, unbound_input: int, unbound_default_input: int = 1000
    ) -> Tuple[dict, State]:
        await asyncio.sleep(0.01)
        res = state["input_variable"] + bound_input + unbound_input + unbound_default_input
        return {"output_variable": res}, state.update(output_variable=res)

    fn_based_action: SingleStepAction = create_action(
        my_action.bind(bound_input=10), name="my_action"
    )
    assert fn_based_action.inputs == (["unbound_input"], ["unbound_default_input"])
    result, state = await fn_based_action.run_and_update(
        State({"input_variable": 1}), unbound_input=100
    )
    assert state.get_all() == {"input_variable": 1, "output_variable": 1111}
    assert result == {"output_variable": 1111}


def test_create_action_class_api():
    raw_action = BasicAction()
    created_action = create_action(raw_action, name="my_action")
    assert created_action.name == "my_action"
    assert created_action.reads == raw_action.reads
    assert created_action.writes == raw_action.writes
    assert created_action.tags == raw_action.tags


def test_create_action_fn_api():
    @action(reads=["input_variable"], writes=["output_variable"], tags=["tag1", "tag2"])
    def test_action(state: State) -> Tuple[dict, State]:
        result = {"output_variable": state["input_variable"]}
        return result, state.update(output_variable=result["output_variable"])

    created_action = create_action(test_action, name="my_action")
    assert created_action.name == "my_action"
    assert created_action.reads == ["input_variable"]
    assert created_action.writes == ["output_variable"]
    assert created_action.single_step
    assert created_action.tags == ["tag1", "tag2"]
    result, state = created_action.run_and_update(State({"input_variable": "foo"}))
    assert result == {"output_variable": "foo"}
    assert state.get_all() == {"input_variable": "foo", "output_variable": "foo"}


def test_create_action_fn_api_with_bind():
    @action(reads=["input_variable_1"], writes=["output_variable_1", "output_variable_2"])
    def test_action(state: State, input_variable_2: int) -> Tuple[dict, State]:
        result = {
            "output_variable_1": state["input_variable_1"],
            "output_variable_2": input_variable_2,
        }
        return result, state.update(**result)

    bound = test_action.bind(input_variable_2=2)
    created_action = create_action(bound, name="my_action")

    assert created_action.name == "my_action"
    result, state = created_action.run_and_update(State({"input_variable_1": "foo"}))
    assert result == {
        "output_variable_1": "foo",
        "output_variable_2": 2,
    }
    assert state.get_all() == {
        "input_variable_1": "foo",
        "output_variable_1": "foo",
        "output_variable_2": 2,
    }


def test_create_action_streaming_fn_api_with_bind():
    @streaming_action(reads=["input_variable"], writes=["output_variable"], tags=["tag1", "tag2"])
    def test_action(
        state: State, prefix: str
    ) -> Generator[Tuple[dict, Optional[State]], None, None]:
        buffer = []
        for c in prefix + state["input_variable"]:
            buffer.append(c)
            yield {"output_variable": c}, None  # intermediate results
        result = {"output_variable": "".join(buffer)}
        yield result, state.update(output_variable=result["output_variable"])

    created_action = create_action(test_action.bind(prefix="prefix_"), name="my_action")
    assert created_action.streaming
    assert isinstance(created_action, SingleStepStreamingAction)
    assert created_action.name == "my_action"
    assert created_action.reads == ["input_variable"]
    assert created_action.writes == ["output_variable"]
    assert created_action.tags == ["tag1", "tag2"]
    assert created_action.single_step
    gen = created_action.stream_run_and_update(State({"input_variable": "foo"}))
    out = [item for item in gen]
    final_result, state = out[-1]
    intermediate_results = [item[0]["output_variable"] for item in out[:-1]]
    assert final_result == {"output_variable": "prefix_foo"}
    assert state.get_all() == {"input_variable": "foo", "output_variable": "prefix_foo"}
    assert intermediate_results == list("prefix_foo")


async def test_create_action_streaming_fn_api_with_bind_async():
    @streaming_action(reads=["input_variable"], writes=["output_variable"], tags=["tag1", "tag2"])
    async def test_action(
        state: State, prefix: str
    ) -> AsyncGenerator[Tuple[dict, Optional[State]], None]:
        buffer = []
        for c in prefix + state["input_variable"]:
            buffer.append(c)
            yield {"output_variable": c}, None  # intermediate results
            await asyncio.sleep(0.01)
        joined = "".join(buffer)
        yield {"output_variable": joined}, state.update(output_variable=joined)

    created_action = create_action(test_action.bind(prefix="prefix_"), name="my_action")
    assert created_action.streaming  # streaming
    assert created_action.is_async()  # async
    assert isinstance(created_action, SingleStepStreamingAction)
    assert created_action.name == "my_action"
    assert created_action.reads == ["input_variable"]
    assert created_action.writes == ["output_variable"]
    assert created_action.single_step
    assert created_action.tags == ["tag1", "tag2"]
    gen = created_action.stream_run_and_update(State({"input_variable": "foo"}))
    out = [item async for item in gen]
    final_result, state = out[-1]
    intermediate_results = [item[0]["output_variable"] for item in out[:-1]]
    assert final_result == {"output_variable": "prefix_foo"}
    assert state.get_all() == {"input_variable": "foo", "output_variable": "prefix_foo"}
    assert intermediate_results == list("prefix_foo")


def test_create_action_undecorated_function():
    def test_action(state: State) -> Tuple[dict, State]:
        result = {"output_variable": state["input_variable"]}
        return result, state.update(output_variable=result["output_variable"])

    with pytest.raises(ValueError, match="not a valid action"):
        create_action(test_action, name="my_action")


def test_streaming_action_stream_run():
    class SimpleStreamingAction(StreamingAction):
        def stream_run(
            self, state: State, **run_kwargs
        ) -> Generator[Tuple[dict, Optional[State]], None, None]:
            buffer = []
            for char in state["echo"]:
                yield {"response": char}, None
                buffer.append(char)
            yield {"response": "".join(buffer)}, state.update(response="".join(buffer))

        @property
        def reads(self) -> list[str]:
            return []

        @property
        def writes(self) -> list[str]:
            return ["response"]

        def update(self, result: dict, state: State) -> State:
            return state.update(**result)

    action = SimpleStreamingAction()
    STR = "test streaming action"
    result, state_update = action.run(State({"echo": STR}))
    assert result["response"] == STR
    assert state_update["response"] == STR


async def test_streaming_action_stream_run_async():
    class SimpleAsyncStreamingAction(AsyncStreamingAction):
        async def stream_run(self, state: State, **run_kwargs) -> AsyncGenerator[dict, None]:
            buffer = []
            for char in state["echo"]:
                yield {"response": char}, None
                await asyncio.sleep(0.01)
                buffer.append(char)
            yield {"response": "".join(buffer)}

        @property
        def reads(self) -> list[str]:
            return []

        @property
        def writes(self) -> list[str]:
            return ["response"]

        def update(self, result: dict, state: State) -> State:
            return state.update(**result)

    action = SimpleAsyncStreamingAction()
    STR = "test streaming action"
    result = await action.run(State({"echo": STR}))
    state_update = action.update(result, State({"echo": STR}))
    assert result["response"] == STR
    assert state_update["response"] == STR


def sample_generator(chars: str) -> Generator[Tuple[dict, Optional[State]], None, None]:
    buffer = []
    for c in chars:
        buffer.append(c)
        yield {"response": c}, None
    joined = "".join(buffer)
    yield {"response": joined}, State({"response": joined})


def test_streaming_result_container_iterate():
    string_value = "test streaming action"
    container = StreamingResultContainer(
        sample_generator(string_value),
        initial_state=State(),
        process_result=lambda r, s: (r, s),
        callback=lambda s, r, e: None,
    )
    assert [item["response"] for item in list(container)] == list(string_value)
    result, state = container.get()
    assert result["response"] == string_value


def test_streaming_result_get_runs_through():
    string_value = "test streaming action"
    container = StreamingResultContainer(
        sample_generator(string_value),
        initial_state=State(),
        process_result=lambda r, s: (r, s),
        callback=lambda s, r, e: None,
    )
    result, state = container.get()
    assert result["response"] == string_value


def test_streaming_result_callback_called():
    called = []
    string_value = "test streaming action"

    container = StreamingResultContainer(
        sample_generator(string_value),
        # initial state is here solely for returning debugging so we can return an
        # state to the user in the case of failure
        initial_state=State({"foo": "bar"}),
        process_result=lambda r, s: (r, s),
        callback=lambda s, r, e: called.append((s, r, e)),
    )
    container.get()
    assert len(called) == 1
    state, result, error = called[0]
    assert result["response"] == string_value
    assert state["response"] == string_value
    assert error is None


def test_streaming_result_callback_error():
    """This tests whether the callback is called when an error occurs in the generator. Note that try/except
    blocks -- this is required so we can end up delegating to the generators closing capability"""

    class SentinelError(Exception):
        pass

    try:
        called = []
        string_value = "test streaming action"
        container = StreamingResultContainer(
            sample_generator(string_value),
            initial_state=State({"foo": "bar"}),
            process_result=lambda r, s: (r, s),
            callback=lambda r, s, e: called.append((r, s, e)),
        )
        try:
            for _ in container:
                raise SentinelError("error")
        finally:
            assert len(called) == 1
            ((result, state, error),) = called
            assert state["foo"] == "bar"
            assert result is None
            # Exception is currently not exactly what we want, so won't assert on that.
            # See note in StreamingResultContainer
    except SentinelError:
        pass


async def sample_async_generator(chars: str) -> AsyncGenerator[Tuple[dict, Optional[State]], None]:
    buffer = []
    for c in chars:
        buffer.append(c)
        yield {"response": c}, None
        await asyncio.sleep(0.01)
    joined = "".join(buffer)
    yield {"response": joined}, State({"response": joined})


async def test_streaming_result_container_iterate_async():
    async def callback(r: dict, s: State, e: Exception):
        pass

    string_value = "test streaming action"
    container = AsyncStreamingResultContainer(
        sample_async_generator(string_value),
        initial_state=State(),
        process_result=lambda r, s: (r, s),
        callback=callback,
    )
    assert [item["response"] async for item in container] == list(string_value)
    result, state = await container.get()
    assert result["response"] == string_value


async def test_streaming_result_get_runs_through_async():
    async def callback(r: dict, s: State, e: Exception):
        pass

    string_value = "test streaming action"
    container = AsyncStreamingResultContainer(
        sample_async_generator(string_value),
        initial_state=State(),
        process_result=lambda r, s: (r, s),
        callback=callback,
    )
    result, state = await container.get()
    assert result["response"] == string_value


async def test_streaming_result_callback_called_async():
    called = []
    string_value = "test streaming action"

    async def callback(r: Optional[dict], s: State, e: Exception):
        called.append((s, r, e))

    container = AsyncStreamingResultContainer(
        sample_async_generator(string_value),
        # initial state is here solely for returning debugging so we can return an
        # state to the user in the case of failure
        initial_state=State({"foo": "bar"}),
        process_result=lambda r, s: (r, s),
        callback=callback,
    )
    await container.get()
    assert len(called) == 1
    result, state, error = called[0]
    assert result["response"] == string_value
    assert state["response"] == string_value
    assert error is None


def test_streaming_result_callback_error_async():
    """Oi. This can't use pytest-asyncio because pytest-asyncio doesn't shutdown async gens.
    I sure hope our customer's stuff shuts down async gens.

    See https://github.com/pytest-dev/pytest-asyncio/issues/759 for more details.

    """
    called = []

    class SentinelError(Exception):
        pass

    async def test_fn():
        try:

            async def callback(r: Optional[dict], s: State, e: Exception):
                called.append((r, s, e))

            string_value = "test streaming action"
            container = AsyncStreamingResultContainer(
                sample_async_generator(string_value),
                initial_state=State({"foo": "bar"}),
                process_result=lambda r, s: (r, s),
                callback=callback,
            )
            async for _ in container:
                raise SentinelError("error")
                # Exception is currently not exactly what we want, so won't assert on that.
                # See note in StreamingResultContainer
        except SentinelError:
            pass

    asyncio.run(test_fn())
    assert len(called) == 1
    ((result, state, error),) = called
    assert state["foo"] == "bar"
    assert result is None


def test_derive_inputs_from_fn_state_only():
    def fn(state):
        ...

    bound_params = {}
    required, optional = derive_inputs_from_fn(bound_params, fn)
    assert required == []
    assert optional == []


def test_derive_inputs_from_fn_state_and_required():
    def fn(state, a, b):
        ...

    bound_params = {"state": 1}
    required, optional = derive_inputs_from_fn(bound_params, fn)
    assert required == ["a", "b"]
    assert optional == []


def test_derive_inputs_from_fn_state_required_and_optional():
    def fn(state, a, b=2):
        ...

    bound_params = {"state": 1}
    required, optional = derive_inputs_from_fn(bound_params, fn)
    assert required == ["a"]
    assert optional == ["b"]


def test_derive_inputs_from_fnh_state_and_all_bound_except_state():
    def fn(state, a, b):
        ...

    bound_params = {"a": 1, "b": 2}
    required, optional = derive_inputs_from_fn(bound_params, fn)
    assert required == []
    assert optional == []


def test_non_existent_bound_parameters():
    def fn(state, a):
        ...

    bound_params = {"a": 1, "non_existent": 2}
    required, optional = derive_inputs_from_fn(bound_params, fn)
    assert required == []
    assert optional == []



---
File: /burr/tests/core/test_application.py
---

import asyncio
import collections
import datetime
import logging
import typing
import uuid
from typing import Any, Awaitable, Callable, Dict, Generator, Literal, Optional, Tuple, Union

import pytest

from burr.core import State
from burr.core.action import (
    DEFAULT_SCHEMA,
    Action,
    AsyncGenerator,
    AsyncStreamingAction,
    Condition,
    Reducer,
    Result,
    SingleStepAction,
    SingleStepStreamingAction,
    StreamingAction,
    action,
    default,
    expr,
)
from burr.core.application import (
    PRIOR_STEP,
    Application,
    ApplicationBuilder,
    ApplicationContext,
    _adjust_single_step_output,
    _arun_function,
    _arun_multi_step_streaming_action,
    _arun_single_step_action,
    _arun_single_step_streaming_action,
    _remap_dunder_parameters,
    _run_function,
    _run_multi_step_streaming_action,
    _run_reducer,
    _run_single_step_action,
    _run_single_step_streaming_action,
    _validate_start,
)
from burr.core.graph import Graph, GraphBuilder, Transition
from burr.core.persistence import (
    AsyncDevNullPersister,
    BaseStatePersister,
    DevNullPersister,
    PersistedStateData,
    SQLLitePersister,
)
from burr.core.typing import TypingSystem
from burr.lifecycle import (
    PostRunStepHook,
    PostRunStepHookAsync,
    PreRunStepHook,
    PreRunStepHookAsync,
    internal,
)
from burr.lifecycle.base import (
    ExecuteMethod,
    PostApplicationCreateHook,
    PostApplicationExecuteCallHook,
    PostApplicationExecuteCallHookAsync,
    PostEndStreamHook,
    PostStreamItemHook,
    PostStreamItemHookAsync,
    PreApplicationExecuteCallHook,
    PreApplicationExecuteCallHookAsync,
    PreStartStreamHook,
    PreStartStreamHookAsync,
)
from burr.lifecycle.internal import LifecycleAdapterSet
from burr.tracking.base import SyncTrackingClient


class PassedInAction(Action):
    def __init__(
        self,
        reads: list[str],
        writes: list[str],
        fn: Callable[..., dict],
        update_fn: Callable[[dict, State], State],
        inputs: list[str],
    ):
        super(PassedInAction, self).__init__()
        self._reads = reads
        self._writes = writes
        self._fn = fn
        self._update_fn = update_fn
        self._inputs = inputs

    def run(self, state: State, **run_kwargs) -> dict:
        return self._fn(state, **run_kwargs)

    @property
    def inputs(self) -> list[str]:
        return self._inputs

    def update(self, result: dict, state: State) -> State:
        return self._update_fn(result, state)

    @property
    def reads(self) -> list[str]:
        return self._reads

    @property
    def writes(self) -> list[str]:
        return self._writes


class PassedInActionAsync(PassedInAction):
    def __init__(
        self,
        reads: list[str],
        writes: list[str],
        fn: Callable[..., Awaitable[dict]],
        update_fn: Callable[[dict, State], State],
        inputs: list[str],
    ):
        super().__init__(reads=reads, writes=writes, fn=fn, update_fn=update_fn, inputs=inputs)  # type: ignore

    async def run(self, state: State, **run_kwargs) -> dict:
        return await self._fn(state, **run_kwargs)


base_counter_action = PassedInAction(
    reads=["count"],
    writes=["count"],
    fn=lambda state: {"count": state.get("count", 0) + 1},
    update_fn=lambda result, state: state.update(**result),
    inputs=[],
)

base_counter_action_with_inputs = PassedInAction(
    reads=["count"],
    writes=["count"],
    fn=lambda state, additional_increment: {
        "count": state.get("count", 0) + 1 + additional_increment
    },
    update_fn=lambda result, state: state.update(**result),
    inputs=["additional_increment"],
)


class StreamEventCaptureTracker(PreStartStreamHook, PostStreamItemHook, PostEndStreamHook):
    def post_end_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        self.post_end_stream_calls.append((action, locals()))

    def __init__(self):
        self.pre_start_stream_calls = []
        self.post_stream_item_calls = []
        self.post_end_stream_calls = []

    def pre_start_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        self.pre_start_stream_calls.append((action, locals()))

    def post_stream_item(
        self,
        *,
        item: Any,
        item_index: int,
        stream_initialize_time: datetime.datetime,
        first_stream_item_start_time: datetime.datetime,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        self.post_stream_item_calls.append((action, locals()))


class StreamEventCaptureTrackerAsync(
    PreStartStreamHookAsync, PostStreamItemHookAsync, PostEndStreamHook
):
    def __init__(self):
        self.pre_start_stream_calls = []
        self.post_stream_item_calls = []
        self.post_end_stream_calls = []

    async def pre_start_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        self.pre_start_stream_calls.append((action, locals()))

    async def post_stream_item(
        self,
        *,
        item: Any,
        item_index: int,
        stream_initialize_time: datetime.datetime,
        first_stream_item_start_time: datetime.datetime,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        self.post_stream_item_calls.append((action, locals()))

    def post_end_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        self.post_end_stream_calls.append((action, locals()))


class CallCaptureTracker(
    PreRunStepHook,
    PostRunStepHook,
    PreApplicationExecuteCallHook,
    PostApplicationExecuteCallHook,
):
    def __init__(self):
        self.pre_called = []
        self.post_called = []
        self.pre_run_execute_calls = []
        self.post_run_execute_calls = []

    def pre_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        inputs: Dict[str, Any],
        **future_kwargs: Any,
    ):
        self.pre_called.append((action.name, locals()))

    def post_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        result: Optional[Dict[str, Any]],
        exception: Exception,
        **future_kwargs: Any,
    ):
        self.post_called.append((action.name, locals()))

    def pre_run_execute_call(
        self,
        *,
        app_id: str,
        partition_key: str,
        state: "State",
        method: ExecuteMethod,
        **future_kwargs: Any,
    ):
        self.pre_run_execute_calls.append(("pre", locals()))

    def post_run_execute_call(
        self,
        *,
        app_id: str,
        partition_key: str,
        state: "State",
        method: ExecuteMethod,
        exception: Optional[Exception],
        **future_kwargs,
    ):
        self.post_run_execute_calls.append(("post", locals()))


class ExecuteMethodTrackerAsync(
    PreApplicationExecuteCallHookAsync, PostApplicationExecuteCallHookAsync
):
    def __init__(self):
        self.pre_run_execute_calls = []
        self.post_run_execute_calls = []

    async def pre_run_execute_call(
        self,
        *,
        app_id: str,
        partition_key: str,
        state: "State",
        method: ExecuteMethod,
        **future_kwargs: Any,
    ):
        self.pre_run_execute_calls.append(("pre", locals()))

    async def post_run_execute_call(
        self,
        *,
        app_id: str,
        partition_key: str,
        state: "State",
        method: ExecuteMethod,
        exception: Optional[Exception],
        **future_kwargs,
    ):
        self.post_run_execute_calls.append(("post", locals()))


class ActionTrackerAsync(PreRunStepHookAsync, PostRunStepHookAsync):
    def __init__(self):
        self.pre_called = []
        self.post_called = []

    async def pre_run_step(self, *, action: Action, **future_kwargs):
        await asyncio.sleep(0.0001)
        self.pre_called.append((action.name, future_kwargs))

    async def post_run_step(self, *, action: Action, **future_kwargs):
        await asyncio.sleep(0.0001)
        self.post_called.append((action.name, future_kwargs))


async def _counter_update_async(state: State, additional_increment: int = 0) -> dict:
    await asyncio.sleep(0.0001)  # just so we can make this *truly* async
    # does not matter, but more accurately simulates an async function
    return {"count": state.get("count", 0) + 1 + additional_increment}


base_counter_action_async = PassedInActionAsync(
    reads=["count"],
    writes=["count"],
    fn=_counter_update_async,
    update_fn=lambda result, state: state.update(**result),
    inputs=[],
)

base_counter_action_with_inputs_async = PassedInActionAsync(
    reads=["count"],
    writes=["count"],
    fn=lambda state, additional_increment: _counter_update_async(
        state, additional_increment=additional_increment
    ),
    update_fn=lambda result, state: state.update(**result),
    inputs=["additional_increment"],
)


class BrokenStepException(Exception):
    pass


base_broken_action = PassedInAction(
    reads=[],
    writes=[],
    fn=lambda x: exec("raise(BrokenStepException(x))"),
    update_fn=lambda result, state: state,
    inputs=[],
)

base_broken_action_async = PassedInActionAsync(
    reads=[],
    writes=[],
    fn=lambda x: exec("raise(BrokenStepException(x))"),
    update_fn=lambda result, state: state,
    inputs=[],
)


async def incorrect(x):
    return "not a dict"


base_action_incorrect_result_type = PassedInAction(
    reads=[],
    writes=[],
    fn=lambda x: "not a dict",
    update_fn=lambda result, state: state,
    inputs=[],
)

base_action_incorrect_result_type_async = PassedInActionAsync(
    reads=[],
    writes=[],
    fn=incorrect,
    update_fn=lambda result, state: state,
    inputs=[],
)


def test__run_function():
    """Tests that we can run a function"""
    action = base_counter_action
    state = State({})
    result = _run_function(action, state, inputs={}, name=action.name)
    assert result == {"count": 1}


def test__run_function_with_inputs():
    """Tests that we can run a function"""
    action = base_counter_action_with_inputs
    state = State({})
    result = _run_function(action, state, inputs={"additional_increment": 1}, name=action.name)
    assert result == {"count": 2}


def test__run_function_cant_run_async():
    """Tests that we can't run an async function"""
    action = base_counter_action_async
    state = State({})
    with pytest.raises(ValueError, match="async"):
        _run_function(action, state, inputs={}, name=action.name)


def test__run_function_incorrect_result_type():
    """Tests that we can run an async function"""
    action = base_action_incorrect_result_type
    state = State({})
    with pytest.raises(ValueError, match="returned a non-dict"):
        _run_function(action, state, inputs={}, name=action.name)


def test__run_reducer_modifies_state():
    """Tests that we can run a reducer and it behaves as expected"""
    reducer = PassedInAction(
        reads=["count"],
        writes=["count"],
        fn=...,
        update_fn=lambda result, state: state.update(**result),
        inputs=[],
    )
    state = State({"count": 0})
    state = _run_reducer(reducer, state, {"count": 1}, "reducer")
    assert state["count"] == 1


def test__run_reducer_deletes_state():
    """Tests that we can run a reducer that deletes an item from state"""
    reducer = PassedInAction(
        reads=["count"],
        writes=[],  # TODO -- figure out how we can better know that it deletes items...ÃŸ
        fn=...,
        update_fn=lambda result, state: state.wipe(delete=["count"]),
        inputs=[],
    )
    state = State({"count": 0})
    state = _run_reducer(reducer, state, {}, "deletion_reducer")
    assert "count" not in state


async def test__arun_function():
    """Tests that we can run an async function"""
    action = base_counter_action_async
    state = State({})
    result = await _arun_function(action, state, inputs={}, name=action.name)
    assert result == {"count": 1}


async def test__arun_function_incorrect_result_type():
    """Tests that we can run an async function"""
    action = base_action_incorrect_result_type_async
    state = State({})
    with pytest.raises(ValueError, match="returned a non-dict"):
        await _arun_function(action, state, inputs={}, name=action.name)


async def test__arun_function_with_inputs():
    """Tests that we can run an async function"""
    action = base_counter_action_with_inputs_async
    state = State({})
    result = await _arun_function(
        action, state, inputs={"additional_increment": 1}, name=action.name
    )
    assert result == {"count": 2}


def test_run_reducer_errors_missing_writes():
    class BrokenReducer(Reducer):
        def update(self, result: dict, state: State) -> State:
            return state.update(present_value=1)

        @property
        def writes(self) -> list[str]:
            return ["missing_value", "present_value"]

    reducer = BrokenReducer()
    state = State()
    with pytest.raises(ValueError, match="missing_value"):
        _run_reducer(reducer, state, {}, "broken_reducer")


def test_run_single_step_action_errors_missing_writes():
    class BrokenAction(SingleStepAction):
        @property
        def reads(self) -> list[str]:
            return []

        def run_and_update(self, state: State, **run_kwargs) -> Tuple[dict, State]:
            return {"present_value": 1}, state.update(present_value=1)

        @property
        def writes(self) -> list[str]:
            return ["missing_value", "present_value"]

    action = BrokenAction()
    state = State()
    with pytest.raises(ValueError, match="missing_value"):
        _run_single_step_action(action, state, inputs={})


async def test_arun_single_step_action_errors_missing_writes():
    class BrokenAction(SingleStepAction):
        @property
        def reads(self) -> list[str]:
            return []

        async def run_and_update(self, state: State, **run_kwargs) -> Tuple[dict, State]:
            await asyncio.sleep(0.0001)  # just so we can make this *truly* async
            return {"present_value": 1}, state.update(present_value=1)

        @property
        def writes(self) -> list[str]:
            return ["missing_value", "present_value"]

    action = BrokenAction()
    state = State()
    with pytest.raises(ValueError, match="missing_value"):
        await _arun_single_step_action(action, state, inputs={})


def test_run_single_step_streaming_action_errors_missing_write():
    class BrokenAction(SingleStepStreamingAction):
        def stream_run_and_update(
            self, state: State, **run_kwargs
        ) -> Generator[Tuple[dict, Optional[State]], None, None]:
            yield {}, None
            yield {"present_value": 1}, state.update(present_value=1)

        @property
        def reads(self) -> list[str]:
            return []

        @property
        def writes(self) -> list[str]:
            return ["missing_value", "present_value"]

    action = BrokenAction()
    state = State()
    with pytest.raises(ValueError, match="missing_value"):
        gen = _run_single_step_streaming_action(
            action, state, inputs={}, sequence_id=0, partition_key="partition_key", app_id="app_id"
        )
        collections.deque(gen, maxlen=0)  # exhaust the generator


async def test_run_single_step_streaming_action_errors_missing_write_async():
    class BrokenAction(SingleStepStreamingAction):
        async def stream_run_and_update(
            self, state: State, **run_kwargs
        ) -> AsyncGenerator[Tuple[dict, Optional[State]], None]:
            yield {}, None
            yield {"present_value": 1}, state.update(present_value=1)

        @property
        def reads(self) -> list[str]:
            return []

        @property
        def writes(self) -> list[str]:
            return ["missing_value", "present_value"]

    action = BrokenAction()
    state = State()
    with pytest.raises(ValueError, match="missing_value"):
        gen = _arun_single_step_streaming_action(
            action,
            state,
            inputs={},
            sequence_id=0,
            app_id="app_id",
            partition_key="partition_key",
            lifecycle_adapters=LifecycleAdapterSet(),
        )
        [result async for result in gen]  # exhaust the generator


def test_run_multi_step_streaming_action_errors_missing_write():
    class BrokenAction(StreamingAction):
        def stream_run(self, state: State, **run_kwargs) -> Generator[dict, None, None]:
            yield {}
            yield {"present_value": 1}

        def update(self, result: dict, state: State) -> State:
            return state.update(present_value=1)

        @property
        def reads(self) -> list[str]:
            return []

        @property
        def writes(self) -> list[str]:
            return ["missing_value", "present_value"]

    action = BrokenAction()
    state = State()
    with pytest.raises(ValueError, match="missing_value"):
        gen = _run_multi_step_streaming_action(
            action, state, inputs={}, sequence_id=0, partition_key="partition_key", app_id="app_id"
        )
        collections.deque(gen, maxlen=0)  # exhaust the generator


class SingleStepCounter(SingleStepAction):
    def run_and_update(self, state: State, **run_kwargs) -> Tuple[dict, State]:
        result = {"count": state["count"] + 1 + sum([0] + list(run_kwargs.values()))}
        return result, state.update(**result).append(tracker=result["count"])

    @property
    def reads(self) -> list[str]:
        return ["count"]

    @property
    def writes(self) -> list[str]:
        return ["count", "tracker"]


class SingleStepCounterWithInputs(SingleStepCounter):
    @property
    def inputs(self) -> list[str]:
        return ["additional_increment"]


class SingleStepActionIncorrectResultType(SingleStepAction):
    def run_and_update(self, state: State, **run_kwargs) -> Tuple[dict, State]:
        return "not a dict", state

    @property
    def reads(self) -> list[str]:
        return []

    @property
    def writes(self) -> list[str]:
        return []


class SingleStepActionIncorrectResultTypeAsync(SingleStepActionIncorrectResultType):
    async def run_and_update(self, state: State, **run_kwargs) -> Tuple[dict, State]:
        return "not a dict", state


class SingleStepCounterAsync(SingleStepCounter):
    async def run_and_update(self, state: State, **run_kwargs) -> Tuple[dict, State]:
        await asyncio.sleep(0.0001)  # just so we can make this *truly* async
        return super(SingleStepCounterAsync, self).run_and_update(state, **run_kwargs)

    @property
    def reads(self) -> list[str]:
        return ["count"]

    @property
    def writes(self) -> list[str]:
        return ["count", "tracker"]


class SingleStepCounterWithInputsAsync(SingleStepCounterAsync):
    @property
    def inputs(self) -> list[str]:
        return ["additional_increment"]


class StreamingCounter(StreamingAction):
    def stream_run(self, state: State, **run_kwargs) -> Generator[dict, None, None]:
        if "steps_per_count" in run_kwargs:
            steps_per_count = run_kwargs["granularity"]
        else:
            steps_per_count = 10
        count = state["count"]
        for i in range(steps_per_count):
            yield {"count": count + ((i + 1) / 10)}
        yield {"count": count + 1}

    @property
    def reads(self) -> list[str]:
        return ["count"]

    @property
    def writes(self) -> list[str]:
        return ["count", "tracker"]

    def update(self, result: dict, state: State) -> State:
        return state.update(**result).append(tracker=result["count"])


class AsyncStreamingCounter(AsyncStreamingAction):
    async def stream_run(self, state: State, **run_kwargs) -> AsyncGenerator[dict, None]:
        if "steps_per_count" in run_kwargs:
            steps_per_count = run_kwargs["granularity"]
        else:
            steps_per_count = 10
        count = state["count"]
        for i in range(steps_per_count):
            await asyncio.sleep(0.01)
            yield {"count": count + (i + 1) / 10}
        await asyncio.sleep(0.01)
        yield {"count": count + 1}

    @property
    def reads(self) -> list[str]:
        return ["count"]

    @property
    def writes(self) -> list[str]:
        return ["count", "tracker"]

    def update(self, result: dict, state: State) -> State:
        return state.update(**result).append(tracker=result["count"])


class SingleStepStreamingCounter(SingleStepStreamingAction):
    def stream_run_and_update(
        self, state: State, **run_kwargs
    ) -> Generator[Tuple[dict, Optional[State]], None, None]:
        steps_per_count = run_kwargs.get("granularity", 10)
        count = state["count"]
        for i in range(steps_per_count):
            yield {"count": count + ((i + 1) / 10)}, None
        yield {"count": count + 1}, state.update(count=count + 1).append(tracker=count + 1)

    @property
    def reads(self) -> list[str]:
        return ["count"]

    @property
    def writes(self) -> list[str]:
        return ["count", "tracker"]


class SingleStepStreamingCounterAsync(SingleStepStreamingAction):
    async def stream_run_and_update(
        self, state: State, **run_kwargs
    ) -> AsyncGenerator[Tuple[dict, Optional[State]], None]:
        steps_per_count = run_kwargs.get("granularity", 10)
        count = state["count"]
        for i in range(steps_per_count):
            await asyncio.sleep(0.01)
            yield {"count": count + ((i + 1) / 10)}, None
        await asyncio.sleep(0.01)
        yield {"count": count + 1}, state.update(count=count + 1).append(tracker=count + 1)

    @property
    def reads(self) -> list[str]:
        return ["count"]

    @property
    def writes(self) -> list[str]:
        return ["count", "tracker"]


class StreamingActionIncorrectResultType(StreamingAction):
    def stream_run(self, state: State, **run_kwargs) -> Generator[dict, None, dict]:
        yield {}
        yield "not a dict"

    @property
    def reads(self) -> list[str]:
        return []

    @property
    def writes(self) -> list[str]:
        return []

    def update(self, result: dict, state: State) -> State:
        return state


class StreamingActionIncorrectResultTypeAsync(AsyncStreamingAction):
    async def stream_run(self, state: State, **run_kwargs) -> AsyncGenerator[dict, None]:
        yield {}
        yield "not a dict"

    @property
    def reads(self) -> list[str]:
        return []

    @property
    def writes(self) -> list[str]:
        return []

    def update(self, result: dict, state: State) -> State:
        return state


class StreamingSingleStepActionIncorrectResultType(SingleStepStreamingAction):
    def stream_run_and_update(
        self, state: State, **run_kwargs
    ) -> Generator[Tuple[dict, Optional[State]], None, None]:
        yield {}, State
        yield "not a dict", state

    @property
    def reads(self) -> list[str]:
        return []

    @property
    def writes(self) -> list[str]:
        return []


class StreamingSingleStepActionIncorrectResultTypeAsync(SingleStepStreamingAction):
    async def stream_run_and_update(
        self, state: State, **run_kwargs
    ) -> typing.AsyncGenerator[Tuple[dict, Optional[State]], None]:
        yield {}, None
        yield "not a dict", state

    @property
    def reads(self) -> list[str]:
        return []

    @property
    def writes(self) -> list[str]:
        return []


base_single_step_counter = SingleStepCounter()
base_single_step_counter_async = SingleStepCounterAsync()
base_single_step_counter_with_inputs = SingleStepCounterWithInputs()
base_single_step_counter_with_inputs_async = SingleStepCounterWithInputsAsync()

base_streaming_counter = StreamingCounter()
base_streaming_single_step_counter = SingleStepStreamingCounter()

base_streaming_counter_async = AsyncStreamingCounter()
base_streaming_single_step_counter_async = SingleStepStreamingCounterAsync()

base_single_step_action_incorrect_result_type = SingleStepActionIncorrectResultType()
base_single_step_action_incorrect_result_type_async = SingleStepActionIncorrectResultTypeAsync()


def test__run_single_step_action():
    action = base_single_step_counter.with_name("counter")
    state = State({"count": 0, "tracker": []})
    result, state = _run_single_step_action(action, state, inputs={})
    assert result == {"count": 1}
    assert state.subset("count", "tracker").get_all() == {"count": 1, "tracker": [1]}
    result, state = _run_single_step_action(action, state, inputs={})
    assert result == {"count": 2}
    assert state.subset("count", "tracker").get_all() == {"count": 2, "tracker": [1, 2]}


def test__run_single_step_action_incorrect_result_type():
    action = base_single_step_action_incorrect_result_type.with_name("counter")
    state = State({"count": 0, "tracker": []})
    with pytest.raises(ValueError, match="returned a non-dict"):
        _run_single_step_action(action, state, inputs={})


async def test__arun_single_step_action_incorrect_result_type():
    action = base_single_step_action_incorrect_result_type_async.with_name("counter")
    state = State({"count": 0, "tracker": []})
    with pytest.raises(ValueError, match="returned a non-dict"):
        await _arun_single_step_action(action, state, inputs={})


def test__run_single_step_action_with_inputs():
    action = base_single_step_counter_with_inputs.with_name("counter")
    state = State({"count": 0, "tracker": []})
    result, state = _run_single_step_action(action, state, inputs={"additional_increment": 1})
    assert result == {"count": 2}
    assert state.subset("count", "tracker").get_all() == {"count": 2, "tracker": [2]}
    result, state = _run_single_step_action(action, state, inputs={"additional_increment": 1})
    assert result == {"count": 4}
    assert state.subset("count", "tracker").get_all() == {"count": 4, "tracker": [2, 4]}


async def test__arun_single_step_action():
    action = base_single_step_counter_async.with_name("counter")
    state = State({"count": 0, "tracker": []})
    result, state = await _arun_single_step_action(action, state, inputs={})
    assert result == {"count": 1}
    assert state.subset("count", "tracker").get_all() == {"count": 1, "tracker": [1]}
    result, state = await _arun_single_step_action(action, state, inputs={})
    assert result == {"count": 2}
    assert state.subset("count", "tracker").get_all() == {"count": 2, "tracker": [1, 2]}


async def test__arun_single_step_action_with_inputs():
    action = base_single_step_counter_with_inputs_async.with_name("counter")
    state = State({"count": 0, "tracker": []})
    result, state = await _arun_single_step_action(
        action, state, inputs={"additional_increment": 1}
    )
    assert result == {"count": 2}
    assert state.subset("count", "tracker").get_all() == {"count": 2, "tracker": [2]}
    result, state = await _arun_single_step_action(
        action, state, inputs={"additional_increment": 1}
    )
    assert result == {"count": 4}
    assert state.subset("count", "tracker").get_all() == {"count": 4, "tracker": [2, 4]}


class SingleStepActionWithDeletion(SingleStepAction):
    def run_and_update(self, state: State, **run_kwargs) -> Tuple[dict, State]:
        return {}, state.wipe(delete=["to_delete"])

    @property
    def reads(self) -> list[str]:
        return []

    @property
    def writes(self) -> list[str]:
        return []


def test__run_single_step_action_deletes_state():
    action = SingleStepActionWithDeletion()
    state = State({"to_delete": 0})
    result, state = _run_single_step_action(action, state, inputs={})
    assert "to_delete" not in state


def test__run_multistep_streaming_action():
    action = base_streaming_counter.with_name("counter")
    state = State({"count": 0, "tracker": []})
    generator = _run_multi_step_streaming_action(
        action, state, inputs={}, sequence_id=0, partition_key="partition_key", app_id="app_id"
    )
    last_result = -1
    result = None
    for result, state in generator:
        if last_result < 1:
            # Otherwise you hit floating poit comparison problems
            assert result["count"] > last_result
        last_result = result["count"]
    assert result == {"count": 1}
    assert state.subset("count", "tracker").get_all() == {"count": 1, "tracker": [1]}


def test__run_multistep_streaming_action_callbacks():
    class TrackingCallback(PostStreamItemHook):
        def __init__(self):
            self.items = []

        def post_stream_item(self, item: Any, **future_kwargs: Any):
            self.items.append(item)

    hook = TrackingCallback()

    action = base_streaming_counter.with_name("counter")
    state = State({"count": 0, "tracker": []})
    generator = _run_multi_step_streaming_action(
        action,
        state,
        inputs={},
        sequence_id=0,
        partition_key="partition_key",
        app_id="app_id",
        lifecycle_adapters=LifecycleAdapterSet(hook),
    )
    last_result = -1
    result = None
    for result, state in generator:
        if last_result < 1:
            # Otherwise you hit floating poit comparison problems
            assert result["count"] > last_result
        last_result = result["count"]
    assert result == {"count": 1}
    assert state.subset("count", "tracker").get_all() == {"count": 1, "tracker": [1]}
    assert len(hook.items) == 10  # one for each streaming callback


async def test__run_multistep_streaming_action_async():
    action = base_streaming_counter_async.with_name("counter")
    state = State({"count": 0, "tracker": []})
    generator = _arun_multi_step_streaming_action(
        action=action,
        state=state,
        inputs={},
        sequence_id=0,
        app_id="app_id",
        partition_key="partition_key",
        lifecycle_adapters=LifecycleAdapterSet(),
    )
    last_result = -1
    result = None
    async for result, state in generator:
        if last_result < 1:
            # Otherwise you hit floating poit comparison problems
            assert result["count"] > last_result
        last_result = result["count"]
    assert result == {"count": 1}
    assert state.subset("count", "tracker").get_all() == {"count": 1, "tracker": [1]}


async def test__run_multistep_streaming_action_async_callbacks():
    class TrackingCallback(PostStreamItemHookAsync):
        def __init__(self):
            self.items = []

        async def post_stream_item(self, item: Any, **future_kwargs: Any):
            self.items.append(item)

    hook = TrackingCallback()
    action = base_streaming_counter_async.with_name("counter")
    state = State({"count": 0, "tracker": []})
    generator = _arun_multi_step_streaming_action(
        action=action,
        state=state,
        inputs={},
        sequence_id=0,
        app_id="app_id",
        partition_key="partition_key",
        lifecycle_adapters=LifecycleAdapterSet(hook),
    )
    last_result = -1
    result = None
    async for result, state in generator:
        if last_result < 1:
            # Otherwise you hit floating poit comparison problems
            assert result["count"] > last_result
        last_result = result["count"]
    assert result == {"count": 1}
    assert state.subset("count", "tracker").get_all() == {"count": 1, "tracker": [1]}
    assert len(hook.items) == 10  # one for each streaming callback


def test__run_streaming_action_incorrect_result_type():
    action = StreamingActionIncorrectResultType()
    state = State()
    with pytest.raises(ValueError, match="returned a non-dict"):
        gen = _run_multi_step_streaming_action(
            action, state, inputs={}, sequence_id=0, partition_key="partition_key", app_id="app_id"
        )
        collections.deque(gen, maxlen=0)  # exhaust the generator


async def test__run_streaming_action_incorrect_result_type_async():
    action = StreamingActionIncorrectResultTypeAsync()
    state = State()
    with pytest.raises(ValueError, match="returned a non-dict"):
        gen = _arun_multi_step_streaming_action(
            action=action,
            state=state,
            inputs={},
            sequence_id=0,
            app_id="app_id",
            partition_key="partition_key",
            lifecycle_adapters=LifecycleAdapterSet(),
        )
        async for _ in gen:
            pass


def test__run_single_step_streaming_action_incorrect_result_type():
    action = StreamingSingleStepActionIncorrectResultType()
    state = State()
    with pytest.raises(ValueError, match="returned a non-dict"):
        gen = _run_single_step_streaming_action(
            action=action,
            state=state,
            inputs={},
            sequence_id=0,
            partition_key="partition_key",
            app_id="app_id",
        )
        collections.deque(gen, maxlen=0)  # exhaust the generator


async def test__run_single_step_streaming_action_incorrect_result_type_async():
    action = StreamingSingleStepActionIncorrectResultTypeAsync()
    state = State()
    with pytest.raises(ValueError, match="returned a non-dict"):
        gen = _arun_single_step_streaming_action(
            action,
            state,
            inputs={},
            sequence_id=0,
            partition_key="partition_key",
            app_id="app_id",
            lifecycle_adapters=LifecycleAdapterSet(),
        )
        _ = [item async for item in gen]


def test__run_single_step_streaming_action():
    action = base_streaming_single_step_counter.with_name("counter")
    state = State({"count": 0, "tracker": []})
    generator = _run_single_step_streaming_action(
        action, state, inputs={}, sequence_id=0, partition_key="partition_key", app_id="app_id"
    )
    last_result = -1
    result, state = None, None
    for result, state in generator:
        if last_result < 1:
            # Otherwise you hit comparison issues
            # This is because we get to the last one, which is the final result
            assert result["count"] > last_result
        last_result = result["count"]
    assert result == {"count": 1}
    assert state.subset("count", "tracker").get_all() == {"count": 1, "tracker": [1]}


def test__run_single_step_streaming_action_calls_callbacks():
    action = base_streaming_single_step_counter.with_name("counter")

    class TrackingCallback(PostStreamItemHook):
        def __init__(self):
            self.items = []

        def post_stream_item(self, item: Any, **future_kwargs: Any):
            self.items.append(item)

    hook = TrackingCallback()

    state = State({"count": 0, "tracker": []})
    generator = _run_single_step_streaming_action(
        action,
        state,
        inputs={},
        sequence_id=0,
        partition_key="partition_key",
        app_id="app_id",
        lifecycle_adapters=LifecycleAdapterSet(hook),
    )
    last_result = -1
    result, state = None, None
    for result, state in generator:
        if last_result < 1:
            # Otherwise you hit comparison issues
            # This is because we get to the last one, which is the final result
            assert result["count"] > last_result
        last_result = result["count"]
    assert result == {"count": 1}
    assert state.subset("count", "tracker").get_all() == {"count": 1, "tracker": [1]}
    assert len(hook.items) == 10  # one for each streaming callback


async def test__run_single_step_streaming_action_async():
    async_action = base_streaming_single_step_counter_async.with_name("counter")
    state = State({"count": 0, "tracker": []})
    generator = _arun_single_step_streaming_action(
        action=async_action,
        state=state,
        inputs={},
        sequence_id=0,
        app_id="app_id",
        partition_key="partition_key",
        lifecycle_adapters=LifecycleAdapterSet(),
    )
    last_result = -1
    result, state = None, None
    async for result, state in generator:
        if last_result < 1:
            # Otherwise you hit comparison issues
            # This is because we get to the last one, which is the final result
            assert result["count"] > last_result
        last_result = result["count"]
    assert result == {"count": 1}
    assert state.subset("count", "tracker").get_all() == {"count": 1, "tracker": [1]}


async def test__run_single_step_streaming_action_async_callbacks():
    class TrackingCallback(PostStreamItemHookAsync):
        def __init__(self):
            self.items = []

        async def post_stream_item(self, item: Any, **future_kwargs: Any):
            self.items.append(item)

    hook = TrackingCallback()

    async_action = base_streaming_single_step_counter_async.with_name("counter")
    state = State({"count": 0, "tracker": []})
    generator = _arun_single_step_streaming_action(
        action=async_action,
        state=state,
        inputs={},
        sequence_id=0,
        app_id="app_id",
        partition_key="partition_key",
        lifecycle_adapters=LifecycleAdapterSet(hook),
    )
    last_result = -1
    result, state = None, None
    async for result, state in generator:
        if last_result < 1:
            # Otherwise you hit comparison issues
            # This is because we get to the last one, which is the final result
            assert result["count"] > last_result
        last_result = result["count"]
    assert result == {"count": 1}
    assert state.subset("count", "tracker").get_all() == {"count": 1, "tracker": [1]}
    assert len(hook.items) == 10  # one for each streaming callback


class SingleStepActionWithDeletionAsync(SingleStepActionWithDeletion):
    async def run_and_update(self, state: State, **run_kwargs) -> Tuple[dict, State]:
        return {}, state.wipe(delete=["to_delete"])


async def test__arun_single_step_action_deletes_state():
    action = SingleStepActionWithDeletionAsync()
    state = State({"to_delete": 0})
    result, state = await _arun_single_step_action(action, state, inputs={})
    assert "to_delete" not in state


def test_app_step():
    """Tests that we can run a step in an app"""
    counter_action = base_counter_action.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action],
            transitions=[Transition(counter_action, counter_action, default)],
        ),
    )
    action, result, state = app.step()
    assert app.sequence_id == 1
    assert action.name == "counter"
    assert result == {"count": 1}
    assert state[PRIOR_STEP] == "counter"  # internal contract, not part of the public API


def test_app_step_with_inputs():
    """Tests that we can run a step in an app"""
    counter_action = base_single_step_counter_with_inputs.with_name("counter")
    app = Application(
        state=State({"count": 0, "tracker": []}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action],
            transitions=[Transition(counter_action, counter_action, default)],
        ),
    )
    action, result, state = app.step(inputs={"additional_increment": 1})
    assert action.name == "counter"
    assert result == {"count": 2}
    assert state.subset("count", "tracker").get_all() == {"count": 2, "tracker": [2]}


def test_app_step_with_inputs_missing():
    """Tests that we can run a step in an app"""
    counter_action = base_single_step_counter_with_inputs.with_name("counter")
    app = Application(
        state=State({"count": 0, "tracker": []}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action],
            transitions=[Transition(counter_action, counter_action, default)],
        ),
    )
    with pytest.raises(ValueError, match="missing required inputs"):
        app.step(inputs={})


def test_app_step_broken(caplog):
    """Tests that we can run a step in an app"""
    broken_action = base_broken_action.with_name("broken_action_unique_name")
    app = Application(
        state=State({}),
        entrypoint="broken_action_unique_name",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[broken_action],
            transitions=[Transition(broken_action, broken_action, default)],
        ),
    )
    with caplog.at_level(logging.ERROR):  # it should say the name, that's the only contract for now
        with pytest.raises(BrokenStepException):
            app.step()
    assert "broken_action_unique_name" in caplog.text


def test_app_step_done():
    """Tests that when we cannot run a step, we return None"""
    counter_action = base_counter_action.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action],
            transitions=[],
        ),
    )
    app.step()
    assert app.step() is None


async def test_app_astep():
    """Tests that we can run an async step in an app"""
    counter_action = base_counter_action_async.with_name("counter_async")
    app = Application(
        state=State({}),
        entrypoint="counter_async",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action],
            transitions=[Transition(counter_action, counter_action, default)],
        ),
    )
    action, result, state = await app.astep()
    assert app.sequence_id == 1
    assert action.name == "counter_async"
    assert result == {"count": 1}
    assert state[PRIOR_STEP] == "counter_async"  # internal contract, not part of the public API


def test_app_step_context():
    APP_ID = str(uuid.uuid4())
    PARTITION_KEY = str(uuid.uuid4())

    @action(reads=[], writes=[])
    def test_action(state: State, __context: ApplicationContext) -> State:
        assert __context.sequence_id == 0
        assert __context.partition_key == PARTITION_KEY
        assert __context.app_id == APP_ID
        assert __context.action_name == "test_action"
        return state

    app = (
        ApplicationBuilder()
        .with_actions(test_action)
        .with_entrypoint("test_action")
        .with_transitions()
        .with_identifiers(
            app_id=APP_ID,
            partition_key=PARTITION_KEY,
        )
        .build()
    )
    app.step()


async def test_app_astep_context():
    """Tests that app.astep correctly passes context."""
    APP_ID = str(uuid.uuid4())
    PARTITION_KEY = str(uuid.uuid4())

    @action(reads=[], writes=[])
    def test_action(state: State, __context: ApplicationContext) -> State:
        assert __context.sequence_id == 0
        assert __context.partition_key == PARTITION_KEY
        assert __context.app_id == APP_ID
        assert __context.action_name == "test_action"
        return state

    app = (
        ApplicationBuilder()
        .with_actions(test_action)
        .with_entrypoint("test_action")
        .with_transitions()
        .with_identifiers(
            app_id=APP_ID,
            partition_key=PARTITION_KEY,
        )
        .build()
    )
    await app.astep()


async def test_app_astep_with_inputs():
    """Tests that we can run an async step in an app"""
    counter_action = base_single_step_counter_with_inputs_async.with_name("counter_async")
    app = Application(
        state=State({"count": 0, "tracker": []}),
        entrypoint="counter_async",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action],
            transitions=[Transition(counter_action, counter_action, default)],
        ),
    )
    action, result, state = await app.astep(inputs={"additional_increment": 1})
    assert action.name == "counter_async"
    assert result == {"count": 2}
    assert state.subset("count", "tracker").get_all() == {"count": 2, "tracker": [2]}


async def test_app_astep_with_inputs_missing():
    """Tests that we can run an async step in an app"""
    counter_action = base_single_step_counter_with_inputs_async.with_name("counter_async")
    app = Application(
        state=State({"count": 0, "tracker": []}),
        entrypoint="counter_async",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action],
            transitions=[Transition(counter_action, counter_action, default)],
        ),
    )
    with pytest.raises(ValueError, match="missing required inputs"):
        await app.astep(inputs={})


async def test_app_astep_broken(caplog):
    """Tests that we can run a step in an app"""
    broken_action = base_broken_action_async.with_name("broken_action_unique_name")
    app = Application(
        state=State({}),
        entrypoint="broken_action_unique_name",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[broken_action],
            transitions=[Transition(broken_action, broken_action, default)],
        ),
    )
    with caplog.at_level(logging.ERROR):  # it should say the name, that's the only contract for now
        with pytest.raises(BrokenStepException):
            await app.astep()
    assert "broken_action_unique_name" in caplog.text


async def test_app_astep_done():
    """Tests that when we cannot run a step, we return None"""
    counter_action = base_counter_action_async.with_name("counter_async")
    app = Application(
        state=State({}),
        entrypoint="counter_async",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action],
            transitions=[],
        ),
    )
    await app.astep()
    assert await app.astep() is None


# internal API
def test_app_many_steps():
    counter_action = base_counter_action.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action],
            transitions=[Transition(counter_action, counter_action, default)],
        ),
    )
    action, result = None, None
    for i in range(100):
        action, result, state = app.step()
    assert action.name == "counter"
    assert result == {"count": 100}


async def test_app_many_a_steps():
    counter_action = base_counter_action_async.with_name("counter_async")
    app = Application(
        state=State({}),
        entrypoint="counter_async",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action],
            transitions=[Transition(counter_action, counter_action, default)],
        ),
    )
    action, result = None, None
    for i in range(100):
        action, result, state = await app.astep()
    assert action.name == "counter_async"
    assert result == {"count": 100}


def test_iterate():
    result_action = Result("count").with_name("result")
    counter_action = base_counter_action.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, counter_action, Condition.expr("count < 10")),
                Transition(counter_action, result_action, default),
            ],
        ),
    )
    res = []
    gen = app.iterate(halt_after=["result"])
    counter = 0
    try:
        while True:
            action, result, state = next(gen)
            if action.name == "counter":
                assert state["count"] == counter + 1
                assert result["count"] == state["count"]
                counter = result["count"]
            else:
                res.append(result)
                assert state["count"] == 10
                assert result["count"] == 10
    except StopIteration as e:
        stop_iteration_error = e
    generator_result = stop_iteration_error.value
    action, result, state = generator_result
    assert state["count"] == 10
    assert result["count"] == 10
    assert app.sequence_id == 11


def test_iterate_with_inputs():
    result_action = Result("count").with_name("result")
    counter_action = base_counter_action_with_inputs.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, counter_action, Condition.expr("count < 2")),
                Transition(counter_action, result_action, default),
            ],
        ),
    )
    gen = app.iterate(
        halt_after=["result"], inputs={"additional_increment": 10}
    )  # make it go quicly to the end
    while True:
        try:
            action, result, state = next(gen)
        except StopIteration as e:
            a, r, s = e.value
            assert r["count"] == 11  # 1 + 10, for the first one
            break


async def test_aiterate():
    result_action = Result("count").with_name("result")
    counter_action = base_counter_action_async.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, counter_action, Condition.expr("count < 10")),
                Transition(counter_action, result_action, default),
            ],
        ),
    )
    gen = app.aiterate(halt_after=["result"])
    assert app.sequence_id == 0
    counter = 0
    # Note that we use an async-for loop cause the API is different, this doesn't
    # return anything (async generators are not allowed to).
    async for action_, result, state in gen:
        print("si", app.sequence_id, action_.name, state)
        if action_.name == "counter":
            assert state["count"] == result["count"] == counter + 1
            counter = result["count"]
        else:
            assert state["count"] == result["count"] == 10
    assert app.sequence_id == 11


async def test_aiterate_halt_before():
    result_action = Result("count").with_name("result")
    counter_action = base_counter_action_async.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, counter_action, Condition.expr("count < 10")),
                Transition(counter_action, result_action, default),
            ],
        ),
    )
    gen = app.aiterate(halt_before=["result"])
    counter = 0
    # Note that we use an async-for loop cause the API is different, this doesn't
    # return anything (async generators are not allowed to).
    async for action_, result, state in gen:
        if action_.name == "counter":
            assert state["count"] == counter + 1
            counter = result["count"]
        else:
            assert result is None
            assert state["count"] == 10


async def test_app_aiterate_with_inputs():
    result_action = Result("count").with_name("result")
    counter_action = base_counter_action_with_inputs_async.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, counter_action, Condition.expr("count < 10")),
                Transition(counter_action, result_action, default),
            ],
        ),
    )
    gen = app.aiterate(halt_after=["result"], inputs={"additional_increment": 10})
    async for action_, result, state in gen:
        if action_.name == "counter":
            assert result["count"] == state["count"] == 11
        else:
            assert state["count"] == result["count"] == 11


def test_run():
    result_action = Result("count").with_name("result")
    counter_action = base_counter_action.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, counter_action, Condition.expr("count < 10")),
                Transition(counter_action, result_action, default),
            ],
        ),
    )
    action, result, state = app.run(halt_after=["result"])
    assert state["count"] == 10
    assert result["count"] == 10


def test_run_halt_before():
    result_action = Result("count").with_name("result")
    counter_action = base_counter_action.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, counter_action, Condition.expr("count < 10")),
                Transition(counter_action, result_action, default),
            ],
        ),
    )
    action, result, state = app.run(halt_before=["result"])
    assert state["count"] == 10
    assert result is None
    assert action.name == "result"


def test_run_with_inputs():
    result_action = Result("count").with_name("result")
    counter_action = base_counter_action_with_inputs.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, counter_action, Condition.expr("count < 10")),
                Transition(counter_action, result_action, default),
            ],
        ),
    )
    action_, result, state = app.run(halt_after=["result"], inputs={"additional_increment": 10})
    assert action_.name == "result"
    assert state["count"] == result["count"] == 11


def test_run_with_inputs_multiple_actions():
    """Tests that inputs aren't popped off and are passed through to multiple actions."""
    result_action = Result("count").with_name("result")
    counter_action1 = base_counter_action_with_inputs.with_name("counter1")
    counter_action2 = base_counter_action_with_inputs.with_name("counter2")
    app = Application(
        state=State({}),
        entrypoint="counter1",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action1, counter_action2, result_action],
            transitions=[
                Transition(counter_action1, counter_action1, Condition.expr("count < 10")),
                Transition(counter_action1, counter_action2, Condition.expr("count >= 10")),
                Transition(counter_action2, counter_action2, Condition.expr("count < 20")),
                Transition(counter_action2, result_action, default),
            ],
        ),
    )
    action_, result, state = app.run(halt_after=["result"], inputs={"additional_increment": 8})
    assert action_.name == "result"
    assert state["count"] == result["count"] == 27
    assert state["__SEQUENCE_ID"] == 4


async def test_arun():
    result_action = Result("count").with_name("result")
    counter_action = base_counter_action_async.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, counter_action, Condition.expr("count < 10")),
                Transition(counter_action, result_action, default),
            ],
        ),
    )
    action_, result, state = await app.arun(halt_after=["result"])
    assert state["count"] == result["count"] == 10
    assert action_.name == "result"


async def test_arun_halt_before():
    result_action = Result("count").with_name("result")
    counter_action = base_counter_action_async.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, counter_action, Condition.expr("count < 10")),
                Transition(counter_action, result_action, default),
            ],
        ),
    )
    action_, result, state = await app.arun(halt_before=["result"])
    assert state["count"] == 10
    assert result is None
    assert action_.name == "result"


async def test_arun_with_inputs():
    result_action = Result("count").with_name("result")
    counter_action = base_counter_action_with_inputs_async.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, counter_action, Condition.expr("count < 10")),
                Transition(counter_action, result_action, default),
            ],
        ),
    )
    action_, result, state = await app.arun(
        halt_after=["result"], inputs={"additional_increment": 10}
    )
    assert state["count"] == result["count"] == 11
    assert action_.name == "result"


async def test_arun_with_inputs_multiple_actions():
    result_action = Result("count").with_name("result")
    counter_action1 = base_counter_action_with_inputs_async.with_name("counter1")
    counter_action2 = base_counter_action_with_inputs_async.with_name("counter2")
    app = Application(
        state=State({}),
        entrypoint="counter1",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action1, counter_action2, result_action],
            transitions=[
                Transition(counter_action1, counter_action1, Condition.expr("count < 10")),
                Transition(counter_action1, counter_action2, Condition.expr("count >= 10")),
                Transition(counter_action2, counter_action2, Condition.expr("count < 20")),
                Transition(counter_action2, result_action, default),
            ],
        ),
    )
    action_, result, state = await app.arun(
        halt_after=["result"], inputs={"additional_increment": 8}
    )
    assert state["count"] == result["count"] == 27
    assert action_.name == "result"
    assert state["__SEQUENCE_ID"] == 4


async def test_app_a_run_async_and_sync():
    result_action = Result("count").with_name("result")
    counter_action_sync = base_counter_action_async.with_name("counter_sync")
    counter_action_async = base_counter_action_async.with_name("counter_async")
    app = Application(
        state=State({}),
        entrypoint="counter_sync",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action_sync, counter_action_async, result_action],
            transitions=[
                Transition(counter_action_sync, counter_action_async, Condition.expr("count < 20")),
                Transition(counter_action_async, counter_action_sync, default),
                Transition(counter_action_sync, result_action, default),
            ],
        ),
    )
    action_, result, state = await app.arun(halt_after=["result"])
    assert state["count"] > 20
    assert result["count"] > 20


def test_stream_result_halt_after_unique_ordered_sequence_id():
    action_tracker = CallCaptureTracker()
    stream_event_tracker = StreamEventCaptureTracker()
    counter_action = base_streaming_counter.with_name("counter")
    counter_action_2 = base_streaming_counter.with_name("counter_2")
    app = Application(
        state=State({"count": 0}),
        entrypoint="counter",
        adapter_set=LifecycleAdapterSet(action_tracker, stream_event_tracker),
        partition_key="test",
        uid="test-123",
        graph=Graph(
            actions=[counter_action, counter_action_2],
            transitions=[
                Transition(counter_action, counter_action_2, default),
            ],
        ),
    )
    action_, streaming_container = app.stream_result(halt_after=["counter_2"])
    results = list(streaming_container)
    assert len(results) == 10
    result, state = streaming_container.get()
    assert result["count"] == state["count"] == 2
    assert state["tracker"] == [1, 2]
    assert len(action_tracker.pre_called) == 2
    assert len(action_tracker.post_called) == 2
    assert set(dict(action_tracker.pre_called).keys()) == {"counter", "counter_2"}
    assert set(dict(action_tracker.post_called).keys()) == {"counter", "counter_2"}
    assert [item["sequence_id"] for _, item in action_tracker.pre_called] == [
        0,
        1,
    ]  # ensure sequence ID is respected
    assert [item["sequence_id"] for _, item in action_tracker.post_called] == [
        0,
        1,
    ]  # ensure sequence ID is respected
    # One post call/one pre-call, as we call stream_result once
    assert len(action_tracker.post_run_execute_calls) == 1
    assert len(action_tracker.pre_run_execute_calls) == 1
    # One call for streaming
    assert len(stream_event_tracker.pre_start_stream_calls) == 1
    # 10 streaming items
    assert len(stream_event_tracker.post_stream_item_calls) == 10
    # One call for streaming
    assert len(stream_event_tracker.post_end_stream_calls) == 1


async def test_astream_result_halt_after_unique_ordered_sequence_id():
    action_tracker = CallCaptureTracker()
    stream_event_tracker = StreamEventCaptureTrackerAsync()
    stream_event_tracker_sync = StreamEventCaptureTracker()
    counter_action = base_streaming_counter_async.with_name("counter")
    counter_action_2 = base_streaming_counter_async.with_name("counter_2")
    app = Application(
        state=State({"count": 0}),
        entrypoint="counter",
        adapter_set=LifecycleAdapterSet(
            action_tracker, stream_event_tracker, stream_event_tracker_sync
        ),
        partition_key="test",
        uid="test-123",
        graph=Graph(
            actions=[counter_action, counter_action_2],
            transitions=[
                Transition(counter_action, counter_action_2, default),
            ],
        ),
    )
    action_, streaming_async_container = await app.astream_result(halt_after=["counter_2"])
    results = [
        item async for item in streaming_async_container
    ]  # this should just have the intermediate results
    # results = list(streaming_container)
    assert len(results) == 10
    result, state = await streaming_async_container.get()
    assert result["count"] == state["count"] == 2
    assert state["tracker"] == [1, 2]
    assert len(action_tracker.pre_called) == 2
    assert len(action_tracker.post_called) == 2
    assert set(dict(action_tracker.pre_called).keys()) == {"counter", "counter_2"}
    assert set(dict(action_tracker.post_called).keys()) == {"counter", "counter_2"}
    assert [item["sequence_id"] for _, item in action_tracker.pre_called] == [
        0,
        1,
    ]  # ensure sequence ID is respected
    assert [item["sequence_id"] for _, item in action_tracker.post_called] == [
        0,
        1,
    ]  # ensure sequence ID is respected
    assert len(action_tracker.post_run_execute_calls) == 1
    assert len(action_tracker.pre_run_execute_calls) == 1

    # One call for streaming
    assert (
        len(stream_event_tracker.pre_start_stream_calls)
        == len(stream_event_tracker_sync.pre_start_stream_calls)
        == 1
    )
    # 10 streaming items
    assert (
        len(stream_event_tracker.post_stream_item_calls)
        == len(stream_event_tracker_sync.post_stream_item_calls)
        == 10
    )
    # One call for streaming
    assert (
        len(stream_event_tracker.post_end_stream_calls)
        == len(stream_event_tracker_sync.post_end_stream_calls)
        == 1
    )


def test_stream_result_halt_after_run_through_streaming():
    """Tests that we can pass through streaming results,
    fully realize them, then get to the streaming results at the end and return the stream"""
    action_tracker = CallCaptureTracker()
    stream_event_tracker = StreamEventCaptureTracker()
    counter_action = base_streaming_single_step_counter.with_name("counter")
    counter_action_2 = base_streaming_single_step_counter.with_name("counter_2")
    app = Application(
        state=State({"count": 0}),
        entrypoint="counter",
        adapter_set=LifecycleAdapterSet(action_tracker, stream_event_tracker),
        partition_key="test",
        uid="test-123",
        graph=Graph(
            actions=[counter_action, counter_action_2],
            transitions=[
                Transition(counter_action, counter_action_2, default),
            ],
        ),
    )
    action_, streaming_container = app.stream_result(halt_after=["counter_2"])
    results = list(streaming_container)
    assert len(results) == 10
    result, state = streaming_container.get()
    assert result["count"] == state["count"] == 2
    assert state["tracker"] == [1, 2]
    assert len(action_tracker.pre_called) == 2
    assert len(action_tracker.post_called) == 2
    assert set(dict(action_tracker.pre_called).keys()) == {"counter", "counter_2"}
    assert set(dict(action_tracker.post_called).keys()) == {"counter", "counter_2"}
    assert [item["sequence_id"] for _, item in action_tracker.pre_called] == [
        0,
        1,
    ]  # ensure sequence ID is respected
    assert [item["sequence_id"] for _, item in action_tracker.post_called] == [
        0,
        1,
    ]  # ensure sequence ID is respected
    assert len(action_tracker.post_run_execute_calls) == 1
    assert len(action_tracker.pre_run_execute_calls) == 1
    # One call for streaming
    assert len(stream_event_tracker.pre_start_stream_calls) == 1
    # 10 streaming items
    assert len(stream_event_tracker.post_stream_item_calls) == 10
    # One call for streaming
    assert len(stream_event_tracker.post_end_stream_calls) == 1


@pytest.mark.parametrize("exhaust_intermediate_generators", [True, False])
def test_stream_iterate(exhaust_intermediate_generators: bool):
    """Tests that we can pass through streaming results in streaming iterate. Note that this tests two cases:
    1. We exhaust the intermediate generators, and then call get() to get the final result
    2. We don't exhaust the intermediate generators, and then call get() to get the final result
    This ensures that the application effectively does it for us.
    """
    action_tracker = CallCaptureTracker()
    stream_event_tracker = StreamEventCaptureTracker()
    counter_action = base_streaming_single_step_counter.with_name("counter")
    counter_action_2 = base_streaming_single_step_counter.with_name("counter_2")
    app = Application(
        state=State({"count": 0}),
        entrypoint="counter",
        adapter_set=LifecycleAdapterSet(action_tracker, stream_event_tracker),
        partition_key="test",
        uid="test-123",
        graph=Graph(
            actions=[counter_action, counter_action_2],
            transitions=[
                Transition(counter_action, counter_action_2, default),
            ],
        ),
    )
    for _, streaming_container in app.stream_iterate(halt_after=["counter_2"]):
        if exhaust_intermediate_generators:
            results = list(streaming_container)
            assert len(results) == 10
    result, state = streaming_container.get()
    assert result["count"] == state["count"] == 2
    assert state["tracker"] == [1, 2]
    assert len(action_tracker.pre_called) == 2
    assert len(action_tracker.post_called) == 2
    assert set(dict(action_tracker.pre_called).keys()) == {"counter", "counter_2"}
    assert set(dict(action_tracker.post_called).keys()) == {"counter", "counter_2"}
    assert [item["sequence_id"] for _, item in action_tracker.pre_called] == [
        0,
        1,
    ]  # ensure sequence ID is respected
    assert [item["sequence_id"] for _, item in action_tracker.post_called] == [
        0,
        1,
    ]  # ensure sequence ID is respected

    assert len(stream_event_tracker.pre_start_stream_calls) == 2
    assert len(stream_event_tracker.post_end_stream_calls) == 2
    assert len(stream_event_tracker.post_stream_item_calls) == 20
    assert len(stream_event_tracker.post_stream_item_calls) == 20


@pytest.mark.asyncio
@pytest.mark.parametrize("exhaust_intermediate_generators", [True, False])
async def test_astream_iterate(exhaust_intermediate_generators: bool):
    """Tests that we can pass through streaming results in astream_iterate. Note that this tests two cases:
    1. We exhaust the intermediate generators, and then call get() to get the final result
    2. We don't exhaust the intermediate generators, and then call get() to get the final result
    This ensures that the application effectively does it for us.
    """
    action_tracker = CallCaptureTracker()
    stream_event_tracker = StreamEventCaptureTracker()
    counter_action = base_streaming_single_step_counter_async.with_name(
        "counter"
    )  # Use async action
    counter_action_2 = base_streaming_single_step_counter_async.with_name(
        "counter_2"
    )  # Use async action
    app = Application(
        state=State({"count": 0}),
        entrypoint="counter",
        adapter_set=LifecycleAdapterSet(action_tracker, stream_event_tracker),
        partition_key="test",
        uid="test-123",
        graph=Graph(
            actions=[counter_action, counter_action_2],
            transitions=[
                Transition(counter_action, counter_action_2, default),
            ],
        ),
    )
    streaming_container = None  # Define outside the loop to access later
    async for _, streaming_container in app.astream_iterate(halt_after=["counter_2"]):
        if exhaust_intermediate_generators:
            results = []
            async for item in streaming_container:  # Use async for
                results.append(item)
            assert len(results) == 10
    assert streaming_container is not None  # Ensure the loop ran
    result, state = await streaming_container.get()  # Use await
    assert result["count"] == state["count"] == 2
    assert state["tracker"] == [1, 2]
    assert len(action_tracker.pre_called) == 2
    assert len(action_tracker.post_called) == 2
    assert set(dict(action_tracker.pre_called).keys()) == {"counter", "counter_2"}
    assert set(dict(action_tracker.post_called).keys()) == {"counter", "counter_2"}
    assert [item["sequence_id"] for _, item in action_tracker.pre_called] == [
        0,
        1,
    ]  # ensure sequence ID is respected
    assert [item["sequence_id"] for _, item in action_tracker.post_called] == [
        0,
        1,
    ]  # ensure sequence ID is respected

    assert len(stream_event_tracker.pre_start_stream_calls) == 2
    assert len(stream_event_tracker.post_end_stream_calls) == 2
    assert len(stream_event_tracker.post_stream_item_calls) == 20


async def test_astream_result_halt_after_run_through_streaming():
    action_tracker = CallCaptureTracker()
    stream_event_tracker = StreamEventCaptureTrackerAsync()
    sync_stream_event_tracker = StreamEventCaptureTracker()

    counter_action = base_streaming_single_step_counter_async.with_name("counter")
    counter_action_2 = base_streaming_single_step_counter_async.with_name("counter_2")
    assert counter_action.is_async()
    assert counter_action_2.is_async()
    app = Application(
        state=State({"count": 0}),
        entrypoint="counter",
        adapter_set=LifecycleAdapterSet(
            action_tracker, stream_event_tracker, sync_stream_event_tracker
        ),
        partition_key="test",
        uid="test-123",
        graph=Graph(
            actions=[counter_action, counter_action_2],
            transitions=[
                Transition(counter_action, counter_action_2, default),
            ],
        ),
    )
    action_, streaming_container = await app.astream_result(halt_after=["counter_2"])
    results = [
        item async for item in streaming_container
    ]  # this should just have the intermediate results
    assert len(results) == 10
    result, state = await streaming_container.get()
    assert result["count"] == state["count"] == 2
    assert state["tracker"] == [1, 2]
    assert len(action_tracker.pre_called) == 2
    assert len(action_tracker.post_called) == 2
    assert set(dict(action_tracker.pre_called).keys()) == {"counter", "counter_2"}
    assert set(dict(action_tracker.post_called).keys()) == {"counter", "counter_2"}
    assert [item["sequence_id"] for _, item in action_tracker.pre_called] == [
        0,
        1,
    ]  # ensure sequence ID is respected
    assert [item["sequence_id"] for _, item in action_tracker.post_called] == [
        0,
        1,
    ]  # ensure sequence ID is respected
    assert len(action_tracker.post_run_execute_calls) == 1
    assert len(action_tracker.pre_run_execute_calls) == 1

    # One call for streaming
    assert (
        len(stream_event_tracker.pre_start_stream_calls)
        == len(sync_stream_event_tracker.pre_start_stream_calls)
        == 1
    )
    # 10 streaming items
    assert (
        len(stream_event_tracker.post_stream_item_calls)
        == len(sync_stream_event_tracker.post_stream_item_calls)
        == 10
    )
    # One call for streaming
    assert (
        len(stream_event_tracker.post_end_stream_calls)
        == len(sync_stream_event_tracker.post_end_stream_calls)
        == 1
    )


def test_stream_result_halt_after_run_through_non_streaming():
    """Tests what happens when we have an app that runs through non-streaming
    results before hitting a final streaming result specified by halt_after"""
    action_tracker = CallCaptureTracker()
    stream_event_tracker = StreamEventCaptureTracker()
    counter_non_streaming = base_counter_action.with_name("counter_non_streaming")
    counter_streaming = base_streaming_single_step_counter.with_name("counter_streaming")

    app = Application(
        state=State({"count": 0}),
        entrypoint="counter_non_streaming",
        adapter_set=LifecycleAdapterSet(action_tracker, stream_event_tracker),
        partition_key="test",
        uid="test-123",
        graph=Graph(
            actions=[counter_non_streaming, counter_streaming],
            transitions=[
                Transition(counter_non_streaming, counter_non_streaming, expr("count < 10")),
                Transition(counter_non_streaming, counter_streaming, default),
            ],
        ),
    )
    action_, streaming_container = app.stream_result(halt_after=["counter_streaming"])
    results = list(streaming_container)
    assert len(results) == 10
    result, state = streaming_container.get()
    assert result["count"] == state["count"] == 11
    assert len(action_tracker.pre_called) == 11
    assert len(action_tracker.post_called) == 11
    assert set(dict(action_tracker.pre_called).keys()) == {
        "counter_streaming",
        "counter_non_streaming",
    }
    assert set(dict(action_tracker.post_called).keys()) == {
        "counter_streaming",
        "counter_non_streaming",
    }
    assert [item["sequence_id"] for _, item in action_tracker.pre_called] == list(
        range(0, 11)
    )  # ensure sequence ID is respected
    assert [item["sequence_id"] for _, item in action_tracker.post_called] == list(
        range(0, 11)
    )  # ensure sequence ID is respected
    assert len(action_tracker.post_run_execute_calls) == 1
    assert len(action_tracker.pre_run_execute_calls) == 1

    # One call for streaming
    assert len(stream_event_tracker.pre_start_stream_calls) == 1
    # 10 streaming items
    assert len(stream_event_tracker.post_stream_item_calls) == 10
    # One call for streaming
    assert len(stream_event_tracker.post_end_stream_calls) == 1


async def test_astream_result_halt_after_run_through_non_streaming():
    action_tracker = CallCaptureTracker()
    stream_event_tracker = StreamEventCaptureTrackerAsync()
    stream_event_tracker_sync = StreamEventCaptureTracker()
    counter_non_streaming = base_counter_action_async.with_name("counter_non_streaming")
    counter_streaming = base_streaming_single_step_counter_async.with_name("counter_streaming")

    app = Application(
        state=State({"count": 0}),
        entrypoint="counter_non_streaming",
        adapter_set=LifecycleAdapterSet(
            action_tracker, stream_event_tracker, stream_event_tracker_sync
        ),
        partition_key="test",
        uid="test-123",
        graph=Graph(
            actions=[counter_non_streaming, counter_streaming],
            transitions=[
                Transition(counter_non_streaming, counter_non_streaming, expr("count < 10")),
                Transition(counter_non_streaming, counter_streaming, default),
            ],
        ),
    )
    action_, async_streaming_container = await app.astream_result(halt_after=["counter_streaming"])
    results = [
        item async for item in async_streaming_container
    ]  # this should just have the intermediate results
    assert len(results) == 10
    result, state = await async_streaming_container.get()
    assert result["count"] == state["count"] == 11
    assert len(action_tracker.pre_called) == 11
    assert len(action_tracker.post_called) == 11
    assert set(dict(action_tracker.pre_called).keys()) == {
        "counter_streaming",
        "counter_non_streaming",
    }
    assert set(dict(action_tracker.post_called).keys()) == {
        "counter_streaming",
        "counter_non_streaming",
    }
    assert [item["sequence_id"] for _, item in action_tracker.pre_called] == list(
        range(0, 11)
    )  # ensure sequence ID is respected
    assert [item["sequence_id"] for _, item in action_tracker.post_called] == list(
        range(0, 11)
    )  # ensure sequence ID is respected
    assert len(action_tracker.post_run_execute_calls) == 1
    assert len(action_tracker.pre_run_execute_calls) == 1

    # One call for streaming
    assert (
        len(stream_event_tracker.pre_start_stream_calls)
        == len(stream_event_tracker_sync.pre_start_stream_calls)
        == 1
    )
    # 10 streaming items
    assert (
        len(stream_event_tracker.post_stream_item_calls)
        == len(stream_event_tracker_sync.post_stream_item_calls)
        == 10
    )
    # One call for streaming
    assert (
        len(stream_event_tracker.post_end_stream_calls)
        == len(stream_event_tracker_sync.post_end_stream_calls)
        == 1
    )


def test_stream_result_halt_after_run_through_final_non_streaming():
    """Tests that we can pass through non-streaming results when streaming is called"""
    action_tracker = CallCaptureTracker()
    counter_non_streaming = base_counter_action.with_name("counter_non_streaming")
    counter_final_non_streaming = base_counter_action.with_name("counter_final_non_streaming")

    app = Application(
        state=State({"count": 0}),
        entrypoint="counter_non_streaming",
        adapter_set=LifecycleAdapterSet(action_tracker),
        partition_key="test",
        uid="test-123",
        graph=Graph(
            actions=[counter_non_streaming, counter_final_non_streaming],
            transitions=[
                Transition(counter_non_streaming, counter_non_streaming, expr("count < 10")),
                Transition(counter_non_streaming, counter_final_non_streaming, default),
            ],
        ),
    )
    action, streaming_container = app.stream_result(halt_after=["counter_final_non_streaming"])
    results = list(streaming_container)
    assert len(results) == 0  # nothing to steram
    result, state = streaming_container.get()
    assert result["count"] == state["count"] == 11
    assert len(action_tracker.pre_called) == 11
    assert len(action_tracker.post_called) == 11
    assert set(dict(action_tracker.pre_called).keys()) == {
        "counter_non_streaming",
        "counter_final_non_streaming",
    }
    assert set(dict(action_tracker.post_called).keys()) == {
        "counter_non_streaming",
        "counter_final_non_streaming",
    }
    assert [item["sequence_id"] for _, item in action_tracker.pre_called] == list(
        range(0, 11)
    )  # ensure sequence ID is respected
    assert [item["sequence_id"] for _, item in action_tracker.post_called] == list(
        range(0, 11)
    )  # ensure sequence ID is respected
    assert len(action_tracker.pre_run_execute_calls) == 1
    assert len(action_tracker.post_run_execute_calls) == 1


async def test_astream_result_halt_after_run_through_final_streaming():
    """Tests that we can pass through non-streaming results when streaming is called"""
    action_tracker = CallCaptureTracker()

    counter_non_streaming = base_counter_action_async.with_name("counter_non_streaming")
    counter_final_non_streaming = base_counter_action_async.with_name("counter_final_non_streaming")

    app = Application(
        state=State({"count": 0}),
        entrypoint="counter_non_streaming",
        adapter_set=LifecycleAdapterSet(action_tracker),
        partition_key="test",
        uid="test-123",
        graph=Graph(
            actions=[counter_non_streaming, counter_final_non_streaming],
            transitions=[
                Transition(counter_non_streaming, counter_non_streaming, expr("count < 10")),
                Transition(counter_non_streaming, counter_final_non_streaming, default),
            ],
        ),
    )
    action, streaming_container = await app.astream_result(
        halt_after=["counter_final_non_streaming"]
    )
    results = [
        item async for item in streaming_container
    ]  # this should just have the intermediate results
    assert len(results) == 0  # nothing to stream
    result, state = await streaming_container.get()
    assert result["count"] == state["count"] == 11
    assert len(action_tracker.pre_called) == 11
    assert len(action_tracker.post_called) == 11
    assert set(dict(action_tracker.pre_called).keys()) == {
        "counter_non_streaming",
        "counter_final_non_streaming",
    }
    assert set(dict(action_tracker.post_called).keys()) == {
        "counter_non_streaming",
        "counter_final_non_streaming",
    }
    assert [item["sequence_id"] for _, item in action_tracker.pre_called] == list(
        range(0, 11)
    )  # ensure sequence ID is respected
    assert [item["sequence_id"] for _, item in action_tracker.post_called] == list(
        range(0, 11)
    )  # ensure sequence ID is respected
    assert len(action_tracker.post_run_execute_calls) == 1
    assert len(action_tracker.pre_run_execute_calls) == 1


def test_stream_result_halt_before():
    action_tracker = CallCaptureTracker()
    counter_non_streaming = base_counter_action.with_name("counter_non_streaming")
    counter_streaming = base_streaming_single_step_counter.with_name("counter_final")

    app = Application(
        state=State({"count": 0}),
        entrypoint="counter_non_streaming",
        partition_key="test",
        uid="test-123",
        adapter_set=LifecycleAdapterSet(action_tracker),
        graph=Graph(
            actions=[counter_non_streaming, counter_streaming],
            transitions=[
                Transition(counter_non_streaming, counter_non_streaming, expr("count < 10")),
                Transition(counter_non_streaming, counter_streaming, default),
            ],
        ),
    )
    action, streaming_container = app.stream_result(halt_after=[], halt_before=["counter_final"])
    results = list(streaming_container)
    assert len(results) == 0  # nothing to steram
    result, state = streaming_container.get()
    assert action.name == "counter_final"  # halt before this one
    assert result is None
    assert state["count"] == 10
    assert [item["sequence_id"] for _, item in action_tracker.pre_called] == list(
        range(0, 10)
    )  # ensure sequence ID is respected
    assert [item["sequence_id"] for _, item in action_tracker.post_called] == list(
        range(0, 10)
    )  # ensure sequence ID is respected
    assert len(action_tracker.post_run_execute_calls) == 1
    assert len(action_tracker.pre_run_execute_calls) == 1


async def test_astream_result_halt_before():
    action_tracker = CallCaptureTracker()
    counter_non_streaming = base_counter_action_async.with_name("counter_non_streaming")
    counter_streaming = base_streaming_single_step_counter_async.with_name("counter_final")

    app = Application(
        state=State({"count": 0}),
        entrypoint="counter_non_streaming",
        partition_key="test",
        uid="test-123",
        adapter_set=LifecycleAdapterSet(action_tracker),
        graph=Graph(
            actions=[counter_non_streaming, counter_streaming],
            transitions=[
                Transition(counter_non_streaming, counter_non_streaming, expr("count < 10")),
                Transition(counter_non_streaming, counter_streaming, default),
            ],
        ),
    )
    action, streaming_container = await app.astream_result(
        halt_after=[], halt_before=["counter_final"]
    )
    results = [
        item async for item in streaming_container
    ]  # this should just have the intermediate results
    assert len(results) == 0  # nothing to stream
    result, state = await streaming_container.get()
    assert action.name == "counter_final"  # halt before this one
    assert result is None
    assert state["count"] == 10
    assert [item["sequence_id"] for _, item in action_tracker.pre_called] == list(
        range(0, 10)
    )  # ensure sequence ID is respected
    assert [item["sequence_id"] for _, item in action_tracker.post_called] == list(
        range(0, 10)
    )  # ensure sequence ID is respected
    assert len(action_tracker.post_run_execute_calls) == 1
    assert len(action_tracker.pre_run_execute_calls) == 1


def test_app_set_state():
    counter_action = base_counter_action.with_name("counter")
    app = Application(
        state=State(),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action],
            transitions=[Transition(counter_action, counter_action, default)],
        ),
    )
    assert "counter" not in app.state  # initial value
    app.step()
    assert app.state["count"] == 1  # updated value
    state = app.state
    app.update_state(state.update(count=2))
    assert app.state["count"] == 2  # updated value


def test_app_get_next_step():
    counter_action_1 = base_counter_action.with_name("counter_1")
    counter_action_2 = base_counter_action.with_name("counter_2")
    counter_action_3 = base_counter_action.with_name("counter_3")
    app = Application(
        state=State(),
        entrypoint="counter_1",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action_1, counter_action_2, counter_action_3],
            transitions=[
                Transition(counter_action_1, counter_action_2, default),
                Transition(counter_action_2, counter_action_3, default),
                Transition(counter_action_3, counter_action_1, default),
            ],
        ),
    )
    # uninitialized -- counter_1
    assert app.get_next_action().name == "counter_1"
    app.step()
    # ran counter_1 -- counter_2
    assert app.get_next_action().name == "counter_2"
    app.step()
    # ran counter_2 -- counter_3
    assert app.get_next_action().name == "counter_3"
    app.step()
    # ran counter_3 -- back to counter_1
    assert app.get_next_action().name == "counter_1"


def test_application_builder_complete():
    app = (
        ApplicationBuilder()
        .with_state(count=0)
        .with_actions(counter=base_counter_action, result=Result("count"))
        .with_transitions(
            ("counter", "counter", Condition.expr("count < 10")), ("counter", "result")
        )
        .with_entrypoint("counter")
        .build()
    )
    graph = app.graph
    assert len(graph.actions) == 2
    assert len(graph.transitions) == 2
    assert app.get_next_action().name == "counter"


def test__validate_start_valid():
    _validate_start("counter", {"counter", "result"})


def test__validate_start_not_found():
    with pytest.raises(ValueError, match="not found"):
        _validate_start("counter", {"result"})


def test__adjust_single_step_output_result_and_state():
    state = State({"count": 1})
    result = {"count": 1}
    assert _adjust_single_step_output((result, state), "test_action", DEFAULT_SCHEMA) == (
        result,
        state,
    )


def test__adjust_single_step_output_just_state():
    state = State({"count": 1})
    assert _adjust_single_step_output(state, "test_action", DEFAULT_SCHEMA) == ({}, state)


def test__adjust_single_step_output_errors_incorrect_type():
    state = "foo"
    with pytest.raises(ValueError, match="must return either"):
        _adjust_single_step_output(state, "test_action", DEFAULT_SCHEMA)


def test__adjust_single_step_output_errors_incorrect_result_type():
    state = State()
    result = "bar"
    with pytest.raises(ValueError, match="non-dict"):
        _adjust_single_step_output((state, result), "test_action", DEFAULT_SCHEMA)


def test_application_builder_unset():
    with pytest.raises(ValueError):
        ApplicationBuilder().build()


def test_application_run_step_hooks_sync():
    action_tracker = CallCaptureTracker()
    counter_action = base_counter_action.with_name("counter")
    result_action = Result("count").with_name("result")
    app = Application(
        state=State({}),
        entrypoint="counter",
        adapter_set=internal.LifecycleAdapterSet(action_tracker),
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, result_action, Condition.expr("count >= 10")),
                Transition(counter_action, counter_action, default),
            ],
        ),
    )
    app.run(halt_after=["result"])
    assert set(dict(action_tracker.pre_called).keys()) == {"counter", "result"}
    assert set(dict(action_tracker.post_called).keys()) == {"counter", "result"}
    # assert sequence id is incremented
    assert action_tracker.pre_called[0][1]["sequence_id"] == 1
    assert action_tracker.post_called[0][1]["sequence_id"] == 1
    assert {
        "action",
        "sequence_id",
        "state",
        "inputs",
        "app_id",
        "partition_key",
    }.issubset(set(action_tracker.pre_called[0][1].keys()))
    assert {
        "sequence_id",
        "result",
        "state",
        "exception",
        "app_id",
        "partition_key",
    }.issubset(set(action_tracker.post_called[0][1].keys()))
    assert len(action_tracker.pre_called) == 11
    assert len(action_tracker.post_called) == 11
    # quick inclusion to ensure that the action is not called when we're done running
    assert len(action_tracker.post_run_execute_calls) == 1
    assert len(action_tracker.pre_run_execute_calls) == 1
    assert app.step() is None  # should be None
    assert len(action_tracker.post_run_execute_calls) == 2
    assert len(action_tracker.pre_run_execute_calls) == 2


async def test_application_run_step_hooks_async():
    tracker = ActionTrackerAsync()
    counter_action = base_counter_action.with_name("counter")
    result_action = Result("count").with_name("result")
    app = Application(
        state=State({}),
        entrypoint="counter",
        adapter_set=internal.LifecycleAdapterSet(tracker),
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, result_action, Condition.expr("count >= 10")),
                Transition(counter_action, counter_action, default),
            ],
        ),
    )
    await app.arun(halt_after=["result"])
    assert set(dict(tracker.pre_called).keys()) == {"counter", "result"}
    assert set(dict(tracker.post_called).keys()) == {"counter", "result"}
    # assert sequence id is incremented
    assert tracker.pre_called[0][1]["sequence_id"] == 1
    assert tracker.post_called[0][1]["sequence_id"] == 1
    assert {
        "sequence_id",
        "state",
        "inputs",
        "app_id",
        "partition_key",
    }.issubset(set(tracker.pre_called[0][1].keys()))
    assert {
        "sequence_id",
        "result",
        "state",
        "exception",
        "app_id",
        "partition_key",
    }.issubset(set(tracker.post_called[0][1].keys()))
    assert len(tracker.pre_called) == 11
    assert len(tracker.post_called) == 11


async def test_application_run_step_runs_hooks():
    hooks = [CallCaptureTracker(), ActionTrackerAsync()]

    counter_action = base_counter_action.with_name("counter")
    app = Application(
        state=State({}),
        entrypoint="counter",
        adapter_set=internal.LifecycleAdapterSet(*hooks),
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action],
            transitions=[
                Transition(counter_action, counter_action, default),
            ],
        ),
    )
    await app.astep()
    assert len(hooks[0].pre_called) == 1
    assert len(hooks[0].post_called) == 1
    # assert sequence id is incremented
    assert hooks[0].pre_called[0][1]["sequence_id"] == 1
    assert hooks[0].post_called[0][1]["sequence_id"] == 1
    assert {
        "sequence_id",
        "state",
        "inputs",
        "app_id",
        "partition_key",
        "action",
    }.issubset(set(hooks[0].pre_called[0][1].keys()))
    assert {
        "sequence_id",
        "state",
        "inputs",
        "app_id",
        "partition_key",
    }.issubset(set(hooks[1].pre_called[0][1].keys()))
    assert {
        "sequence_id",
        "result",
        "state",
        "exception",
        "app_id",
        "partition_key",
    }.issubset(set(hooks[0].post_called[0][1].keys()))
    assert {
        "sequence_id",
        "result",
        "state",
        "exception",
        "app_id",
        "partition_key",
    }.issubset(set(hooks[1].post_called[0][1].keys()))
    assert len(hooks[1].pre_called) == 1
    assert len(hooks[1].post_called) == 1
    assert len(hooks[0].post_run_execute_calls) == 1
    assert len(hooks[0].pre_run_execute_calls) == 1


def test_application_post_application_create_hook():
    class PostApplicationCreateTracker(PostApplicationCreateHook):
        def __init__(self):
            self.called_args = None
            self.call_count = 0

        def post_application_create(self, **kwargs):
            self.called_args = kwargs
            self.call_count += 1

    tracker = PostApplicationCreateTracker()
    counter_action = base_counter_action.with_name("counter")
    result_action = Result("count").with_name("result")
    Application(
        state=State({}),
        entrypoint="counter",
        adapter_set=internal.LifecycleAdapterSet(tracker),
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, result_action, Condition.expr("count >= 10")),
                Transition(counter_action, counter_action, default),
            ],
        ),
    )
    assert "state" in tracker.called_args
    assert "application_graph" in tracker.called_args
    assert tracker.call_count == 1


async def test_application_gives_graph():
    counter_action = base_counter_action.with_name("counter")
    result_action = Result("count").with_name("result")
    app = Application(
        state=State({}),
        entrypoint="counter",
        partition_key="test",
        uid="test-123",
        sequence_id=0,
        graph=Graph(
            actions=[counter_action, result_action],
            transitions=[
                Transition(counter_action, result_action, Condition.expr("count >= 10")),
                Transition(counter_action, counter_action, default),
            ],
        ),
    )
    graph = app.graph
    assert len(graph.actions) == 2
    assert len(graph.transitions) == 2
    assert graph.entrypoint.name == "counter"


def test_application_builder_initialize_does_not_allow_state_setting():
    with pytest.raises(ValueError, match="Cannot call initialize_from"):
        ApplicationBuilder().with_entrypoint("foo").with_state(**{"foo": "bar"}).initialize_from(
            DevNullPersister(),
            resume_at_next_action=True,
            default_state={},
            default_entrypoint="foo",
        )


class BrokenPersister(BaseStatePersister):
    """Broken persistor."""

    def load(
        self, partition_key: str, app_id: Optional[str], sequence_id: Optional[int] = None, **kwargs
    ) -> Optional[PersistedStateData]:
        return dict(
            partition_key="key",
            app_id="id",
            sequence_id=0,
            position="foo",
            state=None,
            created_at="",
            status="completed",
        )

    def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        return []

    def save(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int,
        position: str,
        state: State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        return


def test_application_builder_initialize_raises_on_broken_persistor():
    """Persisters should return None when there is no state to be loaded and the default used."""
    counter_action = base_counter_action.with_name("counter")
    result_action = Result("count").with_name("result")
    with pytest.raises(ValueError, match="but value for state was None"):
        (
            ApplicationBuilder()
            .with_actions(counter_action, result_action)
            .with_transitions(("counter", "result", default))
            .initialize_from(
                BrokenPersister(),
                resume_at_next_action=True,
                default_state={},
                default_entrypoint="foo",
            )
            .build()
        )


def test_load_from_sync_cannot_have_async_persistor_error():
    builder = ApplicationBuilder()
    builder.initialize_from(
        AsyncDevNullPersister(),
        resume_at_next_action=True,
        default_state={},
        default_entrypoint="foo",
    )
    with pytest.raises(
        ValueError, match="are building the sync application, but have used an async initializer."
    ):
        # we have not initialized
        builder._load_from_sync_persister()


async def test_load_from_async_cannot_have_sync_persistor_error():
    await asyncio.sleep(0.00001)
    builder = ApplicationBuilder()
    builder.initialize_from(
        DevNullPersister(),
        resume_at_next_action=True,
        default_state={},
        default_entrypoint="foo",
    )
    with pytest.raises(
        ValueError, match="are building the async application, but have used an sync initializer."
    ):
        # we have not initialized
        await builder._load_from_async_persister()


def test_application_builder_assigns_correct_actions_with_dual_api():
    counter_action = base_counter_action.with_name("counter")
    result_action = Result("count")

    @action(reads=[], writes=[])
    def test_action(state: State) -> State:
        return state

    app = (
        ApplicationBuilder()
        .with_state(count=0)
        .with_actions(counter_action, test_action, result=result_action)
        .with_transitions()
        .with_entrypoint("counter")
        .build()
    )
    graph = app.graph
    assert {a.name for a in graph.actions} == {"counter", "result", "test_action"}


def test__validate_halt_conditions():
    counter_action = base_counter_action.with_name("counter")
    result_action = Result("count")

    @action(reads=[], writes=[])
    def test_action(state: State) -> State:
        return state

    app = (
        ApplicationBuilder()
        .with_state(count=0)
        .with_actions(counter_action, test_action, result=result_action)
        .with_transitions()
        .with_entrypoint("counter")
        .build()
    )
    with pytest.raises(ValueError, match="(?=.*no_exist_1)(?=.*no_exist_2)"):
        app._validate_halt_conditions(halt_after=["no_exist_1"], halt_before=["no_exist_2"])


def test_application_builder_initialize_raises_on_fork_app_id_not_provided():
    """Can't pass in fork_from* without an app_id."""
    with pytest.raises(ValueError, match="If you set fork_from_partition_key"):
        counter_action = base_counter_action.with_name("counter")
        result_action = Result("count").with_name("result")
        (
            ApplicationBuilder()
            .with_actions(counter_action, result_action)
            .with_transitions(("counter", "result", default))
            .initialize_from(
                BrokenPersister(),
                resume_at_next_action=True,
                default_state={},
                default_entrypoint="foo",
                fork_from_sequence_id=1,
                fork_from_partition_key="foo-bar",
            )
            .build()
        )


class DummyPersister(BaseStatePersister):
    """Dummy persistor."""

    def load(
        self, partition_key: str, app_id: Optional[str], sequence_id: Optional[int] = None, **kwargs
    ) -> Optional[PersistedStateData]:
        return PersistedStateData(
            partition_key="user123",
            app_id="123",
            sequence_id=5,
            position="counter",
            state=State({"count": 5}),
            created_at="",
            status="completed",
        )

    def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        return ["123"]

    def save(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int,
        position: str,
        state: State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        return


def test_application_builder_initialize_fork_errors_on_same_app_id():
    """Tests that we can't have an app_id and fork_from_app_id that's the same"""
    with pytest.raises(ValueError, match="Cannot fork and save"):
        counter_action = base_counter_action.with_name("counter")
        result_action = Result("count").with_name("result")
        (
            ApplicationBuilder()
            .with_actions(counter_action, result_action)
            .with_transitions(("counter", "result", default))
            .initialize_from(
                DummyPersister(),
                resume_at_next_action=True,
                default_state={},
                default_entrypoint="foo",
                fork_from_app_id="123",
                fork_from_partition_key="user123",
            )
            .with_identifiers(app_id="123")
            .build()
        )


def test_application_builder_initialize_fork_app_id_happy_pth():
    """Tests that forking properly works"""
    counter_action = base_counter_action.with_name("counter")
    result_action = Result("count").with_name("result")
    old_app_id = "123"
    app = (
        ApplicationBuilder()
        .with_actions(counter_action, result_action)
        .with_transitions(("counter", "result", default))
        .initialize_from(
            DummyPersister(),
            resume_at_next_action=True,
            default_state={},
            default_entrypoint="counter",
            fork_from_app_id=old_app_id,
            fork_from_partition_key="user123",
        )
        .with_identifiers(app_id="test123")
        .build()
    )
    assert app.uid != old_app_id
    assert app.state == State({"count": 5, "__PRIOR_STEP": "counter", "__SEQUENCE_ID": 5})
    assert app.parent_pointer.app_id == old_app_id


class NoOpTracker(SyncTrackingClient):
    def copy(self):
        pass

    def pre_start_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass

    def post_stream_item(
        self,
        *,
        item: Any,
        item_index: int,
        stream_initialize_time: datetime.datetime,
        first_stream_item_start_time: datetime.datetime,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass

    def post_end_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass

    def do_log_attributes(self, **future_kwargs: Any):
        pass

    def __init__(self, unique_id: str):
        self.unique_id = unique_id

    def post_application_create(self, **future_kwargs: Any):
        pass

    def pre_run_step(self, **future_kwargs: Any):
        pass

    def post_run_step(self, **future_kwargs: Any):
        pass

    def pre_start_span(self, **future_kwargs: Any):
        pass

    def post_end_span(self, **future_kwargs: Any):
        pass


def test_application_exposes_app_context():
    """Tests that we can get the context from the application correctly"""
    counter_action = base_counter_action.with_name("counter")
    result_action = Result("count").with_name("result")
    app = (
        ApplicationBuilder()
        .with_actions(counter_action, result_action)
        .with_transitions(("counter", "result", default))
        .with_tracker(NoOpTracker("unique_tracker_name"))
        .with_identifiers(app_id="test123", partition_key="user123", sequence_id=5)
        .with_entrypoint("counter")
        .with_state(count=0)
        .build()
    )
    context = app.context
    assert context.app_id == "test123"
    assert context.partition_key == "user123"
    assert context.sequence_id == 5
    assert context.tracker.unique_id == "unique_tracker_name"


def test_application_exposes_app_context_through_context_manager_sync():
    """Tests that we can get the context from the application correctly"""

    @action(reads=["count"], writes=["count"])
    def counter(state: State) -> State:
        app_context = ApplicationContext.get()
        assert app_context is not None
        assert app_context.tracker is not None  # NoOpTracker is used
        assert isinstance(app_context.sequence_id, int)
        assert app_context.app_id == "test123"
        return state.update(count=state["count"] + 1)

    result_action = Result("count").with_name("result")
    app = (
        ApplicationBuilder()
        .with_actions(result_action, counter=counter)
        .with_transitions(("counter", "result", default))
        .with_tracker(NoOpTracker("unique_tracker_name"))
        .with_identifiers(app_id="test123", partition_key="user123", sequence_id=5)
        .with_entrypoint("counter")
        .with_state(count=0)
        .build()
    )
    app.run(halt_after=["result"])


async def test_application_exposes_app_context_through_context_manager_async():
    """Tests that we can get the context from the application correctly"""

    @action(reads=["count"], writes=["count"])
    async def counter(state: State) -> State:
        app_context = ApplicationContext.get()
        assert app_context is not None
        assert app_context.tracker is not None  # NoOpTracker is used
        assert isinstance(app_context.sequence_id, int)
        assert app_context.app_id == "test123"
        return state.update(count=state["count"] + 1)

    result_action = Result("count").with_name("result")
    app = (
        ApplicationBuilder()
        .with_actions(result_action, counter=counter)
        .with_transitions(("counter", "result", default))
        .with_tracker(NoOpTracker("unique_tracker_name"))
        .with_identifiers(app_id="test123", partition_key="user123", sequence_id=5)
        .with_entrypoint("counter")
        .with_state(count=0)
        .build()
    )
    await app.arun(halt_after=["result"])


def test_application_passes_context_when_declared():
    """Tests that the context is passed to the function correctly"""
    context_list = []

    @action(reads=["count"], writes=["count"])
    def context_counter(state: State, __context: ApplicationContext) -> State:
        context_list.append(__context)
        return state.update(count=state["count"] + 1)

    result_action = Result("count")
    app = (
        ApplicationBuilder()
        .with_actions(counter=context_counter, result=result_action)
        .with_transitions(("counter", "counter", expr("count < 10")), ("counter", "result"))
        .with_tracker(NoOpTracker("unique_tracker_name"))
        .with_identifiers(app_id="test123", partition_key="user123", sequence_id=5)
        .with_entrypoint("counter")
        .with_state(count=0)
        .build()
    )
    app.run(halt_after=["result"])
    sequence_ids = [context.sequence_id for context in context_list]
    assert sequence_ids == list(range(6, 16))
    app_ids = set(context.app_id for context in context_list)
    assert app_ids == {"test123"}
    trackers = set(context.tracker.unique_id for context in context_list)
    assert trackers == {"unique_tracker_name"}


def test_optional_context_in_dependency_factories():
    """Tests that the context is passed to the function correctly when nulled out.
    TODO -- get this to test without instantiating an application through the builder --
    this is slightly overkill for a bit of code"""
    context_list = []

    @action(reads=["count"], writes=["count"])
    def context_counter(state: State, __context: ApplicationContext = None) -> State:
        context_list.append(__context)
        return state.update(count=state["count"] + 1)

    result_action = Result("count")
    app = (
        ApplicationBuilder()
        .with_actions(counter=context_counter, result=result_action)
        .with_transitions(("counter", "counter", expr("count < 10")), ("counter", "result"))
        .with_tracker(NoOpTracker("unique_tracker_name"))
        .with_identifiers(app_id="test123", partition_key="user123", sequence_id=5)
        .with_entrypoint("counter")
        .with_state(count=0)
        .build()
    )
    inputs = app._process_inputs({}, app.get_next_action())
    assert "__context" in inputs  # it should be there
    assert inputs["__context"] is not None  # it should not be None
    assert inputs["__context"].app_id == "test123"  # it should be the correct context
    assert (
        inputs["__context"].tracker.unique_id == "unique_tracker_name"
    )  # it should be the correct context


def test_application_with_no_spawning_parent():
    """Test that the application does not have a spawning parent when it is not specified"""
    counter_action = base_counter_action.with_name("counter")
    result_action = Result("count").with_name("result")
    app = (
        ApplicationBuilder()
        .with_actions(counter_action, result_action)
        .with_transitions(("counter", "result", default))
        .with_identifiers(app_id="test123", partition_key="user123", sequence_id=5)
        .with_entrypoint("counter")
        .with_state(count=0)
        .build()
    )
    spawned_by = app.spawning_parent_pointer
    assert spawned_by is None


def test_application_with_spawning_parent():
    """Tests that the application builder can specify a spawning
    parent and it gets wired through to the app."""
    counter_action = base_counter_action.with_name("counter")
    result_action = Result("count").with_name("result")
    app = (
        ApplicationBuilder()
        .with_actions(counter_action, result_action)
        .with_transitions(("counter", "result", default))
        .with_identifiers(app_id="test123", partition_key="user123")
        .with_spawning_parent(app_id="test123", partition_key="user123", sequence_id=5)
        .with_entrypoint("counter")
        .with_state(count=0)
        .build()
    )
    spawned_by = app.spawning_parent_pointer
    assert spawned_by is not None
    assert spawned_by.app_id == "test123"
    assert spawned_by.partition_key == "user123"
    assert spawned_by.sequence_id == 5


def test_application_does_not_allow_dunderscore_inputs():
    """Tests that the context is passed to the function correctly when nulled out.
    TODO -- get this to test without instantiating an application through the builder --
    this is slightly overkill for a bit of code"""
    result_action = Result("count").with_name("result")
    app = (
        ApplicationBuilder()
        .with_actions(result_action)
        .with_entrypoint("result")
        .with_transitions()
        .with_state(count=0)
        .build()
    )
    with pytest.raises(ValueError, match="double underscore"):
        app._process_inputs({"__not_allowed": ...}, app.get_next_action())


def test_application_recursive_action_lifecycle_hooks():
    """Tests that calling burr within burr works as expected"""

    class TestingHook(PreApplicationExecuteCallHook, PostApplicationExecuteCallHook):
        def __init__(self):
            self.pre_called = []
            self.post_called = []

        def pre_run_execute_call(
            self,
            *,
            app_id: str,
            partition_key: str,
            state: "State",
            method: ExecuteMethod,
            **future_kwargs: Any,
        ):
            self.pre_called.append((app_id, partition_key, state, method))

        def post_run_execute_call(
            self,
            *,
            app_id: str,
            partition_key: str,
            state: "State",
            method: ExecuteMethod,
            exception: Optional[Exception],
            **future_kwargs,
        ):
            self.post_called.append((app_id, partition_key, state, method, exception))

    hook = TestingHook()
    foo = []

    @action(reads=["recursion_count", "total_count"], writes=["recursion_count", "total_count"])
    def recursive_action(state: State) -> State:
        foo.append(1)
        recursion_count = state["recursion_count"]
        if recursion_count == 5:
            return state
        # Fork bomb!
        total_counts = []
        for i in range(2):
            app = (
                ApplicationBuilder()
                .with_graph(graph)
                .with_hooks(hook)
                .with_entrypoint("recursive_action")
                .with_state(recursion_count=recursion_count + 1, total_count=1)
                .build()
            )
            action, result, state = app.run(halt_after=["recursive_action"])
            total_counts.append(state["total_count"])
        return state.update(
            recursion_count=state["recursion_count"], total_count=sum(total_counts) + 1
        )

    graph = GraphBuilder().with_actions(recursive_action).with_transitions().build()

    # initial to kick it off
    result = recursive_action(State({"recursion_count": 0, "total_count": 0}))
    # Basic sanity checks to demonstrate
    assert result["recursion_count"] == 5
    assert result["total_count"] == 63  # One for each of the calls (sum(2**n for n in range(6)))
    assert (
        len(hook.pre_called) == 62
    )  # 63 - the initial one from the call to recursive_action outside the application
    assert len(hook.post_called) == 62  # ditto


class CounterState(State):
    count: int


class SimpleTypingSystem(TypingSystem[CounterState]):
    def state_type(self) -> type[CounterState]:
        return CounterState

    def state_pre_action_run_type(self, action: Action, graph: Graph) -> type[Any]:
        raise NotImplementedError

    def state_post_action_run_type(self, action: Action, graph: Graph) -> type[Any]:
        raise NotImplementedError

    def construct_data(self, state: State[Any]) -> CounterState:
        return CounterState({"count": state["count"]})

    def construct_state(self, data: Any) -> State[Any]:
        raise NotImplementedError


def test_builder_captures_typing_system():
    """Tests that the typing system is captured correctly"""
    counter_action = base_counter_action.with_name("counter")
    result_action = Result("count").with_name("result")
    app = (
        ApplicationBuilder()
        .with_actions(counter_action, result_action)
        .with_transitions(("counter", "counter", expr("count < 10")))
        .with_transitions(("counter", "result", default))
        .with_entrypoint("counter")
        .with_state(count=0)
        .with_typing(SimpleTypingSystem())
        .build()
    )
    assert isinstance(app.state.data, CounterState)
    _, _, state = app.run(halt_after=["result"])
    assert isinstance(state.data, CounterState)
    assert state.data["count"] == 10


def test_set_sync_state_persister_cannot_have_async_error():
    builder = ApplicationBuilder()
    persister = AsyncDevNullPersister()
    builder.with_state_persister(persister)
    with pytest.raises(
        ValueError, match="are building the sync application, but have used an async persister."
    ):
        # we have not initialized
        builder._set_sync_state_persister()


def test_set_sync_state_persister_is_not_initialized_error(tmp_path):
    builder = ApplicationBuilder()
    persister = SQLLitePersister(db_path=":memory:", table_name="test_table")
    builder.with_state_persister(persister)
    with pytest.raises(RuntimeError):
        # we have not initialized
        builder._set_sync_state_persister()


async def test_set_async_state_persister_cannot_have_sync_error():
    await asyncio.sleep(0.00001)
    builder = ApplicationBuilder()
    persister = DevNullPersister()
    builder.with_state_persister(persister)
    with pytest.raises(
        ValueError, match="are building the async application, but have used an sync persister."
    ):
        # we have not initialized
        await builder._set_async_state_persister()


async def test_set_async_state_persister_is_not_initialized_error(tmp_path):
    await asyncio.sleep(0.00001)
    builder = ApplicationBuilder()

    class FakePersister(AsyncDevNullPersister):
        async def is_initialized(self):
            return False

    persister = FakePersister()
    builder.with_state_persister(persister)
    with pytest.raises(RuntimeError):
        # we have not initialized
        await builder._set_async_state_persister()


def test_with_state_persister_is_initialized_not_implemented():
    builder = ApplicationBuilder()

    class FakePersister(BaseStatePersister):
        # does not implement is_initialized
        def list_app_ids(self):
            return []

        def save(
            self,
            partition_key: Optional[str],
            app_id: str,
            sequence_id: int,
            position: str,
            state: State,
            status: Literal["completed", "failed"],
            **kwargs,
        ):
            pass

        def load(
            self,
            partition_key: str,
            app_id: Optional[str],
            sequence_id: Optional[int] = None,
            **kwargs,
        ):
            return None

    persister = FakePersister()
    # Add the persister to the builder, expecting no exceptions
    builder.with_state_persister(persister)


class ActionWithoutContext(Action):
    def run(self, other_param, foo):
        pass

    @property
    def reads(self) -> list[str]:
        pass

    @property
    def writes(self) -> list[str]:
        pass

    def update(self, result: dict, state: State) -> State:
        pass

    def inputs(self) -> Union[list[str], tuple[list[str], list[str]]]:
        return ["other_param", "foo"]


class ActionWithContext(ActionWithoutContext):
    def run(self, __context, other_param, foo):
        pass

    def inputs(self) -> Union[list[str], tuple[list[str], list[str]]]:
        return ["other_param", "foo", "__context"]


class ActionWithKwargs(ActionWithoutContext):
    def run(self, other_param, foo, **kwargs):
        pass

    def inputs(self) -> Union[list[str], tuple[list[str], list[str]]]:
        return ["other_param", "foo", "__context"]


class ActionWithContextTracer(ActionWithoutContext):
    def run(self, __context, other_param, foo, __tracer):
        pass

    def inputs(self) -> Union[list[str], tuple[list[str], list[str]]]:
        return ["other_param", "foo", "__context", "__tracer"]


def test_remap_context_variable_with_mangled_context_kwargs():
    _action = ActionWithKwargs()

    inputs = {"__context": "context_value", "other_key": "other_value", "foo": "foo_value"}
    expected = {"__context": "context_value", "other_key": "other_value", "foo": "foo_value"}
    assert _remap_dunder_parameters(_action.run, inputs, ["__context", "__tracer"]) == expected


def test_remap_context_variable_with_mangled_context():
    _action = ActionWithContext()

    inputs = {"__context": "context_value", "other_key": "other_value", "foo": "foo_value"}
    expected = {
        f"_{ActionWithContext.__name__}__context": "context_value",
        "other_key": "other_value",
        "foo": "foo_value",
    }
    assert _remap_dunder_parameters(_action.run, inputs, ["__context", "__tracer"]) == expected


def test_remap_context_variable_with_mangled_contexttracer():
    _action = ActionWithContextTracer()

    inputs = {
        "__context": "context_value",
        "__tracer": "tracer_value",
        "other_key": "other_value",
        "foo": "foo_value",
    }
    expected = {
        f"_{ActionWithContextTracer.__name__}__context": "context_value",
        "other_key": "other_value",
        "foo": "foo_value",
        f"_{ActionWithContextTracer.__name__}__tracer": "tracer_value",
    }
    assert _remap_dunder_parameters(_action.run, inputs, ["__context", "__tracer"]) == expected


def test_remap_context_variable_without_mangled_context():
    _action = ActionWithoutContext()
    inputs = {"__context": "context_value", "other_key": "other_value", "foo": "foo_value"}
    expected = {"__context": "context_value", "other_key": "other_value", "foo": "foo_value"}
    assert _remap_dunder_parameters(_action.run, inputs, ["__context", "__tracer"]) == expected


async def test_async_application_builder_initialize_raises_on_broken_persistor():
    """Persisters should return None when there is no state to be loaded and the default used."""
    await asyncio.sleep(0.00001)
    counter_action = base_counter_action_async.with_name("counter")
    result_action = Result("count").with_name("result")

    class AsyncBrokenPersister(AsyncDevNullPersister):
        async def load(
            self,
            partition_key: str,
            app_id: Optional[str],
            sequence_id: Optional[int] = None,
            **kwargs,
        ) -> Optional[PersistedStateData]:
            await asyncio.sleep(0.0001)
            return dict(
                partition_key="key",
                app_id="id",
                sequence_id=0,
                position="foo",
                state=None,
                created_at="",
                status="completed",
            )

    with pytest.raises(ValueError, match="but value for state was None"):
        await (
            ApplicationBuilder()
            .with_actions(counter_action, result_action)
            .with_transitions(("counter", "result", default))
            .initialize_from(
                AsyncBrokenPersister(),
                resume_at_next_action=True,
                default_state={},
                default_entrypoint="foo",
            )
            .abuild()
        )


def test_application__process_control_flow_params():
    @action(reads=[], writes=[], tags=["tag1", "tag2"])
    def test_action(state: State) -> State:
        return state

    @action(reads=[], writes=[], tags=["tag1", "tag3"])
    def test_action_2(state: State) -> State:
        return state

    app = (
        ApplicationBuilder()
        .with_state(count=0)
        .with_actions(test_action, test_action_2)
        .with_transitions(("test_action", "test_action_2"))
        .with_entrypoint("test_action")
        .build()
    )
    halt_before, halt_after, inputs = app._process_control_flow_params(
        halt_after=["@tag:tag1"], halt_before=["@tag:tag2"]
    )

    assert sorted(halt_after) == ["test_action", "test_action_2"]
    assert halt_before == ["test_action"]
    assert inputs == {}



---
File: /burr/tests/core/test_graph.py
---

from typing import Callable

import pytest

from burr.core import Action, Condition, Result, State, default
from burr.core.graph import GraphBuilder, _validate_actions, _validate_transitions


# TODO -- share versions among tests, this is duplicated in test_application.py
class PassedInAction(Action):
    def __init__(
        self,
        reads: list[str],
        writes: list[str],
        fn: Callable[..., dict],
        update_fn: Callable[[dict, State], State],
        inputs: list[str],
        tags: list[str] = None,
    ):
        super(PassedInAction, self).__init__()
        self._reads = reads
        self._writes = writes
        self._fn = fn
        self._update_fn = update_fn
        self._inputs = inputs
        self._tags = tags

    def run(self, state: State, **run_kwargs) -> dict:
        return self._fn(state, **run_kwargs)

    @property
    def inputs(self) -> list[str]:
        return self._inputs

    def update(self, result: dict, state: State) -> State:
        return self._update_fn(result, state)

    @property
    def reads(self) -> list[str]:
        return self._reads

    @property
    def writes(self) -> list[str]:
        return self._writes

    @property
    def tags(self) -> list[str]:
        return self._tags or []


def test__validate_transitions_correct():
    _validate_transitions(
        [("counter", "counter", Condition.expr("count < 10")), ("counter", "result", default)],
        {"counter", "result"},
    )


def test__validate_transitions_missing_action():
    with pytest.raises(ValueError, match="not found"):
        _validate_transitions(
            [
                ("counter", "counter", Condition.expr("count < 10")),
                ("counter", "result", default),
            ],
            {"counter"},
        )


def test__validate_transitions_redundant_transition():
    with pytest.raises(ValueError, match="redundant"):
        _validate_transitions(
            [
                ("counter", "counter", Condition.expr("count < 10")),
                ("counter", "result", default),
                ("counter", "counter", default),  # this is unreachable as we already have a default
            ],
            {"counter", "result"},
        )


def test__validate_actions_valid():
    _validate_actions([Result("test")])


def test__validate_actions_empty():
    with pytest.raises(ValueError, match="at least one"):
        _validate_actions([])


def test__validate_actions_duplicated():
    with pytest.raises(ValueError, match="duplicated"):
        _validate_actions([Result("test"), Result("test")])


base_counter_action = PassedInAction(
    reads=["count"],
    writes=["count"],
    fn=lambda state: {"count": state.get("count", 0) + 1},
    update_fn=lambda result, state: state.update(**result),
    inputs=[],
    tags=["tag1", "tag2"],
)


def test_graph_builder_builds():
    graph = (
        GraphBuilder()
        .with_actions(counter=base_counter_action, result=Result("count"))
        .with_transitions(
            ("counter", "counter", Condition.expr("count < 10")), ("counter", "result")
        )
        .build()
    )
    assert len(graph.actions) == 2
    assert len(graph.transitions) == 2


def test_graph_builder_with_graph():
    graph1 = (
        GraphBuilder()
        .with_actions(counter=base_counter_action)
        .with_transitions(("counter", "counter", Condition.expr("count < 10")))
        .build()
    )
    graph2 = (
        GraphBuilder()
        .with_actions(counter2=base_counter_action)
        .with_transitions(("counter2", "counter2", Condition.expr("count < 20")))
        .build()
    )
    graph = (
        GraphBuilder()
        .with_graph(graph1)
        .with_graph(graph2)
        .with_actions(result=Result("count"))
        .with_transitions(
            ("counter", "counter2"),
            ("counter2", "result"),
        )
    )
    assert len(graph.actions) == 3
    assert len(graph.transitions) == 4


def test_graph_builder_get_next_node():
    graph = (
        GraphBuilder()
        .with_actions(counter=base_counter_action, result=Result("count"))
        .with_transitions(
            ("counter", "counter", Condition.expr("count < 10")), ("counter", "result")
        )
        .build()
    )
    assert len(graph.actions) == 2
    assert len(graph.transitions) == 2
    assert graph.get_next_node(None, State({"count": 0}), entrypoint="counter").name == "counter"


def test_get_actions_by_tag():
    action_with_tags = PassedInAction(
        reads=["count"],
        writes=["count"],
        fn=lambda state: {"count": state.get("count", 0) + 1},
        update_fn=lambda result, state: state.update(**result),
        inputs=[],
        tags=["tag1", "tag2"],
    )

    action_with_tags_2 = PassedInAction(
        reads=["count"],
        writes=["count"],
        fn=lambda state: {"count": state.get("count", 0) + 1},
        update_fn=lambda result, state: state.update(**result),
        inputs=[],
        tags=["tag1", "tag3"],
    )
    graph = (
        GraphBuilder()
        .with_actions(counter1=action_with_tags, counter2=action_with_tags_2)
        .with_transitions(("counter1", "counter2"))
        .with_transitions(("counter2", "counter1"))
        .build()
    )
    # tag1 is in both actions, tag2 is in one, tag3 is in one
    assert len(graph.get_actions_by_tag("tag1")) == 2
    assert len(graph.get_actions_by_tag("tag2")) == 1
    assert len(graph.get_actions_by_tag("tag3")) == 1
    with pytest.raises(ValueError, match="not found"):
        graph.get_actions_by_tag("tag4")



---
File: /burr/tests/core/test_graphviz_display.py
---

import pathlib

import pytest

from burr.core.graph import GraphBuilder

from tests.core.test_graph import PassedInAction


@pytest.fixture
def base_counter_action():
    yield PassedInAction(
        reads=["count"],
        writes=["count"],
        fn=lambda state: {"count": state.get("count", 0) + 1},
        update_fn=lambda result, state: state.update(**result),
        inputs=[],
    )


@pytest.fixture
def graph(base_counter_action):
    yield (
        GraphBuilder()
        .with_actions(counter=base_counter_action)
        .with_transitions(("counter", "counter"))
        .build()
    )


@pytest.mark.parametrize(
    "filename, write_dot", [("app", False), ("app.png", False), ("app", True), ("app.png", True)]
)
def test_visualize_dot_output(graph, tmp_path: pathlib.Path, filename: str, write_dot: bool):
    """Handle file generation with `graph.Digraph` `.render()` and `.pipe()`"""
    output_file_path = f"{tmp_path}/{filename}"

    graph.visualize(
        output_file_path=output_file_path,
        write_dot=write_dot,
    )

    # assert pathlib.Path(tmp_path, "app.png").exists()
    assert pathlib.Path(tmp_path, "app").exists() == write_dot


def test_visualize_no_dot_output(graph, tmp_path: pathlib.Path):
    """Check that no dot file is generated when output_file_path=None"""
    dot_file_path = tmp_path / "dag"

    graph.visualize(output_file_path=None)

    assert not dot_file_path.exists()



---
File: /burr/tests/core/test_implementations.py
---

import pytest

from burr.core import State
from burr.core.implementations import Placeholder


def test_placedholder_action():
    action = Placeholder(reads=["foo"], writes=["bar"]).with_name("test")
    assert action.reads == ["foo"]
    assert action.writes == ["bar"]
    with pytest.raises(NotImplementedError):
        action.run(State({}))

    with pytest.raises(NotImplementedError):
        action.update({}, State({}))



---
File: /burr/tests/core/test_parallelism.py
---

import asyncio
import concurrent.futures
import dataclasses
import datetime
from random import random
from typing import Any, AsyncGenerator, Callable, Dict, Generator, List, Literal, Optional, Union

import pytest

from burr.common import types as burr_types
from burr.core import (
    Action,
    ApplicationBuilder,
    ApplicationContext,
    ApplicationGraph,
    State,
    action,
)
from burr.core.action import Input, Result
from burr.core.graph import GraphBuilder
from burr.core.parallelism import (
    MapActions,
    MapActionsAndStates,
    MapStates,
    RunnableGraph,
    SubGraphTask,
    TaskBasedParallelAction,
    _cascade_adapter,
    map_reduce_action,
)
from burr.core.persistence import BaseStateLoader, BaseStateSaver, PersistedStateData
from burr.tracking.base import SyncTrackingClient
from burr.visibility import ActionSpan

old_action = action


async def sleep_random():
    await asyncio.sleep(random() / 1000)


# Single action/callable subgraph
@action(reads=["input_number", "number_to_add"], writes=["output_number"])
def simple_single_fn_subgraph(
    state: State, additional_number: int = 1, identifying_number: int = 1000
) -> State:
    return state.update(
        output_number=state["input_number"]
        + state["number_to_add"]
        + additional_number
        + identifying_number
    )


# Single action/callable subgraph
@action(reads=["input_number", "number_to_add"], writes=["output_number"])
async def simple_single_fn_subgraph_async(
    state: State, additional_number: int = 1, identifying_number: int = 1000
) -> State:
    await sleep_random()
    return state.update(
        output_number=state["input_number"]
        + state["number_to_add"]
        + additional_number
        + identifying_number
    )


class ClassBasedAction(Action):
    def __init__(self, identifying_number: int, name: str = "class_based_action"):
        super().__init__()
        self._name = name
        self.identifying_number = identifying_number

    @property
    def reads(self) -> list[str]:
        return ["input_number", "number_to_add"]

    def run(self, state: State, **run_kwargs) -> dict:
        return {
            "output_number": state["input_number"]
            + state["number_to_add"]
            + run_kwargs.get("additional_number", 1)
            + self.identifying_number
        }

    @property
    def writes(self) -> list[str]:
        return ["output_number"]

    def update(self, result: dict, state: State) -> State:
        return state.update(**result)


class ClassBasedActionAsync(ClassBasedAction):
    async def run(self, state: State, **run_kwargs) -> dict:
        await sleep_random()
        return super().run(state, **run_kwargs)


@action(reads=["input_number"], writes=["current_number"])
def entry_action_for_subgraph(state: State) -> State:
    return state.update(current_number=state["input_number"])


@action(reads=["current_number", "number_to_add"], writes=["current_number"])
def add_number_to_add(state: State) -> State:
    return state.update(current_number=state["current_number"] + state["number_to_add"])


@action(reads=["current_number"], writes=["current_number"])
def add_additional_number_to_add(
    state: State, additional_number: int = 1, identifying_number: int = 3000
) -> State:
    return state.update(
        current_number=state["current_number"] + additional_number + identifying_number
    )  # 1000 is the one that marks this as different


@action(reads=["current_number"], writes=["output_number"])
def final_result(state: State) -> State:
    return state.update(output_number=state["current_number"])


@action(reads=["input_number"], writes=["current_number"])
async def entry_action_for_subgraph_async(state: State) -> State:
    await sleep_random()
    return entry_action_for_subgraph(state)


@action(reads=["current_number", "number_to_add"], writes=["current_number"])
async def add_number_to_add_async(state: State) -> State:
    await sleep_random()
    return add_number_to_add(state)


@action(reads=["current_number"], writes=["current_number"])
async def add_additional_number_to_add_async(
    state: State, additional_number: int = 1, identifying_number: int = 3000
) -> State:
    await sleep_random()
    return add_additional_number_to_add(
        state, additional_number=additional_number, identifying_number=identifying_number
    )  # 1000 is the one that marks this as different


@action(reads=["current_number"], writes=["output_number"])
async def final_result_async(state: State) -> State:
    await sleep_random()
    return final_result(state)


SubGraphType = Union[Action, Callable, RunnableGraph]


def create_full_subgraph(identifying_number: int = 0) -> SubGraphType:
    return RunnableGraph(
        graph=(
            GraphBuilder()
            .with_actions(
                entry_action_for_subgraph,
                add_number_to_add,
                add_additional_number_to_add.bind(identifying_number=identifying_number),
                final_result,
            )
            .with_transitions(
                ("entry_action_for_subgraph", "add_number_to_add"),
                ("add_number_to_add", "add_additional_number_to_add"),
                ("add_additional_number_to_add", "final_result"),
            )
            .build()
        ),
        entrypoint="entry_action_for_subgraph",
        halt_after=["final_result"],
    )


def create_full_subgraph_async(identifying_number: int = 0) -> SubGraphType:
    return RunnableGraph(
        graph=GraphBuilder()
        .with_actions(
            entry_action_for_subgraph=entry_action_for_subgraph_async,
            add_number_to_add=add_number_to_add_async,
            add_additional_number_to_add=add_additional_number_to_add_async.bind(
                identifying_number=identifying_number
            ),
            final_result=final_result_async,
        )
        .with_transitions(
            ("entry_action_for_subgraph", "add_number_to_add"),
            ("add_number_to_add", "add_additional_number_to_add"),
            ("add_additional_number_to_add", "final_result"),
        )
        .build(),
        entrypoint="entry_action_for_subgraph",
        halt_after=["final_result"],
    )


FULL_SUBGRAPH: SubGraphType = create_full_subgraph(identifying_number=3000)
FULL_SUBGRAPH_ASYNC: SubGraphType = create_full_subgraph_async(identifying_number=3000)


@dataclasses.dataclass
class RecursiveActionTracked:
    state_before: Optional[State]
    state_after: Optional[State]
    action: Action
    app_id: str
    partition_key: str
    sequence_id: int
    children: List["RecursiveActionTracked"] = dataclasses.field(default_factory=list)


class RecursiveActionTracker(SyncTrackingClient):
    """Simple test tracking client for a recursive action"""

    def __init__(
        self,
        events: List[RecursiveActionTracked],
        parent: Optional["RecursiveActionTracker"] = None,
    ):
        self.events = events
        self.parent = parent

    def copy(self):
        """Quick way to copy from the current state. This assumes linearity (which is true in this case, as parallelism is delegated)"""
        if self.events:
            current_event = self.events[-1]
            if current_event.state_after is not None:
                raise ValueError("Don't copy if you're not in the middle of an event")
            return RecursiveActionTracker(current_event.children, parent=self)
        raise ValueError("Don't copy if you're not in the middle of an event")

    def post_application_create(
        self,
        *,
        app_id: str,
        partition_key: Optional[str],
        state: "State",
        application_graph: "ApplicationGraph",
        parent_pointer: Optional[burr_types.ParentPointer],
        spawning_parent_pointer: Optional[burr_types.ParentPointer],
        **future_kwargs: Any,
    ):
        pass

    def pre_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        inputs: Dict[str, Any],
        **future_kwargs: Any,
    ):
        self.events.append(
            RecursiveActionTracked(
                state_before=state,
                state_after=None,
                action=action,
                app_id=app_id,
                partition_key=partition_key,
                sequence_id=sequence_id,
            )
        )

    def post_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        result: Optional[Dict[str, Any]],
        exception: Exception,
        **future_kwargs: Any,
    ):
        self.events[-1].state_after = state

    def pre_start_span(
        self,
        *,
        action: str,
        action_sequence_id: int,
        span: "ActionSpan",
        span_dependencies: list[str],
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass

    def post_end_span(
        self,
        *,
        action: str,
        action_sequence_id: int,
        span: "ActionSpan",
        span_dependencies: list[str],
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass

    def do_log_attributes(
        self,
        *,
        attributes: Dict[str, Any],
        action: str,
        action_sequence_id: int,
        span: Optional["ActionSpan"],
        tags: dict,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass

    def pre_start_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass

    def post_stream_item(
        self,
        *,
        item: Any,
        item_index: int,
        stream_initialize_time: datetime.datetime,
        first_stream_item_start_time: datetime.datetime,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass

    def post_end_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


def _group_events_by_app_id(
    events: List[RecursiveActionTracked],
) -> Dict[str, List[RecursiveActionTracked]]:
    grouped_events = {}
    for event in events:
        if event.app_id not in grouped_events:
            grouped_events[event.app_id] = []
        grouped_events[event.app_id].append(event)
    return grouped_events


def test_map_actions_default_state():
    class MapActionsAllApproaches(MapActions):
        def actions(
            self, state: State, inputs: Dict[str, Any], context: ApplicationContext
        ) -> Generator[Union[Action, Callable, RunnableGraph], None, None]:
            ...

        def reduce(self, state: State, states: Generator[State, None, None]) -> State:
            ...

        @property
        def writes(self) -> list[str]:
            return []

        @property
        def reads(self) -> list[str]:
            return []

    state_to_test = State({"foo": "bar", "baz": "qux"})
    assert MapActionsAllApproaches().state(state_to_test, {}).get_all() == state_to_test.get_all()


def test_e2e_map_actions_sync_subgraph():
    """Tests map actions over multiple action types (runnable graph, function, action class...)"""

    class MapActionsAllApproaches(MapActions):
        def actions(
            self, state: State, inputs: Dict[str, Any], context: ApplicationContext
        ) -> Generator[Union[Action, Callable, RunnableGraph], None, None]:
            for graph_ in [
                simple_single_fn_subgraph.bind(identifying_number=1000),
                ClassBasedAction(2000),
                create_full_subgraph(3000),
            ]:
                yield graph_

        def state(self, state: State, inputs: Dict[str, Any]):
            return state.update(input_number=state["input_number_in_state"], number_to_add=10)

        def reduce(self, state: State, states: Generator[State, None, None]) -> State:
            # TODO -- ensure that states is in the correct order...
            # Or decide to key it?
            new_state = state
            for output_state in states:
                new_state = new_state.append(output_numbers_in_state=output_state["output_number"])
            return new_state

        @property
        def writes(self) -> list[str]:
            return ["output_numbers_in_state"]

        @property
        def reads(self) -> list[str]:
            return ["input_number_in_state"]

    app = (
        ApplicationBuilder()
        .with_actions(
            initial_action=Input("input_number_in_state"),
            map_action=MapActionsAllApproaches(),
            final_action=Result("output_numbers_in_state"),
        )
        .with_transitions(("initial_action", "map_action"), ("map_action", "final_action"))
        .with_entrypoint("initial_action")
        .with_tracker(RecursiveActionTracker(events := []))
        .build()
    )
    action, result, state = app.run(
        halt_after=["final_action"], inputs={"input_number_in_state": 100}
    )
    assert state["output_numbers_in_state"] == [1111, 2111, 3111]  # esnsure order correct
    assert len(events) == 3  # three parent actions
    _, map_event, __ = events
    grouped_events = _group_events_by_app_id(map_event.children)
    assert len(grouped_events) == 3  # three unique App IDs, one for each launching subgraph


async def test_e2e_map_actions_async_subgraph():
    """Tests map actions over multiple action types (runnable graph, function, action class...)"""

    class MapActionsAllApproachesAsync(MapActions):
        def actions(
            self, state: State, inputs: Dict[str, Any], context: ApplicationContext
        ) -> Generator[Union[Action, Callable, RunnableGraph], None, None]:
            for graph_ in [
                simple_single_fn_subgraph_async.bind(identifying_number=1000),
                ClassBasedActionAsync(2000),
                create_full_subgraph_async(3000),
            ]:
                yield graph_

        def is_async(self) -> bool:
            return True

        def state(self, state: State, inputs: Dict[str, Any]):
            return state.update(input_number=state["input_number_in_state"], number_to_add=10)

        async def reduce(self, state: State, states: AsyncGenerator[State, None]) -> State:
            # TODO -- ensure that states is in the correct order...
            # Or decide to key it?
            new_state = state
            async for output_state in states:
                new_state = new_state.append(output_numbers_in_state=output_state["output_number"])
            return new_state

        @property
        def writes(self) -> list[str]:
            return ["output_numbers_in_state"]

        @property
        def reads(self) -> list[str]:
            return ["input_number_in_state"]

    app = (
        ApplicationBuilder()
        .with_actions(
            initial_action=Input("input_number_in_state"),
            map_action=MapActionsAllApproachesAsync(),
            final_action=Result("output_numbers_in_state"),
        )
        .with_transitions(("initial_action", "map_action"), ("map_action", "final_action"))
        .with_entrypoint("initial_action")
        .with_tracker(RecursiveActionTracker(events := []))
        .build()
    )
    action, result, state = await app.arun(
        halt_after=["final_action"], inputs={"input_number_in_state": 100}
    )
    assert state["output_numbers_in_state"] == [1111, 2111, 3111]  # ensure order correct
    assert len(events) == 3  # three parent actions
    _, map_event, __ = events
    grouped_events = _group_events_by_app_id(map_event.children)
    assert len(grouped_events) == 3  # three unique App IDs, one for each launching subgraph


@pytest.mark.parametrize(
    "action",
    [
        simple_single_fn_subgraph.bind(identifying_number=0),
        ClassBasedAction(0),
        create_full_subgraph(0),
    ],
)
def test_e2e_map_states_sync_subgraph(action: SubGraphType):
    """Tests the map states action with a subgraph that is run in parallel.
    Collatz conjecture over different starting points"""

    class MapStatesSync(MapStates):
        def states(
            self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
        ) -> Generator[State, None, None]:
            for input_number in state["input_numbers_in_state"]:
                yield state.update(input_number=input_number, number_to_add=10)

        def action(
            self, state: State, inputs: Dict[str, Any]
        ) -> Union[Action, Callable, RunnableGraph]:
            return action

        def is_async(self) -> bool:
            return False

        def reduce(self, state: State, states: Generator[State, None, None]) -> State:
            # TODO -- ensure that states is in the correct order...
            # Or decide to key it?
            new_state = state
            for output_state in states:
                new_state = new_state.append(output_numbers_in_state=output_state["output_number"])
            return new_state

        @property
        def writes(self) -> list[str]:
            return ["output_numbers_in_state"]

        @property
        def reads(self) -> list[str]:
            return ["input_numbers_in_state"]

    app = (
        ApplicationBuilder()
        .with_actions(
            initial_action=Input("input_numbers_in_state"),
            map_action=MapStatesSync(),
            final_action=Result("output_numbers_in_state"),
        )
        .with_transitions(("initial_action", "map_action"), ("map_action", "final_action"))
        .with_entrypoint("initial_action")
        .with_tracker(RecursiveActionTracker(events := []))
        .build()
    )
    action, result, state = app.run(
        halt_after=["final_action"], inputs={"input_numbers_in_state": [100, 200, 300]}
    )
    assert state["output_numbers_in_state"] == [111, 211, 311]  # ensure order correct
    assert len(events) == 3
    _, map_event, __ = events
    grouped_events = _group_events_by_app_id(map_event.children)
    assert len(grouped_events) == 3


@pytest.mark.parametrize(
    "action",
    [
        simple_single_fn_subgraph_async.bind(identifying_number=0),
        ClassBasedActionAsync(0),
        create_full_subgraph_async(0),
    ],
)
async def test_e2e_map_states_async_subgraph(action: SubGraphType):
    """Tests the map states action with a subgraph that is run in parallel.
    Collatz conjecture over different starting points"""

    class MapStatesAsync(MapStates):
        def states(
            self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
        ) -> Generator[State, None, None]:
            for input_number in state["input_numbers_in_state"]:
                yield state.update(input_number=input_number, number_to_add=10)

        def action(
            self, state: State, inputs: Dict[str, Any]
        ) -> Union[Action, Callable, RunnableGraph]:
            return action

        def is_async(self) -> bool:
            return True

        async def reduce(self, state: State, states: AsyncGenerator[State, None]) -> State:
            # TODO -- ensure that states is in the correct order...
            # Or decide to key it?
            new_state = state
            async for output_state in states:
                new_state = new_state.append(output_numbers_in_state=output_state["output_number"])
            return new_state

        @property
        def writes(self) -> list[str]:
            return ["output_numbers_in_state"]

        @property
        def reads(self) -> list[str]:
            return ["input_numbers_in_state"]

    app = (
        ApplicationBuilder()
        .with_actions(
            initial_action=Input("input_numbers_in_state"),
            map_action=MapStatesAsync(),
            final_action=Result("output_numbers_in_state"),
        )
        .with_transitions(("initial_action", "map_action"), ("map_action", "final_action"))
        .with_entrypoint("initial_action")
        .with_tracker(RecursiveActionTracker(events := []))
        .build()
    )
    action, result, state = await app.arun(
        halt_after=["final_action"], inputs={"input_numbers_in_state": [100, 200, 300]}
    )
    assert state["output_numbers_in_state"] == [111, 211, 311]  # ensure order correct
    assert len(events) == 3
    _, map_event, __ = events
    grouped_events = _group_events_by_app_id(map_event.children)
    assert len(grouped_events) == 3


def test_e2e_map_actions_and_states_sync():
    """Tests the map states action with a subgraph that is run in parallel.
    Collatz conjecture over different starting points"""

    class MapStatesSync(MapActionsAndStates):
        def actions(
            self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
        ) -> Generator[Union[Action, Callable, RunnableGraph], None, None]:
            for graph_ in [
                simple_single_fn_subgraph.bind(identifying_number=1000),
                ClassBasedAction(2000),
                create_full_subgraph(3000),
            ]:
                yield graph_

        def states(
            self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
        ) -> Generator[State, None, None]:
            for input_number in state["input_numbers_in_state"]:
                yield state.update(input_number=input_number, number_to_add=10)

        def is_async(self) -> bool:
            return False

        def reduce(self, state: State, states: Generator[State, None, None]) -> State:
            # TODO -- ensure that states is in the correct order...
            # Or decide to key it?
            new_state = state
            for output_state in states:
                new_state = new_state.append(output_numbers_in_state=output_state["output_number"])
            return new_state

        @property
        def writes(self) -> list[str]:
            return ["output_numbers_in_state"]

        @property
        def reads(self) -> list[str]:
            return ["input_numbers_in_state"]

    app = (
        ApplicationBuilder()
        .with_actions(
            initial_action=Input("input_numbers_in_state"),
            map_action=MapStatesSync(),
            final_action=Result("output_numbers_in_state"),
        )
        .with_transitions(("initial_action", "map_action"), ("map_action", "final_action"))
        .with_entrypoint("initial_action")
        .with_tracker(RecursiveActionTracker(events := []))
        .build()
    )
    action, result, state = app.run(
        halt_after=["final_action"], inputs={"input_numbers_in_state": [100, 200, 300]}
    )
    assert state["output_numbers_in_state"] == [
        1111,
        1211,
        1311,
        2111,
        2211,
        2311,
        3111,
        3211,
        3311,
    ]
    assert len(events) == 3
    _, map_event, __ = events
    grouped_events = _group_events_by_app_id(map_event.children)
    assert len(grouped_events) == 9  # cartesian product of 3 actions and 3 states


async def test_e2e_map_actions_and_states_async():
    """Tests the map states action with a subgraph that is run in parallel.
    Collatz conjecture over different starting points"""

    class MapStatesAsync(MapActionsAndStates):
        def actions(
            self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
        ) -> Generator[Union[Action, Callable, RunnableGraph], None, None]:
            for graph_ in [
                simple_single_fn_subgraph_async.bind(identifying_number=1000),
                ClassBasedActionAsync(2000),
                create_full_subgraph_async(3000),
            ]:
                yield graph_

        def states(
            self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
        ) -> AsyncGenerator[State, None]:
            for input_number in state["input_numbers_in_state"]:
                yield state.update(input_number=input_number, number_to_add=10)

        def is_async(self) -> bool:
            return True

        async def reduce(self, state: State, states: AsyncGenerator[State, None]) -> State:
            # TODO -- ensure that states is in the correct order...
            # Or decide to key it?
            new_state = state
            async for output_state in states:
                new_state = new_state.append(output_numbers_in_state=output_state["output_number"])
            return new_state

        @property
        def writes(self) -> list[str]:
            return ["output_numbers_in_state"]

        @property
        def reads(self) -> list[str]:
            return ["input_numbers_in_state"]

    app = (
        ApplicationBuilder()
        .with_actions(
            initial_action=Input("input_numbers_in_state"),
            map_action=MapStatesAsync(),
            final_action=Result("output_numbers_in_state"),
        )
        .with_transitions(("initial_action", "map_action"), ("map_action", "final_action"))
        .with_entrypoint("initial_action")
        .with_tracker(RecursiveActionTracker(events := []))
        .build()
    )
    action, result, state = await app.arun(
        halt_after=["final_action"], inputs={"input_numbers_in_state": [100, 200, 300]}
    )
    assert state["output_numbers_in_state"] == [
        1111,
        1211,
        1311,
        2111,
        2211,
        2311,
        3111,
        3211,
        3311,
    ]
    assert len(events) == 3
    _, map_event, __ = events
    grouped_events = _group_events_by_app_id(map_event.children)
    assert len(grouped_events) == 9  # cartesian product of 3 actions and 3 states


def test_task_level_API_e2e_sync():
    """Tests the map states action with a subgraph that is run in parallel.
    Collatz conjecture over different starting points"""

    class TaskBasedAction(TaskBasedParallelAction):
        def tasks(
            self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
        ) -> Generator[SubGraphTask, None, None]:
            for j, action in enumerate(
                [
                    simple_single_fn_subgraph.bind(identifying_number=1000),
                    ClassBasedAction(2000),
                    create_full_subgraph(3000),
                ]
            ):
                for i, input_number in enumerate(state["input_numbers_in_state"]):
                    yield SubGraphTask(
                        graph=RunnableGraph.create(action),
                        inputs={},
                        state=state.update(input_number=input_number, number_to_add=10),
                        application_id=f"{i}_{j}",
                        tracker=context.tracker.copy(),
                    )

        def reduce(self, state: State, states: Generator[State, None, None]) -> State:
            # TODO -- ensure that states is in the correct order...
            # Or decide to key it?
            new_state = state
            for output_state in states:
                new_state = new_state.append(output_numbers_in_state=output_state["output_number"])
            return new_state

        @property
        def writes(self) -> list[str]:
            return ["output_numbers_in_state"]

        @property
        def reads(self) -> list[str]:
            return ["input_numbers_in_state"]

    app = (
        ApplicationBuilder()
        .with_actions(
            initial_action=Input("input_numbers_in_state"),
            map_action=TaskBasedAction(),
            final_action=Result("output_numbers_in_state"),
        )
        .with_transitions(("initial_action", "map_action"), ("map_action", "final_action"))
        .with_entrypoint("initial_action")
        .with_tracker(RecursiveActionTracker(events := []))
        .build()
    )
    action, result, state = app.run(
        halt_after=["final_action"], inputs={"input_numbers_in_state": [100, 200, 300]}
    )
    assert state["output_numbers_in_state"] == [
        1111,
        1211,
        1311,
        2111,
        2211,
        2311,
        3111,
        3211,
        3311,
    ]
    assert len(events) == 3
    _, map_event, __ = events
    grouped_events = _group_events_by_app_id(map_event.children)
    assert len(grouped_events) == 9  # cartesian product of 3 actions and 3 states


async def test_task_level_API_e2e_async():
    """Tests the map states action with a subgraph that is run in parallel.
    Collatz conjecture over different starting points"""

    class TaskBasedActionAsync(TaskBasedParallelAction):
        async def tasks(
            self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
        ) -> AsyncGenerator[SubGraphTask, None]:
            for j, action in enumerate(
                [
                    simple_single_fn_subgraph.bind(identifying_number=1000),
                    ClassBasedAction(2000),
                    create_full_subgraph(3000),
                ]
            ):
                for i, input_number in enumerate(state["input_numbers_in_state"]):
                    yield SubGraphTask(
                        graph=RunnableGraph.create(action),
                        inputs={},
                        state=state.update(input_number=input_number, number_to_add=10),
                        application_id=f"{i}_{j}",
                        tracker=context.tracker.copy(),
                    )

        async def reduce(self, state: State, states: AsyncGenerator[State, None]) -> State:
            # TODO -- ensure that states is in the correct order...
            # Or decide to key it?
            new_state = state
            async for output_state in states:
                new_state = new_state.append(output_numbers_in_state=output_state["output_number"])
            return new_state

        @property
        def writes(self) -> list[str]:
            return ["output_numbers_in_state"]

        @property
        def reads(self) -> list[str]:
            return ["input_numbers_in_state"]

        def is_async(self) -> bool:
            return True

    app = (
        ApplicationBuilder()
        .with_actions(
            initial_action=Input("input_numbers_in_state"),
            map_action=TaskBasedActionAsync(),
            final_action=Result("output_numbers_in_state"),
        )
        .with_transitions(("initial_action", "map_action"), ("map_action", "final_action"))
        .with_entrypoint("initial_action")
        .with_tracker(RecursiveActionTracker(events := []))
        .build()
    )
    action, result, state = await app.arun(
        halt_after=["final_action"], inputs={"input_numbers_in_state": [100, 200, 300]}
    )
    assert state["output_numbers_in_state"] == [
        1111,
        1211,
        1311,
        2111,
        2211,
        2311,
        3111,
        3211,
        3311,
    ]
    assert len(events) == 3
    _, map_event, __ = events
    grouped_events = _group_events_by_app_id(map_event.children)
    assert len(grouped_events) == 9  # cartesian product of 3 actions and 3 states


def test_map_reduce_function_e2e():
    mre = map_reduce_action(
        action=[
            simple_single_fn_subgraph.bind(identifying_number=1000),
            ClassBasedAction(2000),
            create_full_subgraph(3000),
        ],
        reads=["input_numbers_in_state"],
        writes=["output_numbers_in_state"],
        state=lambda state, context, inputs: (
            state.update(input_number=input_number, number_to_add=10)
            for input_number in state["input_numbers_in_state"]
        ),
        inputs=[],
        reducer=lambda state, states: state.extend(
            output_numbers_in_state=[output_state["output_number"] for output_state in states]
        ),
    )

    app = (
        ApplicationBuilder()
        .with_actions(
            initial_action=Input("input_numbers_in_state"),
            map_action=mre,
            final_action=Result("output_numbers_in_state"),
        )
        .with_transitions(("initial_action", "map_action"), ("map_action", "final_action"))
        .with_entrypoint("initial_action")
        .with_tracker(RecursiveActionTracker(events := []))
        .build()
    )
    action, result, state = app.run(
        halt_after=["final_action"], inputs={"input_numbers_in_state": [100, 200, 300]}
    )
    assert state["output_numbers_in_state"] == [
        1111,
        1211,
        1311,
        2111,
        2211,
        2311,
        3111,
        3211,
        3311,
    ]
    assert len(events) == 3
    _, map_event, __ = events
    grouped_events = _group_events_by_app_id(map_event.children)
    assert len(grouped_events) == 9  # cartesian product of 3 actions and 3 states


class DummyTracker(SyncTrackingClient):
    def __init__(self, parent: Optional["DummyTracker"] = None):
        self.parent = parent

    def copy(self):
        return DummyTracker(parent=self)

    def post_application_create(
        self,
        *,
        app_id: str,
        partition_key: Optional[str],
        state: "State",
        application_graph: "ApplicationGraph",
        parent_pointer: Optional[burr_types.ParentPointer],
        spawning_parent_pointer: Optional[burr_types.ParentPointer],
        **future_kwargs: Any,
    ):
        pass

    def pre_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        inputs: Dict[str, Any],
        **future_kwargs: Any,
    ):
        pass

    def post_run_step(
        self,
        *,
        app_id: str,
        partition_key: str,
        sequence_id: int,
        state: "State",
        action: "Action",
        result: Optional[Dict[str, Any]],
        exception: Exception,
        **future_kwargs: Any,
    ):
        pass

    def pre_start_span(
        self,
        *,
        action: str,
        action_sequence_id: int,
        span: "ActionSpan",
        span_dependencies: list[str],
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass

    def post_end_span(
        self,
        *,
        action: str,
        action_sequence_id: int,
        span: "ActionSpan",
        span_dependencies: list[str],
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass

    def do_log_attributes(
        self,
        *,
        attributes: Dict[str, Any],
        action: str,
        action_sequence_id: int,
        span: Optional["ActionSpan"],
        tags: dict,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass

    def pre_start_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass

    def post_stream_item(
        self,
        *,
        item: Any,
        item_index: int,
        stream_initialize_time: datetime.datetime,
        first_stream_item_start_time: datetime.datetime,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass

    def post_end_stream(
        self,
        *,
        action: str,
        sequence_id: int,
        app_id: str,
        partition_key: Optional[str],
        **future_kwargs: Any,
    ):
        pass


class DummyPersister(BaseStateSaver, BaseStateLoader):
    def __init__(self, parent: Optional["DummyPersister"] = None):
        self.parent = parent

    def copy(self) -> "DummyPersister":
        return DummyPersister(parent=self)

    def save(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int,
        position: str,
        state: State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        pass

    def load(
        self, partition_key: str, app_id: Optional[str], sequence_id: Optional[int] = None, **kwargs
    ) -> Optional[PersistedStateData]:
        pass

    def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        pass


def test_cascade_adapter_cascade():
    # Tests that cascading the adapter results in a cloned adapter with `copy()` called
    adapter = DummyTracker()
    cascaded = _cascade_adapter("cascade", adapter)
    assert cascaded.parent is adapter


def test_cascade_adapter_none():
    # Tests that setting the adapter behavior to None results in no adapter
    adapter = DummyTracker()
    cascaded = _cascade_adapter(None, adapter)
    assert cascaded is None


def test_cascade_adapter_fixed():
    # Tests that setting the adapter behavior to a fixed value results in that value
    current_adapter = DummyTracker()
    next_adapter = DummyTracker()
    cascaded = _cascade_adapter(next_adapter, current_adapter)
    assert cascaded is next_adapter


def test_map_actions_and_states_uses_same_persister_as_loader():
    """This tests the MapActionsAndStates functionality of using the correct persister. Specifically
    we want it to use the same instance for the saver as it does the loader, as that is
    what the parent app does."""

    class SimpleMapStates(MapActionsAndStates):
        def actions(
            self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
        ) -> Generator[Union[Action, Callable, RunnableGraph], None, None]:
            for graph_ in [
                simple_single_fn_subgraph.bind(identifying_number=1000),
            ]:
                yield graph_

        def states(
            self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
        ) -> Generator[State, None, None]:
            yield state.update(input_number=0, number_to_add=0)

        def reduce(self, state: State, states: Generator[State, None, None]) -> State:
            # TODO -- ensure that states is in the correct order...
            # Or decide to key it?
            new_state = state
            for output_state in states:
                new_state = new_state.append(output_numbers_in_state=output_state["output_number"])
            return new_state

        @property
        def writes(self) -> list[str]:
            return ["output_numbers_in_state"]

        @property
        def reads(self) -> list[str]:
            return ["input_numbers_in_state"]

    action = SimpleMapStates()
    persister = DummyPersister()
    tracker = DummyTracker()

    task_generator = action.tasks(
        state=State(),
        context=ApplicationContext(
            app_id="app_id",
            partition_key="partition_key",
            sequence_id=0,
            tracker=tracker,
            state_persister=persister,
            state_initializer=persister,
            parallel_executor_factory=lambda: concurrent.futures.ThreadPoolExecutor(),
            action_name=action.name,
        ),
        inputs={},
    )
    (task,) = task_generator  # one task
    assert task.state_persister is not None
    assert task.state_initializer is not None
    assert task.tracker is not None
    assert task.state_persister is task.state_initializer  # This ensures they're the same



---
File: /burr/tests/core/test_persistence.py
---

import pytest

from burr.core import State
from burr.core.persistence import InMemoryPersister, SQLLitePersister


@pytest.fixture(
    params=[
        {"which": "sqlite"},
        {"which": "memory"},
    ]
)
def persistence(request):
    which = request.param["which"]
    if which == "sqlite":
        persister = SQLLitePersister(db_path=":memory:", table_name="test_table")
        yield persister
        persister.cleanup()
    elif which == "memory":
        yield InMemoryPersister()
    # return SQLLitePersister(db_path=":memory:", table_name="test_table")


@pytest.fixture()
def initializing_persistence():
    persister = SQLLitePersister(db_path=":memory:", table_name="test_table")
    yield persister
    persister.cleanup()


def test_persistence_initialization_creates_table(initializing_persistence):
    initializing_persistence.initialize()
    assert initializing_persistence.list_app_ids("partition_key") == []


def test_persistence_saves_and_loads_state(persistence):
    if hasattr(persistence, "initialize"):
        persistence.initialize()
    persistence.save("partition_key", "app_id", 1, "position", State({"key": "value"}), "status")
    loaded_state = persistence.load("partition_key", "app_id")
    assert loaded_state["state"] == State({"key": "value"})


def test_persistence_returns_none_when_no_state(persistence):
    if hasattr(persistence, "initialize"):
        persistence.initialize()
    loaded_state = persistence.load("partition_key", "app_id")
    assert loaded_state is None


def test_persistence_lists_app_ids(persistence):
    if hasattr(persistence, "initialize"):
        persistence.initialize()
    persistence.save("partition_key", "app_id1", 1, "position", State({"key": "value"}), "status")
    persistence.save("partition_key", "app_id2", 1, "position", State({"key": "value"}), "status")
    app_ids = persistence.list_app_ids("partition_key")
    assert set(app_ids) == set(["app_id1", "app_id2"])


def test_persistence_is_initialized_false(initializing_persistence):
    assert not initializing_persistence.is_initialized()


def test_persistence_is_initialized_true(initializing_persistence):
    initializing_persistence.initialize()
    assert initializing_persistence.is_initialized()


def test_sqlite_persistence_is_initialized_true_new_connection(tmp_path):
    db_path = tmp_path / "test.db"
    p = SQLLitePersister(db_path=db_path, table_name="test_table")
    p.initialize()
    assert p.is_initialized()
    p2 = SQLLitePersister(db_path=db_path, table_name="test_table")
    assert p2.is_initialized()
    p.cleanup()
    p2.cleanup()


@pytest.mark.parametrize(
    "method_name,kwargs",
    [
        ("list_app_ids", {"partition_key": None}),
        ("load", {"partition_key": None, "app_id": "foo"}),
        (
            "save",
            {
                "partition_key": None,
                "app_id": "foo",
                "sequence_id": 1,
                "position": "position",
                "state": State({"key": "value"}),
                "status": "status",
            },
        ),
    ],
)
def test_persister_methods_none_partition_key(persistence, method_name: str, kwargs: dict):
    if hasattr(persistence, "initialize"):
        persistence.initialize()
    method = getattr(persistence, method_name)
    # method can be executed with `partition_key=None`
    method(**kwargs)
    # this doesn't guarantee that the results of `partition_key=None` and
    # `partition_key=persistence.PARTITION_KEY_DEFAULT`. This is hard to test because
    # these operations are stateful (i.e., read/write to a db)


import asyncio
from typing import Tuple

import aiosqlite
import pytest

from burr.core import ApplicationBuilder, State, action
from burr.core.persistence import AsyncInMemoryPersister
from burr.integrations.persisters.b_aiosqlite import AsyncSQLitePersister

"""Asyncio integration for sqlite persister + """


class AsyncSQLiteContextManager:
    def __init__(self, sqlite_object):
        self.client = sqlite_object

    async def __aenter__(self):
        return self.client

    async def __aexit__(self, exc_type, exc, tb):
        await self.client.close()


@pytest.fixture()
async def async_persistence(request):
    yield AsyncInMemoryPersister()


async def test_async_persistence_saves_and_loads_state(async_persistence):
    await asyncio.sleep(0.00001)
    if hasattr(async_persistence, "initialize"):
        await async_persistence.initialize()
    await async_persistence.save(
        "partition_key", "app_id", 1, "position", State({"key": "value"}), "status"
    )
    loaded_state = await async_persistence.load("partition_key", "app_id")
    assert loaded_state["state"] == State({"key": "value"})


async def test_async_persistence_returns_none_when_no_state(async_persistence):
    await asyncio.sleep(0.00001)
    if hasattr(async_persistence, "initialize"):
        await async_persistence.initialize()
    loaded_state = await async_persistence.load("partition_key", "app_id")
    assert loaded_state is None


async def test_async_persistence_lists_app_ids(async_persistence):
    await asyncio.sleep(0.00001)
    if hasattr(async_persistence, "initialize"):
        await async_persistence.initialize()
    await async_persistence.save(
        "partition_key", "app_id1", 1, "position", State({"key": "value"}), "status"
    )
    await async_persistence.save(
        "partition_key", "app_id2", 1, "position", State({"key": "value"}), "status"
    )
    app_ids = await async_persistence.list_app_ids("partition_key")
    assert set(app_ids) == set(["app_id1", "app_id2"])


@pytest.mark.parametrize(
    "method_name,kwargs",
    [
        ("list_app_ids", {"partition_key": None}),
        ("load", {"partition_key": None, "app_id": "foo"}),
        (
            "save",
            {
                "partition_key": None,
                "app_id": "foo",
                "sequence_id": 1,
                "position": "position",
                "state": State({"key": "value"}),
                "status": "status",
            },
        ),
    ],
)
async def test_async_persister_methods_none_partition_key(
    async_persistence, method_name: str, kwargs: dict
):
    await asyncio.sleep(0.00001)
    if hasattr(async_persistence, "initialize"):
        await async_persistence.initialize()
    method = getattr(async_persistence, method_name)
    # method can be executed with `partition_key=None`
    await method(**kwargs)
    # this doesn't guarantee that the results of `partition_key=None` and
    # `partition_key=persistence.PARTITION_KEY_DEFAULT`. This is hard to test because
    # these operations are stateful (i.e., read/write to a db)


async def test_AsyncSQLitePersister_from_values():
    await asyncio.sleep(0.00001)
    connection = await aiosqlite.connect(":memory:")
    sqlite_persister_init = AsyncSQLitePersister(connection=connection, table_name="test_table")
    sqlite_persister_from_values = await AsyncSQLitePersister.from_values(
        db_path=":memory:", table_name="test_table"
    )

    try:
        sqlite_persister_init.connection == sqlite_persister_from_values.connection
    except Exception as e:
        raise e
    finally:
        await sqlite_persister_init.close()
        await sqlite_persister_from_values.close()


async def test_AsyncSQLitePersister_connection_shutdown():
    await asyncio.sleep(0.00001)
    sqlite_persister = await AsyncSQLitePersister.from_values(
        db_path=":memory:", table_name="test_table"
    )
    await sqlite_persister.close()


@pytest.fixture()
async def initializing_async_persistence():
    sqlite_persister = await AsyncSQLitePersister.from_values(
        db_path=":memory:", table_name="test_table"
    )
    async_context_manager = AsyncSQLiteContextManager(sqlite_persister)
    async with async_context_manager as client:
        yield client


async def test_async_persistence_initialization_creates_table(initializing_async_persistence):
    await asyncio.sleep(0.00001)
    await initializing_async_persistence.initialize()
    assert await initializing_async_persistence.list_app_ids("partition_key") == []


async def test_async_persistence_is_initialized_false(initializing_async_persistence):
    await asyncio.sleep(0.00001)
    assert not await initializing_async_persistence.is_initialized()


async def test_async_persistence_is_initialized_true(initializing_async_persistence):
    await asyncio.sleep(0.00001)
    await initializing_async_persistence.initialize()
    assert await initializing_async_persistence.is_initialized()


async def test_asyncsqlite_persistence_is_initialized_true_new_connection(tmp_path):
    await asyncio.sleep(0.00001)
    db_path = tmp_path / "test.db"
    p = await AsyncSQLitePersister.from_values(db_path=db_path, table_name="test_table")
    await p.initialize()
    p2 = await AsyncSQLitePersister.from_values(db_path=db_path, table_name="test_table")
    try:
        assert await p.is_initialized()
        assert await p2.is_initialized()
    except Exception as e:
        raise e
    finally:
        await p.close()
        await p2.close()


async def test_async_save_and_load_from_sqlite_persister_end_to_end(tmp_path):
    await asyncio.sleep(0.00001)

    @action(reads=[], writes=["prompt", "chat_history"])
    async def dummy_input(state: State) -> Tuple[dict, State]:
        await asyncio.sleep(0.0001)
        if state["chat_history"]:
            new = state["chat_history"][-1] + 1
        else:
            new = 1
        return (
            {"prompt": "PROMPT"},
            state.update(prompt="PROMPT").append(chat_history=new),
        )

    @action(reads=["chat_history"], writes=["response", "chat_history"])
    async def dummy_response(state: State) -> Tuple[dict, State]:
        await asyncio.sleep(0.0001)
        if state["chat_history"]:
            new = state["chat_history"][-1] + 1
        else:
            new = 1
        return (
            {"response": "RESPONSE"},
            state.update(response="RESPONSE").append(chat_history=new),
        )

    db_path = tmp_path / "test.db"
    sqlite_persister = await AsyncSQLitePersister.from_values(
        db_path=db_path, table_name="test_table"
    )
    await sqlite_persister.initialize()
    app = await (
        ApplicationBuilder()
        .with_actions(dummy_input, dummy_response)
        .with_transitions(("dummy_input", "dummy_response"), ("dummy_response", "dummy_input"))
        .initialize_from(
            initializer=sqlite_persister,
            resume_at_next_action=True,
            default_state={"chat_history": []},
            default_entrypoint="dummy_input",
        )
        .with_state_persister(sqlite_persister)
        .with_identifiers(app_id="test_1", partition_key="sqlite")
        .abuild()
    )

    try:
        *_, state = await app.arun(halt_after=["dummy_response"])
        assert state["chat_history"][0] == 1
        assert state["chat_history"][1] == 2
        del app
    except Exception as e:
        raise e
    finally:
        await sqlite_persister.close()
        del sqlite_persister

    sqlite_persister_2 = await AsyncSQLitePersister.from_values(
        db_path=db_path, table_name="test_table"
    )
    await sqlite_persister_2.initialize()
    new_app = await (
        ApplicationBuilder()
        .with_actions(dummy_input, dummy_response)
        .with_transitions(("dummy_input", "dummy_response"), ("dummy_response", "dummy_input"))
        .initialize_from(
            initializer=sqlite_persister_2,
            resume_at_next_action=True,
            default_state={"chat_history": []},
            default_entrypoint="dummy_input",
        )
        .with_state_persister(sqlite_persister_2)
        .with_identifiers(app_id="test_1", partition_key="sqlite")
        .abuild()
    )

    try:
        assert new_app.state["chat_history"][0] == 1
        assert new_app.state["chat_history"][1] == 2

        *_, state = await new_app.arun(halt_after=["dummy_response"])
        assert state["chat_history"][2] == 3
        assert state["chat_history"][3] == 4
    except Exception as e:
        raise e
    finally:
        await sqlite_persister_2.close()



---
File: /burr/tests/core/test_serde.py
---

import pytest

from burr.core.serde import StringDispatch, deserialize, serialize


def test_serialize_primitive_types():
    assert serialize(1) == 1
    assert serialize(1.0) == 1.0
    assert serialize("test") == "test"
    assert serialize(True) is True


def test_serialize_list():
    assert serialize([1, 2, 3]) == [1, 2, 3]
    assert serialize(["a", "b", "c"]) == ["a", "b", "c"]


def test_serialize_dict():
    assert serialize({"key": "value"}) == {"key": "value"}
    assert serialize({"key1": 1, "key2": 2}) == {"key1": 1, "key2": 2}


def test_deserialize_primitive_types():
    assert deserialize(1) == 1
    assert deserialize(1.0) == 1.0
    assert deserialize("test") == "test"
    assert deserialize(True) is True


def test_deserialize_list():
    assert deserialize([1, 2, 3]) == [1, 2, 3]
    assert deserialize(["a", "b", "c"]) == ["a", "b", "c"]


def test_deserialize_dict():
    assert deserialize({"key": "value"}) == {"key": "value"}
    assert deserialize({"key1": 1, "key2": 2}) == {"key1": 1, "key2": 2}


def test_string_dispatch_no_key():
    dispatch = StringDispatch()
    with pytest.raises(ValueError):
        dispatch.call("nonexistent_key")


def test_string_dispatch_with_key():
    dispatch = StringDispatch()
    dispatch.register("test_key")(lambda x: x)
    assert dispatch.call("test_key", "test_value") == "test_value"



---
File: /burr/tests/core/test_state.py
---

from typing import Any

import pytest

from burr.core import Action, Graph
from burr.core.state import State, register_field_serde
from burr.core.typing import TypingSystem


def test_state_access():
    state = State({"foo": "bar"})
    assert state["foo"] == "bar"


def test_state_access_missing():
    state = State({"foo": "bar"})
    with pytest.raises(KeyError):
        _ = state["baz"]


def test_state_get():
    state = State({"foo": "bar"})
    assert state.get("foo") == "bar"


def test_state_get_missing():
    state = State({"foo": "bar"})
    assert state.get("baz") is None


def test_state_get_missing_default():
    state = State({"foo": "bar"})
    assert state.get("baz", "qux") == "qux"


def test_state_in():
    state = State({"foo": "bar"})
    assert "foo" in state
    assert "baz" not in state


def test_state_get_all():
    state = State({"foo": "bar", "baz": "qux"})
    assert state.get_all() == {"foo": "bar", "baz": "qux"}


def test_state_merge():
    state = State({"foo": "bar", "baz": "qux"})
    other = State({"foo": "baz", "quux": "corge"})
    merged = state.merge(other)
    assert merged.get_all() == {"foo": "baz", "baz": "qux", "quux": "corge"}


def test_state_subset():
    state = State({"foo": "bar", "baz": "qux"})
    subset = state.subset("foo")
    assert subset.get_all() == {"foo": "bar"}


def test_state_append():
    state = State({"foo": ["bar"]})
    appended = state.append(foo="baz")
    assert appended.get_all() == {"foo": ["bar", "baz"]}


def test_state_extend():
    state = State({"foo": ["bar"]})
    extended = state.extend(foo=["baz", "qux"])
    assert extended.get_all() == {"foo": ["bar", "baz", "qux"]}


def test_state_append_multiple_keys():
    state = State({"foo": ["bar"], "baz": [1]})
    appended = state.append(foo="baz", baz=2)
    assert appended.get_all() == {"foo": ["bar", "baz"], "baz": [1, 2]}


def test_state_extend_multiple_keys():
    state = State({"foo": ["bar"], "baz": [1]})
    extended = state.extend(foo=["baz"], baz=[2, 3])
    assert extended.get_all() == {"foo": ["bar", "baz"], "baz": [1, 2, 3]}


def test_state_update():
    state = State({"foo": "bar", "baz": "qux"})
    updated = state.update(foo="baz")
    assert updated.get_all() == {"foo": "baz", "baz": "qux"}


def test_state_init():
    state = State({"foo": "bar", "baz": "qux"})
    assert state.get_all() == {"foo": "bar", "baz": "qux"}


def test_state_wipe_delete():
    state = State({"foo": "bar", "baz": "qux"})
    wiped = state.wipe(delete=["foo"])
    assert wiped.get_all() == {"baz": "qux"}


def test_state_wipe_keep():
    state = State({"foo": "bar", "baz": "qux"})
    wiped = state.wipe(keep=["foo"])
    assert wiped.get_all() == {"foo": "bar"}


def test_state_append_validate_failure():
    state = State({"foo": "bar"})
    with pytest.raises(ValueError, match="non-appendable"):
        state.append(foo="baz")


def test_state_extend_validate_failure():
    state = State({"foo": "bar"})
    with pytest.raises(ValueError, match="non-extendable"):
        state.extend(foo=["baz", "qux"], bar=["quux"])


def test_state_increment():
    state = State({"foo": 1})
    incremented = state.increment(foo=2, bar=5)
    assert incremented.get_all() == {"foo": 3, "bar": 5}


def test_state_increment_validate_failure():
    state = State({"foo": "bar"})
    with pytest.raises(ValueError, match="non-integer"):
        state.increment(foo="baz", bar="qux")


def test_field_level_serde():
    def my_field_serializer(value: str, **kwargs) -> dict:
        serde_value = f"serialized_{value}"
        return {"value": serde_value}

    def my_field_deserializer(value: dict, **kwargs) -> str:
        serde_value = value["value"]
        return serde_value.replace("serialized_", "")

    register_field_serde("my_field", my_field_serializer, my_field_deserializer)
    state = State({"foo": {"hi": "world"}, "baz": "qux", "my_field": "testing 123"})
    assert state.serialize() == {
        "foo": {"hi": "world"},
        "baz": "qux",
        "my_field": {"value": "serialized_testing 123"},
    }
    state = State.deserialize(
        {"foo": {"hi": "world"}, "baz": "qux", "my_field": {"value": "serialized_testing 123"}}
    )
    assert state.get_all() == {"foo": {"hi": "world"}, "baz": "qux", "my_field": "testing 123"}


def test_field_level_serde_bad_serde_function():
    def my_field_serializer(value: str, **kwargs) -> str:
        # bad function
        serde_value = f"serialized_{value}"
        return serde_value

    def my_field_deserializer(value: dict, **kwargs) -> str:
        serde_value = value["value"]
        return serde_value.replace("serialized_", "")

    register_field_serde("my_field", my_field_serializer, my_field_deserializer)
    state = State({"foo": {"hi": "world"}, "baz": "qux", "my_field": "testing 123"})
    with pytest.raises(ValueError):
        state.serialize()


def test_register_field_serde_check_no_kwargs():
    def my_field_serializer(value: str) -> dict:
        serde_value = f"serialized_{value}"
        return {"value": serde_value}

    def my_field_deserializer(value: dict) -> str:
        serde_value = value["value"]
        return serde_value.replace("serialized_", "")

    with pytest.raises(ValueError):
        # serializer & deserializer missing kwargs
        register_field_serde("my_field", my_field_serializer, my_field_deserializer)

    def my_field_serializer(value: str, **kwargs) -> dict:
        serde_value = f"serialized_{value}"
        return {"value": serde_value}

    with pytest.raises(ValueError):
        # deserializer still bad
        register_field_serde("my_field", my_field_serializer, my_field_deserializer)


class SimpleTypingSystem(TypingSystem[Any]):
    def state_type(self) -> type[Any]:
        raise NotImplementedError

    def state_pre_action_run_type(self, action: Action, graph: Graph) -> type[Any]:
        raise NotImplementedError

    def state_post_action_run_type(self, action: Action, graph: Graph) -> type[Any]:
        raise NotImplementedError

    def construct_data(self, state: State[Any]) -> Any:
        raise NotImplementedError

    def construct_state(self, data: State[Any]) -> State[Any]:
        raise NotImplementedError


def test_state_apply_keeps_typing_system():
    state = State({"foo": "bar"}, typing_system=SimpleTypingSystem())
    assert state.update(foo="baz").typing_system is state.typing_system
    assert state.subset("foo").typing_system is state.typing_system



---
File: /burr/tests/core/test_validation.py
---

import pytest

from burr.core.validation import assert_set


def test__assert_set():
    assert_set("foo", "foo", "bar")


def test__assert_set_unset():
    with pytest.raises(ValueError, match="bar"):
        assert_set(None, "foo", "bar")



---
File: /burr/tests/integration_tests/test_app.py
---

import pydantic
from langchain_core import documents

from burr import core
from burr.core import State, action, expr, persistence, state
from burr.tracking import client as tracking_client


@action(reads=[], writes=["dict"])
def basic_action(state: State, user_input: str) -> tuple[dict, State]:
    v = {"foo": 1, "bar": "2", "bool": True, "None": None, "input": user_input}
    return {"dict": v}, state.update(dict=v)


class PydanticField(pydantic.BaseModel):
    f1: int = 0
    f2: bool = False


@action(reads=["dict"], writes=["pydantic_field"])
def pydantic_action(state: State) -> tuple[dict, State]:
    v = PydanticField(f1=state["dict"]["foo"], f2=state["dict"]["bool"])
    return {"pydantic_field": v}, state.update(pydantic_field=v)


@action(reads=["pydantic_field"], writes=["lc_doc"])
def langchain_action(state: State) -> tuple[dict, State]:
    v = documents.Document(
        page_content=f"foo: {state['pydantic_field'].f1}, bar: {state['pydantic_field'].f2}"
    )
    return {"lc_doc": v}, state.update(lc_doc=v)


@action(reads=["lc_doc"], writes=[])
def terminal_action(state: State) -> tuple[dict, State]:
    return {"output": state["lc_doc"].page_content}, state


def build_application(sqllite_persister, tracker, partition_key, app_id):
    persister = sqllite_persister or tracker
    app_builder = (
        core.ApplicationBuilder()
        .with_actions(basic_action, pydantic_action, langchain_action, terminal_action)
        .with_transitions(
            ("basic_action", "terminal_action", expr("dict['foo'] == 0")),
            ("basic_action", "pydantic_action"),
            ("pydantic_action", "langchain_action"),
            ("langchain_action", "terminal_action"),
        )
        .with_identifiers(partition_key=partition_key, app_id=app_id)
        .initialize_from(
            persister,
            resume_at_next_action=True,
            default_state={
                "custom_field": documents.Document(
                    page_content="this is a custom field to serialize"
                )
            },
            default_entrypoint="basic_action",
        )
    )
    if sqllite_persister:
        app_builder.with_state_persister(sqllite_persister)
    if tracker:
        app_builder.with_tracker(tracker)
    return app_builder.build()


def register_custom_serde_for_lc_document(field_name: str):
    """Register a custom serde for a field"""

    def my_field_serializer(value: documents.Document, **kwargs) -> dict:
        serde_value = f"serialized::{value.page_content}"
        return {"value": serde_value}

    def my_field_deserializer(value: dict, **kwargs) -> documents.Document:
        serde_value = value["value"]
        return documents.Document(page_content=serde_value.replace("serialized::", ""))

    state.register_field_serde(field_name, my_field_serializer, my_field_deserializer)


def test_whole_application_tracker(tmp_path):
    """This test creates an application and then steps through it rebuilding the
    application each time. This is a test of things being serialized and deserialized."""
    tracker = tracking_client.LocalTrackingClient("integration-test", tmp_path)
    app_id = "integration-test"
    partition_key = ""
    # step 1
    app = build_application(None, tracker, partition_key, app_id)
    register_custom_serde_for_lc_document("custom_field")
    action1, result1, state1 = app.step(inputs={"user_input": "hello"})
    # check custom serde
    assert state1.serialize() == {
        "__PRIOR_STEP": "basic_action",
        "__SEQUENCE_ID": 0,
        "custom_field": {"value": "serialized::this is a custom field to serialize"},
        "dict": {"None": None, "bar": "2", "bool": True, "foo": 1, "input": "hello"},
    }
    assert action1.name == "basic_action"
    # step 2
    app = build_application(None, tracker, partition_key, app_id)
    action2, result2, state2 = app.step()
    assert action2.name == "pydantic_action"
    # step 3
    app = build_application(None, tracker, partition_key, app_id)
    action3, result3, state3 = app.step()
    assert action3.name == "langchain_action"
    # step 4
    app = build_application(None, tracker, partition_key, app_id)
    action4, result4, state4 = app.step()
    assert action4.name == "terminal_action"

    # assert that state is basically the same across different steps
    assert state1["dict"] == {"foo": 1, "bar": "2", "bool": True, "None": None, "input": "hello"}
    assert state1["dict"] == state4["dict"]

    assert state2["pydantic_field"].f1 == 1
    assert state2["pydantic_field"].f2 is True
    assert state2["pydantic_field"] == state3["pydantic_field"]

    assert state3["lc_doc"].page_content == "foo: 1, bar: True"
    assert state3["lc_doc"] == state4["lc_doc"]

    # assert that tracker has things in it too
    final_tracker_state = tracker.load(partition_key, app_id=app_id)
    for k, v in final_tracker_state["state"].items():
        assert v == state4[k]


def test_whole_application_sqllite(tmp_path):
    """This test creates an application and then steps through it rebuilding the
    application each time. This is a test of things being serialized and deserialized."""
    sqllite_persister = persistence.SQLLitePersister(tmp_path / "test.db")
    sqllite_persister.initialize()
    app_id = "integration-test"
    partition_key = ""
    # step 1
    app = build_application(sqllite_persister, None, partition_key, app_id)
    register_custom_serde_for_lc_document("custom_field")
    action1, result1, state1 = app.step(inputs={"user_input": "hello"})
    # check custom serde
    assert state1.serialize() == {
        "__PRIOR_STEP": "basic_action",
        "__SEQUENCE_ID": 0,
        "custom_field": {"value": "serialized::this is a custom field to serialize"},
        "dict": {"None": None, "bar": "2", "bool": True, "foo": 1, "input": "hello"},
    }
    # check actions
    assert action1.name == "basic_action"
    # step 2
    app = build_application(sqllite_persister, None, partition_key, app_id)
    action2, result2, state2 = app.step()
    assert action2.name == "pydantic_action"
    # step 3
    app = build_application(sqllite_persister, None, partition_key, app_id)
    action3, result3, state3 = app.step()
    assert action3.name == "langchain_action"
    # step 4
    app = build_application(sqllite_persister, None, partition_key, app_id)
    action4, result4, state4 = app.step()
    assert action4.name == "terminal_action"

    # assert that state is basically the same across different steps
    assert state1["dict"] == {"foo": 1, "bar": "2", "bool": True, "None": None, "input": "hello"}
    assert state1["dict"] == state4["dict"]

    assert state2["pydantic_field"].f1 == 1
    assert state2["pydantic_field"].f2 is True
    assert state2["pydantic_field"] == state3["pydantic_field"]

    assert state3["lc_doc"].page_content == "foo: 1, bar: True"
    assert state3["lc_doc"] == state4["lc_doc"]

    final_sqllite_state = sqllite_persister.load("", app_id=app_id)
    assert final_sqllite_state["state"] == state4
    assert sqllite_persister.list_app_ids(partition_key="") == ["integration-test"]



---
File: /burr/tests/integrations/persisters/test_b_aiosqlite.py
---

import asyncio
from typing import Tuple

import aiosqlite
import pytest

from burr.core import ApplicationBuilder, State, action
from burr.integrations.persisters.b_aiosqlite import AsyncSQLitePersister


class AsyncSQLiteContextManager:
    def __init__(self, sqlite_object):
        self.client = sqlite_object

    async def __aenter__(self):
        return self.client

    async def __aexit__(self, exc_type, exc, tb):
        await self.client.cleanup()


async def test_copy_persister(async_persistence: AsyncSQLitePersister):
    copy = async_persistence.copy()
    assert copy.table_name == async_persistence.table_name
    assert copy.serde_kwargs == async_persistence.serde_kwargs
    assert copy.connection is not None


@pytest.fixture()
async def async_persistence(request):
    sqlite_persister = await AsyncSQLitePersister.from_values(
        db_path=":memory:", table_name="test_table"
    )
    async_context_manager = AsyncSQLiteContextManager(sqlite_persister)
    async with async_context_manager as client:
        yield client


async def test_async_persistence_saves_and_loads_state(async_persistence):
    await asyncio.sleep(0.00001)
    if hasattr(async_persistence, "initialize"):
        await async_persistence.initialize()
    await async_persistence.save(
        "partition_key", "app_id", 1, "position", State({"key": "value"}), "status"
    )
    loaded_state = await async_persistence.load("partition_key", "app_id")
    assert loaded_state["state"] == State({"key": "value"})


async def test_async_persistence_returns_none_when_no_state(async_persistence):
    await asyncio.sleep(0.00001)
    if hasattr(async_persistence, "initialize"):
        await async_persistence.initialize()
    loaded_state = await async_persistence.load("partition_key", "app_id")
    assert loaded_state is None


async def test_async_persistence_lists_app_ids(async_persistence):
    await asyncio.sleep(0.00001)
    if hasattr(async_persistence, "initialize"):
        await async_persistence.initialize()
    await async_persistence.save(
        "partition_key", "app_id1", 1, "position", State({"key": "value"}), "status"
    )
    await async_persistence.save(
        "partition_key", "app_id2", 1, "position", State({"key": "value"}), "status"
    )
    app_ids = await async_persistence.list_app_ids("partition_key")
    assert set(app_ids) == set(["app_id1", "app_id2"])


@pytest.mark.parametrize(
    "method_name,kwargs",
    [
        ("list_app_ids", {"partition_key": None}),
        ("load", {"partition_key": None, "app_id": "foo"}),
        (
            "save",
            {
                "partition_key": None,
                "app_id": "foo",
                "sequence_id": 1,
                "position": "position",
                "state": State({"key": "value"}),
                "status": "status",
            },
        ),
    ],
)
async def test_async_persister_methods_none_partition_key(
    async_persistence, method_name: str, kwargs: dict
):
    await asyncio.sleep(0.00001)
    if hasattr(async_persistence, "initialize"):
        await async_persistence.initialize()
    method = getattr(async_persistence, method_name)
    # method can be executed with `partition_key=None`
    await method(**kwargs)
    # this doesn't guarantee that the results of `partition_key=None` and
    # `partition_key=persistence.PARTITION_KEY_DEFAULT`. This is hard to test because
    # these operations are stateful (i.e., read/write to a db)


async def test_AsyncSQLitePersister_from_values():
    await asyncio.sleep(0.00001)
    connection = await aiosqlite.connect(":memory:")
    sqlite_persister_init = AsyncSQLitePersister(connection=connection, table_name="test_table")
    sqlite_persister_from_values = await AsyncSQLitePersister.from_values(
        db_path=":memory:", table_name="test_table"
    )

    try:
        sqlite_persister_init.connection == sqlite_persister_from_values.connection
    except Exception as e:
        raise e
    finally:
        await sqlite_persister_init.close()
        await sqlite_persister_from_values.close()


async def test_AsyncSQLitePersister_connection_shutdown():
    await asyncio.sleep(0.00001)
    sqlite_persister = await AsyncSQLitePersister.from_values(
        db_path=":memory:", table_name="test_table"
    )
    await sqlite_persister.close()


@pytest.fixture()
async def initializing_async_persistence():
    sqlite_persister = await AsyncSQLitePersister.from_values(
        db_path=":memory:", table_name="test_table"
    )
    async_context_manager = AsyncSQLiteContextManager(sqlite_persister)
    async with async_context_manager as client:
        yield client


async def test_async_persistence_initialization_creates_table(initializing_async_persistence):
    await asyncio.sleep(0.00001)
    await initializing_async_persistence.initialize()
    assert await initializing_async_persistence.list_app_ids("partition_key") == []


async def test_async_persistence_is_initialized_false(initializing_async_persistence):
    await asyncio.sleep(0.00001)
    assert not await initializing_async_persistence.is_initialized()


async def test_async_persistence_is_initialized_true(initializing_async_persistence):
    await asyncio.sleep(0.00001)
    await initializing_async_persistence.initialize()
    assert await initializing_async_persistence.is_initialized()


async def test_asyncsqlite_persistence_is_initialized_true_new_connection(tmp_path):
    await asyncio.sleep(0.00001)
    db_path = tmp_path / "test.db"
    p = await AsyncSQLitePersister.from_values(db_path=db_path, table_name="test_table")
    await p.initialize()
    p2 = await AsyncSQLitePersister.from_values(db_path=db_path, table_name="test_table")
    try:
        assert await p.is_initialized()
        assert await p2.is_initialized()
    except Exception as e:
        raise e
    finally:
        await p.close()
        await p2.close()


async def test_async_save_and_load_from_sqlite_persister_end_to_end(tmp_path):
    await asyncio.sleep(0.00001)

    @action(reads=[], writes=["prompt", "chat_history"])
    async def dummy_input(state: State) -> Tuple[dict, State]:
        await asyncio.sleep(0.0001)
        if state["chat_history"]:
            new = state["chat_history"][-1] + 1
        else:
            new = 1
        return (
            {"prompt": "PROMPT"},
            state.update(prompt="PROMPT").append(chat_history=new),
        )

    @action(reads=["chat_history"], writes=["response", "chat_history"])
    async def dummy_response(state: State) -> Tuple[dict, State]:
        await asyncio.sleep(0.0001)
        if state["chat_history"]:
            new = state["chat_history"][-1] + 1
        else:
            new = 1
        return (
            {"response": "RESPONSE"},
            state.update(response="RESPONSE").append(chat_history=new),
        )

    db_path = tmp_path / "test.db"
    sqlite_persister = await AsyncSQLitePersister.from_values(
        db_path=db_path, table_name="test_table"
    )
    await sqlite_persister.initialize()
    app = await (
        ApplicationBuilder()
        .with_actions(dummy_input, dummy_response)
        .with_transitions(("dummy_input", "dummy_response"), ("dummy_response", "dummy_input"))
        .initialize_from(
            initializer=sqlite_persister,
            resume_at_next_action=True,
            default_state={"chat_history": []},
            default_entrypoint="dummy_input",
        )
        .with_state_persister(sqlite_persister)
        .with_identifiers(app_id="test_1", partition_key="sqlite")
        .abuild()
    )

    try:
        *_, state = await app.arun(halt_after=["dummy_response"])
        assert state["chat_history"][0] == 1
        assert state["chat_history"][1] == 2
        del app
    except Exception as e:
        raise e
    finally:
        await sqlite_persister.close()
        del sqlite_persister

    sqlite_persister_2 = await AsyncSQLitePersister.from_values(
        db_path=db_path, table_name="test_table"
    )
    await sqlite_persister_2.initialize()
    new_app = await (
        ApplicationBuilder()
        .with_actions(dummy_input, dummy_response)
        .with_transitions(("dummy_input", "dummy_response"), ("dummy_response", "dummy_input"))
        .initialize_from(
            initializer=sqlite_persister_2,
            resume_at_next_action=True,
            default_state={"chat_history": []},
            default_entrypoint="dummy_input",
        )
        .with_state_persister(sqlite_persister_2)
        .with_identifiers(app_id="test_1", partition_key="sqlite")
        .abuild()
    )

    try:
        assert new_app.state["chat_history"][0] == 1
        assert new_app.state["chat_history"][1] == 2

        *_, state = await new_app.arun(halt_after=["dummy_response"])
        assert state["chat_history"][2] == 3
        assert state["chat_history"][3] == 4
    except Exception as e:
        raise e
    finally:
        await sqlite_persister_2.close()



---
File: /burr/tests/integrations/persisters/test_b_mongodb.py
---

import os
import pickle

import pytest

from burr.core import state
from burr.integrations.persisters.b_mongodb import MongoDBPersister
from burr.integrations.persisters.b_pymongo import MongoDBBasePersister

if not os.environ.get("BURR_CI_INTEGRATION_TESTS") == "true":
    pytest.skip("Skipping integration tests", allow_module_level=True)


@pytest.fixture
def mongodb_persister():
    persister = MongoDBBasePersister.from_values(
        uri="mongodb://localhost:27017", db_name="testdb", collection_name="testcollection"
    )
    yield persister
    persister.collection.drop()


def test_save_and_load_state(mongodb_persister):
    mongodb_persister.save("pk", "app_id", 1, "pos", state.State({"a": 1, "b": 2}), "completed")
    data = mongodb_persister.load("pk", "app_id", 1)
    assert data["state"].get_all() == {"a": 1, "b": 2}


def test_list_app_ids(mongodb_persister):
    mongodb_persister.save("pk", "app_id1", 1, "pos1", state.State({"a": 1}), "completed")
    mongodb_persister.save("pk", "app_id2", 2, "pos2", state.State({"b": 2}), "completed")
    app_ids = mongodb_persister.list_app_ids("pk")
    assert "app_id1" in app_ids
    assert "app_id2" in app_ids


def test_load_nonexistent_key(mongodb_persister):
    state_data = mongodb_persister.load("pk", "nonexistent_key")
    assert state_data is None


def test_backwards_compatible_persister():
    persister = MongoDBPersister(
        uri="mongodb://localhost:27017", db_name="testdb", collection_name="backwardscompatible"
    )
    persister.save("pk", "app_id", 5, "pos", state.State({"a": 5, "b": 5}), "completed")
    data = persister.load("pk", "app_id", 5)
    assert data["state"].get_all() == {"a": 5, "b": 5}

    persister.collection.drop()


def test_serialization_with_pickle(mongodb_persister):
    # Save some state
    mongodb_persister.save(
        "pk", "app_id_serde", 1, "pos", state.State({"a": 1, "b": 2}), "completed"
    )

    # Serialize the persister
    serialized_persister = pickle.dumps(mongodb_persister)

    # Deserialize the persister
    deserialized_persister = pickle.loads(serialized_persister)

    # Load the state from the deserialized persister
    data = deserialized_persister.load("pk", "app_id_serde", 1)

    assert data["state"].get_all() == {"a": 1, "b": 2}


def test_partition_key_is_optional(mongodb_persister):
    # 1. Save and load with partition key = None
    mongodb_persister.save(
        None, "app_id_none", 1, "pos1", state.State({"foo": "bar"}), "in_progress"
    )
    loaded_data = mongodb_persister.load(None, "app_id_none", 1)
    assert loaded_data is not None
    assert loaded_data["state"].get_all() == {"foo": "bar"}

    # 2. Save and load again (different key/index) with partition key = None
    mongodb_persister.save(
        None, "app_id_none2", 2, "pos2", state.State({"hello": "world"}), "completed"
    )
    loaded_data2 = mongodb_persister.load(None, "app_id_none2", 2)
    assert loaded_data2 is not None
    assert loaded_data2["state"].get_all() == {"hello": "world"}



---
File: /burr/tests/integrations/persisters/test_b_redis.py
---

import os
import pickle

import pytest

from burr.core import state
from burr.integrations.persisters.b_redis import (
    AsyncRedisBasePersister,
    RedisBasePersister,
    RedisPersister,
)

if not os.environ.get("BURR_CI_INTEGRATION_TESTS") == "true":
    pytest.skip("Skipping integration tests", allow_module_level=True)


@pytest.fixture
def redis_persister():
    persister = RedisBasePersister.from_values(host="localhost", port=6379, db=0)
    yield persister
    persister.cleanup()


@pytest.fixture
def redis_persister_with_ns():
    persister = RedisBasePersister.from_values(host="localhost", port=6379, db=0, namespace="test")
    yield persister
    persister.cleanup()


def test_save_and_load_state(redis_persister):
    redis_persister.save("pk", "app_id", 1, "pos", state.State({"a": 1, "b": 2}), "completed")
    data = redis_persister.load("pk", "app_id", 1)
    assert data["state"].get_all() == {"a": 1, "b": 2}


def test_list_app_ids(redis_persister):
    redis_persister.save("pk", "app_id1", 1, "pos1", state.State({"a": 1}), "completed")
    redis_persister.save("pk", "app_id2", 2, "pos2", state.State({"b": 2}), "completed")
    app_ids = redis_persister.list_app_ids("pk")
    assert "app_id1" in app_ids
    assert "app_id2" in app_ids


def test_load_nonexistent_key(redis_persister):
    state_data = redis_persister.load("pk", "nonexistent_key")
    assert state_data is None


def test_save_and_load_state_ns(redis_persister_with_ns):
    redis_persister_with_ns.save(
        "pk", "app_id", 1, "pos", state.State({"a": 1, "b": 2}), "completed"
    )
    data = redis_persister_with_ns.load("pk", "app_id", 1)
    assert data["state"].get_all() == {"a": 1, "b": 2}


def test_list_app_ids_with_ns(redis_persister_with_ns):
    redis_persister_with_ns.save("pk", "app_id1", 1, "pos1", state.State({"a": 1}), "completed")
    redis_persister_with_ns.save("pk", "app_id2", 2, "pos2", state.State({"b": 2}), "completed")
    app_ids = redis_persister_with_ns.list_app_ids("pk")
    assert "app_id1" in app_ids
    assert "app_id2" in app_ids


def test_load_nonexistent_key_with_ns(redis_persister_with_ns):
    state_data = redis_persister_with_ns.load("pk", "nonexistent_key")
    assert state_data is None


def test_redis_persister_class_backwards_compatible():
    """Tests that the RedisPersister class is still backwards compatible."""
    persister = RedisPersister(host="localhost", port=6379, db=0, namespace="backwardscompatible")
    persister.save("pk", "app_id", 2, "pos", state.State({"a": 4, "b": 5}), "completed")
    data = persister.load("pk", "app_id", 2)
    assert data["state"].get_all() == {"a": 4, "b": 5}
    persister.connection.close()


def test_serialization_with_pickle(redis_persister_with_ns):
    # Save some state
    redis_persister_with_ns.save(
        "pk", "app_id_serde", 1, "pos", state.State({"a": 1, "b": 2}), "completed"
    )

    # Serialize the persister
    serialized_persister = pickle.dumps(redis_persister_with_ns)

    # Deserialize the persister
    deserialized_persister = pickle.loads(serialized_persister)

    # Load the state from the deserialized persister
    data = deserialized_persister.load("pk", "app_id_serde", 1)

    assert data["state"].get_all() == {"a": 1, "b": 2}


@pytest.fixture
async def async_redis_persister():
    persister = AsyncRedisBasePersister.from_values(host="localhost", port=6379, db=1)
    yield persister
    await persister.cleanup()


@pytest.fixture
async def async_redis_persister_with_ns():
    persister = AsyncRedisBasePersister.from_values(
        host="localhost", port=6379, db=1, namespace="test_async"
    )
    yield persister
    await persister.cleanup()


async def test_async_save_and_load_state(async_redis_persister):
    await async_redis_persister.save(
        "pk", "app_id", 1, "pos", state.State({"a": 1, "b": 2}), "completed"
    )
    data = await async_redis_persister.load("pk", "app_id", 1)
    assert data["state"].get_all() == {"a": 1, "b": 2}


async def test_async_list_app_ids(async_redis_persister):
    await async_redis_persister.save("pk", "app_id1", 1, "pos1", state.State({"a": 1}), "completed")
    await async_redis_persister.save("pk", "app_id2", 2, "pos2", state.State({"b": 2}), "completed")
    app_ids = await async_redis_persister.list_app_ids("pk")
    assert "app_id1" in app_ids
    assert "app_id2" in app_ids


async def test_async_load_nonexistent_key(async_redis_persister):
    state_data = await async_redis_persister.load("pk", "nonexistent_key")
    assert state_data is None


async def test_async_save_and_load_state_ns(async_redis_persister_with_ns):
    await async_redis_persister_with_ns.save(
        "pk", "app_id", 1, "pos", state.State({"a": 1, "b": 2}), "completed"
    )
    data = await async_redis_persister_with_ns.load("pk", "app_id", 1)
    assert data["state"].get_all() == {"a": 1, "b": 2}


async def test_async_list_app_ids_with_ns(async_redis_persister_with_ns):
    await async_redis_persister_with_ns.save(
        "pk", "app_id1", 1, "pos1", state.State({"a": 1}), "completed"
    )
    await async_redis_persister_with_ns.save(
        "pk", "app_id2", 2, "pos2", state.State({"b": 2}), "completed"
    )
    app_ids = await async_redis_persister_with_ns.list_app_ids("pk")
    assert "app_id1" in app_ids
    assert "app_id2" in app_ids


async def test_async_load_nonexistent_key_with_ns(async_redis_persister_with_ns):
    state_data = await async_redis_persister_with_ns.load("pk", "nonexistent_key")
    assert state_data is None



---
File: /burr/tests/integrations/persisters/test_postgresql.py
---

import os
import pickle

import pytest

from burr.core import state
from burr.integrations.persisters.b_asyncpg import AsyncPostgreSQLPersister
from burr.integrations.persisters.b_psycopg2 import PostgreSQLPersister

if not os.environ.get("BURR_CI_INTEGRATION_TESTS") == "true":
    pytest.skip("Skipping integration tests", allow_module_level=True)


@pytest.fixture
def postgresql_persister():
    persister = PostgreSQLPersister.from_values(
        db_name="postgres",
        user="postgres",
        password="postgres",
        host="localhost",
        port=5432,
        table_name="testtable",
    )
    persister.initialize()
    yield persister
    persister.cleanup()


def test_save_and_load_state(postgresql_persister):
    postgresql_persister.save("pk", "app_id", 1, "pos", state.State({"a": 1, "b": 2}), "completed")
    data = postgresql_persister.load("pk", "app_id", 1)
    assert data["state"].get_all() == {"a": 1, "b": 2}


def test_list_app_ids(postgresql_persister):
    postgresql_persister.save("pk", "app_id1", 1, "pos1", state.State({"a": 1}), "completed")
    postgresql_persister.save("pk", "app_id2", 2, "pos2", state.State({"b": 2}), "completed")
    app_ids = postgresql_persister.list_app_ids("pk")
    assert "app_id1" in app_ids
    assert "app_id2" in app_ids


def test_load_nonexistent_key(postgresql_persister):
    state_data = postgresql_persister.load("pk", "nonexistent_key")
    assert state_data is None


def test_is_initialized(postgresql_persister):
    """Tests that a new connection also returns True for is_initialized."""
    assert postgresql_persister.is_initialized()
    persister2 = PostgreSQLPersister.from_values(
        db_name="postgres",
        user="postgres",
        password="postgres",
        host="localhost",
        port=5432,
        table_name="testtable",
    )
    assert persister2.is_initialized()


def test_is_initialized_false():
    persister = PostgreSQLPersister.from_values(
        db_name="postgres",
        user="postgres",
        password="postgres",
        host="localhost",
        port=5432,
        table_name="testtable2",
    )
    assert not persister.is_initialized()


def test_serialization_with_pickle(postgresql_persister):
    # Save some state
    postgresql_persister.save(
        "pk", "app_id_serde", 1, "pos", state.State({"a": 1, "b": 2}), "completed"
    )

    # Serialize the persister
    serialized_persister = pickle.dumps(postgresql_persister)

    # Deserialize the persister
    deserialized_persister = pickle.loads(serialized_persister)

    # Load the state from the deserialized persister
    data = deserialized_persister.load("pk", "app_id_serde", 1)

    assert data["state"].get_all() == {"a": 1, "b": 2}


@pytest.fixture
async def asyncpostgresql_persister():
    persister = await AsyncPostgreSQLPersister.from_values(
        db_name="postgres",
        user="postgres",
        password="postgres",
        host="localhost",
        port=5432,
        table_name="testtable_async",
    )
    await persister.initialize()
    yield persister
    await persister.cleanup()


async def test_async_pg_fixture(asyncpostgresql_persister):
    assert await asyncpostgresql_persister.is_initialized()


async def test_async_is_initialized_false():
    persister = await AsyncPostgreSQLPersister.from_values(
        db_name="postgres",
        user="postgres",
        password="postgres",
        host="localhost",
        port=5432,
        table_name="testtable_async2",
    )
    assert not await persister.is_initialized()


async def test_async_save_and_load_state(asyncpostgresql_persister):
    await asyncpostgresql_persister.save(
        "pk", "app_id", 1, "pos", state.State({"a": 1, "b": 2}), "completed"
    )
    data = await asyncpostgresql_persister.load("pk", "app_id", 1)
    print(data)
    assert data["state"].get_all() == {"a": 1, "b": 2}


async def test_async_list_app_ids(asyncpostgresql_persister):
    await asyncpostgresql_persister.save(
        "pk", "app_id1", 1, "pos1", state.State({"a": 1}), "completed"
    )
    await asyncpostgresql_persister.save(
        "pk", "app_id2", 2, "pos2", state.State({"b": 2}), "completed"
    )
    app_ids = await asyncpostgresql_persister.list_app_ids("pk")
    assert "app_id1" in app_ids
    assert "app_id2" in app_ids


async def test_async_load_nonexistent_key(asyncpostgresql_persister):
    state_data = await asyncpostgresql_persister.load("pk", "nonexistent_key")
    assert state_data is None



---
File: /burr/tests/integrations/serde/test_langchain.py
---

from langchain_community.document_transformers.embeddings_redundant_filter import _DocumentWithState
from langchain_core import documents as lc_documents
from langchain_core import messages as lc_messages

from burr.core import serde, state


def test_serde_of_lc_document():
    doc = lc_documents.Document(page_content="test content")
    og = state.State({"doc": doc})
    serialized = og.serialize()
    assert serialized == {
        "doc": {
            serde.KEY: "lc_document",
            "id": ["langchain", "schema", "document", "Document"],
            "kwargs": {"page_content": "test content", "type": "Document"},
            "lc": 1,
            "type": "constructor",
        }
    }
    ng = state.State.deserialize(serialized)
    assert isinstance(ng["doc"], lc_documents.Document)
    assert ng["doc"].page_content == "test content"
    assert serde.KEY not in ng


def test_serde_of_lc_message():
    message = lc_messages.HumanMessage(content="test content")
    og = state.State({"message": message})
    serialized = og.serialize()
    assert serialized == {
        "message": {
            serde.KEY: "lc_message",
            "data": {
                "additional_kwargs": {},
                "content": "test content",
                "example": False,
                "id": None,
                "name": None,
                "response_metadata": {},
                "type": "human",
            },
            "type": "human",
        }
    }
    ng = state.State.deserialize(serialized)
    assert isinstance(ng["message"], lc_messages.HumanMessage)
    assert ng["message"].content == "test content"
    assert serde.KEY not in ng


def test_serde_of_document_with_state():
    """Tests that we can serialize a document that is not serializable to a document."""
    doc = _DocumentWithState(page_content="Hello, World document with state!", state={"foo": "bar"})
    og = state.State({"doc": doc})
    serialized = og.serialize()
    assert serialized == {
        "doc": {
            serde.KEY: "lc_document_with_state",
            "doc": {
                serde.KEY: "lc_document",
                "id": ["langchain", "schema", "document", "Document"],
                "kwargs": {"page_content": "Hello, World document with state!", "type": "Document"},
                "lc": 1,
                "type": "constructor",
            },
            "state": {"foo": "bar"},
        }
    }
    ng = state.State.deserialize(serialized)
    assert isinstance(ng["doc"], lc_documents.Document)
    assert ng["doc"].page_content == "Hello, World document with state!"
    assert serde.KEY not in ng



---
File: /burr/tests/integrations/serde/test_pandas.py
---

import pandas as pd

from burr.core import serde, state


def test_serde_of_pandas_dataframe(tmp_path):
    df = pd.DataFrame({"a": [1, 2, 3], "b": [4, 5, 6]})
    og = state.State({"df": df})
    serialized = og.serialize(pandas_kwargs={"path": tmp_path})
    assert serialized["df"][serde.KEY] == "pandas.DataFrame"
    assert serialized["df"]["path"].startswith(str(tmp_path))
    assert (
        "df_a23d165ed4a2b8c6ccf24ac6276e35a9dc312e2828b4d0810416f4d47c614c7f.parquet"
        in serialized["df"]["path"]
    )
    ng = state.State.deserialize(serialized, pandas_kwargs={"path": tmp_path})
    assert isinstance(ng["df"], pd.DataFrame)
    pd.testing.assert_frame_equal(ng["df"], df)



---
File: /burr/tests/integrations/serde/test_pickle.py
---

from burr.core import serde, state
from burr.integrations.serde import pickle


class User:
    def __init__(self, name, email):
        self.name = name
        self.email = email


def test_serde_of_pickle_object():
    pickle.register_type_to_pickle(User)
    user = User(name="John Doe", email="john.doe@example.com")
    og = state.State({"user": user, "test": "test"})
    serialized = og.serialize()
    assert serialized == {
        "user": {
            serde.KEY: "pickle",
            "value": b"\x80\x04\x95Q\x00\x00\x00\x00\x00\x00\x00\x8c\x0btest_pi"
            b"ckle\x94\x8c\x04User\x94\x93\x94)\x81\x94}\x94(\x8c\x04na"
            b"me\x94\x8c\x08John Doe\x94\x8c\x05email\x94\x8c\x14john"
            b".doe@example.com\x94ub.",
        },
        "test": "test",
    }
    ng = state.State.deserialize(serialized)
    assert isinstance(ng["user"], User)
    assert ng["user"].name == "John Doe"
    assert ng["user"].email == "john.doe@example.com"



---
File: /burr/tests/integrations/serde/test_pydantic.py
---

from pydantic import BaseModel

from burr.core import serde, state


class User(BaseModel):
    name: str
    email: str


def test_serde_of_pydantic_model():
    user = User(name="John Doe", email="john.doe@example.com")
    og = state.State({"user": user})
    serialized = og.serialize()
    assert serialized == {
        "user": {
            serde.KEY: "pydantic",
            "__pydantic_class": "test_pydantic.User",
            "email": "john.doe@example.com",
            "name": "John Doe",
        }
    }
    ng = state.State.deserialize(serialized)
    assert isinstance(ng["user"], User)
    assert ng["user"].name == "John Doe"
    assert ng["user"].email == "john.doe@example.com"



---
File: /burr/tests/integrations/test_burr_hamilton.py
---

import pytest
from hamilton import ad_hoc_utils, driver

from burr.core import State
from burr.integrations.hamilton import Hamilton, from_state, from_value, update_state


def _incrementing_driver():
    def incremented_count(current_count: int) -> int:
        return current_count + 1

    def incremented_count_2(current_count: int, increment_by: int = 1) -> int:
        return current_count + increment_by

    def sum_of_counts(incremented_count: int, incremented_count_2: int) -> int:
        return incremented_count + incremented_count_2

    mod = ad_hoc_utils.create_temporary_module(
        incremented_count, incremented_count_2, sum_of_counts
    )
    dr = driver.Driver({}, mod)
    return dr


def test_set_driver():
    dr = _incrementing_driver()
    Hamilton.set_driver(dr)
    h = Hamilton({}, {}, driver=dr)
    assert h.driver == dr


def test__extract_inputs_overrides():
    dr = _incrementing_driver()
    h = Hamilton(
        inputs={"current_count": from_state("count"), "incremented_count_2": from_value(2)},
        outputs={"sum_of_counts": update_state("count")},
        driver=dr,
    )
    inputs, overrides = h._extract_inputs_overrides(State({"count": 0}))
    assert inputs == {"current_count": 0}
    assert overrides == {"incremented_count_2": 2}


def test__extract_inputs_overrides_missing_inputs():
    dr = _incrementing_driver()
    h = Hamilton(
        inputs={"current_count_not_present": from_state("count")},
        outputs={"sum_of_counts": update_state("count")},
        driver=dr,
    )
    with pytest.raises(ValueError, match="not available"):
        inputs, _ = h._extract_inputs_overrides(State({"count": 0}))


def test_reads():
    dr = _incrementing_driver()
    h = Hamilton(
        inputs={"current_count": from_state("count"), "incremented_count_2": from_value(2)},
        outputs={"sum_of_counts": update_state("count")},
        driver=dr,
    )
    assert h.reads == ["count"]


def test_writes():
    dr = _incrementing_driver()
    h = Hamilton(
        inputs={"current_count": from_state("count"), "incremented_count_2": from_value(2)},
        outputs={"sum_of_counts": update_state("count")},
        driver=dr,
    )
    assert h.writes == ["count"]


def test_run_step_with_multiple_inputs():
    dr = _incrementing_driver()
    h = Hamilton(
        inputs={"current_count": from_state("count"), "increment_by": from_value(5)},
        outputs={"sum_of_counts": update_state("count")},
        driver=dr,
    )
    result = h.run(State({"count": 1}))
    assert result == {"sum_of_counts": 8}
    new_state = h.update(result, State({"count": 1}))
    assert new_state.get_all() == {"count": 8}


def test_run_step_with_overrides():
    dr = _incrementing_driver()
    h = Hamilton(
        inputs={"current_count": from_state("count"), "incremented_count_2": from_value(2)},
        outputs={"sum_of_counts": update_state("count")},
        driver=dr,
    )
    result = h.run(State({"count": 1}))
    assert result == {"sum_of_counts": 4}
    new_state = h.update(result, State({"count": 1}))
    assert new_state.get_all() == {"count": 4}



---
File: /burr/tests/integrations/test_burr_haystack.py
---

import pytest
from haystack import Pipeline, component
from haystack.components.embedders import OpenAITextEmbedder
from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
from haystack.document_stores.in_memory import InMemoryDocumentStore
from haystack.utils.auth import Secret

from burr.core import State, action
from burr.core.application import ApplicationBuilder
from burr.core.graph import GraphBuilder
from burr.integrations.haystack import HaystackAction, haystack_pipeline_to_burr_graph


@component
class MockComponent:
    def __init__(self, required_init: str, optional_init: str = "default"):
        self.required_init = required_init
        self.optional_init = optional_init

    @component.output_types(output_1=str, output_2=str)
    def run(self, required_input: str, optional_input: str = "default") -> dict:
        return {
            "output_1": required_input,
            "output_2": optional_input,
        }


@component
class MockComponentWithWarmup:
    def __init__(self, required_init: str, optional_init: str = "default"):
        self.required_init = required_init
        self.optional_init = optional_init
        self.is_warm = False

    def warm_up(self):
        self.is_warm = True

    @component.output_types(output_1=str, output_2=str)
    def run(self, required_input: str, optional_input: str = "default") -> dict:
        if self.is_warm is False:
            raise RuntimeError("You must call ``warm_up()`` before running.")

        return {
            "output_1": required_input,
            "output_2": optional_input,
        }


@action(reads=["query_embedding"], writes=["documents"])
def retrieve_documents(state: State) -> State:
    query_embedding = state["query_embedding"]

    document_store = InMemoryDocumentStore()
    retriever = InMemoryEmbeddingRetriever(document_store)

    results = retriever.run(query_embedding=query_embedding)
    return state.update(documents=results["documents"])


def test_input_socket_mapping():
    # {input_socket_name: state_field}
    reads = {"required_input": "foo"}

    haction = HaystackAction(
        component=MockComponent(required_init="init"), name="mock", reads=reads, writes=[]
    )

    assert haction.reads == list(set(reads.values())) == ["foo"]


def test_input_socket_sequence():
    # {input_socket_name: input_socket_name}
    reads = ["required_input"]

    haction = HaystackAction(
        component=MockComponent(required_init="init"), name="mock", reads=reads, writes=[]
    )

    assert haction.reads == list(reads) == ["required_input"]


def test_output_socket_mapping():
    # {state_field: output_socket_name}
    writes = {"bar": "output_1"}

    haction = HaystackAction(
        component=MockComponent(required_init="init"), name="mock", reads=[], writes=writes
    )

    assert haction.writes == list(writes.keys()) == ["bar"]


def test_output_socket_sequence():
    # {output_socket_name: output_socket_name}
    writes = ["output_1"]

    haction = HaystackAction(
        component=MockComponent(required_init="init"), name="mock", reads=[], writes=writes
    )

    assert haction.writes == writes == ["output_1"]


def test_get_component_source():
    haction = HaystackAction(
        component=MockComponent(required_init="init"), name="mock", reads=[], writes=[]
    )

    expected_source = """\
@component
class MockComponent:
    def __init__(self, required_init: str, optional_init: str = "default"):
        self.required_init = required_init
        self.optional_init = optional_init

    @component.output_types(output_1=str, output_2=str)
    def run(self, required_input: str, optional_input: str = "default") -> dict:
        return {
            "output_1": required_input,
            "output_2": optional_input,
        }
"""

    assert haction.get_source() == expected_source


def test_run_with_external_inputs():
    state = State(initial_values={})
    haction = HaystackAction(
        component=MockComponent(required_init="init"), name="mock", reads=[], writes=[]
    )

    results = haction.run(state=state, required_input="as_input")

    assert results == {"output_1": "as_input", "output_2": "default"}


def test_run_with_state_inputs():
    state = State(initial_values={"foo": "bar"})
    haction = HaystackAction(
        component=MockComponent(required_init="init"),
        name="mock",
        reads={"required_input": "foo"},
        writes=[],
    )

    results = haction.run(state=state)

    assert results == {"output_1": "bar", "output_2": "default"}


def test_run_with_bound_params():
    state = State(initial_values={})
    haction = HaystackAction(
        component=MockComponent(required_init="init"),
        name="mock",
        reads=[],
        writes=[],
        bound_params={"required_input": "baz"},
    )

    results = haction.run(state=state)

    assert results == {"output_1": "baz", "output_2": "default"}


def test_run_mixed_params():
    state = State(initial_values={"foo": "bar"})
    haction = HaystackAction(
        component=MockComponent(required_init="init"),
        name="mock",
        reads={"required_input": "foo"},
        writes=[],
        bound_params={"optional_input": "baz"},
    )

    results = haction.run(state=state)

    assert results == {"output_1": "bar", "output_2": "baz"}


def test_run_with_sequence():
    state = State(initial_values={"required_input": "bar"})
    haction = HaystackAction(
        component=MockComponent(required_init="init"),
        name="mock",
        reads=["required_input"],
        writes=[],
    )

    results = haction.run(state=state)

    assert results == {"output_1": "bar", "output_2": "default"}


def test_update_with_writes_mapping():
    state = State(initial_values={})
    results = {"output_1": 1, "output_2": 2}
    haction = HaystackAction(
        component=MockComponent(required_init="init"),
        name="mock",
        reads=[],
        writes={"foo": "output_1"},
    )

    new_state = haction.update(result=results, state=state)

    assert new_state["foo"] == 1


def test_update_with_writes_sequence():
    state = State(initial_values={})
    results = {"output_1": 1, "output_2": 2}
    haction = HaystackAction(
        component=MockComponent(required_init="init"),
        name="mock",
        reads=[],
        writes=["output_1"],
    )

    new_state = haction.update(result=results, state=state)

    assert new_state["output_1"] == 1


def test_component_is_warmed_up():
    state = State(initial_values={})
    haction = HaystackAction(
        component=MockComponentWithWarmup(required_init="init"),
        name="mock",
        reads=[],
        writes=[],
        do_warm_up=True,
    )
    results = haction.run(state=state, required_input="as_input")
    assert results == {"output_1": "as_input", "output_2": "default"}


def test_component_is_not_warmed_up():
    state = State(initial_values={})
    haction = HaystackAction(
        component=MockComponentWithWarmup(required_init="init"),
        name="mock",
        reads=[],
        writes=[],
        do_warm_up=False,
    )
    with pytest.raises(RuntimeError):
        haction.run(state=state, required_input="as_input")


def test_pipeline_converter():
    # create haystack Pipeline
    retriever = InMemoryEmbeddingRetriever(InMemoryDocumentStore())
    text_embedder = OpenAITextEmbedder(
        model="text-embedding-3-small", api_key=Secret.from_token("mock-key")
    )

    basic_rag_pipeline = Pipeline()
    basic_rag_pipeline.add_component("text_embedder", text_embedder)
    basic_rag_pipeline.add_component("retriever", retriever)
    basic_rag_pipeline.connect("text_embedder.embedding", "retriever.query_embedding")

    # create Burr application
    embed_text = HaystackAction(
        component=text_embedder,
        name="text_embedder",
        reads=[],
        writes={"query_embedding": "embedding"},
    )

    retrieve_documents = HaystackAction(
        component=retriever,
        name="retriever",
        reads=["query_embedding"],
        writes=["documents"],
    )

    burr_graph = (
        GraphBuilder()
        .with_actions(embed_text, retrieve_documents)
        .with_transitions(("text_embedder", "retriever"))
        .build()
    )

    # convert the Haystack Pipeline to a Burr graph
    haystack_graph = haystack_pipeline_to_burr_graph(basic_rag_pipeline)

    converted_action_names = [action.name for action in haystack_graph.actions]
    for graph_action in burr_graph.actions:
        assert graph_action.name in converted_action_names

    for burr_t in burr_graph.transitions:
        assert any(
            burr_t.from_.name == haystack_t.from_.name and burr_t.to.name == haystack_t.to.name
            for haystack_t in haystack_graph.transitions
        )


def test_run_application():
    app = (
        ApplicationBuilder()
        .with_actions(
            HaystackAction(
                component=MockComponent(required_init="init"),
                name="mock",
                reads=[],
                writes=["output_1"],
            )
        )
        .with_transitions()
        .with_entrypoint("mock")
        .build()
    )

    _, _, state = app.run(halt_after=["mock"], inputs={"required_input": "runtime"})
    assert state["output_1"] == "runtime"


def test_run_application_is_warm_up():
    app = (
        ApplicationBuilder()
        .with_actions(
            HaystackAction(
                component=MockComponentWithWarmup(required_init="init"),
                name="mock",
                reads=[],
                writes=["output_1"],
            )
        )
        .with_transitions()
        .with_entrypoint("mock")
        .build()
    )

    _, _, state = app.run(halt_after=["mock"], inputs={"required_input": "runtime"})
    assert state["output_1"] == "runtime"


def test_run_application_is_not_warmed_up():
    app = (
        ApplicationBuilder()
        .with_actions(
            HaystackAction(
                component=MockComponentWithWarmup(required_init="init"),
                name="mock",
                reads=[],
                writes=["output_1"],
                do_warm_up=False,
            )
        )
        .with_transitions()
        .with_entrypoint("mock")
        .build()
    )
    with pytest.raises(RuntimeError):
        app.run(halt_after=["mock"], inputs={"required_input": "runtime"})



---
File: /burr/tests/integrations/test_burr_opentelemetry.py
---

import json

import pydantic
import pytest

from burr.core import serde
from burr.integrations.opentelemetry import convert_to_otel_attribute


class SampleModel(pydantic.BaseModel):
    foo: int
    bar: bool


@pytest.mark.parametrize(
    "value, expected",
    [
        ("hello", "hello"),
        (1, 1),
        ((1, 1), [1, 1]),
        ((1.0, 1.0), [1.0, 1.0]),
        ((True, True), [True, True]),
        (("hello", "hello"), ["hello", "hello"]),
        (SampleModel(foo=1, bar=True), json.dumps(serde.serialize(SampleModel(foo=1, bar=True)))),
    ],
)
def test_convert_to_otel_attribute(value, expected):
    assert convert_to_otel_attribute(value) == expected



---
File: /burr/tests/integrations/test_burr_pydantic_future_annotations.py
---

from __future__ import annotations

import os

"""This tests that the pydantic integration allows for future import of annotations"""

file_name = os.path.join(os.path.dirname(__file__), "test_burr_pydantic.py")
eval(compile(open(file_name).read(), file_name, "exec"))



---
File: /burr/tests/integrations/test_burr_pydantic.py
---

import asyncio
from typing import AsyncGenerator, Generator, List, Optional, Tuple

import pydantic
import pytest
from pydantic import BaseModel, ConfigDict, EmailStr, Field
from pydantic.fields import FieldInfo

from burr.core import expr
from burr.core.action import (
    AsyncStreamingResultContainer,
    FunctionBasedAction,
    StreamingResultContainer,
    action,
    streaming_action,
)
from burr.core.application import ApplicationBuilder
from burr.core.state import State
from burr.integrations.pydantic import (
    PydanticTypingSystem,
    _validate_and_extract_signature_types,
    _validate_keys,
    merge_to_state,
    model_from_state,
    model_to_dict,
    pydantic_action,
    pydantic_streaming_action,
    subset_model,
)


# Define a nested Pydantic model
class NestedModel(BaseModel):
    nested_field1: int
    nested_field2: Optional[str] = "default_string"


# Expanded OriginalModel
class OriginalModel(BaseModel):
    foo: int
    bar: str
    baz: Optional[int] = None
    qux: Optional[EmailStr] = Field(None, pattern=r"^[a-z0-9]+@[a-z]+\.[a-z]{2,3}$")
    nested: NestedModel
    list_field: List[int] = [1, 2, 3]


def _assert_fields_match(field_original: FieldInfo, field_new: FieldInfo, made_optional: bool):
    if not made_optional:
        assert field_original.annotation == field_new.annotation
        assert field_new.is_required() == field_original.is_required()
    else:
        assert field_original.annotation == Optional[field_new.annotation]
        assert not field_new.is_required()
    assert field_original.default == field_new.default


@pytest.mark.parametrize(
    "fields,force_optional_fields,model_name_suffix,expected_fields,expected_optionals,expected_name",
    [
        # Test case 1: Subset with optional fields maintained
        (["foo", "qux"], [], "Subset", ["foo", "qux"], [], "OriginalModelSubset"),
        # Test case 2: Subset with forced optional fields
        (["foo", "baz"], ["baz"], "Test", ["foo", "baz"], ["baz"], "OriginalModelTest"),
        # Test case 3: Handling nested models and default values
        (
            ["nested", "list_field"],
            [],
            "NestedSubset",
            ["nested", "list_field"],
            [],
            "OriginalModelNestedSubset",
        ),
        # Test case 4: forcing optional with something that wasn't optional
        (["foo"], ["bar"], "Test", ["foo"], ["bar"], "OriginalModelTest"),
    ],
)
def test_subset_model_success(
    fields: List[str],
    force_optional_fields: List[str],
    model_name_suffix: str,
    expected_fields: List[str],
    expected_optionals: List[str],
    expected_name: str,
):
    """Test the successful creation of subset models with proper fields and optionality."""
    SubsetModel = subset_model(OriginalModel, fields, force_optional_fields, model_name_suffix)
    assert SubsetModel.__name__ == expected_name  # Ensure the model name is correct
    for field in expected_fields:
        assert field in SubsetModel.model_fields  # Ensure the field is in the subset
        if field in expected_optionals:
            _assert_fields_match(
                OriginalModel.model_fields[field],
                SubsetModel.model_fields[field],
                made_optional=False,
            )
        elif field in force_optional_fields:
            _assert_fields_match(
                OriginalModel.model_fields[field],
                SubsetModel.model_fields[field],
                made_optional=True,
            )


def test_subset_model_copy_config():
    class Arbitrary:
        pass

    class MyModelWithConfig(pydantic.BaseModel):
        foo: int
        arbitrary: Arbitrary

        model_config = ConfigDict(arbitrary_types_allowed=True)

    SubsetModel = subset_model(MyModelWithConfig, ["foo", "bar"], [], "Subset")
    assert SubsetModel.__name__ == "MyModelWithConfigSubset"
    assert SubsetModel.model_config == {"arbitrary_types_allowed": True}


def test_merge_to_state():
    model = OriginalModel(
        foo=1,
        bar="bar",
        baz=2,
        qux="email@email.io",
        nested=NestedModel(nested_field1=1),
        list_field=[1, 2],
    )
    write_keys = ["foo", "baz", "nested"]
    state = State(dict(foo=2, list_field=[3, 4], not_written="prior_value"))
    new_state = merge_to_state(model, write_keys, state)
    assert new_state.get_all() == {
        **model_to_dict(model, include=write_keys),
        "not_written": "prior_value",
        "list_field": [3, 4],
    }


def test_model_from_state():
    model_data = dict(
        foo=1,
        bar="bar",
        baz=2,
        qux="email@email.io",
        nested=NestedModel(nested_field1=1),
        list_field=[1, 2],
    )
    state = State(
        model_data,
    )
    model = model_from_state(OriginalModel, state)
    assert model_to_dict(model) == state.get_all()


def _fn_without_state_arg(foo: OriginalModel) -> OriginalModel:
    ...


def _fn_with_incorrect_state_arg(state: int) -> OriginalModel:
    ...


def _fn_with_incorrect_return_type(state: OriginalModel) -> int:
    ...


def _fn_with_no_return_type(state: OriginalModel):
    ...


def _fn_correct_same_itype_otype(state: OriginalModel, input_1: int) -> OriginalModel:
    ...


def _fn_correct_diff_itype_otype(state: OriginalModel, input_1: int) -> NestedModel:
    ...


@pytest.mark.parametrize(
    "fn,expected_exception",
    [
        (_fn_without_state_arg, ValueError),
        (_fn_with_incorrect_state_arg, ValueError),
        (_fn_with_incorrect_return_type, ValueError),
        (_fn_with_no_return_type, ValueError),
    ],
)
def test__validate_and_extract_signature_types_error(fn, expected_exception):
    with pytest.raises(expected_exception=expected_exception):
        _validate_and_extract_signature_types(fn)


@pytest.mark.parametrize(
    "fn,expected",
    [
        (_fn_correct_same_itype_otype, (OriginalModel, OriginalModel)),
        (_fn_correct_diff_itype_otype, (OriginalModel, NestedModel)),
    ],
)
def test__validate_and_extract_signature_types_success(fn, expected):
    itype, otype = _validate_and_extract_signature_types(fn)
    assert itype == expected[0]
    assert otype == expected[1]


def test__validate_keys():
    _validate_keys(OriginalModel, ["foo", "baz", "nested"], _fn_correct_same_itype_otype)


class StateModelIn(BaseModel):
    foo: int
    bar: str


class StateModelOut(BaseModel):
    baz: Optional[int] = None
    qux: Optional[EmailStr] = Field(default=None)


class StateModel(StateModelIn, StateModelOut):
    pass


class ModelWithDefaults(BaseModel):
    a: int
    b: str = "b"
    c: list[int] = pydantic.Field(default_factory=list)
    d: Optional[str] = None


def test_subset_model_with_defaults():
    SubsetModel = subset_model(ModelWithDefaults, [], ["a", "b", "c", "d"], "Subset")
    assert SubsetModel.__name__ == "ModelWithDefaultsSubset"
    mod = SubsetModel(a=1)
    assert mod.a == 1
    assert mod.b == "b"
    assert mod.c == []
    assert mod.d is None


def test_pydantic_action_returns_correct_results_same_io_modified():
    @pydantic_action(reads=["foo", "bar"], writes=["baz", "qux"])
    def act(state: StateModel, tld: str) -> StateModel:
        state.baz = state.foo + 1
        state.qux = f"{state.bar}@{state.bar}.{tld}"
        return state

    assert hasattr(act, "bind")  # has to have bind
    assert (action_function := getattr(act, FunctionBasedAction.ACTION_FUNCTION, None)) is not None
    assert action_function.inputs == (["tld"], [])
    assert action_function.reads == ["foo", "bar"]
    assert action_function.writes == ["baz", "qux"]
    result = action_function.fn(
        State(dict(foo=1, bar="bar")),
        tld="com",
    )
    # TODO - figure out if we want the old state objects lying around
    # For now we don't care, this will be handled by the state merge operations
    assert result["baz"] == 2
    assert result["qux"] == "bar@bar.com"


async def test_pydantic_action_returns_correct_results_same_io_modified_async():
    @pydantic_action(reads=["foo", "bar"], writes=["baz", "qux"])
    async def act(state: StateModel, tld: str) -> StateModel:
        await asyncio.sleep(0.0001)
        state.baz = state.foo + 1
        state.qux = f"{state.bar}@{state.bar}.{tld}"
        return state

    assert hasattr(act, "bind")  # has to have bind
    assert (action_function := getattr(act, FunctionBasedAction.ACTION_FUNCTION, None)) is not None
    assert action_function.reads == ["foo", "bar"]
    assert action_function.writes == ["baz", "qux"]
    result = await action_function.fn(
        State(dict(foo=1, bar="bar")),
        tld="com",
    )
    # TODO - figure out if we want the old state objects lying around
    # For now we don't care, this will be handled by the state merge operations
    assert result["baz"] == 2
    assert result["qux"] == "bar@bar.com"


def test_pydantic_action_returns_correct_results_different_io_modified():
    @pydantic_action(reads=["foo", "bar"], writes=["baz", "qux"])
    def act(state: StateModelIn, tld: str) -> StateModelOut:
        return StateModelOut(baz=state.foo + 1, qux=f"{state.bar}@{state.bar}.{tld}")

    assert hasattr(act, "bind")  # has to have bind
    assert (action_function := getattr(act, FunctionBasedAction.ACTION_FUNCTION, None)) is not None
    assert action_function.reads == ["foo", "bar"]
    assert action_function.writes == ["baz", "qux"]
    result = action_function.fn(
        State(dict(foo=1, bar="bar")),
        tld="com",
    )
    # TODO - figure out if we want the old state objects lying around
    # For now we don't care, this will be handled by the state merge operations
    assert result["baz"] == 2
    assert result["qux"] == "bar@bar.com"


def test_pydantic_action_returns_correct_results_different_io_modified_specified_type_in_decorator():
    @pydantic_action(
        reads=["foo", "bar"],
        writes=["baz", "qux"],
        state_input_type=StateModelIn,
        state_output_type=StateModelOut,
    )
    def act(state, tld):
        return StateModelOut(baz=state.foo + 1, qux=f"{state.bar}@{state.bar}.{tld}")

    assert hasattr(act, "bind")  # has to have bind
    assert (action_function := getattr(act, FunctionBasedAction.ACTION_FUNCTION, None)) is not None
    assert action_function.reads == ["foo", "bar"]
    assert action_function.writes == ["baz", "qux"]
    result = action_function.fn(
        State(dict(foo=1, bar="bar")),
        tld="com",
    )
    # TODO - figure out if we want the old state objects lying around
    # For now we don't care, this will be handled by the state merge operations
    assert result["baz"] == 2
    assert result["qux"] == "bar@bar.com"


async def test_pydantic_action_returns_correct_results_different_io_modified_async():
    @pydantic_action(reads=["foo", "bar"], writes=["baz", "qux"])
    async def act(state: StateModelIn, tld: str) -> StateModelOut:
        await asyncio.sleep(0.0001)
        return StateModelOut(baz=state.foo + 1, qux=f"{state.bar}@{state.bar}.{tld}")

    assert hasattr(act, "bind")  # has to have bind
    assert (action_function := getattr(act, FunctionBasedAction.ACTION_FUNCTION, None)) is not None
    assert action_function.inputs == (["tld"], [])
    assert action_function.reads == ["foo", "bar"]
    assert action_function.writes == ["baz", "qux"]
    result = await action_function.fn(
        State(dict(foo=1, bar="bar")),
        tld="com",
    )
    # TODO - figure out if we want the old state objects lying around
    # For now we don't care, this will be handled by the state merge operations
    assert result["baz"] == 2
    assert result["qux"] == "bar@bar.com"


def test_pydantic_action_incorrect_reads():
    def act(state: StateModel, tld: str) -> StateModel:
        ...

    with pytest.raises(ValueError, match="are not present in the model"):
        pydantic_action(reads=["foo", "bar", "not_present"], writes=["baz", "qux"])(act)


def test_pydantic_action_incorrect_writes():
    def act(state: StateModel, tld: str) -> StateModel:
        ...

    with pytest.raises(ValueError, match="are not present in the model"):
        pydantic_action(reads=["foo", "bar"], writes=["baz", "qux", "not_prsent"])(act)


# Simple model to test streaming pydantic model
class AppStateModel(BaseModel):
    count: int
    times_called: int
    other: bool = Field(default=False)
    yet_another: float = Field(default=0.0)


class IntermediateModel(BaseModel):
    result: int


def test_streaming_pydantic_action_same_io():
    @pydantic_streaming_action(
        reads=["count", "times_called"],
        writes=["count", "times_called"],
        stream_type=IntermediateModel,
        state_input_type=AppStateModel,
        state_output_type=AppStateModel,
    )
    def act(
        state: AppStateModel, total_count: int
    ) -> Generator[Tuple[IntermediateModel, Optional[AppStateModel]], None, None]:
        initial_value = state.count
        for i in range(initial_value, initial_value + total_count):
            yield IntermediateModel(result=i), None
            state.count = i
        state.times_called += 1
        yield IntermediateModel(result=state.count), state

    assert hasattr(act, "bind")  # has to have bind
    assert (action_function := getattr(act, FunctionBasedAction.ACTION_FUNCTION, None)) is not None
    assert action_function.inputs == (["total_count"], [])
    gen = action_function.fn(
        State(dict(count=1, times_called=0), typing_system=PydanticTypingSystem(AppStateModel)),
        total_count=5,
    )
    result = list(gen)
    assert len(result) == 6
    assert [item[0].result for item in result] == [1, 2, 3, 4, 5, 5]
    assert all([isinstance(item[0], IntermediateModel) for item in result])
    assert all([item[1] is None for item in result[:-1]])
    assert isinstance(final_state := result[-1][1], State)
    assert final_state["count"] == 5
    assert final_state["times_called"] == 1
    assert final_state.data.count == 5
    assert final_state.data.times_called == 1


async def test_streaming_pydantic_action_same_io_async():
    @pydantic_streaming_action(
        reads=["count", "times_called"],
        writes=["count", "times_called"],
        stream_type=IntermediateModel,
        state_input_type=AppStateModel,
        state_output_type=AppStateModel,
    )
    async def act(
        state: AppStateModel, total_count: int
    ) -> AsyncGenerator[Tuple[IntermediateModel, Optional[AppStateModel]], None]:
        initial_value = state.count
        for i in range(initial_value, initial_value + total_count):
            await asyncio.sleep(0.0001)
            yield IntermediateModel(result=i), None
            state.count = i
        state.times_called += 1
        await asyncio.sleep(0.0001)
        yield IntermediateModel(result=state.count), state

    assert hasattr(act, "bind")  # has to have bind
    assert (action_function := getattr(act, FunctionBasedAction.ACTION_FUNCTION, None)) is not None
    assert action_function.inputs == (["total_count"], [])
    gen = action_function.fn(
        State(dict(count=1, times_called=0), typing_system=PydanticTypingSystem(AppStateModel)),
        total_count=5,
    )
    result = [item async for item in gen]
    assert len(result) == 6
    assert [item[0].result for item in result] == [1, 2, 3, 4, 5, 5]
    assert all([isinstance(item[0], IntermediateModel) for item in result])
    assert all([item[1] is None for item in result[:-1]])
    assert isinstance(final_state := result[-1][1], State)
    assert final_state["count"] == 5
    assert final_state["times_called"] == 1
    assert final_state.data.count == 5
    assert final_state.data.times_called == 1


class OutputModel(BaseModel):
    completed: bool = Field(default=False)
    sum_of_values: int = Field(default=0)


class InputModel(BaseModel):
    count: int
    increment_by: int
    other_value: Optional[str] = None
    yet_another_value: Optional[float] = None


class StateModelStreaming(InputModel, OutputModel):
    pass


def test_streaming_pydantic_action_different_io():
    @pydantic_streaming_action(
        reads=["count", "increment_by"],
        writes=["completed", "sum_of_values"],
        stream_type=IntermediateModel,
        state_input_type=InputModel,
        state_output_type=OutputModel,
    )
    def act(
        state: InputModel, total_count: int
    ) -> Generator[Tuple[IntermediateModel, Optional[OutputModel]], None, None]:
        sum_of_values = 0
        for i in range(total_count):
            yield IntermediateModel(result=i), None
            sum_of_values += state.increment_by
        yield IntermediateModel(result=total_count), OutputModel(
            completed=True, sum_of_values=sum_of_values
        )

    assert hasattr(act, "bind")  # has to have bind
    assert (action_function := getattr(act, FunctionBasedAction.ACTION_FUNCTION, None)) is not None
    assert action_function.inputs == (["total_count"], [])
    gen = action_function.fn(
        State(
            dict(count=1, increment_by=2), typing_system=PydanticTypingSystem(StateModelStreaming)
        ),
        total_count=5,
    )
    assert action_function.inputs == (["total_count"], [])
    result = list(gen)
    assert len(result) == 6
    assert [item[0].result for item in result] == [0, 1, 2, 3, 4, 5]
    assert all([isinstance(item[0], IntermediateModel) for item in result])
    assert all([item[1] is None for item in result[:-1]])
    assert isinstance(final_state := result[-1][1], State)
    assert final_state["completed"] is True
    assert final_state["sum_of_values"] == 10
    assert final_state.data.completed is True
    assert final_state.data.sum_of_values == 10


async def test_streaming_pydantic_action_different_io_async():
    @pydantic_streaming_action(
        reads=["count", "increment_by"],
        writes=["completed", "sum_of_values"],
        stream_type=IntermediateModel,
        state_input_type=InputModel,
        state_output_type=OutputModel,
    )
    async def act(
        state: InputModel, total_count: int
    ) -> AsyncGenerator[Tuple[IntermediateModel, Optional[OutputModel]], None]:
        sum_of_values = 0
        for i in range(total_count):
            await asyncio.sleep(0.0001)
            yield IntermediateModel(result=i), None
            sum_of_values += state.increment_by
        await asyncio.sleep(0.0001)
        yield IntermediateModel(result=total_count), OutputModel(
            completed=True, sum_of_values=sum_of_values
        )

    assert hasattr(act, "bind")  # has to have bind
    assert (action_function := getattr(act, FunctionBasedAction.ACTION_FUNCTION, None)) is not None
    assert action_function.inputs == (["total_count"], [])
    gen = action_function.fn(
        State(
            dict(count=1, increment_by=2), typing_system=PydanticTypingSystem(StateModelStreaming)
        ),
        total_count=5,
    )
    result = [item async for item in gen]
    assert len(result) == 6
    assert [item[0].result for item in result] == [0, 1, 2, 3, 4, 5]
    assert all([isinstance(item[0], IntermediateModel) for item in result])
    assert all([item[1] is None for item in result[:-1]])
    assert isinstance(final_state := result[-1][1], State)
    assert final_state["completed"] is True
    assert final_state["sum_of_values"] == 10
    assert final_state.data.completed is True
    assert final_state.data.sum_of_values == 10


class CounterExampleAppState(pydantic.BaseModel):
    count: int = 0
    counter_called_times: int = 0
    num_counter_calls: int = 10
    final_result: int = 0


def test_end_to_end_pydantic(tmpdir):
    @action.pydantic(
        reads=["count", "counter_called_times"],
        writes=["count", "counter_called_times"],
    )
    def counter(state: CounterExampleAppState, increment_by: int) -> CounterExampleAppState:
        state.count += increment_by
        state.counter_called_times += 1
        return state

    @action.pydantic(
        reads=["count"],
        writes=["final_result"],
    )
    def final_result(state: CounterExampleAppState) -> CounterExampleAppState:
        state.final_result = state.count
        return state

    app = (
        ApplicationBuilder()
        .with_actions(counter=counter, final_result=final_result)
        .with_entrypoint("counter")
        .with_tracker(tracker="local", params={"storage_dir": tmpdir})
        .with_typing(PydanticTypingSystem(CounterExampleAppState))
        .with_state(CounterExampleAppState())
        .with_transitions(
            ("counter", "final_result", expr("counter_called_times == num_counter_calls")),
            ("counter", "counter"),
        )
        .build()
    )
    act, result, state = app.run(halt_after=["final_result"], inputs={"increment_by": 2})
    assert isinstance(state.data, CounterExampleAppState)

    assert state.data.final_result == 20
    assert state.data.counter_called_times == 10
    assert state.data.count == 20


async def test_end_to_end_pydantic_async(tmpdir):
    @action.pydantic(
        reads=["count", "counter_called_times"],
        writes=["count", "counter_called_times"],
    )
    async def counter(state: CounterExampleAppState, increment_by: int) -> CounterExampleAppState:
        await asyncio.sleep(0.0001)
        state.count += increment_by
        state.counter_called_times += 1
        return state

    @action.pydantic(
        reads=["count"],
        writes=["final_result"],
    )
    async def final_result(state: CounterExampleAppState) -> CounterExampleAppState:
        await asyncio.sleep(0.0001)
        state.final_result = state.count
        return state

    app = (
        ApplicationBuilder()
        .with_actions(counter=counter, final_result=final_result)
        .with_entrypoint("counter")
        .with_tracker(tracker="local", params={"storage_dir": tmpdir})
        .with_typing(PydanticTypingSystem(CounterExampleAppState))
        .with_state(CounterExampleAppState())
        .with_transitions(
            ("counter", "final_result", expr("counter_called_times == num_counter_calls")),
            ("counter", "counter"),
        )
        .build()
    )
    *_, state = await app.arun(halt_after=["final_result"], inputs={"increment_by": 2})
    assert isinstance(state.data, CounterExampleAppState)

    assert state.data.final_result == 20
    assert state.data.counter_called_times == 10
    assert state.data.count == 20


class IntermediateStreamModel(BaseModel):
    result: int


def test_end_to_end_pydantic_streaming(tmpdir):
    @action.pydantic(
        reads=["count", "counter_called_times"],
        writes=["count", "counter_called_times"],
    )
    def counter(state: CounterExampleAppState, increment_by: int) -> CounterExampleAppState:
        state.count += increment_by
        state.counter_called_times += 1
        return state

    @streaming_action.pydantic(
        reads=["count"],
        writes=["final_result"],
        state_output_type=CounterExampleAppState,
        state_input_type=CounterExampleAppState,
        stream_type=IntermediateModel,
    )
    def final_result_streamed(
        state: CounterExampleAppState,
    ) -> Generator[Tuple[IntermediateModel, Optional[CounterExampleAppState]], None, None]:
        for i in range(state.count):
            yield IntermediateModel(result=i), None
        state.final_result = state.count
        yield IntermediateModel(result=state.count), state

    app = (
        ApplicationBuilder()
        .with_actions(counter=counter, final_result=final_result_streamed)
        .with_entrypoint("counter")
        .with_tracker(tracker="local", params={"storage_dir": tmpdir})
        .with_typing(PydanticTypingSystem(CounterExampleAppState))
        .with_state(CounterExampleAppState())
        .with_transitions(
            ("counter", "final_result", expr("counter_called_times == num_counter_calls")),
            ("counter", "counter"),
        )
        .build()
    )
    _, container = app.stream_result(halt_after=["final_result"], inputs={"increment_by": 2})  # type: ignore
    container: StreamingResultContainer[CounterExampleAppState, IntermediateModel]
    results = list(container)
    assert all(isinstance(item, IntermediateModel) for item in results)
    result, state = container.get()

    assert state.data.final_result == 20
    assert state.data.counter_called_times == 10
    assert state.data.count == 20
    assert isinstance(result, IntermediateModel)
    assert result.result == 20


async def test_end_to_end_pydantic_streaming_async(tmpdir):
    @action.pydantic(
        reads=["count", "counter_called_times"],
        writes=["count", "counter_called_times"],
    )
    async def counter(state: CounterExampleAppState, increment_by: int) -> CounterExampleAppState:
        await asyncio.sleep(0.0001)
        state.count += increment_by
        state.counter_called_times += 1
        return state

    @streaming_action.pydantic(
        reads=["count"],
        writes=["final_result"],
        state_output_type=CounterExampleAppState,
        state_input_type=CounterExampleAppState,
        stream_type=IntermediateModel,
    )
    async def final_result_streamed(
        state: CounterExampleAppState,
    ) -> AsyncGenerator[Tuple[IntermediateModel, Optional[CounterExampleAppState]], None]:
        for i in range(state.count):
            yield IntermediateModel(result=i), None
        state.final_result = state.count
        yield IntermediateModel(result=state.count), state

    app = (
        ApplicationBuilder()
        .with_actions(counter=counter, final_result=final_result_streamed)
        .with_entrypoint("counter")
        .with_tracker(tracker="local", params={"storage_dir": tmpdir})
        .with_typing(PydanticTypingSystem(CounterExampleAppState))
        .with_state(CounterExampleAppState())
        .with_transitions(
            ("counter", "final_result", expr("counter_called_times == num_counter_calls")),
            ("counter", "counter"),
        )
        .build()
    )
    _, container = await app.astream_result(halt_after=["final_result"], inputs={"increment_by": 2})  # type: ignore
    container: AsyncStreamingResultContainer[CounterExampleAppState, IntermediateModel]
    results = [item async for item in container]
    assert all(isinstance(item, IntermediateModel) for item in results)
    result, state = await container.get()

    assert state.data.final_result == 20
    assert state.data.counter_called_times == 10
    assert state.data.count == 20
    assert isinstance(result, IntermediateModel)
    assert result.result == 20



---
File: /burr/tests/integrations/test_burr_ray.py
---

import uuid
from typing import Any, Dict, Generator

import pytest
import ray

from burr.core import (
    ApplicationBuilder,
    ApplicationContext,
    GraphBuilder,
    Result,
    State,
    action,
    expr,
)
from burr.core.parallelism import MapStates, RunnableGraph, SubgraphType
from burr.integrations.ray import RayExecutor


@pytest.fixture(scope="module")
def init():
    ray.init()
    yield "initialized"
    ray.shutdown()


def test_ray_executor_simple(init):
    executor = RayExecutor()

    futures = [executor.submit(lambda x: x * x, num) for num in range(0, 10)]
    results = [future.result(timeout=5) for future in futures]  # Adjust timeout as necessary
    expected_results = [num * num for num in range(0, 10)]
    assert results == expected_results, "Each number should be squared correctly"


def test_ray_executor_end_to_end_persistence(init, tmpdir):
    """Largely a duplicate of the test_parallelism test_end_to_end test with a few small changes.
    We don't have persistence as sqlite + ray are not happy together (need to shut down resources
    better, more likely, but really people should use a multi-tennant db. We should consider
    unifying but not worth it now"""

    MIN_NUMBER = 90
    MAX_NUMBER = 100

    # dummy as we want an initial action to decide between odd/even next
    @action(reads=["n"], writes=["n", "original_n"])
    def initial(state: State, __context: ApplicationContext) -> State:
        # This assert ensures we only visit once per app, globally
        # Thus if we're restarting this will break
        return state.update(original_n=state["n"])

    @action(reads=["n"], writes=["n", "n_history"])
    def even(state: State) -> State:
        return state.update(n=state["n"] // 2).append(n_history=state["n"])

    @action(reads=["n"], writes=["n", "n_history"])
    def odd(state: State) -> State:
        return state.update(n=3 * state["n"] + 1).append(n_history=state["n"])

    collatz_graph = (
        GraphBuilder()
        .with_actions(
            initial,
            even,
            odd,
            result=Result("n_history"),
        )
        .with_transitions(
            (["initial", "even"], "result", expr("n == 1")),
            (["initial", "even", "odd"], "even", expr("n % 2 == 0")),
            (["initial", "even", "odd"], "odd", expr("n % 2 != 0")),
        )
        .build()
    )

    @action(reads=[], writes=["ns"])
    def map_step(state: State, min_number: int = MIN_NUMBER, max_number: int = MAX_NUMBER) -> State:
        return state.update(ns=list(range(min_number, max_number)))

    class ParallelCollatz(MapStates):
        def states(
            self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
        ) -> Generator[State, None, None]:
            for item in state["ns"]:
                yield state.update(n=item)

        def action(self, state: State, inputs: Dict[str, Any]) -> SubgraphType:
            return RunnableGraph(
                collatz_graph,
                entrypoint="initial",
                halt_after=["result"],
            )

        def reduce(self, state: State, results: Generator[State, None, None]) -> State:
            new_state = state
            count_mapping = {}
            for result in results:
                count_mapping[result["original_n"]] = len(result["n_history"])
            return new_state.update(counts=count_mapping)

        @property
        def writes(self) -> list[str]:
            return ["counts"]

        @property
        def reads(self) -> list[str]:
            return ["ns"]

    app_id = f"collatz_test_{str(uuid.uuid4())}"
    containing_application = (
        ApplicationBuilder()
        .with_actions(
            map_step,
            parallel_collatz=ParallelCollatz(),
            final=Result("counts"),
        )
        .with_transitions(
            ("map_step", "parallel_collatz"),
            ("parallel_collatz", "final"),
        )
        .with_identifiers(app_id=app_id)
        .with_parallel_executor(RayExecutor)
        .with_entrypoint("map_step")
        .build()
    )
    *_, final_state = containing_application.run(halt_after=["final"])
    assert len(final_state["counts"]) == MAX_NUMBER - MIN_NUMBER



---
File: /burr/tests/integrations/test_opentelemetry.py
---

import typing

import burr.integrations.opentelemetry as burr_otel


def test_instrument_specs_match_instruments_literal():
    assert set(typing.get_args(burr_otel.INSTRUMENTS)) == set(burr_otel.INSTRUMENTS_SPECS.keys())



---
File: /burr/tests/tracking/test_common_models.py
---

from burr.core import Action, State
from burr.tracking.common.models import ActionModel


class ActionWithCustomSource(Action):
    def __init__(self):
        super().__init__()

    @property
    def reads(self) -> list[str]:
        return []

    def run(self, state: State, **run_kwargs) -> dict:
        return {}

    @property
    def writes(self) -> list[str]:
        return []

    def update(self, result: dict, state: State) -> State:
        return state

    def get_source(self) -> str:
        return "custom source code"


def test_action_with_custom_source():
    model = ActionModel.from_action(ActionWithCustomSource().with_name("foo"))
    assert model.code == "custom source code"



---
File: /burr/tests/tracking/test_local_tracking_client.py
---

import json
import os
import uuid
from typing import Literal, Optional, Tuple

import pytest

import burr
from burr import lifecycle
from burr.core import Action, Application, ApplicationBuilder, Result, State, action, default, expr
from burr.core.persistence import BaseStatePersister, PersistedStateData
from burr.tracking import LocalTrackingClient
from burr.tracking.client import _allowed_project_name
from burr.tracking.common.models import (
    ApplicationMetadataModel,
    ApplicationModel,
    AttributeModel,
    BeginEntryModel,
    BeginSpanModel,
    ChildApplicationModel,
    EndEntryModel,
    EndSpanModel,
)
from burr.visibility import TracerFactory


@action(reads=["counter", "break_at"], writes=["counter"])
def counter(state: State, __tracer: TracerFactory) -> Tuple[dict, State]:
    with __tracer("increment") as t:
        result = {"counter": state["counter"] + 1}
        t.log_attributes(counter=result["counter"])
    if state["break_at"] == result["counter"]:
        raise ValueError("Broken")
    return result, state.update(**result)


def sample_application(
    project_name: str,
    log_dir: str,
    app_id: str,
    broken: bool = False,
    spawn_from: Tuple[Optional[str], Optional[int]] = (None, None),
):
    return (
        burr.core.ApplicationBuilder()
        .with_state(counter=0, break_at=2 if broken else -1)
        .with_actions(counter=counter, result=Result("counter"))
        .with_transitions(
            ("counter", "counter", expr("counter < 2")),  # just count to two for testing
            ("counter", "result", default),
        )
        .with_entrypoint("counter")
        .with_tracker(project=project_name, tracker="local", params={"storage_dir": log_dir})
        .with_identifiers(app_id=app_id)
        .with_spawning_parent(
            app_id=spawn_from[0],
            sequence_id=spawn_from[1],  # no need to test the partition key here really
        )
        .build()
    )


def test_application_tracks_end_to_end(tmpdir: str):
    app_id = str(uuid.uuid4())
    log_dir = os.path.join(tmpdir, "tracking")
    project_name = "test_application_tracks_end_to_end"
    app = sample_application(project_name, log_dir, app_id)
    app.run(halt_after=["result"])
    results_dir = os.path.join(log_dir, project_name, app_id)
    assert os.path.exists(results_dir)
    assert os.path.exists(log_output := os.path.join(results_dir, LocalTrackingClient.LOG_FILENAME))
    assert os.path.exists(
        graph_output := os.path.join(results_dir, LocalTrackingClient.GRAPH_FILENAME)
    )
    with open(log_output) as f:
        log_contents = [json.loads(item) for item in f.readlines()]
    with open(graph_output) as f:
        graph_contents = json.load(f)
    assert graph_contents["type"] == "application"
    app_model = ApplicationModel.parse_obj(graph_contents)
    assert app_model.entrypoint == "counter"
    assert app_model.actions[0].name == "counter"
    assert app_model.actions[1].name == "result"
    pre_run = [
        BeginEntryModel.model_validate(line)
        for line in log_contents
        if line["type"] == "begin_entry"
    ]
    post_run = [
        EndEntryModel.model_validate(line) for line in log_contents if line["type"] == "end_entry"
    ]
    span_start_model = [
        BeginSpanModel.model_validate(line) for line in log_contents if line["type"] == "begin_span"
    ]
    span_end_model = [
        EndSpanModel.model_validate(line) for line in log_contents if line["type"] == "end_span"
    ]
    attributes = [
        AttributeModel.model_validate(line) for line in log_contents if line["type"] == "attribute"
    ]
    assert len(pre_run) == 3
    assert len(post_run) == 3
    assert len(span_start_model) == 2  # two custom-defined spans
    assert len(span_end_model) == 2  # ditto
    assert not any(item.exception for item in post_run)
    assert len(attributes) == 2  # two attributes logged


def test_application_tracks_end_to_end_broken(tmpdir: str):
    app_id = str(uuid.uuid4())
    log_dir = os.path.join(tmpdir, "tracking")
    project_name = "test_application_tracks_end_to_end"
    app = sample_application(project_name, log_dir, app_id, broken=True)
    with pytest.raises(ValueError):
        app.run(halt_after=["result"])
    results_dir = os.path.join(log_dir, project_name, app_id)
    assert os.path.exists(results_dir)
    assert os.path.exists(log_output := os.path.join(results_dir, LocalTrackingClient.LOG_FILENAME))
    assert os.path.exists(
        graph_output := os.path.join(results_dir, LocalTrackingClient.GRAPH_FILENAME)
    )
    with open(log_output) as f:
        log_contents = [json.loads(item) for item in f.readlines()]
    with open(graph_output) as f:
        graph_contents = json.load(f)
    assert graph_contents["type"] == "application"
    app_model = ApplicationModel.model_validate(graph_contents)
    assert app_model.entrypoint == "counter"
    assert app_model.actions[0].name == "counter"
    assert app_model.actions[1].name == "result"
    pre_run = [
        BeginEntryModel.model_validate(line)
        for line in log_contents
        if line["type"] == "begin_entry"
    ]
    post_run = [
        EndEntryModel.model_validate(line) for line in log_contents if line["type"] == "end_entry"
    ]
    assert len(pre_run) == 2
    assert len(post_run) == 2
    assert len(post_run[-1].exception) > 0 and "Broken" in post_run[-1].exception


@pytest.mark.parametrize(
    "input_string, on_windows, expected_result",
    [
        ("Hello-World_123", False, True),
        ("Hello:World_123", False, True),
        ("Hello:World_123", True, False),
        ("Invalid:Chars*", False, False),
        ("Just$ymbols", True, False),
        ("Normal_Text", True, True),
    ],
)
def test__allowed_project_name(input_string, on_windows, expected_result):
    assert _allowed_project_name(input_string, on_windows) == expected_result


class DummyPersister(BaseStatePersister):
    """Dummy persistor."""

    def load(
        self, partition_key: str, app_id: Optional[str], sequence_id: Optional[int] = None, **kwargs
    ) -> Optional[PersistedStateData]:
        return PersistedStateData(
            partition_key="user123",
            app_id="123",
            sequence_id=5,
            position="counter",
            state=State({"count": 5}),
            created_at="",
            status="completed",
        )

    def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
        return ["123"]

    def save(
        self,
        partition_key: Optional[str],
        app_id: str,
        sequence_id: int,
        position: str,
        state: State,
        status: Literal["completed", "failed"],
        **kwargs,
    ):
        return


def test_persister_tracks_parent(tmpdir):
    result = Result("count").with_name("result")
    old_app_id = "old"
    new_app_id = "new"
    log_dir = os.path.join(tmpdir, "tracking")
    results_dir = os.path.join(log_dir, "test_persister_tracks_parent", new_app_id)
    project_name = "test_persister_tracks_parent"
    app: Application = (
        ApplicationBuilder()
        .with_actions(counter, result)
        .with_transitions(("counter", "result", default))
        .initialize_from(
            DummyPersister(),
            resume_at_next_action=True,
            default_state={},
            default_entrypoint="counter",
            fork_from_app_id=old_app_id,
            fork_from_partition_key="user123",
            fork_from_sequence_id=5,
        )
        .with_identifiers(app_id=new_app_id, partition_key="user123")
        .with_tracker(project=project_name, tracker="local", params={"storage_dir": log_dir})
        .build()
    )
    app.run(halt_after=["result"])
    assert os.path.exists(
        graph_output := os.path.join(results_dir, LocalTrackingClient.METADATA_FILENAME)
    )
    with open(graph_output) as f:
        metadata = json.load(f)
    metadata_parsed = ApplicationMetadataModel.model_validate(metadata)
    assert metadata_parsed.partition_key == "user123"
    assert metadata_parsed.parent_pointer.app_id == old_app_id
    assert metadata_parsed.parent_pointer.sequence_id == 5
    assert metadata_parsed.parent_pointer.partition_key == "user123"


def test_multi_fork_tracking_client(tmpdir):
    """This is more of an end-to-end test. We shoudl probably break it out
    into smaller tests but the local tracking client being used as a persister is
    a bit of a complex case, and we don't want to get lost in the details.
    """
    common_app_id = uuid.uuid4()
    initial_app_id = f"new_{common_app_id}"
    # newer_app_id = "newer"
    log_dir = os.path.join(tmpdir, "tracking")
    # results_dir = os.path.join(log_dir, "test_persister_tracks_parent", new_app_id)
    project_name = "test_persister_tracks_parent"

    tracking_client = LocalTrackingClient(project=project_name, storage_dir=log_dir)

    class CallTracker(lifecycle.PostRunStepHook):
        def __init__(self):
            self.count = 0

        def post_run_step(self, action: Action, **kwargs):
            if action.name == "counter":
                self.count += 1

    def create_application(
        old_app_id: Optional[str], new_app_id: str, old_sequence_id: Optional[int], max_count: int
    ) -> Tuple[Application, CallTracker]:
        tracker = CallTracker()
        app: Application = (
            ApplicationBuilder()
            .with_actions(counter, Result("count").with_name("result"))
            .with_transitions(
                ("counter", "counter", expr(f"counter < {max_count}")),
                ("counter", "result", default),
            )
            .initialize_from(
                tracking_client,
                resume_at_next_action=True,
                default_state={"counter": 0, "break_at": -1},  # never break
                default_entrypoint="counter",
                fork_from_app_id=old_app_id,
                fork_from_sequence_id=old_sequence_id,
            )
            .with_identifiers(app_id=new_app_id)
            .with_tracker(tracking_client)
            .with_hooks(tracker)
            .build()
        )
        return app, tracker

    # create an initial one
    app_initial, tracker = create_application(None, initial_app_id, None, max_count=10)
    action_, result, state = app_initial.run(halt_after=["result"])  # Run all the way through
    assert state["counter"] == 10  # should have counted to 10
    assert tracker.count == 10  # 10 counts

    # create a new one from position 5

    forked_app_id = f"fork_1_{common_app_id}"
    forked_app_1, tracker = create_application(initial_app_id, forked_app_id, 5, max_count=15)
    assert forked_app_1.sequence_id == 5
    action_, result, state = forked_app_1.run(halt_after=["result"])  # Run all the way through
    assert state["counter"] == 15  # should have counted to 15
    assert tracker.count == 9  # start at 6, go to 15
    assert forked_app_1.parent_pointer.app_id == initial_app_id
    assert forked_app_1.parent_pointer.sequence_id == 5

    forked_forked_app_id = f"fork_2_{common_app_id}"
    forked_app_2, tracker = create_application(
        forked_app_id, forked_forked_app_id, 10, max_count=25
    )
    assert forked_app_2.sequence_id == 10
    action_, result, state = forked_app_2.run(halt_after=["result"])  # Run all the way through
    assert state["counter"] == 25  # should have counted to 15
    assert tracker.count == 14  # start at 11, go to 20

    assert forked_app_2.parent_pointer.app_id == forked_app_id
    assert forked_app_2.parent_pointer.sequence_id == 10

    # fork from latest
    # TODO -- break this up -- this test tests too much at once
    # This is a quick addition to test that forking from sequence_id=None picks up where the last one left off

    forked_forked_forked_app_id = f"fork_3_{common_app_id}"
    forked_app_3, tracker = create_application(
        forked_forked_app_id, forked_forked_forked_app_id, None, max_count=35
    )
    assert (
        forked_app_3.sequence_id == forked_app_2.sequence_id == 25
    )  # this should pick up where the last one left off
    assert forked_app_3.parent_pointer.app_id == forked_forked_app_id


def test_application_tracks_link_to_spawning_parent(tmpdir: str):
    """Tests that we record the parent of the spawned application in the metadata file for the spawned application."""
    app_id = str(uuid.uuid4())
    log_dir = os.path.join(tmpdir, "tracking_parent_test")
    project_name = "test_application_tracks_end_to_end_with_spawning_parent"
    # constructing this will cause the desired side-effect
    sample_application(project_name, log_dir, app_id, spawn_from=(f"spawn_{app_id}", 5))
    results_dir = os.path.join(log_dir, project_name, app_id)
    assert os.path.exists(results_dir)
    assert os.path.exists(
        metadata_output := os.path.join(results_dir, LocalTrackingClient.METADATA_FILENAME)
    )
    with open(metadata_output) as f:
        metadata = json.load(f)
    metadata_parsed = ApplicationMetadataModel.model_validate(metadata)
    assert metadata_parsed.spawning_parent_pointer.app_id == f"spawn_{app_id}"
    assert metadata_parsed.spawning_parent_pointer.sequence_id == 5


def test_application_tracks_link_from_spawning_parent(tmpdir: str):
    """Tests that we record the child in the parent's directory when instantiated."""
    spawning_parent_app_id = str(uuid.uuid4())
    project_name = "test_application_tracks_link_from_spawning_parent"
    log_dir = os.path.join(tmpdir, "tracking_child_test")
    # creates the directory for the parent
    # technically not needed (it'll create an empty directory), but nice to have
    sample_application(project_name, log_dir, spawning_parent_app_id)
    parent_result_dir = os.path.join(log_dir, project_name, spawning_parent_app_id)
    spawned_children = [str(uuid.uuid4()), str(uuid.uuid4())]
    for child_app_id in spawned_children:
        # constructing this will cause the desired side effect -- crating the pointer to the child in the parent's directory
        sample_application(
            project_name, log_dir, child_app_id, spawn_from=(spawning_parent_app_id, 5)
        )
        assert os.path.exists(
            children_output := os.path.join(
                parent_result_dir, LocalTrackingClient.CHILDREN_FILENAME
            )
        )
        with open(children_output) as f:
            children = [json.loads(line) for line in f.readlines()]
    children_parsed = [ChildApplicationModel.model_validate(child) for child in children]
    assert set(child.child.app_id for child in children_parsed) == set(spawned_children)
    assert all(child.event_type == "spawn_start" for child in children_parsed)


def test_that_we_fail_on_non_unicode_characters(tmp_path):
    """This is a test to log expected behavior.

    Right now it is on the developer to ensure that state can be encoded into UTF-8.

    This test is here to capture this assumption.
    """

    @action(reads=["test"], writes=["test"])
    def state_1(state: State) -> State:
        return state.update(test="test")

    @action(reads=["test"], writes=["test"])
    def state_2(state: State) -> State:
        return state.update(test="\uD800")  # Invalid UTF-8 byte sequence

    tracker = LocalTrackingClient(project="test", storage_dir=tmp_path)
    app: Application = (
        ApplicationBuilder()
        .with_actions(state_1, state_2)
        .with_transitions(("state_1", "state_2"), ("state_2", "state_1"))
        .with_tracker(tracker=tracker)
        .initialize_from(
            initializer=tracker,
            resume_at_next_action=False,
            default_entrypoint="state_1",
            default_state={},
        )
        .with_identifiers(app_id="3")
        .build()
    )

    with pytest.raises(ValueError):
        app.run(halt_after=["state_2"])


def test_that_we_can_read_write_local_tracker(tmp_path):
    """Integration like test to ensure we can write and then read what was written"""

    @action(
        reads=[],
        writes=[
            "text",
            "greek",
            "cyrillic",
            "hebrew",
            "arabic",
            "hindi",
            "chinese",
            "japanese",
            "korean",
            "emoji",
        ],
    )
    def state_1(state: State) -> State:
        text = "Ã¡, Ã©, Ã­, Ã³, Ãº, Ã±, Ã¼"
        greek = "Î±, Î², Î³, Î´"
        cyrillic = "Ð¶, Ñ‹, Ð±, ÑŠ"
        hebrew = "×, ×‘, ×’, ×“"
        arabic = "Ø®, Ø¯, Ø°, Ø±"
        hindi = "à¤…, à¤†, à¤‡, à¤ˆ"
        chinese = "ä¸­, å›½, æ–‡"
        japanese = "æ—¥, æœ¬, èªž"
        korean = "í•œ, êµ­, ì–´"
        emoji = "ðŸ˜€, ðŸ‘, ðŸš€, ðŸŒ"
        return state.update(
            text=text,
            greek=greek,
            cyrillic=cyrillic,
            hebrew=hebrew,
            arabic=arabic,
            hindi=hindi,
            chinese=chinese,
            japanese=japanese,
            korean=korean,
            emoji=emoji,
        )

    @action(reads=["text"], writes=["text"])
    def state_2(state: State) -> State:
        return state.update(text="\x9d")  # encode-able UTF-8 sequence

    tracker = LocalTrackingClient(
        project="test",
        storage_dir=tmp_path,
    )

    for i in range(2):
        # reloads from log.jsonl in the second run and errors
        app: Application = (
            ApplicationBuilder()
            .with_actions(state_1, state_2)
            .with_transitions(("state_1", "state_2"), ("state_2", "state_1"))
            .with_tracker(tracker=tracker)
            .initialize_from(
                initializer=tracker,
                resume_at_next_action=False,
                default_entrypoint="state_1",
                default_state={},
            )
            .with_identifiers(app_id="3")
            .build()
        )

        app.run(halt_after=["state_2"])


def test_local_tracking_client_copy():
    """Tests tracking client .copy() method for serialization/parallelism.
    Internal-facing contracts but we want coverage here."""
    tracking_client = LocalTrackingClient("foo", "storage_dir", serde_kwargs={"foo": "bar"})
    copy = tracking_client.copy()
    assert copy.project_id == tracking_client.project_id
    assert copy.serde_kwargs == tracking_client.serde_kwargs
    assert copy.storage_dir == tracking_client.storage_dir



---
File: /burr/tests/visibility/test_tracing.py
---

import asyncio
from contextvars import ContextVar
from typing import Any, Dict, Optional

from burr.lifecycle.base import (
    DoLogAttributeHook,
    PostEndSpanHook,
    PostEndSpanHookAsync,
    PreStartSpanHook,
    PreStartSpanHookAsync,
)
from burr.lifecycle.internal import LifecycleAdapterSet
from burr.visibility import tracing
from burr.visibility.tracing import ActionSpan, TracerFactory

# There are a lot of state-specific assertions in this file
# We try to comment them. We should also probably organize them to be
# smaller and test specific things, but this is easier to validate for now


def is_subset(dict_subset, dict_full):
    return all(item in dict_full.items() for item in dict_subset.items())


def test_action_span_tracer_correct_span_count(request):
    context_var: ContextVar[Optional[ActionSpan]] = ContextVar(
        request.node.name,
        default=None,
    )
    tracer_factory = TracerFactory(
        action="test_action",
        sequence_id=0,
        lifecycle_adapters=LifecycleAdapterSet(),
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )
    assert context_var.get() is None  # nothing to start
    assert tracer_factory.top_level_span_count == 0  # and thus no top-level spans

    with tracer_factory("0") as span_0:
        assert span_0.span_name == "0"  # span name should match the factory call
        context = context_var.get()
        assert context is not None  # context is now set as we entered the manager
        assert context.child_count == 0  # no children yet
        assert context.action == "test_action"  # action name should match the factory
        assert context.parent is None  # one of the top-level/roots
        assert context.sequence_id == 0  # first one

    context = context_var.get()
    assert context is None  # context is now unset
    assert tracer_factory.top_level_span_count == 1  # we have one now


async def test_action_span_tracer_correct_span_count_async(request):
    context_var: ContextVar[Optional[ActionSpan]] = ContextVar(
        request.node.name,
        default=None,
    )
    tracer_factory = TracerFactory(
        action="test_action",
        sequence_id=0,
        lifecycle_adapters=LifecycleAdapterSet(),
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )
    assert context_var.get() is None  # nothing to start
    assert tracer_factory.top_level_span_count == 0  # and thus no top-level spans

    async with tracer_factory("0") as span_0:
        assert span_0.span_name == "0"  # span name should match the factory call
        context = context_var.get()
        assert context is not None  # context is now set as we entered the manager
        assert context.child_count == 0  # no children yet
        assert context.action == "test_action"  # action name should match the factory
        assert context.parent is None  # one of the top-level/roots
        assert context.sequence_id == 0  # first one

    context = context_var.get()
    assert context is None  # context is now unset
    assert tracer_factory.top_level_span_count == 1  # we have one now


def test_action_span_tracer_correct_span_count_nested(request):
    context_var: ContextVar[Optional[ActionSpan]] = ContextVar(
        request.node.name,
        default=None,
    )
    tracer_factory = TracerFactory(
        action="test_action",
        sequence_id=0,
        lifecycle_adapters=LifecycleAdapterSet(),
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )

    with tracer_factory("0") as outside_span_0:
        with tracer_factory("0.0"):
            context = context_var.get()
            assert context.child_count == 0  # no children
            assert context.parent.name == outside_span_0.span_name  # one of the top-level/roots
            assert context.sequence_id == 0  # first one

        with tracer_factory("0.1"):
            context = context_var.get()
            assert context.sequence_id == 1  # this is the second one

        assert outside_span_0.top_level_span_count == 1  # we only have one top-level span

        context = context_var.get()
        assert context.child_count == 2  # two children we spawned above
        assert context.name == outside_span_0.span_name  # we're back to the outside span

    with tracer_factory("1") as outside_span_1:
        assert outside_span_1.top_level_span_count == 2  # now we have another
    assert tracer_factory.top_level_span_count == 2  # the tracer factory holds state

    context = context_var.get()
    assert context is None  # context gets reset at the end


async def test_action_span_tracer_correct_span_count_nested_async(request):
    context_var: ContextVar[Optional[ActionSpan]] = ContextVar(
        request.node.name,
        default=None,
    )
    tracer_factory = TracerFactory(
        action="test_action",
        sequence_id=0,
        lifecycle_adapters=LifecycleAdapterSet(),
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )

    async with tracer_factory("0") as outside_span_0:
        async with tracer_factory("0.0"):
            context = context_var.get()
            assert context.child_count == 0  # no children
            assert context.parent.name == outside_span_0.span_name  # one of the top-level/roots
            assert context.sequence_id == 0  # first one

        async with tracer_factory("0.1"):
            context = context_var.get()
            assert context.sequence_id == 1  # this is the second one

        assert outside_span_0.top_level_span_count == 1  # we only have one top-level span

        context = context_var.get()
        assert context.child_count == 2  # two children we spawned above
        assert context.name == outside_span_0.span_name  # we're back to the outside span

    async with tracer_factory("1") as outside_span_1:
        assert outside_span_1.top_level_span_count == 2  # now we have another
    assert tracer_factory.top_level_span_count == 2  # the tracer factory holds state

    context = context_var.get()
    assert context is None  # context gets reset at the end


def test_action_span_spawn():
    action_span = ActionSpan.create_initial("test_action", "0", 0, action_sequence_id=0)
    assert action_span.parent is None  # this is the top-level span
    spawned_0 = action_span.spawn("0.0")
    assert spawned_0.sequence_id == 0
    assert spawned_0.parent is action_span  # this is a child of the top-level span
    spawned_1 = action_span.spawn("0.1")  # another child
    assert spawned_1.sequence_id == 1
    assert action_span.child_count == 2
    assert action_span.sequence_id == 0


def test_pre_span_lifecycle_hooks_called(request):
    class TrackingHook(PreStartSpanHook, PostEndSpanHook):
        def __init__(self):
            self.uids_pre = []
            self.uids_post = []

        def pre_start_span(
            self,
            *,
            span: "ActionSpan",
            **future_kwargs: Any,
        ):
            self.uids_pre.append(span.uid)

        def post_end_span(
            self,
            *,
            span: "ActionSpan",
            **future_kwargs: Any,
        ):
            self.uids_post.append(span.uid)

    hook = TrackingHook()
    adapter_set = LifecycleAdapterSet(hook)
    context_var: ContextVar[Optional[ActionSpan]] = ContextVar(request.node.name, default=None)
    tracer_factory_0 = TracerFactory(
        action="test_action",
        sequence_id=0,
        lifecycle_adapters=adapter_set,
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )
    # 0:0
    with tracer_factory_0("0"):
        # 0:0.0
        with tracer_factory_0("0.0"):
            pass
        # 0:0.1
        with tracer_factory_0("0.1"):
            pass
    # 0:1
    with tracer_factory_0("1"):
        # 0:1.0
        with tracer_factory_0("1.0"):
            # 0:1.0.0
            with tracer_factory_0("1.0.0"):
                pass
            # 0:1.0.1
            with tracer_factory_0("1.0.1"):
                pass
    tracer_factory_1 = TracerFactory(
        action="test_action",
        sequence_id=tracer_factory_0.action_sequence_id + 1,
        lifecycle_adapters=adapter_set,
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )
    # 1:0
    with tracer_factory_1("2"):
        # 1:0.0
        with tracer_factory_1("2.0"):
            pass
        # 1:0.1
        with tracer_factory_1("2.1"):
            pass

    # order of this is exactly as expected -- in order traversal
    assert hook.uids_pre == [
        "0:0",
        "0:0.0",
        "0:0.1",
        "0:1",
        "0:1.0",
        "0:1.0.0",
        "0:1.0.1",
        "1:0",
        "1:0.0",
        "1:0.1",
    ]

    # order of this is a little trickier -- the containing span closes at the end so its post-order traversal
    assert hook.uids_post == [
        "0:0.0",
        "0:0.1",
        "0:0",
        "0:1.0.0",
        "0:1.0.1",
        "0:1.0",
        "0:1",
        "1:0.0",
        "1:0.1",
        "1:0",
    ]


async def test_pre_span_lifecycle_hooks_called_async(request):
    class AsyncTrackingHook(PreStartSpanHookAsync, PostEndSpanHookAsync):
        def __init__(self):
            self.uids_pre = []
            self.uids_post = []

        async def pre_start_span(
            self,
            *,
            span: "ActionSpan",
            **future_kwargs: Any,
        ):
            await asyncio.sleep(0.01)
            self.uids_pre.append(span.uid)

        async def post_end_span(
            self,
            *,
            span: "ActionSpan",
            **future_kwargs: Any,
        ):
            await asyncio.sleep(0.01)
            self.uids_post.append(span.uid)

    hook = AsyncTrackingHook()
    adapter_set = LifecycleAdapterSet(hook)
    context_var: ContextVar[Optional[ActionSpan]] = ContextVar(request.node.name, default=None)
    tracer_factory_0 = TracerFactory(
        action="test_action",
        sequence_id=0,
        lifecycle_adapters=adapter_set,
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )
    # 0:0
    async with tracer_factory_0("0"):
        # 0:0.0
        async with tracer_factory_0("0.0"):
            pass
        # 0:0.1
        async with tracer_factory_0("0.1"):
            pass
    # 0:1
    async with tracer_factory_0("1"):
        # 0:1.0
        async with tracer_factory_0("1.0"):
            # 0:1.0.0
            async with tracer_factory_0("1.0.0"):
                pass
            # 0:1.0.1
            async with tracer_factory_0("1.0.1"):
                pass
    tracer_factory_1 = TracerFactory(
        action="test_action",
        sequence_id=tracer_factory_0.action_sequence_id + 1,
        lifecycle_adapters=adapter_set,
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )
    # 1:0
    async with tracer_factory_1("2"):
        # 1:0.0
        async with tracer_factory_1("2.0"):
            pass
        # 1:0.1
        async with tracer_factory_1("2.1"):
            pass

    # order of this is exactly as expected -- in order traversal
    assert hook.uids_pre == [
        "0:0",
        "0:0.0",
        "0:0.1",
        "0:1",
        "0:1.0",
        "0:1.0.0",
        "0:1.0.1",
        "1:0",
        "1:0.0",
        "1:0.1",
    ]

    # order of this is a little trickier -- the containing span closes at the end so its post-order traversal
    assert hook.uids_post == [
        "0:0.0",
        "0:0.1",
        "0:0",
        "0:1.0.0",
        "0:1.0.1",
        "0:1.0",
        "0:1",
        "1:0.0",
        "1:0.1",
        "1:0",
    ]


class AttributeHook(DoLogAttributeHook):
    def do_log_attributes(
        self,
        *,
        attributes: Dict[str, Any],
        action: str,
        action_sequence_id: int,
        span: Optional["ActionSpan"],
        tags: dict,
        **future_kwargs: Any,
    ):
        self.attributes.append(
            (attributes, action, action_sequence_id, span.uid if span is not None else None)
        )

    def __init__(self):
        self.attributes = []


async def test_log_attributes_called(request):
    hook = AttributeHook()
    adapter_set = LifecycleAdapterSet(hook)
    context_var: ContextVar[Optional[ActionSpan]] = ContextVar(request.node.name, default=None)
    tracer_factory_0 = TracerFactory(
        action="test_action",
        sequence_id=0,
        lifecycle_adapters=adapter_set,
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )
    # 0:0
    tracer_factory_0.log_attributes(key="root:0")
    async with tracer_factory_0("0") as t:
        t.log_attributes(key="0:0")
        # 0:0.0
        async with tracer_factory_0("0.0") as t:
            t.log_attributes(key="0.0:0")
            t.log_attributes(key="0.0:1")
        # 0:0.1
        async with tracer_factory_0("0.1") as t:
            t.log_attributes(key="0:0.1")
    # 0:1
    async with tracer_factory_0("1") as t:
        t.log_attributes(key="1:0")
        t.log_attributes(key="1:1")
        # 0:1.0
        async with tracer_factory_0("1.0") as t:
            t.log_attributes(key="1.0:0")
            t.log_attributes(key="1.0:1")
            # 0:1.0.0
            async with tracer_factory_0("1.0.0") as t:
                t.log_attributes(key="1.0.0:0")
                t.log_attributes(key="1.0.0:1")
            # 0:1.0.1
            async with tracer_factory_0("1.0.1") as t:
                t.log_attributes(key="1.0.1:0")
                t.log_attributes(key="1.0.1:1")
    tracer_factory_1 = TracerFactory(
        action="test_action",
        sequence_id=tracer_factory_0.action_sequence_id + 1,
        lifecycle_adapters=adapter_set,
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )
    # 1:0
    async with tracer_factory_1("2"):
        # 1:0.0
        async with tracer_factory_1("2.0"):
            pass
        # 1:0.1
        async with tracer_factory_1("2.1"):
            pass

    # order of this is exactly as expected -- in order traversal
    assert hook.attributes == [
        ({"key": "root:0"}, "test_action", 0, None),
        ({"key": "0:0"}, "test_action", 0, "0:0"),
        ({"key": "0.0:0"}, "test_action", 0, "0:0.0"),
        ({"key": "0.0:1"}, "test_action", 0, "0:0.0"),
        ({"key": "0:0.1"}, "test_action", 0, "0:0.1"),
        ({"key": "1:0"}, "test_action", 0, "0:1"),
        ({"key": "1:1"}, "test_action", 0, "0:1"),
        ({"key": "1.0:0"}, "test_action", 0, "0:1.0"),
        ({"key": "1.0:1"}, "test_action", 0, "0:1.0"),
        ({"key": "1.0.0:0"}, "test_action", 0, "0:1.0.0"),
        ({"key": "1.0.0:1"}, "test_action", 0, "0:1.0.0"),
        ({"key": "1.0.1:0"}, "test_action", 0, "0:1.0.1"),
        ({"key": "1.0.1:1"}, "test_action", 0, "0:1.0.1"),
    ]


def test_trace_decorator_single_fn(request):
    # Mock an action span context var
    # This is set by the tracer factory, which uses the default in tracing
    # This is so we know the current (context aware) span
    context_var: ContextVar[Optional[ActionSpan]] = ContextVar(
        request.node.name,
        default=None,
    )
    # Mock a tracer factory context var
    # This is set usually by the application so the decorator knows what to get
    tracer_factory_context_var = ContextVar[TracerFactory](
        request.node.name + "_tracer", default=None
    )
    # Create a hook so we can wire it through and ensure the right thing gets called
    hook = AttributeHook()
    adapter_set = LifecycleAdapterSet(hook)

    # Create and set the var to a tracer factory that is context-aware
    tracer_factory = TracerFactory(
        action="test_action",
        sequence_id=0,
        lifecycle_adapters=adapter_set,
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )
    tracer_factory_context_var.set(tracer_factory)
    # Nothing set now
    assert context_var.get() is None  # nothing to start
    assert tracer_factory.top_level_span_count == 0  # and thus no top-level spans
    # Instantiate a decorator that will set the action span when it enters, and unset once it leaves
    decorator = tracing.trace(
        _context_var=tracer_factory_context_var, input_filterlist=["filtered_out"]
    )

    # Then the function can assert things for us
    # We ensure it gets called by checking the output
    def foo(a: int, b: int, filtered_out: str = "not_present") -> int:
        action_span_context = context_var.get()
        assert action_span_context is not None  # context is now set as we entered the manager
        assert action_span_context.name == foo.__name__  # span name should match the function name
        assert action_span_context.child_count == 0  # no children (now)
        return a + b

    decorated_foo = decorator(foo)
    result = decorated_foo(1, 2)
    assert result == 3
    assert is_subset({"a": 1, "b": 2}, hook.attributes[0][0])
    assert is_subset({"return": 3}, hook.attributes[2][0])


async def test_trace_decorator_single_fn_async(request):
    context_var: ContextVar[Optional[ActionSpan]] = ContextVar(
        request.node.name,
        default=None,
    )
    tracer_factory_context_var = ContextVar[TracerFactory](
        request.node.name + "_tracer", default=None
    )
    hook = AttributeHook()
    adapter_set = LifecycleAdapterSet(hook)

    tracer_factory = TracerFactory(
        action="test_action",
        sequence_id=0,
        lifecycle_adapters=adapter_set,
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )
    tracer_factory_context_var.set(tracer_factory)
    assert context_var.get() is None  # nothing to start
    assert tracer_factory.top_level_span_count == 0  # and thus no top-level spans
    decorator = tracing.trace(
        _context_var=tracer_factory_context_var, input_filterlist=["filtered_out"]
    )

    async def foo(a: int, b: int, filtered_out: str = "not_present") -> int:
        action_span_context = context_var.get()
        assert action_span_context is not None  # context is now set as we entered the manager
        assert action_span_context.name == foo.__name__  # span name should match the function name
        assert action_span_context.child_count == 0  # no children (now)
        await asyncio.sleep(0.001)
        return a + b

    decorated_foo = decorator(foo)
    result = await decorated_foo(1, 2)
    assert result == 3
    assert is_subset({"a": 1, "b": 2}, hook.attributes[0][0])
    assert is_subset({"return": 3}, hook.attributes[2][0])


def test_trace_decorator_recursive(request):
    context_var: ContextVar[Optional[ActionSpan]] = ContextVar(
        request.node.name,
        default=None,
    )
    hook = AttributeHook()
    tracer_factory_context_var = ContextVar[TracerFactory](
        request.node.name + "_tracer", default=None
    )
    tracer_factory = TracerFactory(
        action="test_action",
        sequence_id=0,
        lifecycle_adapters=LifecycleAdapterSet(hook),
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )
    tracer_factory_context_var.set(tracer_factory)
    assert context_var.get() is None  # nothing to start
    assert tracer_factory.top_level_span_count == 0  # and thus no top-level spans
    decorator = tracing.trace(_context_var=tracer_factory_context_var)

    @decorator
    def foo(a: int, b: int) -> int:
        action_span_context = context_var.get()
        assert action_span_context is not None  # context is now set as we entered the manager
        assert action_span_context.name == foo.__name__  # span name should match the function name
        return bar(a + 1, a + b) + a + b

    @decorator
    def bar(a: int, b: int) -> int:
        action_span_context = context_var.get()
        assert action_span_context is not None  # context is now set as we entered the manager
        assert action_span_context.name == bar.__name__  # span name should match the function name
        assert action_span_context.child_count == 0  # no children (now)
        assert action_span_context.parent.name == foo.__name__  # parent should be foo
        return a * b

    result = foo(1, 2)
    assert result == 9
    assert is_subset({"a": 1, "b": 2}, hook.attributes[0][0])
    assert is_subset({"a": 2, "b": 3}, hook.attributes[2][0])
    assert is_subset({"return": 6}, hook.attributes[4][0])
    assert is_subset({"return": 9}, hook.attributes[5][0])


async def test_trace_decorator_recursive_async(request):
    context_var: ContextVar[Optional[ActionSpan]] = ContextVar(
        request.node.name,
        default=None,
    )
    hook = AttributeHook()
    tracer_factory_context_var = ContextVar[TracerFactory](
        request.node.name + "_tracer", default=None
    )
    tracer_factory = TracerFactory(
        action="test_action",
        sequence_id=0,
        lifecycle_adapters=LifecycleAdapterSet(hook),
        _context_var=context_var,
        app_id="test_app_id",
        partition_key=None,
    )
    tracer_factory_context_var.set(tracer_factory)
    assert context_var.get() is None  # nothing to start
    assert tracer_factory.top_level_span_count == 0  # and thus no top-level spans
    decorator = tracing.trace(_context_var=tracer_factory_context_var)

    @decorator
    async def foo(a: int, b: int) -> int:
        action_span_context = context_var.get()
        assert action_span_context is not None  # context is now set as we entered the manager
        assert action_span_context.name == foo.__name__  # span name should match the function name
        await asyncio.sleep(0.001)
        return await bar(a + 1, a + b) + a + b

    @decorator
    async def bar(a: int, b: int) -> int:
        action_span_context = context_var.get()
        assert action_span_context is not None  # context is now set as we entered the manager
        assert action_span_context.name == bar.__name__  # span name should match the function name
        assert action_span_context.child_count == 0  # no children (now)
        assert action_span_context.parent.name == foo.__name__  # parent should be foo
        await asyncio.sleep(0.001)
        return a * b

    result = await foo(1, 2)
    assert result == 9
    assert is_subset({"a": 1, "b": 2}, hook.attributes[0][0])
    assert is_subset({"a": 2, "b": 3}, hook.attributes[2][0])
    assert is_subset({"return": 6}, hook.attributes[4][0])
    assert is_subset({"return": 9}, hook.attributes[5][0])



---
File: /burr/tests/conftest.py
---

from burr.telemetry import disable_telemetry

disable_telemetry()



---
File: /burr/tests/test_end_to_end.py
---

"""End-to-end tests -- these are more like integration tests,
but they're specifically meant to be a smoke-screen. If you ever
see failures in these tests, you should make a unit test, demonstrate the failure there,
then fix both in that test and the end-to-end test."""
import asyncio
import datetime
import uuid
from concurrent.futures import ThreadPoolExecutor
from io import StringIO
from typing import Any, AsyncGenerator, Dict, Generator, Literal, Optional, Tuple
from unittest.mock import patch

import pytest

from burr.core import (
    Action,
    ApplicationBuilder,
    ApplicationContext,
    GraphBuilder,
    State,
    action,
    persistence,
)
from burr.core.action import Input, Result, expr
from burr.core.parallelism import MapStates, RunnableGraph, SubgraphType
from burr.lifecycle import base


def test_end_to_end_collatz_with_function_api():
    """End-to-end test for collatz conjecture. This is a fun (unproven) finite state machine."""

    class CountHook(base.PostRunStepHook):
        def __init__(self):
            self.count = 0

        def post_run_step(self, action: "Action", **future_kwargs: Any):
            if action.name != "result":
                self.count += 1

    hook = CountHook()

    @action(reads=["n"], writes=["n", "n_history"])
    def even(state: State) -> Tuple[dict, State]:
        result = {"n": state["n"] // 2}
        return result, state.update(**result).append(n_history=result["n"])

    @action(reads=["n"], writes=["n", "n_history"])
    def odd(state: State) -> Tuple[dict, State]:
        result = {"n": 3 * state["n"] + 1}
        return result, state.update(**result).append(n_history=result["n"])

    done = expr("n == 1")
    is_even = expr("n % 2 == 0")
    is_odd = expr("n % 2 != 0")
    application = (
        ApplicationBuilder()
        .with_state(n_history=[])
        .with_actions(
            original=Input("n"),
            even=even,
            odd=odd,
            result=Result("n_history"),
        )
        .with_transitions(
            (["original", "even"], "result", done),
            (["original", "even", "odd"], "even", is_even),
            (["original", "even", "odd"], "odd", is_odd),
        )
        .with_entrypoint("original")
        .with_hooks(hook)
        .build()
    )
    run_action, result, state = application.run(halt_after=["result"], inputs={"n": 1000})
    assert result["n_history"][-1] == 1
    assert hook.count == 112


def test_end_to_end_parallel_collatz_many_unreliable_tasks(tmpdir):
    """Tests collatz conjecture search on multiple tasks.
    Each of these persists its own capabilities and state.
    This uses the in memory persister to store the state of each task.
    Each task is unreliable -- it will break every few times. We will restart
    to ensure it is eventually successful using a while loop.

    This simulates running a complex workflow in parallel.
    """

    MIN_NUMBER = 80
    MAX_NUMBER = 100
    FAILURE_ODDS = 0.05  # 1 in twenty chance of faiulre, will be hit but not all the time

    seen = set()

    class UnreliableFailureError(Exception):
        pass

    def _fail_at_random():
        import random

        if random.random() < FAILURE_ODDS:
            raise UnreliableFailureError("Random failure")

    # dummy as we want an initial action to decide between odd/even next
    @action(reads=["n"], writes=["n", "original_n"])
    def initial(state: State, __context: ApplicationContext) -> State:
        # This assert ensures we only visit once per app, globally
        # Thus if we're restarting this will break
        assert __context.app_id not in seen, f"App id {__context.app_id} already seen"
        seen.add(__context.app_id)
        return state.update(original_n=state["n"])

    @action(reads=["n"], writes=["n", "n_history"])
    def even(state: State) -> State:
        _fail_at_random()
        result = {"n": state["n"] // 2}
        return state.update(**result).append(n_history=result["n"])

    @action(reads=["n"], writes=["n", "n_history"])
    def odd(state: State) -> Tuple[dict, State]:
        _fail_at_random()
        result = {"n": 3 * state["n"] + 1}
        return result, state.update(**result).append(n_history=result["n"])

    collatz_graph = (
        GraphBuilder()
        .with_actions(
            initial,
            even,
            odd,
            result=Result("n_history"),
        )
        .with_transitions(
            (["initial", "even"], "result", expr("n == 1")),
            (["initial", "even", "odd"], "even", expr("n % 2 == 0")),
            (["initial", "even", "odd"], "odd", expr("n % 2 != 0")),
        )
        .build()
    )

    @action(reads=[], writes=["ns"])
    def map_step(state: State, min_number: int = MIN_NUMBER, max_number: int = MAX_NUMBER) -> State:
        return state.update(ns=list(range(min_number, max_number)))

    class ParallelCollatz(MapStates):
        def states(
            self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
        ) -> Generator[State, None, None]:
            for item in state["ns"]:
                yield state.update(n=item)

        def action(self, state: State, inputs: Dict[str, Any]) -> SubgraphType:
            return RunnableGraph(
                collatz_graph,
                entrypoint="initial",
                halt_after=["result"],
            )

        def reduce(self, state: State, results: Generator[State, None, None]) -> State:
            new_state = state
            count_mapping = {}
            for result in results:
                count_mapping[result["original_n"]] = len(result["n_history"])
            return new_state.update(counts=count_mapping)

        @property
        def writes(self) -> list[str]:
            return ["counts"]

        @property
        def reads(self) -> list[str]:
            return ["ns"]

    persister = persistence.InMemoryPersister()

    app_id = f"collatz_test_{str(uuid.uuid4())}"
    final_state = None
    while final_state is None:
        try:
            containing_application = (
                ApplicationBuilder()
                .with_actions(
                    map_step,
                    parallel_collatz=ParallelCollatz(),
                    final=Result("counts"),
                )
                .with_transitions(
                    ("map_step", "parallel_collatz"),
                    ("parallel_collatz", "final"),
                )
                .with_state_persister(persister)
                .with_identifiers(app_id=app_id)
                .initialize_from(
                    persister,
                    resume_at_next_action=True,
                    default_state={},
                    default_entrypoint="map_step",
                )
                # Uncomment for debugging/visualizing
                # .with_tracker("local", project="test_persister")
                .with_parallel_executor(lambda: ThreadPoolExecutor(max_workers=10))
                .build()
            )
            *_, final_state = containing_application.run(halt_after=["final"])
        except UnreliableFailureError:
            continue

        # We want to ensure that initial is called once per app
        assert len(seen) == len(range(MIN_NUMBER, MAX_NUMBER)), "Should have seen all numbers"


async def test_end_to_end_parallel_collatz_many_unreliable_tasks_async(tmpdir):
    """Tests collatz conjecture search on multiple tasks.
    Each of these persists its own capabilities and state.
    This uses the in memory persister to store the state of each task.
    Each task is unreliable -- it will break every few times. We will restart
    to ensure it is eventually successful using a while loop.

    This simulates running a complex workflow in parallel.
    """

    MIN_NUMBER = 80
    MAX_NUMBER = 100
    FAILURE_ODDS = 0.05  # 1 in twenty chance of faiulre, will be hit but not all the time

    seen = set()

    class UnreliableFailureError(Exception):
        pass

    def _fail_at_random():
        import random

        if random.random() < FAILURE_ODDS:
            raise UnreliableFailureError("Random failure")

    # dummy as we want an initial action to decide between odd/even next
    @action(reads=["n"], writes=["n", "original_n"])
    async def initial(state: State, __context: ApplicationContext) -> State:
        # This assert ensures we only visit once per app, globally
        # Thus if we're restarting this will break
        await asyncio.sleep(0.001)
        assert __context.app_id not in seen, f"App id {__context.app_id} already seen"
        seen.add(__context.app_id)
        return state.update(original_n=state["n"])

    @action(reads=["n"], writes=["n", "n_history"])
    async def even(state: State) -> State:
        await asyncio.sleep(0.001)
        _fail_at_random()
        result = {"n": state["n"] // 2}
        return state.update(**result).append(n_history=result["n"])

    @action(reads=["n"], writes=["n", "n_history"])
    async def odd(state: State) -> Tuple[dict, State]:
        await asyncio.sleep(0.001)
        _fail_at_random()
        result = {"n": 3 * state["n"] + 1}
        return result, state.update(**result).append(n_history=result["n"])

    collatz_graph = (
        GraphBuilder()
        .with_actions(
            initial,
            even,
            odd,
            result=Result("n_history"),
        )
        .with_transitions(
            (["initial", "even"], "result", expr("n == 1")),
            (["initial", "even", "odd"], "even", expr("n % 2 == 0")),
            (["initial", "even", "odd"], "odd", expr("n % 2 != 0")),
        )
        .build()
    )

    @action(reads=[], writes=["ns"])
    def map_step(state: State, min_number: int = MIN_NUMBER, max_number: int = MAX_NUMBER) -> State:
        return state.update(ns=list(range(min_number, max_number)))

    class ParallelCollatz(MapStates):
        async def states(
            self, state: State, context: ApplicationContext, inputs: Dict[str, Any]
        ) -> AsyncGenerator[State, None]:
            for item in state["ns"]:
                yield state.update(n=item)

        def action(self, state: State, inputs: Dict[str, Any]) -> SubgraphType:
            return RunnableGraph(
                collatz_graph,
                entrypoint="initial",
                halt_after=["result"],
            )

        async def reduce(self, state: State, results: AsyncGenerator[State, None]) -> State:
            new_state = state
            count_mapping = {}
            async for result in results:
                count_mapping[result["original_n"]] = len(result["n_history"])
            return new_state.update(counts=count_mapping)

        @property
        def writes(self) -> list[str]:
            return ["counts"]

        @property
        def reads(self) -> list[str]:
            return ["ns"]

        def is_async(self) -> bool:
            return True

    persister = persistence.InMemoryPersister()
    app_id = f"collatz_test_{str(uuid.uuid4())}"
    final_state = None
    while final_state is None:
        try:
            containing_application = (
                ApplicationBuilder()
                .with_actions(
                    map_step,
                    parallel_collatz=ParallelCollatz(),
                    final=Result("counts"),
                )
                .with_transitions(
                    ("map_step", "parallel_collatz"),
                    ("parallel_collatz", "final"),
                )
                .with_state_persister(persister)
                .with_identifiers(app_id=app_id)
                .initialize_from(
                    persister,
                    resume_at_next_action=True,
                    default_state={},
                    default_entrypoint="map_step",
                )
                .build()
            )
            *_, final_state = await containing_application.arun(halt_after=["final"])
        except UnreliableFailureError:
            continue

        # We want to ensure that initial is called once per app
        assert len(seen) == len(range(MIN_NUMBER, MAX_NUMBER)), "Should have seen all numbers"


def test_echo_bot():
    @action(reads=["prompt"], writes=["response"])
    def echo(state: State) -> Tuple[dict, State]:
        return {"response": state["prompt"]}, state.update(response=state["prompt"])

    application = (
        ApplicationBuilder()
        .with_actions(
            prompt=Input("prompt"),
            response=echo,
        )
        .with_transitions(("prompt", "response"))
        .with_entrypoint("prompt")
        .build()
    )
    prompt = "hello"
    with patch("sys.stdin", new=StringIO(prompt)):
        run_action, result, state = application.run(
            halt_after=["response"], inputs={"prompt": input()}
        )

    application.visualize(
        output_file_path="digraph",
        include_conditions=True,
        view=False,
        include_state=True,
        format="png",
    )
    assert result["response"] == prompt


async def test_async_save_and_load_from_persister_end_to_end():
    await asyncio.sleep(0.00001)

    @action(reads=[], writes=["prompt", "chat_history"])
    async def dummy_input(state: State) -> Tuple[dict, State]:
        await asyncio.sleep(0.0001)
        if state["chat_history"]:
            new = state["chat_history"][-1] + 1
        else:
            new = 1
        return (
            {"prompt": "PROMPT"},
            state.update(prompt="PROMPT").append(chat_history=new),
        )

    @action(reads=["chat_history"], writes=["response", "chat_history"])
    async def dummy_response(state: State) -> Tuple[dict, State]:
        await asyncio.sleep(0.0001)
        if state["chat_history"]:
            new = state["chat_history"][-1] + 1
        else:
            new = 1
        return (
            {"response": "RESPONSE"},
            state.update(response="RESPONSE").append(chat_history=new),
        )

    class AsyncDummyPersister(persistence.AsyncBaseStatePersister):
        def __init__(self):
            self.persisted_state = None

        async def load(
            self,
            partition_key: str,
            app_id: Optional[str],
            sequence_id: Optional[int] = None,
            **kwargs,
        ) -> Optional[persistence.PersistedStateData]:
            await asyncio.sleep(0.0001)
            return self.persisted_state

        async def list_app_ids(self, partition_key: str, **kwargs) -> list[str]:
            return []

        async def save(
            self,
            partition_key: Optional[str],
            app_id: str,
            sequence_id: int,
            position: str,
            state: State,
            status: Literal["completed", "failed"],
            **kwargs,
        ):
            await asyncio.sleep(0.0001)
            self.persisted_state: persistence.PersistedStateData = {
                "partition_key": partition_key or "",
                "app_id": app_id,
                "sequence_id": sequence_id,
                "position": position,
                "state": state,
                "created_at": datetime.datetime.now().isoformat(),
                "status": status,
            }

    dummy_persister = AsyncDummyPersister()
    app = await (
        ApplicationBuilder()
        .with_actions(dummy_input, dummy_response)
        .with_transitions(("dummy_input", "dummy_response"), ("dummy_response", "dummy_input"))
        .initialize_from(
            initializer=dummy_persister,
            resume_at_next_action=True,
            default_state={"chat_history": []},
            default_entrypoint="dummy_input",
        )
        .with_state_persister(dummy_persister)
        .abuild()
    )

    *_, state = await app.arun(halt_after=["dummy_response"])

    assert state["chat_history"][0] == 1
    assert state["chat_history"][1] == 2
    del app

    new_app = await (
        ApplicationBuilder()
        .with_actions(dummy_input, dummy_response)
        .with_transitions(("dummy_input", "dummy_response"), ("dummy_response", "dummy_input"))
        .initialize_from(
            initializer=dummy_persister,
            resume_at_next_action=True,
            default_state={"chat_history": []},
            default_entrypoint="dummy_input",
        )
        .with_state_persister(dummy_persister)
        .abuild()
    )

    assert new_app.state["chat_history"][0] == 1
    assert new_app.state["chat_history"][1] == 2

    *_, state = await new_app.arun(halt_after=["dummy_response"])
    assert state["chat_history"][2] == 3
    assert state["chat_history"][3] == 4

    with pytest.raises(ValueError, match="The application was build with .abuild()"):
        *_, state = new_app.run(halt_after=["dummy_response"])



---
File: /burr/__init__.py
---


